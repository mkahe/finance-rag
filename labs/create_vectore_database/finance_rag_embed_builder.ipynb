{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Finance RAG ‚Äì Embedding DB Builder (Per-Embedding Chroma DB, Per-Chunk Collections)\n",
        "\n",
        "This notebook creates **one Chroma database per embedding model** and stores **one collection per chunk size**. It also lists existing databases/collections and shows basic metadata.\n",
        "\n",
        "**Recommended Python:** 3.11 or 3.12\n",
        "\n",
        "**What you get**\n",
        "- Unified `EmbeddingBundle` loader with families: `openai`, `hf` (HuggingFace), `ollama`\n",
        "- Chunking via LlamaIndex `SentenceSplitter`\n",
        "- Per-embedding Chroma **persist directory** (configurable root & prefix)\n",
        "- Per-chunk-size **collection** inside that DB\n",
        "- Progress bars (`tqdm`) for embedding and upserting\n",
        "- Utilities to **list databases** and **inspect collections**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# If you are running locally, uncomment and run this cell once to install dependencies.\n",
        "# It's safer to do it in a clean virtualenv (Python 3.11/3.12).\n",
        "# Note: We intentionally omit tiktoken to avoid Rust build requirements on some Python versions.\n",
        "# %pip install -q \"langchain>=0.3.7,<0.4.0\" \"langchain-community>=0.3.7,<0.4.0\" \"langchain-openai>=0.2.0,<0.3.0\" \\\n",
        "\n",
        "#                \"chromadb>=0.5.16,<0.6.0\" \"llama-index==0.11.18\" \\\n",
        "\n",
        "#                \"llama-index-embeddings-huggingface>=0.3.0,<0.4.0\" \"llama-index-embeddings-openai>=0.2.0,<0.4.0\" \\\n",
        "\n",
        "#                \"sentence-transformers>=3.1.1,<3.2.0\" \"tqdm>=4.66\" \"python-dotenv>=1.0,<2.0\" \\\n",
        "\n",
        "#                \"pydantic>=2.7,<3.0\" \"openai>=1.51.0,<2.0.0\" \"ollama>=0.3.0,<0.4.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import uuid\n",
        "import datetime as dt\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Sequence, Literal\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Chroma\n",
        "import chromadb\n",
        "from chromadb.config import Settings as ChromaSettings\n",
        "\n",
        "# Embeddings (LangChain wrappers)\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings, OllamaEmbeddings\n",
        "\n",
        "# Chunking\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ---- Configuration defaults (you can change these in the next cell) ----\n",
        "BASE_DB_ROOT = \"../../vector_databases\"   # where all per-embedding DBs will live\n",
        "DB_PREFIX    = \"\"          # prefix on each embedding-specific persist dir\n",
        "DATASET_TAG  = \"financebench_v1\"  # used in collection names, e.g., financebench_v1__c256\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class EmbeddingBundle:\n",
        "    family: str\n",
        "    model: str\n",
        "    name: str\n",
        "    lc: Any          # LangChain embeddings object\n",
        "    dim: Optional[int]  # embedding dimension (best effort); may be None if probing fails\n",
        "\n",
        "def _safe_name(s: str) -> str:\n",
        "    return s.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
        "\n",
        "def _probe_dim(emb) -> Optional[int]:\n",
        "    try:\n",
        "        v = emb.embed_query(\"dimension probe\")\n",
        "        return len(v) if isinstance(v, list) else (len(v[0]) if v is not None else None)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def load_embeddings(family: Literal[\"openai\", \"hf\", \"ollama\"], model: Optional[str] = None, **kwargs) -> EmbeddingBundle:\n",
        "    \"\"\"Return a unified EmbeddingBundle for the requested family/model.\"\n",
        "    Supported:\n",
        "      - family=\"openai\", model like \"text-embedding-3-small\"\n",
        "      - family=\"hf\", model like \"BAAI/bge-m3\" or \"nomic-ai/nomic-embed-text-v1\"\n",
        "      - family=\"ollama\", model like \"nomic-embed-text\" (Ollama must be running)\n",
        "    \"\"\"\n",
        "    family = family.lower()\n",
        "    if family == \"openai\":\n",
        "        model = model or \"text-embedding-3-small\"\n",
        "        emb = OpenAIEmbeddings(model=model, **kwargs)\n",
        "        dim = _probe_dim(emb)\n",
        "        return EmbeddingBundle(family=\"openai\", model=model, name=f\"openai__{model}\", lc=emb, dim=dim)\n",
        "    elif family == \"hf\":\n",
        "        if not model:\n",
        "            raise ValueError(\"For family='hf', please provide a HuggingFace model name.\")\n",
        "        # normalize embeddings tends to help cosine distances\n",
        "        kwargs.setdefault(\"encode_kwargs\", {\"normalize_embeddings\": True})\n",
        "        emb = HuggingFaceEmbeddings(model_name=model, **kwargs)\n",
        "        dim = _probe_dim(emb)\n",
        "        return EmbeddingBundle(family=\"hf\", model=model, name=f\"hf__{_safe_name(model)}\", lc=emb, dim=dim)\n",
        "    elif family == \"ollama\":\n",
        "        model = model or \"nomic-embed-text\"\n",
        "        emb = OllamaEmbeddings(model=model, **kwargs)\n",
        "        dim = _probe_dim(emb)\n",
        "        return EmbeddingBundle(family=\"ollama\", model=model, name=f\"ollama__{_safe_name(model)}\", lc=emb, dim=dim)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown embedding family: {family}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def persist_dir_for(db_root: str, db_prefix: str, emb_name: str) -> str:\n",
        "    os.makedirs(db_root, exist_ok=True)\n",
        "    safe = _safe_name(emb_name)\n",
        "    return os.path.join(db_root, f\"{db_prefix}{safe}\")\n",
        "\n",
        "def chroma_client(persist_dir: str):\n",
        "    os.makedirs(persist_dir, exist_ok=True)\n",
        "    return chromadb.Client(ChromaSettings(is_persistent=True, allow_reset=True, persist_directory=persist_dir))\n",
        "\n",
        "def collection_name(dataset_tag: str, chunk_size: int) -> str:\n",
        "    return f\"{dataset_tag}__c{chunk_size}\"\n",
        "\n",
        "def chunk_texts_llamaindex(texts: Sequence[Dict[str, Any]], chunk_size: int = 512, chunk_overlap: int = 32):\n",
        "    splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    out = []\n",
        "    for item in texts:\n",
        "        chunks = splitter.split_text(item[\"text\"])\n",
        "        for i, ch in enumerate(chunks):\n",
        "            meta = dict(item.get(\"metadata\", {}))\n",
        "            meta.update({\"chunk_index\": i, \"chunk_size\": chunk_size, \"chunk_overlap\": chunk_overlap})\n",
        "            out.append({\"text\": ch, \"metadata\": meta})\n",
        "    return out\n",
        "\n",
        "def embed_texts_in_batches(emb, texts: List[str], batch_size: int = 64) -> List[List[float]]:\n",
        "    vectors = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\", leave=False):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        # LangChain embeddings implement embed_documents for list inputs\n",
        "        vectors.extend(emb.embed_documents(batch))\n",
        "    return vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def build_collections_for_embedding(\n",
        "    raw_texts: Sequence[Dict[str, Any]],\n",
        "    family: Literal[\"openai\", \"hf\", \"ollama\"],\n",
        "    model: Optional[str] = None,\n",
        "    chunk_sizes: Sequence[int] = (256, 512),\n",
        "    chunk_overlap: int = 32,\n",
        "    db_root: str = BASE_DB_ROOT,\n",
        "    db_prefix: str = DB_PREFIX,\n",
        "    dataset_tag: str = DATASET_TAG,\n",
        "    batch_size: int = 64,\n",
        "    upsert_batch: int = 1000,\n",
        "    skip_existing: bool = True,\n",
        "    embedding_kwargs: Optional[Dict[str, Any]] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Create (or update) one Chroma DB for the embedding, and one collection per chunk size.\n",
        "\n",
        "    Returns a dict summary with persist_dir and created/updated collections.\n",
        "    \"\"\"\n",
        "    embedding_kwargs = embedding_kwargs or {}\n",
        "    bundle = load_embeddings(family, model, **embedding_kwargs)\n",
        "    pdir = persist_dir_for(db_root, db_prefix, bundle.name)\n",
        "    client = chroma_client(pdir)\n",
        "\n",
        "    existing = {c.name for c in client.list_collections()}\n",
        "    created = []\n",
        "\n",
        "    for csz in tqdm(chunk_sizes, desc=f\"Chunk sizes for {bundle.name}\"):\n",
        "        cname = collection_name(dataset_tag, csz)\n",
        "        if skip_existing and cname in existing:\n",
        "            print(f\"‚úì Skipping existing collection: {cname} @ {pdir}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"‚Üí Building collection: {cname} @ {pdir}\")\n",
        "        chunks = chunk_texts_llamaindex(raw_texts, chunk_size=csz, chunk_overlap=chunk_overlap)\n",
        "        texts = [c[\"text\"] for c in chunks]\n",
        "        metadatas = [c[\"metadata\"] for c in chunks]\n",
        "\n",
        "        vectors = embed_texts_in_batches(bundle.lc, texts, batch_size=batch_size)\n",
        "\n",
        "        col = client.get_or_create_collection(\n",
        "            name=cname,\n",
        "            metadata={\n",
        "                \"embedding_name\": bundle.name,\n",
        "                \"embedding_family\": bundle.family,\n",
        "                \"embedding_model\": bundle.model,\n",
        "                \"embedding_dim\": bundle.dim,\n",
        "                \"dataset_tag\": dataset_tag,\n",
        "                \"chunk_size\": csz,\n",
        "                \"chunk_overlap\": chunk_overlap,\n",
        "                \"created_at\": dt.datetime.utcnow().isoformat() + \"Z\",\n",
        "            },\n",
        "        )\n",
        "\n",
        "        ids = [str(uuid.uuid4()) for _ in texts]\n",
        "        for i in tqdm(range(0, len(texts), upsert_batch), desc=\"Upserting\", leave=False):\n",
        "            col.upsert(\n",
        "                ids=ids[i:i+upsert_batch],\n",
        "                documents=texts[i:i+upsert_batch],\n",
        "                metadatas=metadatas[i:i+upsert_batch],\n",
        "                embeddings=vectors[i:i+upsert_batch],\n",
        "            )\n",
        "\n",
        "        created.append({\"collection\": cname, \"count\": len(texts)})\n",
        "\n",
        "    return {\"persist_dir\": pdir, \"embedding\": bundle.name, \"collections\": created}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def list_embedding_databases(db_root: str = BASE_DB_ROOT, db_prefix: str = DB_PREFIX) -> List[Dict[str, Any]]:\n",
        "    \"\"\"List per-embedding Chroma DBs (persist dirs) and their collections/metadata.\"\"\"\n",
        "    dbs = []\n",
        "    if not os.path.isdir(db_root):\n",
        "        return dbs\n",
        "    for entry in sorted(os.listdir(db_root)):\n",
        "        if not entry.startswith(db_prefix):\n",
        "            continue\n",
        "        pdir = os.path.join(db_root, entry)\n",
        "        info = {\"persist_dir\": pdir, \"collections\": []}\n",
        "        try:\n",
        "            client = chroma_client(pdir)\n",
        "            cols = client.list_collections()\n",
        "            for c in cols:\n",
        "                info[\"collections\"].append({\n",
        "                    \"name\": c.name,\n",
        "                    \"metadata\": c.metadata or {},\n",
        "                })\n",
        "        except Exception as e:\n",
        "            info[\"error\"] = str(e)\n",
        "        dbs.append(info)\n",
        "    return dbs\n",
        "\n",
        "def print_db_summary(dbs: List[Dict[str, Any]]):\n",
        "    if not dbs:\n",
        "        print(\"No databases found.\")\n",
        "        return\n",
        "    for db in dbs:\n",
        "        print(f\"\\nüìÅ {db['persist_dir']}\")\n",
        "        if \"error\" in db:\n",
        "            print(\"  ‚ö†Ô∏è \", db[\"error\"])\n",
        "            continue\n",
        "        if not db[\"collections\"]:\n",
        "            print(\"  (no collections)\")\n",
        "        for c in db[\"collections\"]:\n",
        "            md = c.get(\"metadata\", {}) or {}\n",
        "            cs = md.get(\"chunk_size\", \"?\")\n",
        "            fam = md.get(\"embedding_family\", \"?\")\n",
        "            model = md.get(\"embedding_model\", \"?\")\n",
        "            dim = md.get(\"embedding_dim\", \"?\")\n",
        "            print(f\"  ‚Ä¢ {c['name']} | family={fam} model={model} dim={dim} chunk_size={cs}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "Chunk sizes for hf__BAAI_bge-m3:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚Üí Building collection: financebench_v1__c256 @ ../../vector_databases/hf__BAAI_bge-m3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/lj/175ptt0d6knb0gg0lg2h4n2h0000gp/T/ipykernel_33066/2791667283.py:50: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"created_at\": dt.datetime.utcnow().isoformat() + \"Z\",\n",
            "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "Chunk sizes for hf__BAAI_bge-m3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  5.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚Üí Building collection: financebench_v1__c512 @ ../../vector_databases/hf__BAAI_bge-m3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "Chunk sizes for hf__BAAI_bge-m3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  8.54it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'persist_dir': '../../vector_databases/hf__BAAI_bge-m3',\n",
              "  'embedding': 'hf__BAAI_bge-m3',\n",
              "  'collections': [{'collection': 'financebench_v1__c256', 'count': 2},\n",
              "   {'collection': 'financebench_v1__c512', 'count': 2}]}]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# --- Example raw texts (replace with your dataset loader) ---\n",
        "raw_texts = [\n",
        "    {\"text\": \"Acme Corp reported revenue of $12.3B in FY2023.\", \"metadata\": {\"source\": \"10-K 2023\", \"ticker\": \"ACME\"}},\n",
        "    {\"text\": \"Operating margin improved to 18% due to cost controls.\", \"metadata\": {\"source\": \"10-K 2023\", \"ticker\": \"ACME\"}},\n",
        "]\n",
        "\n",
        "# --- Configure which embeddings and chunk sizes to build ---\n",
        "families_and_models = [\n",
        "    {\"family\": \"hf\", \"model\": \"BAAI/bge-m3\"},\n",
        "    # Uncomment if you have OPENAI_API_KEY set in your environment:\n",
        "    # {\"family\": \"openai\", \"model\": \"text-embedding-3-small\"},\n",
        "    # If you run Ollama locally with an embedding model:\n",
        "    # {\"family\": \"ollama\", \"model\": \"nomic-embed-text\"},\n",
        "]\n",
        "\n",
        "chunk_sizes = (256, 512)\n",
        "chunk_overlap = 32\n",
        "\n",
        "# --- Build phase (progress bars will show) ---\n",
        "results = []\n",
        "for fm in families_and_models:\n",
        "    res = build_collections_for_embedding(\n",
        "        raw_texts=raw_texts,\n",
        "        family=fm[\"family\"],\n",
        "        model=fm.get(\"model\"),\n",
        "        chunk_sizes=chunk_sizes,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        db_root=BASE_DB_ROOT,\n",
        "        db_prefix=DB_PREFIX,\n",
        "        dataset_tag=DATASET_TAG,\n",
        "        batch_size=64,\n",
        "        upsert_batch=1000,\n",
        "        skip_existing=True,\n",
        "    )\n",
        "    results.append(res)\n",
        "\n",
        "results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìÅ ../../vector_databases/hf__BAAI_bge-m3\n",
            "  ‚Ä¢ financebench_v1__c256 | family=hf model=BAAI/bge-m3 dim=1024 chunk_size=256\n",
            "  ‚Ä¢ financebench_v1__c512 | family=hf model=BAAI/bge-m3 dim=1024 chunk_size=512\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dbs = list_embedding_databases(BASE_DB_ROOT, DB_PREFIX)\n",
        "print_db_summary(dbs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample from ../../vector_databases/hf__BAAI_bge-m3 :: financebench_v1__c256\n",
            "\n",
            "#1\n",
            " Acme Corp reported revenue of $12.3B in FY2023. \n",
            " {'chunk_index': 0, 'chunk_overlap': 32, 'chunk_size': 256, 'source': '10-K 2023', 'ticker': 'ACME'}\n",
            "\n",
            "#2\n",
            " Operating margin improved to 18% due to cost controls. \n",
            " {'chunk_index': 0, 'chunk_overlap': 32, 'chunk_size': 256, 'source': '10-K 2023', 'ticker': 'ACME'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# OPTIONAL: quick retrieval sanity check\n",
        "# Pick the first DB and first collection, and run a tiny query.\n",
        "try:\n",
        "    if dbs:\n",
        "        any_db = dbs[0]['persist_dir']\n",
        "        client = chroma_client(any_db)\n",
        "        if dbs[0]['collections']:\n",
        "            cname = dbs[0]['collections'][0]['name']\n",
        "            col = client.get_or_create_collection(cname)\n",
        "            # naive query by re-embedding using the same family/model is out of scope here,\n",
        "            # but Chroma can query with raw embeddings if provided.\n",
        "            # For simplicity, we'll just print a few docs:\n",
        "            qs = col.get(include=[\"documents\",\"metadatas\"], limit=3)\n",
        "            print(f\"Sample from {any_db} :: {cname}\")\n",
        "            for i, (doc, meta) in enumerate(zip(qs.get('documents', []), qs.get('metadatas', []))):\n",
        "                print(f\"\\n#{i+1}\\n\", doc, \"\\n\", meta)\n",
        "except Exception as e:\n",
        "    print(\"Retrieval sanity check skipped:\", e)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "create_vector",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
