{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f638d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02561f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 3, 'key_facts_gold': ['AMD', 'higher sales', 'EPYC server processors', 'higher semi-custom product sales', 'inclusion of Xilinx embedded product sales'], 'facts_present': ['AMD', 'revenue change', '64% increase in Data Center segment revenue', '21% increase in Gaming segment revenue', 'significant growth in Embedded segment revenue from Xilinx product sales'], 'facts_missing': ['higher sales of EPYC server processors', 'higher semi-custom product sales'], 'justification': 'The generated answer provides specific percentage increases for the Data Center and Gaming segments, as well as mentioning growth from Xilinx products, which aligns with the gold answer. However, it omits the mention of higher sales of EPYC server processors and higher semi-custom product sales, which are key components of the revenue change.', 'success': True, 'raw_response': {'score': 3, 'key_facts_gold': ['AMD', 'higher sales', 'EPYC server processors', 'higher semi-custom product sales', 'inclusion of Xilinx embedded product sales'], 'facts_present': ['AMD', 'revenue change', '64% increase in Data Center segment revenue', '21% increase in Gaming segment revenue', 'significant growth in Embedded segment revenue from Xilinx product sales'], 'facts_missing': ['higher sales of EPYC server processors', 'higher semi-custom product sales'], 'justification': 'The generated answer provides specific percentage increases for the Data Center and Gaming segments, as well as mentioning growth from Xilinx products, which aligns with the gold answer. However, it omits the mention of higher sales of EPYC server processors and higher semi-custom product sales, which are key components of the revenue change.'}, 'metadata': {'provider': 'openai', 'model': 'gpt-4o-mini', 'temperature': 0.0, 'question': 'What drove revenue change as of the FY22 for AMD?', 'gold_answer': 'In 2022, AMD reported Higher sales of their EPYC server processors, higher semi-custom product sales, and the inclusion of Xilinx embedded product sales', 'generated_answer': 'Revenue change for AMD in FY22 was driven by a 64% increase in Data Center segment revenue, a 21% increase in Gaming segment revenue, and significant growth in Embedded segment revenue from Xilinx product sales.'}}\n"
     ]
    }
   ],
   "source": [
    "from generation_evaluation_suit.llm_as_judge_graded import llm_as_judge_graded\n",
    "\n",
    "question = \"What drove revenue change as of the FY22 for AMD?\"\n",
    "gold_answer = \"In 2022, AMD reported Higher sales of their EPYC server processors, higher semi-custom product sales, and the inclusion of Xilinx embedded product sales\"\n",
    "generated_answer = \"Revenue change for AMD in FY22 was driven by a 64% increase in Data Center segment revenue, a 21% increase in Gaming segment revenue, and significant growth in Embedded segment revenue from Xilinx product sales.\"\n",
    "\n",
    "result = llm_as_judge_graded(\n",
    "    question=question,\n",
    "    gold_answer=gold_answer,\n",
    "    generated_answer=generated_answer,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_retries=3,\n",
    "    retry_delay_ms=1000\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3b5b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'match': True, 'gold_num': 1577.0, 'gen_num': 1580.0, 'relative_error': 0.19023462270133165, 'absolute_error': 3.0, 'error_category': 'within_tolerance', 'justification': 'The generated answer (1580 million) differs from the gold answer (1577 million) by 3 units, resulting in a relative error of 0.191%. Since 0.191% is less than the 1% tolerance, this is within tolerance and is a match.', 'success': True, 'raw_response': {'match': True, 'gold_number': 1577.0, 'generated_number': 1580.0, 'relative_error': 0.191, 'absolute_error': 3.0, 'error_category': 'within_tolerance', 'justification': 'The generated answer (1580 million) differs from the gold answer (1577 million) by 3 units, resulting in a relative error of 0.191%. Since 0.191% is less than the 1% tolerance, this is within tolerance and is a match.'}, 'metadata': {'provider': 'openai', 'model': 'gpt-4o-mini', 'temperature': 0.0, 'tolerance': 0.01, 'question': 'What is the FY2018 capital expenditure amount (in USD millions) for 3M?', 'gold_answer': '$1577.00', 'generated_answer': '1580 million dollars'}}\n"
     ]
    }
   ],
   "source": [
    "from generation_evaluation_suit.llm_as_judge_binary import llm_as_judge_binary\n",
    "\n",
    "question = \"What is the FY2018 capital expenditure amount (in USD millions) for 3M?\"\n",
    "gold = \"$1577.00\"\n",
    "gen = \"1580 million dollars\"\n",
    "\n",
    "result = llm_as_judge_binary(\n",
    "    question=question,\n",
    "    gold_answer=gold,\n",
    "    generated_answer=gen,\n",
    "    tolerance=0.01\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "598b3dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Match! Error: 1.25%\n",
      "{'match': True, 'gold_num': 0.8, 'gen_num': 79.0, 'gold_scale': None, 'gen_scale': None, 'gold_is_percentage': False, 'gen_is_percentage': True, 'relative_error': 1.25, 'absolute_error': 1.0, 'error_category': 'within_tolerance', 'normalized_gold': 80.0, 'normalized_gen': 79.0, 'common_scale': None}\n"
     ]
    }
   ],
   "source": [
    "from generation_evaluation_suit.numerical_exact_match import numerical_exact_match\n",
    "\n",
    "result = numerical_exact_match(\n",
    "    gold_answer=\"$0.8\",\n",
    "    generated_answer=\"79%\",\n",
    "    tolerance=0.05  # 5% tolerance\n",
    ")\n",
    "\n",
    "if result['match']:\n",
    "    print(f\"✓ Match! Error: {result['relative_error']:.2f}%\")\n",
    "else:\n",
    "    print(f\"✗ No match. Category: {result['error_category']}\")\n",
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac2f651d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_refusal': True, 'confidence': 1.0, 'refusal_type': 'explicit', 'matched_pattern': \"\\\\bi\\\\s+(?:do\\\\s+not|don't|cannot|can't|could\\\\s+not|couldn't)\\\\s+(?:know|have|provide|answer|calculate|determine|find)\", 'answer_length': 41}\n"
     ]
    }
   ],
   "source": [
    "from generation_evaluation_suit.detect_refusal import detect_refusal\n",
    "\n",
    "result = detect_refusal(\"I don't know the answer to that question.\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c58b6138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.400\n",
      "Precision: 0.429\n",
      "Recall: 0.375\n",
      "Common tokens: {'segment', 'the', 'consumer'}\n",
      "Missing tokens: {'0', 'organically', 'shrunk', 'by', '9'}\n"
     ]
    }
   ],
   "source": [
    "from generation_evaluation_suit.token_f1 import token_f1\n",
    "result = token_f1(\n",
    "    gold_answer=\"The consumer segment shrunk by 0.9% organically.\",\n",
    "    generated_answer=\"The Consumer segment has dragged down growth.\",\n",
    "    normalize=True,\n",
    "    remove_stopwords=False\n",
    ")\n",
    "\n",
    "print(f\"F1: {result['f1']:.3f}\")\n",
    "print(f\"Precision: {result['precision']:.3f}\")\n",
    "print(f\"Recall: {result['recall']:.3f}\")\n",
    "print(f\"Common tokens: {result['common_tokens']}\")\n",
    "print(f\"Missing tokens: {result['missing_tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c97ce4a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Evaluation failed for metrics-generated: numerical_exact_match failed: name 'numerical_exact_match' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/generation/generation_evaluation_suit/evaluate_answer.py:236\u001b[39m, in \u001b[36m_evaluate_metrics_generated\u001b[39m\u001b[34m(question, gold_answer, generated_answer, config)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     nem_result = \u001b[43mnumerical_exact_match\u001b[49m(\n\u001b[32m    237\u001b[39m         gold_answer=gold_answer,\n\u001b[32m    238\u001b[39m         generated_answer=generated_answer,\n\u001b[32m    239\u001b[39m         tolerance=config[\u001b[33m'\u001b[39m\u001b[33mtolerance\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    240\u001b[39m     )\n\u001b[32m    241\u001b[39m     metrics[\u001b[33m'\u001b[39m\u001b[33mnumerical_exact_match\u001b[39m\u001b[33m'\u001b[39m] = nem_result\n",
      "\u001b[31mNameError\u001b[39m: name 'numerical_exact_match' is not defined",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/generation/generation_evaluation_suit/evaluate_answer.py:181\u001b[39m, in \u001b[36mevaluate_answer\u001b[39m\u001b[34m(question, question_type, gold_answer, generated_answer, config)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m question_type == \u001b[33m'\u001b[39m\u001b[33mmetrics-generated\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     result[\u001b[33m'\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43m_evaluate_metrics_generated\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgold_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m     result[\u001b[33m'\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmetrics_computed\u001b[39m\u001b[33m'\u001b[39m] = [\u001b[33m'\u001b[39m\u001b[33mnumerical_exact_match\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mllm_as_judge_binary\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/generation/generation_evaluation_suit/evaluate_answer.py:243\u001b[39m, in \u001b[36m_evaluate_metrics_generated\u001b[39m\u001b[34m(question, gold_answer, generated_answer, config)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnumerical_exact_match failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m# Metric 2: LLM-as-Judge Binary (LLM-based)\u001b[39;00m\n",
      "\u001b[31mException\u001b[39m: numerical_exact_match failed: name 'numerical_exact_match' is not defined",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeneration_evaluation_suit\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluate_answer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_answer, print_evaluation_summary\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m result = \u001b[43mevaluate_answer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the FY2018 capital expenditure?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquestion_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetrics-generated\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgold_answer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m$1577.00\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerated_answer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1577 million dollars\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Use built-in pretty printer\u001b[39;00m\n\u001b[32m     11\u001b[39m print_evaluation_summary(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/generation/generation_evaluation_suit/evaluate_answer.py:204\u001b[39m, in \u001b[36mevaluate_answer\u001b[39m\u001b[34m(question, question_type, gold_answer, generated_answer, config)\u001b[39m\n\u001b[32m    202\u001b[39m     result[\u001b[33m'\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mevaluation_complete\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    203\u001b[39m     result[\u001b[33m'\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33merrors\u001b[39m\u001b[33m'\u001b[39m].append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluation failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluation failed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mException\u001b[39m: Evaluation failed for metrics-generated: numerical_exact_match failed: name 'numerical_exact_match' is not defined"
     ]
    }
   ],
   "source": [
    "from generation_evaluation_suit.evaluate_answer import evaluate_answer, print_evaluation_summary\n",
    "\n",
    "result = evaluate_answer(\n",
    "    question=\"What is the FY2018 capital expenditure?\",\n",
    "    question_type=\"metrics-generated\",\n",
    "    gold_answer=\"$1577.00\",\n",
    "    generated_answer=\"1577 million dollars\"\n",
    ")\n",
    "\n",
    "# Use built-in pretty printer\n",
    "print_evaluation_summary(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
