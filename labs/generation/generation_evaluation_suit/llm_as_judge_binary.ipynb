{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80c0b376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7e9b38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary LLM-as-Judge Module\n",
      "======================================================================\n",
      "\n",
      "To test, run: _test_llm_as_judge_binary()\n",
      "Make sure OPENAI_API_KEY is set in environment\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Binary LLM-as-Judge for Numerical Answer Validation\n",
    "====================================================\n",
    "This module provides LLM-based binary validation for metrics-generated questions.\n",
    "Supplements numerical_exact_match by handling complex numerical expressions,\n",
    "scale conversions, and providing reasoning for matches/mismatches.\n",
    "\n",
    "Use Cases:\n",
    "- Complex numerical expressions (\"1.577 billion\" vs \"$1,577 million\")\n",
    "- Context-dependent parsing (\"24.5%\" vs \"0.245\")\n",
    "- Debugging numerical_exact_match failures\n",
    "- Comparison studies (rule-based vs LLM approaches)\n",
    "\n",
    "Author: Financial QA Evaluation System\n",
    "Version: 1.0\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from typing import Dict, Any, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class BinaryJudgment(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic schema for binary numerical judgment output.\n",
    "    Ensures structured and parseable response from LLM.\n",
    "    \"\"\"\n",
    "    \n",
    "    match: bool = Field(\n",
    "        description=\"Whether the generated answer matches gold within tolerance\"\n",
    "    )\n",
    "    \n",
    "    gold_number: Optional[float] = Field(\n",
    "        description=\"Numerical value extracted from gold answer (None if unparseable)\"\n",
    "    )\n",
    "    \n",
    "    generated_number: Optional[float] = Field(\n",
    "        description=\"Numerical value extracted from generated answer (None if unparseable)\"\n",
    "    )\n",
    "    \n",
    "    relative_error: Optional[float] = Field(\n",
    "        description=\"Relative error as percentage (None if not applicable)\"\n",
    "    )\n",
    "    \n",
    "    absolute_error: Optional[float] = Field(\n",
    "        description=\"Absolute difference between numbers (None if not applicable)\"\n",
    "    )\n",
    "    \n",
    "    error_category: str = Field(\n",
    "        description=\"Category: exact_match, within_tolerance, out_of_tolerance, refusal, unparseable\"\n",
    "    )\n",
    "    \n",
    "    justification: str = Field(\n",
    "        description=\"Brief explanation (1-2 sentences) of the judgment and reasoning\"\n",
    "    )\n",
    "\n",
    "\n",
    "def llm_as_judge_binary(\n",
    "    question: str,\n",
    "    gold_answer: str,\n",
    "    generated_answer: str,\n",
    "    tolerance: float = 0.01,\n",
    "    provider: str = \"openai\",\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    temperature: float = 0.0,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay_ms: int = 500,\n",
    "    return_details: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate numerical answer using LLM with binary match validation.\n",
    "    \n",
    "    This evaluator is used for metrics-generated questions as a supplement\n",
    "    to rule-based numerical_exact_match. It handles complex numerical\n",
    "    expressions, scale conversions, and provides reasoning.\n",
    "    \n",
    "    Args:\n",
    "        question: The question being answered (provides context)\n",
    "        gold_answer: The gold standard answer (ground truth)\n",
    "        generated_answer: The generated answer to evaluate\n",
    "        tolerance: Relative tolerance for matching (0.01 = 1% difference allowed)\n",
    "        provider: LLM provider ('openai', 'anthropic', 'ollama')\n",
    "        model: Model name (e.g., 'gpt-4o-mini')\n",
    "        temperature: Temperature for generation (0.0 for deterministic)\n",
    "        max_retries: Maximum number of retry attempts on failure\n",
    "        retry_delay_ms: Delay between retries in milliseconds\n",
    "        return_details: If True, include full LLM response and metadata\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - match: bool - Whether answers match within tolerance\n",
    "            - gold_num: Optional[float] - Extracted gold number\n",
    "            - gen_num: Optional[float] - Extracted generated number\n",
    "            - relative_error: Optional[float] - Relative error as percentage\n",
    "            - absolute_error: Optional[float] - Absolute difference\n",
    "            - error_category: str - One of: exact_match, within_tolerance, \n",
    "                                    out_of_tolerance, refusal, unparseable\n",
    "            - justification: str - LLM's reasoning\n",
    "            - success: bool - Whether LLM call succeeded\n",
    "            - raw_response: dict - Full LLM response (if return_details=True)\n",
    "            - metadata: dict - Call information (if return_details=True)\n",
    "    \n",
    "    Error Categories:\n",
    "        - exact_match: Numbers are identical (within floating point precision)\n",
    "        - within_tolerance: Numbers differ but within tolerance threshold\n",
    "        - out_of_tolerance: Numbers differ beyond tolerance\n",
    "        - refusal: Generated answer is a refusal (\"I don't know\", etc.)\n",
    "        - unparseable: Cannot extract valid number from generated answer\n",
    "    \n",
    "    Examples:\n",
    "        >>> # Exact match with different formats\n",
    "        >>> result = llm_as_judge_binary(\n",
    "        ...     question=\"What is the FY2018 capex for 3M in millions?\",\n",
    "        ...     gold_answer=\"$1577.00\",\n",
    "        ...     generated_answer=\"1577 million dollars\",\n",
    "        ...     tolerance=0.01\n",
    "        ... )\n",
    "        >>> print(result['match'])  # True\n",
    "        >>> print(result['error_category'])  # 'exact_match'\n",
    "        \n",
    "        >>> # Within tolerance\n",
    "        >>> result = llm_as_judge_binary(\n",
    "        ...     question=\"What is the operating margin?\",\n",
    "        ...     gold_answer=\"24.5%\",\n",
    "        ...     generated_answer=\"24.48%\",\n",
    "        ...     tolerance=0.01\n",
    "        ... )\n",
    "        >>> print(result['match'])  # True\n",
    "        >>> print(result['error_category'])  # 'within_tolerance'\n",
    "        \n",
    "        >>> # Scale conversion\n",
    "        >>> result = llm_as_judge_binary(\n",
    "        ...     question=\"What is the revenue?\",\n",
    "        ...     gold_answer=\"$1.577 billion\",\n",
    "        ...     generated_answer=\"1577 million\",\n",
    "        ...     tolerance=0.01\n",
    "        ... )\n",
    "        >>> print(result['match'])  # True\n",
    "        >>> print(result['justification'])  # \"1.577 billion equals 1577 million...\"\n",
    "    \"\"\"\n",
    "    \n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    # Create LLM with structured output\n",
    "    llm = ChatOpenAI(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        model_kwargs={\"response_format\": {\"type\": \"json_object\"}} if provider == \"openai\" else {}\n",
    "    )\n",
    "    \n",
    "    # Apply structured output schema\n",
    "    structured_llm = llm.with_structured_output(BinaryJudgment)\n",
    "    \n",
    "    # Construct evaluation prompt with few-shot examples\n",
    "    prompt = _create_binary_prompt(question, gold_answer, generated_answer, tolerance)\n",
    "    \n",
    "    # Call LLM with retry logic\n",
    "    try:\n",
    "        judgment = _call_llm_with_retry(\n",
    "            structured_llm,\n",
    "            prompt,\n",
    "            max_retries=max_retries,\n",
    "            retry_delay_ms=retry_delay_ms\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # If all retries fail, return error result\n",
    "        return {\n",
    "            'match': False,\n",
    "            'gold_num': None,\n",
    "            'gen_num': None,\n",
    "            'relative_error': None,\n",
    "            'absolute_error': None,\n",
    "            'error_category': 'unparseable',\n",
    "            'justification': f\"LLM evaluation failed after {max_retries} retries: {str(e)}\",\n",
    "            'error': str(e),\n",
    "            'success': False\n",
    "        }\n",
    "    \n",
    "    # Build result dictionary - start with LLM's judgment\n",
    "    result = {\n",
    "        'match': judgment.match,\n",
    "        'gold_num': judgment.gold_number,\n",
    "        'gen_num': judgment.generated_number,\n",
    "        'relative_error': judgment.relative_error,\n",
    "        'absolute_error': judgment.absolute_error,\n",
    "        'error_category': judgment.error_category,\n",
    "        'justification': judgment.justification,\n",
    "        'success': True\n",
    "    }\n",
    "    \n",
    "    # POST-PROCESSING: Validate and correct LLM logic errors\n",
    "    # If both numbers exist and relative error is calculable\n",
    "    if judgment.gold_number is not None and judgment.generated_number is not None:\n",
    "        # Calculate what the error SHOULD be\n",
    "        abs_error = abs(judgment.generated_number - judgment.gold_number)\n",
    "        rel_error = (abs_error / abs(judgment.gold_number)) * 100  # as percentage\n",
    "        \n",
    "        # Check if LLM's calculation is reasonable (allow 0.05% variance for rounding)\n",
    "        if judgment.relative_error is not None:\n",
    "            calc_diff = abs(rel_error - judgment.relative_error)\n",
    "            if calc_diff > 0.05:  # More than 0.05% difference suggests miscalculation\n",
    "                result['warning'] = f\"LLM calculated relative_error={judgment.relative_error}%, but should be {rel_error:.3f}%\"\n",
    "        \n",
    "        # CRITICAL: Validate match/category consistency based on ACTUAL calculation\n",
    "        expected_match = rel_error <= (tolerance * 100)\n",
    "        \n",
    "        if expected_match and not judgment.match:\n",
    "            # LLM said NO but should be YES - CORRECT IT\n",
    "            result['match'] = True\n",
    "            result['error_category'] = 'exact_match' if rel_error == 0 else 'within_tolerance'\n",
    "            result['justification'] += f\" [Auto-corrected: relative error {rel_error:.3f}% ≤ {tolerance*100}% tolerance]\"\n",
    "            result['corrected'] = True\n",
    "        elif not expected_match and judgment.match:\n",
    "            # LLM said YES but should be NO - CORRECT IT\n",
    "            result['match'] = False\n",
    "            result['error_category'] = 'out_of_tolerance'\n",
    "            result['justification'] += f\" [Auto-corrected: relative error {rel_error:.3f}% > {tolerance*100}% tolerance]\"\n",
    "            result['corrected'] = True\n",
    "        \n",
    "        # Use our calculation for error values (more reliable than LLM)\n",
    "        result['relative_error'] = rel_error\n",
    "        result['absolute_error'] = abs_error\n",
    "    \n",
    "    if return_details:\n",
    "        result['raw_response'] = judgment.model_dump()\n",
    "        result['metadata'] = {\n",
    "            'provider': provider,\n",
    "            'model': model,\n",
    "            'temperature': temperature,\n",
    "            'tolerance': tolerance,\n",
    "            'question': question,\n",
    "            'gold_answer': gold_answer,\n",
    "            'generated_answer': generated_answer\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def _create_binary_prompt(\n",
    "    question: str,\n",
    "    gold_answer: str,\n",
    "    generated_answer: str,\n",
    "    tolerance: float\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create the evaluation prompt with few-shot examples for binary judgment.\n",
    "    \n",
    "    Args:\n",
    "        question: The question being answered\n",
    "        gold_answer: Gold standard answer\n",
    "        generated_answer: Generated answer to evaluate\n",
    "        tolerance: Relative tolerance (e.g., 0.01 = 1%)\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    \n",
    "    tolerance_percent = tolerance * 100\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert evaluator for a financial question-answering system. Your task is to determine if a generated numerical answer matches the gold standard answer within a specified tolerance.\n",
    "\n",
    "**Your Task:**\n",
    "1. Extract the numerical value from both gold and generated answers\n",
    "2. Handle different formats (currency, percentages, scales like million/billion)\n",
    "3. Calculate the relative error if both numbers are valid\n",
    "4. Determine if the match is within tolerance: {tolerance_percent}%\n",
    "5. Categorize the result and provide justification\n",
    "\n",
    "**Tolerance Definition:**\n",
    "- Relative tolerance: {tolerance_percent}% means the generated number can differ by up to {tolerance_percent}% of the gold number\n",
    "- Formula: relative_error = (|generated - gold| / |gold|) × 100%\n",
    "- The tolerance value {tolerance_percent}% is expressed as a PERCENTAGE, not a decimal\n",
    "- Example 1: If gold=100 and tolerance=1%, generated can be 99-101 (1% of 100 = 1)\n",
    "- Example 2: If gold=1577 and tolerance=1%, generated can be 1561.23-1592.77 (1% of 1577 = 15.77)\n",
    "- Example 3: If relative_error=0.191%, compare 0.191 < 1.0 → WITHIN tolerance\n",
    "- Example 4: If relative_error=2.5%, compare 2.5 > 1.0 → OUT OF tolerance\n",
    "\n",
    "**CRITICAL COMPARISON RULE:**\n",
    "When you calculate relative_error as a percentage (like 0.191%), compare it directly to {tolerance_percent}:\n",
    "- If relative_error ≤ {tolerance_percent} → WITHIN tolerance → match=TRUE\n",
    "- If relative_error > {tolerance_percent} → OUT OF tolerance → match=FALSE\n",
    "\n",
    "Example: relative_error=0.191% and tolerance={tolerance_percent}%\n",
    "→ Is 0.191 ≤ {tolerance_percent}? \n",
    "→ 0.191 ≤ 1.0? → YES → within_tolerance → match=TRUE\n",
    "\n",
    "**Error Categories and Match Rules:**\n",
    "- **exact_match**: Numbers are identical (or effectively identical within floating point precision) → **match=TRUE**\n",
    "- **within_tolerance**: Numbers differ but relative error ≤ {tolerance_percent}% → **match=TRUE**\n",
    "- **out_of_tolerance**: Numbers differ and relative error > {tolerance_percent}% → **match=FALSE**\n",
    "- **refusal**: Generated answer refuses to provide a number (\"I don't know\", \"cannot calculate\", etc.) → **match=FALSE**\n",
    "- **unparseable**: Cannot extract a valid number from generated answer → **match=FALSE**\n",
    "\n",
    "**CRITICAL RULE**: If relative_error ≤ {tolerance_percent}%, then match MUST be TRUE and error_category MUST be either \"exact_match\" (if error is 0) or \"within_tolerance\".\n",
    "\n",
    "**Important Guidelines:**\n",
    "- Handle format variations: \"$1577\", \"1577 dollars\", \"USD 1577\" are all the same\n",
    "- Handle scale conversions: \"1.577 billion\" = \"1577 million\"\n",
    "- Handle percentage formats: \"24.5%\" and \"0.245\" may be the same depending on context\n",
    "- Negative numbers: \"-3.7\" and \"(3.7)\" in accounting notation are the same\n",
    "- Accounting notation: \"(3.7)\" means -3.7\n",
    "- If generated answer has qualifiers like \"approximately\", still extract the number\n",
    "\n",
    "---\n",
    "\n",
    "**Few-Shot Examples:**\n",
    "\n",
    "**Example 1 - Exact Match with Different Formats:**\n",
    "Question: \"What is the FY2018 capital expenditure amount (in USD millions) for 3M?\"\n",
    "Gold Answer: \"$1577.00\"\n",
    "Generated Answer: \"1577 million dollars\"\n",
    "Tolerance: 1%\n",
    "\n",
    "Evaluation:\n",
    "- gold_number: 1577.0\n",
    "- generated_number: 1577.0\n",
    "- absolute_error: 0.0\n",
    "- relative_error: 0.0%\n",
    "- match: true\n",
    "- error_category: \"exact_match\"\n",
    "- justification: \"Both answers represent $1577 million. The format differs but the numerical value is identical.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Example 2 - Within Tolerance:**\n",
    "Question: \"What is Adobe's FY2016 unadjusted operating income margin (as percent of total revenue)?\"\n",
    "Gold Answer: \"24.5%\"\n",
    "Generated Answer: \"24.48%\"\n",
    "Tolerance: 1%\n",
    "\n",
    "Evaluation:\n",
    "- gold_number: 24.5\n",
    "- generated_number: 24.48\n",
    "- absolute_error: 0.02\n",
    "- relative_error: 0.08% (calculated as: |24.48-24.5|/24.5 × 100 = 0.02/24.5 × 100 = 0.0817%)\n",
    "- **Comparison: Is 0.08% ≤ 1%? YES, 0.08 < 1.0**\n",
    "- match: true (BECAUSE relative_error 0.08% is LESS than tolerance 1%)\n",
    "- error_category: \"within_tolerance\"\n",
    "- justification: \"The generated answer (24.48%) is within 1% tolerance of the gold answer (24.5%). The relative error is 0.08%, which is less than 1%, so this is a match.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Example 2b - Within Tolerance (Worked Example with Similar Numbers):**\n",
    "Question: \"What is the capital expenditure?\"\n",
    "Gold Answer: \"1577\"\n",
    "Generated Answer: \"1580\"\n",
    "Tolerance: 1%\n",
    "\n",
    "Evaluation:\n",
    "- gold_number: 1577.0\n",
    "- generated_number: 1580.0\n",
    "- absolute_error: 3.0 (calculated as: |1580-1577| = 3)\n",
    "- relative_error: 0.190% (calculated as: 3/1577 × 100 = 0.190%)\n",
    "- **Comparison: Is 0.190% ≤ 1%? YES, 0.190 < 1.0**\n",
    "- match: true (BECAUSE relative_error 0.190% is LESS than tolerance 1%)\n",
    "- error_category: \"within_tolerance\"\n",
    "- justification: \"The generated answer (1580) differs from gold (1577) by 3 units, resulting in a relative error of 0.190%. Since 0.190% < 1%, this is within tolerance and is a match.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Example 3 - Out of Tolerance (Wrong Number):**\n",
    "Question: \"Roughly how many times has AES Corporation sold its inventory in FY2022?\"\n",
    "Gold Answer: \"9.5\"\n",
    "Generated Answer: \"AES Corporation sold its inventory roughly 12 times in FY2022.\"\n",
    "Tolerance: 1%\n",
    "\n",
    "Evaluation:\n",
    "- gold_number: 9.5\n",
    "- generated_number: 12.0\n",
    "- absolute_error: 2.5\n",
    "- relative_error: 26.3% (2.5/9.5 × 100)\n",
    "- match: false\n",
    "- error_category: \"out_of_tolerance\"\n",
    "- justification: \"The generated number (12.0) differs significantly from the gold answer (9.5) with a relative error of 26.3%, far exceeding the 1% tolerance.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Example 4 - Refusal:**\n",
    "Question: \"What is the FY2019 fixed asset turnover ratio for Activision Blizzard?\"\n",
    "Gold Answer: \"0.66\"\n",
    "Generated Answer: \"I cannot calculate this ratio without access to the specific financial statements.\"\n",
    "Tolerance: 1%\n",
    "\n",
    "Evaluation:\n",
    "- gold_number: 0.66\n",
    "- generated_number: null\n",
    "- absolute_error: null\n",
    "- relative_error: null\n",
    "- match: false\n",
    "- error_category: \"refusal\"\n",
    "- justification: \"The generated answer is a refusal to provide a numerical value rather than an actual answer.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Example 5 - Scale Conversion (Billion vs Million):**\n",
    "Question: \"What is the total revenue for FY2021?\"\n",
    "Gold Answer: \"$1.577 billion\"\n",
    "Generated Answer: \"Total revenue was 1577 million dollars\"\n",
    "Tolerance: 1%\n",
    "\n",
    "Evaluation:\n",
    "- gold_number: 1577.0 (converted to millions for comparison)\n",
    "- generated_number: 1577.0\n",
    "- absolute_error: 0.0\n",
    "- relative_error: 0.0%\n",
    "- match: true\n",
    "- error_category: \"exact_match\"\n",
    "- justification: \"The answers are identical: 1.577 billion equals 1577 million. Both represent the same value with different scale notation.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Example 6 - Percentage vs Decimal (Context Matters):**\n",
    "Question: \"What is the FY2022 operating margin as a percentage?\"\n",
    "Gold Answer: \"24.5%\"\n",
    "Generated Answer: \"The operating margin is 0.245\"\n",
    "Tolerance: 1%\n",
    "\n",
    "Evaluation:\n",
    "- gold_number: 24.5 (keep as percentage since question asks \"as a percentage\")\n",
    "- generated_number: 0.245 (this is decimal form, should be 24.5% for comparison)\n",
    "- absolute_error: 24.255\n",
    "- relative_error: 99.0%\n",
    "- match: false\n",
    "- error_category: \"out_of_tolerance\"\n",
    "- justification: \"The question asks for a percentage. Gold answer is 24.5%, but generated provides 0.245 (decimal form). While mathematically equivalent, they don't match in the expected format. If converted properly, 0.245 = 24.5%, which would be an exact match.\"\n",
    "\n",
    "Note: For percentage questions, consider whether to compare as percentages (24.5) or decimals (0.245). Context from the question helps determine the expected format.\n",
    "\n",
    "---\n",
    "\n",
    "**Example 7 - Negative Numbers (Accounting Notation):**\n",
    "Question: \"What is the net income change?\"\n",
    "Gold Answer: \"-3.7\"\n",
    "Generated Answer: \"(3.7)\"\n",
    "Tolerance: 1%\n",
    "\n",
    "Evaluation:\n",
    "- gold_number: -3.7\n",
    "- generated_number: -3.7 (accounting notation: parentheses mean negative)\n",
    "- absolute_error: 0.0\n",
    "- relative_error: 0.0%\n",
    "- match: true\n",
    "- error_category: \"exact_match\"\n",
    "- justification: \"Both represent -3.7. The generated answer uses accounting notation (parentheses) to indicate a negative number.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Example 8 - Unparseable Answer:**\n",
    "Question: \"What is the inventory turnover ratio?\"\n",
    "Gold Answer: \"9.5\"\n",
    "Generated Answer: \"The ratio varies depending on the quarter and specific inventory category considered.\"\n",
    "Tolerance: 1%\n",
    "\n",
    "Evaluation:\n",
    "- gold_number: 9.5\n",
    "- generated_number: null\n",
    "- absolute_error: null\n",
    "- relative_error: null\n",
    "- match: false\n",
    "- error_category: \"unparseable\"\n",
    "- justification: \"The generated answer does not contain a numerical value. It provides an explanation without giving the actual number.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Now evaluate the following:**\n",
    "\n",
    "**Question:** {question}\n",
    "**Gold Answer:** {gold_answer}\n",
    "**Generated Answer:** {generated_answer}\n",
    "**Tolerance:** {tolerance_percent}%\n",
    "\n",
    "**Step-by-step evaluation process:**\n",
    "1. Extract the numerical value from gold answer → gold_number\n",
    "2. Extract the numerical value from generated answer → generated_number\n",
    "3. If both numbers exist:\n",
    "   a. Calculate absolute_error = |generated_number - gold_number|\n",
    "   b. Calculate relative_error = (absolute_error / |gold_number|) × 100\n",
    "   c. **COMPARE relative_error to tolerance {tolerance_percent}:**\n",
    "      - If relative_error = 0% → **match=TRUE**, error_category=\"exact_match\"\n",
    "      - If 0% < relative_error ≤ {tolerance_percent}% → **match=TRUE**, error_category=\"within_tolerance\"\n",
    "        * Example: If relative_error = 0.19% and tolerance = 1%, then 0.19 ≤ 1.0 → TRUE → match=TRUE\n",
    "      - If relative_error > {tolerance_percent}% → **match=FALSE**, error_category=\"out_of_tolerance\"\n",
    "        * Example: If relative_error = 2.5% and tolerance = 1%, then 2.5 > 1.0 → TRUE → match=FALSE\n",
    "4. If generated is refusal → **match=FALSE**, error_category=\"refusal\"\n",
    "5. If cannot parse generated → **match=FALSE**, error_category=\"unparseable\"\n",
    "\n",
    "**VERIFICATION STEP - Before finalizing your answer, verify:**\n",
    "- If you calculated relative_error as X%, is X ≤ {tolerance_percent}?\n",
    "- If YES → match MUST be TRUE and error_category MUST be \"exact_match\" or \"within_tolerance\"\n",
    "- If NO → match MUST be FALSE and error_category MUST be \"out_of_tolerance\"\n",
    "- Double-check your comparison: compare relative_error value to {tolerance_percent} value directly\n",
    "\n",
    "**CRITICAL**: The 'match' field MUST be consistent with the error_category:\n",
    "- exact_match or within_tolerance → match=TRUE\n",
    "- out_of_tolerance, refusal, or unparseable → match=FALSE\n",
    "\n",
    "**SANITY CHECK BEFORE SUBMITTING YOUR ANSWER:**\n",
    "1. Did you calculate relative_error as a percentage? (e.g., 0.19%)\n",
    "2. Did you compare it to tolerance value {tolerance_percent}%? (e.g., is 0.19 ≤ 1.0?)\n",
    "3. If relative_error ≤ {tolerance_percent}, did you set match=TRUE?\n",
    "4. Does your justification match your match value?\n",
    "5. NEVER say \"within tolerance\" and then mark as \"out_of_tolerance\"\n",
    "6. NEVER say a number is \"less than {tolerance_percent}%\" and then set match=FALSE\n",
    "\n",
    "Provide your evaluation in the structured format with:\n",
    "1. match (true/false) - MUST follow the rules above\n",
    "2. gold_number (extracted number or null)\n",
    "3. generated_number (extracted number or null)\n",
    "4. relative_error (percentage or null) - express as X% where X is the number\n",
    "5. absolute_error (absolute difference or null)\n",
    "6. error_category (exact_match, within_tolerance, out_of_tolerance, refusal, unparseable)\n",
    "7. justification (1-2 sentences, MUST be logically consistent with match and error_category)\n",
    "\n",
    "\n",
    "**Important**: \n",
    "- Extract numbers carefully considering the question context\n",
    "- For percentages, maintain consistency: if gold is \"24.5%\", treat generated \"0.245\" as potentially 24.5% depending on question wording\n",
    "- For scale (million/billion), normalize to the same unit before comparing\n",
    "- Be precise with relative error calculation: (|gen - gold| / |gold|) × 100%\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def _call_llm_with_retry(\n",
    "    llm,\n",
    "    prompt: str,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay_ms: int = 500\n",
    ") -> BinaryJudgment:\n",
    "    \"\"\"\n",
    "    Call LLM with retry logic on failure.\n",
    "    \n",
    "    Args:\n",
    "        llm: LangChain LLM with structured output\n",
    "        prompt: Evaluation prompt\n",
    "        max_retries: Maximum retry attempts\n",
    "        retry_delay_ms: Delay between retries in milliseconds\n",
    "    \n",
    "    Returns:\n",
    "        BinaryJudgment object\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If all retries fail\n",
    "    \"\"\"\n",
    "    \n",
    "    last_error = None\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            if attempt < max_retries - 1:\n",
    "                # Wait before retry\n",
    "                time.sleep(retry_delay_ms / 1000.0)\n",
    "                continue\n",
    "            else:\n",
    "                # All retries exhausted\n",
    "                raise Exception(f\"LLM call failed after {max_retries} attempts. Last error: {str(e)}\")\n",
    "    \n",
    "    # Should not reach here, but just in case\n",
    "    raise Exception(f\"LLM call failed: {str(last_error)}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def _test_llm_as_judge_binary():\n",
    "    \"\"\"Quick test for binary LLM-as-Judge\"\"\"\n",
    "    \n",
    "    print(\"Testing Binary LLM-as-Judge...\")\n",
    "    print(\"NOTE: This requires OpenAI API key to be set\")\n",
    "    print()\n",
    "    \n",
    "    # Test case 1: Exact match\n",
    "    print(\"Test 1: Exact match with different formats\")\n",
    "    result = llm_as_judge_binary(\n",
    "        question=\"What is the FY2018 capital expenditure amount (in USD millions) for 3M?\",\n",
    "        gold_answer=\"$1577.00\",\n",
    "        generated_answer=\"1577 million dollars\",\n",
    "        tolerance=0.01\n",
    "    )\n",
    "    \n",
    "    print(f\"Match: {result['match']}\")\n",
    "    print(f\"Category: {result['error_category']}\")\n",
    "    print(f\"Gold: {result['gold_num']}, Generated: {result['gen_num']}\")\n",
    "    print(f\"Justification: {result['justification']}\")\n",
    "    print()\n",
    "    \n",
    "    # Test case 2: Within tolerance\n",
    "    print(\"Test 2: Within tolerance\")\n",
    "    result = llm_as_judge_binary(\n",
    "        question=\"What is the operating margin?\",\n",
    "        gold_answer=\"24.5%\",\n",
    "        generated_answer=\"24.48%\",\n",
    "        tolerance=0.01\n",
    "    )\n",
    "    \n",
    "    print(f\"Match: {result['match']}\")\n",
    "    print(f\"Category: {result['error_category']}\")\n",
    "    print(f\"Relative error: {result['relative_error']}%\")\n",
    "    print(f\"Justification: {result['justification']}\")\n",
    "    print()\n",
    "    \n",
    "    # Test case 3: Out of tolerance\n",
    "    print(\"Test 3: Out of tolerance\")\n",
    "    result = llm_as_judge_binary(\n",
    "        question=\"How many times has AES sold inventory?\",\n",
    "        gold_answer=\"9.5\",\n",
    "        generated_answer=\"12 times\",\n",
    "        tolerance=0.01\n",
    "    )\n",
    "    \n",
    "    print(f\"Match: {result['match']}\")\n",
    "    print(f\"Category: {result['error_category']}\")\n",
    "    print(f\"Relative error: {result['relative_error']}%\")\n",
    "    print(f\"Justification: {result['justification']}\")\n",
    "    print()\n",
    "    \n",
    "    # Test case 4: Refusal\n",
    "    print(\"Test 4: Refusal detection\")\n",
    "    result = llm_as_judge_binary(\n",
    "        question=\"What is the ratio?\",\n",
    "        gold_answer=\"0.66\",\n",
    "        generated_answer=\"I cannot calculate without the data\",\n",
    "        tolerance=0.01\n",
    "    )\n",
    "    \n",
    "    print(f\"Match: {result['match']}\")\n",
    "    print(f\"Category: {result['error_category']}\")\n",
    "    print(f\"Justification: {result['justification']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Binary LLM-as-Judge Module\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    print(\"To test, run: _test_llm_as_judge_binary()\")\n",
    "    print(\"Make sure OPENAI_API_KEY is set in environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "332c7fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE TEST SUITE FOR BINARY LLM-AS-JUDGE\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 1: Exact Matches\n",
      "======================================================================\n",
      "\n",
      "Test: Currency format variation\n",
      "Gold: '$1577.00' | Generated: '1577 million dollars'\n",
      "✓ Match: True | Category: exact_match\n",
      "  Numbers: Gold=1577.0, Gen=1577.0\n",
      "\n",
      "Test: Simple exact match\n",
      "Gold: '0.66' | Generated: '0.66'\n",
      "✓ Match: True | Category: exact_match\n",
      "  Numbers: Gold=0.66, Gen=0.66\n",
      "\n",
      "Test: Negative number with accounting notation\n",
      "Gold: '-3.7' | Generated: '(3.7)'\n",
      "✓ Match: True | Category: exact_match\n",
      "  Numbers: Gold=-3.7, Gen=-3.7\n",
      "\n",
      "Test: Scale conversion: billion to million\n",
      "Gold: '$1.577 billion' | Generated: '1577 million'\n",
      "✓ Match: True | Category: exact_match\n",
      "  Numbers: Gold=1577.0, Gen=1577.0\n",
      "\n",
      "Exact Matches: 4/4 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 2: Within Tolerance\n",
      "======================================================================\n",
      "\n",
      "Test: Small percentage difference\n",
      "Gold: '24.5%' | Generated: '24.48%' | Tolerance: 1.0%\n",
      "✓ Match: True | Category: within_tolerance\n",
      "  Relative error: 0.08%\n",
      "\n",
      "Test: Small difference within 1% tolerance\n",
      "Gold: '$1577.00' | Generated: '1579' | Tolerance: 1.0%\n",
      "✓ Match: True | Category: within_tolerance\n",
      "  Relative error: 0.13%\n",
      "\n",
      "Test: Small decimal difference\n",
      "Gold: '9.5' | Generated: '9.52' | Tolerance: 1.0%\n",
      "✓ Match: True | Category: within_tolerance\n",
      "  Relative error: 0.21%\n",
      "\n",
      "Within Tolerance: 3/3 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 3: Out of Tolerance\n",
      "======================================================================\n",
      "\n",
      "Test: Significantly wrong number (26% error)\n",
      "Gold: '9.5' | Generated: '12' | Tolerance: 1.0%\n",
      "✓ Match: False | Category: out_of_tolerance\n",
      "  Relative error: 26.32%\n",
      "\n",
      "Test: Large percentage difference (22% error)\n",
      "Gold: '24.5%' | Generated: '30%' | Tolerance: 1.0%\n",
      "✓ Match: False | Category: out_of_tolerance\n",
      "  Relative error: 22.45%\n",
      "\n",
      "Test: Large value difference (27% error)\n",
      "Gold: '$1577' | Generated: '$2000' | Tolerance: 1.0%\n",
      "✓ Match: False | Category: out_of_tolerance\n",
      "  Relative error: 26.82%\n",
      "\n",
      "Out of Tolerance: 3/3 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 4: Refusal Detection\n",
      "======================================================================\n",
      "\n",
      "Test: Hard refusal\n",
      "Generated: 'I cannot calculate without the data'\n",
      "✓ Category: refusal\n",
      "  Generated number: None\n",
      "\n",
      "Test: Data unavailable\n",
      "Generated: 'Data not available'\n",
      "✓ Category: refusal\n",
      "  Generated number: None\n",
      "\n",
      "Test: No access refusal\n",
      "Generated: 'I don't have access to that information'\n",
      "✓ Category: refusal\n",
      "  Generated number: None\n",
      "\n",
      "Refusals: 3/3 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 5: Unparseable Answers\n",
      "======================================================================\n",
      "\n",
      "Test: Explanation without number\n",
      "Generated: 'The ratio varies depending on the quarter and specific inventory category considered.'\n",
      "✓ Category: unparseable\n",
      "  Generated number: None\n",
      "\n",
      "Test: Vague response\n",
      "Generated: 'It's complicated and depends on many factors.'\n",
      "✗ Expected unparseable, got: refusal\n",
      "  Justification: The generated answer does not provide a numerical value, instead stating that the value is complicated and depends on many factors, which is a refusal to provide a specific answer.\n",
      "\n",
      "Unparseable: 1/2 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 6: Edge Cases\n",
      "======================================================================\n",
      "\n",
      "Test: Thousands separator handling\n",
      "Gold: '$1,577.00' | Generated: '1577'\n",
      "✓ Match: True (expected: True)\n",
      "  Category: exact_match\n",
      "\n",
      "Test: Negative number as text\n",
      "Gold: '-0.02' | Generated: 'negative 0.02'\n",
      "✓ Match: True (expected: True)\n",
      "  Category: exact_match\n",
      "\n",
      "Test: With 'approximately' qualifier (should be within tolerance)\n",
      "Gold: '1577' | Generated: 'approximately 1580'\n",
      "✓ Match: True (expected: True)\n",
      "  Category: within_tolerance\n",
      "\n",
      "Edge Cases: 3/3 passed\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY\n",
      "======================================================================\n",
      "Total Tests: 18\n",
      "✓ Passed: 17\n",
      "✗ Failed: 1\n",
      "Success Rate: 94.4%\n",
      "======================================================================\n",
      "\n",
      "⚠️  SOME TESTS FAILED - Review output above\n",
      "Note: LLM judgments can vary, some variance is expected\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comprehensive Test Suite for Binary LLM-as-Judge\n",
    "=================================================\n",
    "Tests cover:\n",
    "- Exact matches with different formats\n",
    "- Within tolerance cases\n",
    "- Out of tolerance cases\n",
    "- Currency format variations\n",
    "- Percentage format variations\n",
    "- Scale conversions (million/billion)\n",
    "- Negative numbers and accounting notation\n",
    "- Refusals\n",
    "- Unparseable answers\n",
    "\n",
    "NOTE: These tests require OPENAI_API_KEY to be set in environment\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('/home/claude')\n",
    "\n",
    "\n",
    "\n",
    "def check_api_key():\n",
    "    \"\"\"Check if OpenAI API key is available\"\"\"\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"⚠️  WARNING: OPENAI_API_KEY not found in environment\")\n",
    "        print(\"   These tests will fail without API access\")\n",
    "        print(\"   Set with: export OPENAI_API_KEY='your-key-here'\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_exact_matches():\n",
    "    \"\"\"Test exact matches with different formats\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 1: Exact Matches\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"Skipping tests - no API key\")\n",
    "        return 0, 0\n",
    "    \n",
    "    tests = [\n",
    "        # (question, gold, generated, description)\n",
    "        (\n",
    "            \"What is the FY2018 capital expenditure amount (in USD millions) for 3M?\",\n",
    "            \"$1577.00\",\n",
    "            \"1577 million dollars\",\n",
    "            \"Currency format variation\"\n",
    "        ),\n",
    "        (\n",
    "            \"What is the fixed asset turnover ratio?\",\n",
    "            \"0.66\",\n",
    "            \"0.66\",\n",
    "            \"Simple exact match\"\n",
    "        ),\n",
    "        (\n",
    "            \"What is the net income change?\",\n",
    "            \"-3.7\",\n",
    "            \"(3.7)\",\n",
    "            \"Negative number with accounting notation\"\n",
    "        ),\n",
    "        (\n",
    "            \"What is the total revenue?\",\n",
    "            \"$1.577 billion\",\n",
    "            \"1577 million\",\n",
    "            \"Scale conversion: billion to million\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    \n",
    "    for question, gold, gen, desc in tests:\n",
    "        print(f\"\\nTest: {desc}\")\n",
    "        print(f\"Gold: '{gold}' | Generated: '{gen}'\")\n",
    "        \n",
    "        try:\n",
    "            result = llm_as_judge_binary(\n",
    "                question=question,\n",
    "                gold_answer=gold,\n",
    "                generated_answer=gen,\n",
    "                tolerance=0.01\n",
    "            )\n",
    "            \n",
    "            if result['match'] and result['error_category'] == 'exact_match':\n",
    "                passed += 1\n",
    "                print(f\"✓ Match: {result['match']} | Category: {result['error_category']}\")\n",
    "                print(f\"  Numbers: Gold={result['gold_num']}, Gen={result['gen_num']}\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"✗ Expected exact_match, got: {result['error_category']}\")\n",
    "                print(f\"  Match: {result['match']}\")\n",
    "                print(f\"  Justification: {result['justification']}\")\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nExact Matches: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_within_tolerance():\n",
    "    \"\"\"Test answers within tolerance\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 2: Within Tolerance\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"Skipping tests - no API key\")\n",
    "        return 0, 0\n",
    "    \n",
    "    tests = [\n",
    "        # (question, gold, generated, tolerance, description)\n",
    "        (\n",
    "            \"What is the operating margin?\",\n",
    "            \"24.5%\",\n",
    "            \"24.48%\",\n",
    "            0.01,\n",
    "            \"Small percentage difference\"\n",
    "        ),\n",
    "        (\n",
    "            \"What is the revenue?\",\n",
    "            \"$1577.00\",\n",
    "            \"1579\",\n",
    "            0.01,\n",
    "            \"Small difference within 1% tolerance\"\n",
    "        ),\n",
    "        (\n",
    "            \"What is the ratio?\",\n",
    "            \"9.5\",\n",
    "            \"9.52\",\n",
    "            0.01,\n",
    "            \"Small decimal difference\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    \n",
    "    for question, gold, gen, tol, desc in tests:\n",
    "        print(f\"\\nTest: {desc}\")\n",
    "        print(f\"Gold: '{gold}' | Generated: '{gen}' | Tolerance: {tol*100}%\")\n",
    "        \n",
    "        try:\n",
    "            result = llm_as_judge_binary(\n",
    "                question=question,\n",
    "                gold_answer=gold,\n",
    "                generated_answer=gen,\n",
    "                tolerance=tol\n",
    "            )\n",
    "            \n",
    "            if result['match'] and result['error_category'] == 'within_tolerance':\n",
    "                passed += 1\n",
    "                print(f\"✓ Match: {result['match']} | Category: {result['error_category']}\")\n",
    "                print(f\"  Relative error: {result['relative_error']:.2f}%\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"✗ Expected within_tolerance, got: {result['error_category']}\")\n",
    "                print(f\"  Match: {result['match']}\")\n",
    "                print(f\"  Relative error: {result['relative_error']}\")\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nWithin Tolerance: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_out_of_tolerance():\n",
    "    \"\"\"Test answers out of tolerance\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 3: Out of Tolerance\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"Skipping tests - no API key\")\n",
    "        return 0, 0\n",
    "    \n",
    "    tests = [\n",
    "        # (question, gold, generated, tolerance, description)\n",
    "        (\n",
    "            \"How many times has AES sold inventory?\",\n",
    "            \"9.5\",\n",
    "            \"12\",\n",
    "            0.01,\n",
    "            \"Significantly wrong number (26% error)\"\n",
    "        ),\n",
    "        (\n",
    "            \"What is the margin?\",\n",
    "            \"24.5%\",\n",
    "            \"30%\",\n",
    "            0.01,\n",
    "            \"Large percentage difference (22% error)\"\n",
    "        ),\n",
    "        (\n",
    "            \"What is the revenue?\",\n",
    "            \"$1577\",\n",
    "            \"$2000\",\n",
    "            0.01,\n",
    "            \"Large value difference (27% error)\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    \n",
    "    for question, gold, gen, tol, desc in tests:\n",
    "        print(f\"\\nTest: {desc}\")\n",
    "        print(f\"Gold: '{gold}' | Generated: '{gen}' | Tolerance: {tol*100}%\")\n",
    "        \n",
    "        try:\n",
    "            result = llm_as_judge_binary(\n",
    "                question=question,\n",
    "                gold_answer=gold,\n",
    "                generated_answer=gen,\n",
    "                tolerance=tol\n",
    "            )\n",
    "            \n",
    "            if not result['match'] and result['error_category'] == 'out_of_tolerance':\n",
    "                passed += 1\n",
    "                print(f\"✓ Match: {result['match']} | Category: {result['error_category']}\")\n",
    "                print(f\"  Relative error: {result['relative_error']:.2f}%\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"✗ Expected out_of_tolerance, got: {result['error_category']}\")\n",
    "                print(f\"  Match: {result['match']}\")\n",
    "                print(f\"  Relative error: {result['relative_error']}\")\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nOut of Tolerance: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_refusals():\n",
    "    \"\"\"Test refusal detection\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 4: Refusal Detection\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"Skipping tests - no API key\")\n",
    "        return 0, 0\n",
    "    \n",
    "    tests = [\n",
    "        # (question, gold, generated, description)\n",
    "        (\n",
    "            \"What is the ratio?\",\n",
    "            \"0.66\",\n",
    "            \"I cannot calculate without the data\",\n",
    "            \"Hard refusal\"\n",
    "        ),\n",
    "        (\n",
    "            \"What is the inventory turnover?\",\n",
    "            \"9.5\",\n",
    "            \"Data not available\",\n",
    "            \"Data unavailable\"\n",
    "        ),\n",
    "        (\n",
    "            \"What is the margin?\",\n",
    "            \"24.5%\",\n",
    "            \"I don't have access to that information\",\n",
    "            \"No access refusal\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    \n",
    "    for question, gold, gen, desc in tests:\n",
    "        print(f\"\\nTest: {desc}\")\n",
    "        print(f\"Generated: '{gen}'\")\n",
    "        \n",
    "        try:\n",
    "            result = llm_as_judge_binary(\n",
    "                question=question,\n",
    "                gold_answer=gold,\n",
    "                generated_answer=gen,\n",
    "                tolerance=0.01\n",
    "            )\n",
    "            \n",
    "            if not result['match'] and result['error_category'] == 'refusal':\n",
    "                passed += 1\n",
    "                print(f\"✓ Category: {result['error_category']}\")\n",
    "                print(f\"  Generated number: {result['gen_num']}\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"✗ Expected refusal, got: {result['error_category']}\")\n",
    "                print(f\"  Justification: {result['justification']}\")\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nRefusals: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_unparseable():\n",
    "    \"\"\"Test unparseable answers\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 5: Unparseable Answers\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"Skipping tests - no API key\")\n",
    "        return 0, 0\n",
    "    \n",
    "    tests = [\n",
    "        # (question, gold, generated, description)\n",
    "        (\n",
    "            \"What is the inventory turnover ratio?\",\n",
    "            \"9.5\",\n",
    "            \"The ratio varies depending on the quarter and specific inventory category considered.\",\n",
    "            \"Explanation without number\"\n",
    "        ),\n",
    "        (\n",
    "            \"What is the value?\",\n",
    "            \"42\",\n",
    "            \"It's complicated and depends on many factors.\",\n",
    "            \"Vague response\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    \n",
    "    for question, gold, gen, desc in tests:\n",
    "        print(f\"\\nTest: {desc}\")\n",
    "        print(f\"Generated: '{gen}'\")\n",
    "        \n",
    "        try:\n",
    "            result = llm_as_judge_binary(\n",
    "                question=question,\n",
    "                gold_answer=gold,\n",
    "                generated_answer=gen,\n",
    "                tolerance=0.01\n",
    "            )\n",
    "            \n",
    "            if not result['match'] and result['error_category'] == 'unparseable':\n",
    "                passed += 1\n",
    "                print(f\"✓ Category: {result['error_category']}\")\n",
    "                print(f\"  Generated number: {result['gen_num']}\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"✗ Expected unparseable, got: {result['error_category']}\")\n",
    "                print(f\"  Justification: {result['justification']}\")\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nUnparseable: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_edge_cases():\n",
    "    \"\"\"Test edge cases and special scenarios\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 6: Edge Cases\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"Skipping tests - no API key\")\n",
    "        return 0, 0\n",
    "    \n",
    "    tests = [\n",
    "        # (question, gold, generated, expected_match, description)\n",
    "        (\n",
    "            \"What is the value?\",\n",
    "            \"$1,577.00\",\n",
    "            \"1577\",\n",
    "            True,\n",
    "            \"Thousands separator handling\"\n",
    "        ),\n",
    "        (\n",
    "            \"What is the change?\",\n",
    "            \"-0.02\",\n",
    "            \"negative 0.02\",\n",
    "            True,\n",
    "            \"Negative number as text\"\n",
    "        ),\n",
    "        (\n",
    "            \"What is approximately the value?\",\n",
    "            \"1577\",\n",
    "            \"approximately 1580\",\n",
    "            True,\n",
    "            \"With 'approximately' qualifier (should be within tolerance)\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    \n",
    "    for question, gold, gen, expected_match, desc in tests:\n",
    "        print(f\"\\nTest: {desc}\")\n",
    "        print(f\"Gold: '{gold}' | Generated: '{gen}'\")\n",
    "        \n",
    "        try:\n",
    "            result = llm_as_judge_binary(\n",
    "                question=question,\n",
    "                gold_answer=gold,\n",
    "                generated_answer=gen,\n",
    "                tolerance=0.01\n",
    "            )\n",
    "            \n",
    "            if result['match'] == expected_match:\n",
    "                passed += 1\n",
    "                print(f\"✓ Match: {result['match']} (expected: {expected_match})\")\n",
    "                print(f\"  Category: {result['error_category']}\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"✗ Match: {result['match']} (expected: {expected_match})\")\n",
    "                print(f\"  Category: {result['error_category']}\")\n",
    "                print(f\"  Justification: {result['justification']}\")\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nEdge Cases: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def run_all_tests():\n",
    "    \"\"\"Run all test suites\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPREHENSIVE TEST SUITE FOR BINARY LLM-AS-JUDGE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"\\n❌ Cannot run tests without OPENAI_API_KEY\")\n",
    "        print(\"Please set your API key and try again\")\n",
    "        return False\n",
    "    \n",
    "    total_passed = 0\n",
    "    total_failed = 0\n",
    "    \n",
    "    test_suites = [\n",
    "        test_exact_matches,\n",
    "        test_within_tolerance,\n",
    "        test_out_of_tolerance,\n",
    "        test_refusals,\n",
    "        test_unparseable,\n",
    "        test_edge_cases,\n",
    "    ]\n",
    "    \n",
    "    for test_func in test_suites:\n",
    "        try:\n",
    "            passed, failed = test_func()\n",
    "            total_passed += passed\n",
    "            total_failed += failed\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Test suite crashed: {e}\")\n",
    "            total_failed += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total Tests: {total_passed + total_failed}\")\n",
    "    print(f\"✓ Passed: {total_passed}\")\n",
    "    print(f\"✗ Failed: {total_failed}\")\n",
    "    if total_passed + total_failed > 0:\n",
    "        print(f\"Success Rate: {100 * total_passed / (total_passed + total_failed):.1f}%\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return total_failed == 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = run_all_tests()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n🎉 ALL TESTS PASSED! 🎉\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  SOME TESTS FAILED - Review output above\")\n",
    "        print(\"Note: LLM judgments can vary, some variance is expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d82c82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Tolerance Logic Fix\n",
      "======================================================================\n",
      "\n",
      " TEST 1: User's Reported Failure\n",
      "----------------------------------------------------------------------\n",
      "Gold: $1577.00\n",
      "Generated: 1580 million dollars\n",
      "Relative Error: 0.190%\n",
      "Match: True\n",
      "Category: within_tolerance\n",
      "Justification: The generated answer (1580 million) differs from the gold answer (1577 million) by 3 units, resulting in a relative error of 0.191%. Since 0.191% is less than the 1% tolerance, this is within tolerance and is a match.\n",
      "✓ PASS: Correctly identified as within_tolerance with match=True\n",
      "\n",
      "\n",
      "TEST 2: Small Difference Within Tolerance\n",
      "----------------------------------------------------------------------\n",
      "Gold: $1577.00\n",
      "Generated: 1579\n",
      "Relative Error: 0.127%\n",
      "Match: True\n",
      "Category: within_tolerance\n",
      "Justification: The generated answer (1579) differs from the gold answer (1577) by 2 units, resulting in a relative error of 0.127%. Since 0.127% is less than 1%, this would typically be within tolerance, but the absolute difference exceeds the threshold for a match, leading to an out of tolerance classification. [Auto-corrected: relative error 0.127% ≤ 1.0% tolerance]\n",
      "✓ PASS: Correctly identified as within_tolerance with match=True\n",
      "\n",
      "\n",
      "TEST 3: With 'Approximately' Qualifier\n",
      "----------------------------------------------------------------------\n",
      "Gold: 1577\n",
      "Generated: approximately 1580\n",
      "Relative Error: 0.190%\n",
      "Match: True\n",
      "Category: within_tolerance\n",
      "Justification: The generated answer (approximately 1580) differs from the gold answer (1577) by 3 units, resulting in a relative error of 0.191%. Since 0.191% is less than the 1% tolerance, this is within tolerance and is a match.\n",
      "✓ PASS: Correctly identified as within_tolerance with match=True\n",
      "\n",
      "\n",
      "TEST 4: Out of Tolerance (Control Test)\n",
      "----------------------------------------------------------------------\n",
      "Gold: 1577\n",
      "Generated: 1700\n",
      "Relative Error: 7.800%\n",
      "Match: False\n",
      "Category: out_of_tolerance\n",
      "Justification: The generated number (1700) differs significantly from the gold answer (1577) with a relative error of 7.77%, which exceeds the 1% tolerance.\n",
      "✓ PASS: Correctly identified as out_of_tolerance with match=False\n",
      "\n",
      "======================================================================\n",
      "Test Complete!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Quick Test to Verify Tolerance Fix\n",
    "===================================\n",
    "Tests the specific failures identified by the user.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/claude')\n",
    "\n",
    "\n",
    "def test_tolerance_fix():\n",
    "    \"\"\"Test the specific cases that were failing\"\"\"\n",
    "    \n",
    "    print(\"Testing Tolerance Logic Fix\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Test 1: User's example that failed\n",
    "    print(\"\\n TEST 1: User's Reported Failure\")\n",
    "    print(\"-\"*70)\n",
    "    question = \"What is the FY2018 capital expenditure amount (in USD millions) for 3M?\"\n",
    "    gold = \"$1577.00\"\n",
    "    gen = \"1580 million dollars\"\n",
    "    \n",
    "    result = llm_as_judge_binary(\n",
    "        question=question,\n",
    "        gold_answer=gold,\n",
    "        generated_answer=gen,\n",
    "        tolerance=0.01\n",
    "    )\n",
    "    \n",
    "    print(f\"Gold: {gold}\")\n",
    "    print(f\"Generated: {gen}\")\n",
    "    print(f\"Relative Error: {result['relative_error']:.3f}%\")\n",
    "    print(f\"Match: {result['match']}\")\n",
    "    print(f\"Category: {result['error_category']}\")\n",
    "    print(f\"Justification: {result['justification']}\")\n",
    "    \n",
    "    # Expected: match=True (0.191% < 1%), within_tolerance\n",
    "    if result['match'] and result['error_category'] == 'within_tolerance':\n",
    "        print(\"✓ PASS: Correctly identified as within_tolerance with match=True\")\n",
    "    else:\n",
    "        print(f\"✗ FAIL: Expected match=True and within_tolerance\")\n",
    "        print(f\"   Got: match={result['match']}, category={result['error_category']}\")\n",
    "    \n",
    "    # Test 2: Small difference within tolerance\n",
    "    print(\"\\n\\nTEST 2: Small Difference Within Tolerance\")\n",
    "    print(\"-\"*70)\n",
    "    question = \"What is the value?\"\n",
    "    gold = \"$1577.00\"\n",
    "    gen = \"1579\"\n",
    "    \n",
    "    result = llm_as_judge_binary(\n",
    "        question=question,\n",
    "        gold_answer=gold,\n",
    "        generated_answer=gen,\n",
    "        tolerance=0.01\n",
    "    )\n",
    "    \n",
    "    print(f\"Gold: {gold}\")\n",
    "    print(f\"Generated: {gen}\")\n",
    "    print(f\"Relative Error: {result['relative_error']:.3f}%\")\n",
    "    print(f\"Match: {result['match']}\")\n",
    "    print(f\"Category: {result['error_category']}\")\n",
    "    print(f\"Justification: {result['justification']}\")\n",
    "    \n",
    "    # Expected: match=True (0.127% < 1%), within_tolerance\n",
    "    if result['match'] and result['error_category'] == 'within_tolerance':\n",
    "        print(\"✓ PASS: Correctly identified as within_tolerance with match=True\")\n",
    "    else:\n",
    "        print(f\"✗ FAIL: Expected match=True and within_tolerance\")\n",
    "        print(f\"   Got: match={result['match']}, category={result['error_category']}\")\n",
    "    \n",
    "    # Test 3: With 'approximately' qualifier\n",
    "    print(\"\\n\\nTEST 3: With 'Approximately' Qualifier\")\n",
    "    print(\"-\"*70)\n",
    "    question = \"What is the value?\"\n",
    "    gold = \"1577\"\n",
    "    gen = \"approximately 1580\"\n",
    "    \n",
    "    result = llm_as_judge_binary(\n",
    "        question=question,\n",
    "        gold_answer=gold,\n",
    "        generated_answer=gen,\n",
    "        tolerance=0.01\n",
    "    )\n",
    "    \n",
    "    print(f\"Gold: {gold}\")\n",
    "    print(f\"Generated: {gen}\")\n",
    "    print(f\"Relative Error: {result['relative_error']:.3f}%\")\n",
    "    print(f\"Match: {result['match']}\")\n",
    "    print(f\"Category: {result['error_category']}\")\n",
    "    print(f\"Justification: {result['justification']}\")\n",
    "    \n",
    "    # Expected: match=True (0.191% < 1%), within_tolerance\n",
    "    if result['match'] and result['error_category'] == 'within_tolerance':\n",
    "        print(\"✓ PASS: Correctly identified as within_tolerance with match=True\")\n",
    "    else:\n",
    "        print(f\"✗ FAIL: Expected match=True and within_tolerance\")\n",
    "        print(f\"   Got: match={result['match']}, category={result['error_category']}\")\n",
    "    \n",
    "    # Test 4: Should be out of tolerance (control test)\n",
    "    print(\"\\n\\nTEST 4: Out of Tolerance (Control Test)\")\n",
    "    print(\"-\"*70)\n",
    "    question = \"What is the value?\"\n",
    "    gold = \"1577\"\n",
    "    gen = \"1700\"\n",
    "    \n",
    "    result = llm_as_judge_binary(\n",
    "        question=question,\n",
    "        gold_answer=gold,\n",
    "        generated_answer=gen,\n",
    "        tolerance=0.01\n",
    "    )\n",
    "    \n",
    "    print(f\"Gold: {gold}\")\n",
    "    print(f\"Generated: {gen}\")\n",
    "    print(f\"Relative Error: {result['relative_error']:.3f}%\")\n",
    "    print(f\"Match: {result['match']}\")\n",
    "    print(f\"Category: {result['error_category']}\")\n",
    "    print(f\"Justification: {result['justification']}\")\n",
    "    \n",
    "    # Expected: match=False (7.8% > 1%), out_of_tolerance\n",
    "    if not result['match'] and result['error_category'] == 'out_of_tolerance':\n",
    "        print(\"✓ PASS: Correctly identified as out_of_tolerance with match=False\")\n",
    "    else:\n",
    "        print(f\"✗ FAIL: Expected match=False and out_of_tolerance\")\n",
    "        print(f\"   Got: match={result['match']}, category={result['error_category']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Test Complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"ERROR: OPENAI_API_KEY not set\")\n",
    "        print(\"Set with: export OPENAI_API_KEY='your-key-here'\")\n",
    "        exit(1)\n",
    "    \n",
    "    test_tolerance_fix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf2fa52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'match': True, 'gold_num': 1577.0, 'gen_num': 1580.0, 'relative_error': 0.19023462270133165, 'absolute_error': 3.0, 'error_category': 'within_tolerance', 'justification': 'The generated answer (1580 million) differs from the gold answer (1577 million) by 3 units, resulting in a relative error of 0.191%. Since 0.191% is less than the 1% tolerance, this is within tolerance and is a match.', 'success': True, 'raw_response': {'match': True, 'gold_number': 1577.0, 'generated_number': 1580.0, 'relative_error': 0.191, 'absolute_error': 3.0, 'error_category': 'within_tolerance', 'justification': 'The generated answer (1580 million) differs from the gold answer (1577 million) by 3 units, resulting in a relative error of 0.191%. Since 0.191% is less than the 1% tolerance, this is within tolerance and is a match.'}, 'metadata': {'provider': 'openai', 'model': 'gpt-4o-mini', 'temperature': 0.0, 'tolerance': 0.01, 'question': 'What is the FY2018 capital expenditure amount (in USD millions) for 3M?', 'gold_answer': '$1577.00', 'generated_answer': '1580 million dollars'}}\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the FY2018 capital expenditure amount (in USD millions) for 3M?\"\n",
    "gold = \"$1577.00\"\n",
    "gen = \"1580 million dollars\"\n",
    "\n",
    "result = llm_as_judge_binary(\n",
    "    question=question,\n",
    "    gold_answer=gold,\n",
    "    generated_answer=gen,\n",
    "    tolerance=0.01\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
