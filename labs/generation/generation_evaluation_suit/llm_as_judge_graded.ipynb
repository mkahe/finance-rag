{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baae2951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcffc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graded LLM-as-Judge Module\n",
      "======================================================================\n",
      "\n",
      "To test, run: _test_llm_as_judge_graded()\n",
      "Make sure OPENAI_API_KEY is set in environment\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Graded LLM-as-Judge for Financial QA Evaluation\n",
    "================================================\n",
    "\n",
    "This module provides LLM-based evaluation for domain-relevant and novel-generated\n",
    "questions using a 0-4 graded scoring system.\n",
    "\n",
    "Uses OpenAI's structured output with Pydantic for reliable parsing.\n",
    "\n",
    "Author: Financial QA Evaluation System\n",
    "Version: 1.0\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from typing import Dict, Any, List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class GradedJudgment(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic schema for graded LLM judgment output.\n",
    "    Ensures structured and parseable response from LLM.\n",
    "    \"\"\"\n",
    "    score: int = Field(\n",
    "        description=\"Score from 0-4 based on factual correctness and completeness\",\n",
    "        ge=0,\n",
    "        le=4\n",
    "    )\n",
    "    key_facts_gold: List[str] = Field(\n",
    "        description=\"List of key facts extracted from the gold answer\"\n",
    "    )\n",
    "    facts_present: List[str] = Field(\n",
    "        description=\"List of facts from gold answer that are present in generated answer\"\n",
    "    )\n",
    "    facts_missing: List[str] = Field(\n",
    "        description=\"List of facts from gold answer that are missing in generated answer\"\n",
    "    )\n",
    "    justification: str = Field(\n",
    "        description=\"Brief explanation (2-3 sentences) of why this score was assigned\"\n",
    "    )\n",
    "\n",
    "\n",
    "def llm_as_judge_graded(\n",
    "    question: str,\n",
    "    gold_answer: str,\n",
    "    generated_answer: str,\n",
    "    provider: str = \"openai\",\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    temperature: float = 0.0,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay_ms: int = 500,\n",
    "    return_details: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate answer quality using LLM with 0-4 graded scoring.\n",
    "    \n",
    "    This evaluator is used for:\n",
    "    - Domain-relevant questions (all lengths)\n",
    "    - Novel-generated questions (all)\n",
    "    \n",
    "    The LLM judges semantic equivalence, factual accuracy, and completeness\n",
    "    rather than exact word matching.\n",
    "    \n",
    "    Args:\n",
    "        question: The question being answered\n",
    "        gold_answer: The gold standard answer (ground truth)\n",
    "        generated_answer: The generated answer to evaluate\n",
    "        provider: LLM provider ('openai', 'anthropic', 'ollama')\n",
    "        model: Model name (e.g., 'gpt-4o-mini', 'claude-sonnet-4', 'llama3.1:8b')\n",
    "        temperature: Temperature for generation (0.0 for deterministic)\n",
    "        max_retries: Maximum number of retry attempts on failure\n",
    "        retry_delay_ms: Delay between retries in milliseconds\n",
    "        return_details: If True, include full LLM response and metadata\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - score: int (0-4)\n",
    "            - key_facts_gold: List[str] - Key facts from gold answer\n",
    "            - facts_present: List[str] - Facts present in generated answer\n",
    "            - facts_missing: List[str] - Facts missing from generated answer\n",
    "            - justification: str - Explanation of the score\n",
    "            - raw_response: dict - Full LLM response (if return_details=True)\n",
    "            - metadata: dict - Call information (if return_details=True)\n",
    "    \n",
    "    Scoring Rubric:\n",
    "        4 (Perfect): All key facts present, accurate, comprehensive\n",
    "        3 (Good): Most key facts present, minor omissions\n",
    "        2 (Acceptable): Some key facts present, significant omissions\n",
    "        1 (Poor): Few key facts, mostly incorrect/irrelevant\n",
    "        0 (Wrong): Completely incorrect or refusal to answer\n",
    "    \n",
    "    Examples:\n",
    "        >>> result = llm_as_judge_graded(\n",
    "        ...     question=\"What is 3M's inventory turnover in FY2022?\",\n",
    "        ...     gold_answer=\"AES has converted inventory 9.5 times in FY 2022.\",\n",
    "        ...     generated_answer=\"AES Corporation sold its inventory roughly 12 times in FY2022.\"\n",
    "        ... )\n",
    "        >>> print(f\"Score: {result['score']}/4\")\n",
    "        >>> print(f\"Facts missing: {result['facts_missing']}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Import get_llm function (assumes it's available in the environment)\n",
    "    # from your_module import get_llm\n",
    "    # For now, we'll create the LLM directly\n",
    "    # You can replace this with: llm = get_llm(provider, model, temperature)\n",
    "    \n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    # Create LLM with structured output\n",
    "    llm = ChatOpenAI(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        model_kwargs={\"response_format\": {\"type\": \"json_object\"}} if provider == \"openai\" else {}\n",
    "    )\n",
    "    \n",
    "    # Apply structured output schema\n",
    "    structured_llm = llm.with_structured_output(GradedJudgment)\n",
    "    \n",
    "    # Construct evaluation prompt with few-shot examples\n",
    "    prompt = _create_graded_prompt(question, gold_answer, generated_answer)\n",
    "    \n",
    "    # Call LLM with retry logic\n",
    "    try:\n",
    "        judgment = _call_llm_with_retry(\n",
    "            structured_llm,\n",
    "            prompt,\n",
    "            max_retries=max_retries,\n",
    "            retry_delay_ms=retry_delay_ms\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # If all retries fail, return error result\n",
    "        return {\n",
    "            'score': 0,\n",
    "            'key_facts_gold': [],\n",
    "            'facts_present': [],\n",
    "            'facts_missing': [],\n",
    "            'justification': f\"LLM evaluation failed after {max_retries} retries: {str(e)}\",\n",
    "            'error': str(e),\n",
    "            'success': False\n",
    "        }\n",
    "    \n",
    "    # Build result dictionary\n",
    "    result = {\n",
    "        'score': judgment.score,\n",
    "        'key_facts_gold': judgment.key_facts_gold,\n",
    "        'facts_present': judgment.facts_present,\n",
    "        'facts_missing': judgment.facts_missing,\n",
    "        'justification': judgment.justification,\n",
    "        'success': True\n",
    "    }\n",
    "    \n",
    "    if return_details:\n",
    "        result['raw_response'] = judgment.dict()\n",
    "        result['metadata'] = {\n",
    "            'provider': provider,\n",
    "            'model': model,\n",
    "            'temperature': temperature,\n",
    "            'question': question,\n",
    "            'gold_answer': gold_answer,\n",
    "            'generated_answer': generated_answer\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def _create_graded_prompt(question: str, gold_answer: str, generated_answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Create the evaluation prompt with few-shot examples.\n",
    "    \n",
    "    Args:\n",
    "        question: The question being answered\n",
    "        gold_answer: Gold standard answer\n",
    "        generated_answer: Generated answer to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert evaluator for a financial question-answering system. Your task is to evaluate how well a generated answer matches the gold standard answer.\n",
    "\n",
    "**Scoring Rubric:**\n",
    "- **4 (Perfect)**: All key facts present, accurate, comprehensive. Generated answer fully captures the gold answer's information.\n",
    "- **3 (Good)**: Most key facts present with minor omissions. The core information is correct but some details are missing.\n",
    "- **2 (Acceptable)**: Some key facts present but significant omissions. Partial correctness with important information missing.\n",
    "- **1 (Poor)**: Few key facts correct, mostly incorrect or irrelevant information.\n",
    "- **0 (Wrong)**: Completely incorrect, contradicts gold answer, or is a refusal to answer.\n",
    "\n",
    "**Important Guidelines:**\n",
    "- Focus on FACTUAL CORRECTNESS, not exact wording\n",
    "- Different phrasings of the same fact should be recognized as correct\n",
    "- Numbers must match (with reasonable rounding)\n",
    "- If generated answer includes information not in gold answer, don't penalize unless it contradicts\n",
    "- A refusal to answer (e.g., \"I don't know\", \"Data not available\") should score 0\n",
    "\n",
    "---\n",
    "\n",
    "**Few-Shot Examples:**\n",
    "\n",
    "**Example 1 - Novel-Generated Question:**\n",
    "Question: \"Which segment dragged down 3M's overall growth in 2022 excluding M&A?\"\n",
    "Gold Answer: \"The consumer segment shrunk by 0.9% organically.\"\n",
    "Generated Answer: \"The Consumer segment has dragged down 3M's overall growth in 2022.\"\n",
    "\n",
    "Evaluation:\n",
    "- Key facts in gold: [consumer segment, shrunk/declined, 0.9%, organically]\n",
    "- Facts present: [consumer segment, dragged down growth]\n",
    "- Facts missing: [0.9%, organically]\n",
    "- Score: 2 (Some key facts present - identifies the segment correctly but misses the specific percentage and \"organically\" qualifier)\n",
    "- Justification: \"The generated answer correctly identifies the consumer segment as the problem area but omits the specific 0.9% decline and the 'organically' qualifier, which are important quantitative details.\"\n",
    "\n",
    "**Example 2 - Domain-Relevant Question:**\n",
    "Question: \"Does AMD have a reasonably healthy liquidity profile based on its quick ratio for FY22?\"\n",
    "Gold Answer: \"Yes. The quick ratio is 1.57, calculated as (cash and cash equivalents+Short term investments+Accounts receivable, net+receivables from related parties)/ (current liabilities).\"\n",
    "Generated Answer: \"Yes, AMD has a reasonably healthy liquidity profile based on its quick ratio of approximately 1.57 for FY22.\"\n",
    "\n",
    "Evaluation:\n",
    "- Key facts in gold: [Yes, quick ratio, 1.57, healthy liquidity, calculation formula]\n",
    "- Facts present: [Yes, quick ratio, 1.57, healthy liquidity]\n",
    "- Facts missing: [calculation formula]\n",
    "- Score: 4 (All essential facts present - the calculation formula is supplementary detail, and the core answer is complete)\n",
    "- Justification: \"The generated answer captures all essential information: affirmative answer, the specific quick ratio value (1.57), and the assessment of healthy liquidity. The missing calculation formula is supplementary detail that doesn't affect the core answer quality.\"\n",
    "\n",
    "**Example 3 - Domain-Relevant Question:**\n",
    "Question: \"Roughly how many times has AES Corporation sold its inventory in FY2022?\"\n",
    "Gold Answer: \"AES has converted inventory 9.5 times in FY 2022.\"\n",
    "Generated Answer: \"AES Corporation sold its inventory roughly 12 times in FY2022; however, conventional inventory management may not be meaningful due to the nature of its business in the energy sector.\"\n",
    "\n",
    "Evaluation:\n",
    "- Key facts in gold: [AES, inventory turnover, 9.5 times, FY2022]\n",
    "- Facts present: [AES, inventory turnover, FY2022]\n",
    "- Facts missing: [9.5 times - generated says 12 times which is wrong]\n",
    "- Score: 1 (The number is significantly wrong: 12 vs 9.5, which is a ~26% error. The qualification about energy sector doesn't compensate for the incorrect figure.)\n",
    "- Justification: \"While the generated answer correctly identifies the context and adds useful qualification about the energy sector, it provides an incorrect inventory turnover number (12 vs 9.5 times), which is a significant factual error for a quantitative question.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Now evaluate the following:**\n",
    "\n",
    "**Question:** {question}\n",
    "\n",
    "**Gold Answer:** {gold_answer}\n",
    "\n",
    "**Generated Answer:** {generated_answer}\n",
    "\n",
    "Provide your evaluation in the structured format with:\n",
    "1. score (0-4)\n",
    "2. key_facts_gold (list of key facts from gold answer)\n",
    "3. facts_present (list of facts present in generated answer)\n",
    "4. facts_missing (list of facts missing from generated answer)\n",
    "5. justification (2-3 sentences explaining the score)\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def _call_llm_with_retry(\n",
    "    llm,\n",
    "    prompt: str,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay_ms: int = 500\n",
    ") -> GradedJudgment:\n",
    "    \"\"\"\n",
    "    Call LLM with retry logic on failure.\n",
    "    \n",
    "    Args:\n",
    "        llm: LangChain LLM with structured output\n",
    "        prompt: Evaluation prompt\n",
    "        max_retries: Maximum retry attempts\n",
    "        retry_delay_ms: Delay between retries in milliseconds\n",
    "    \n",
    "    Returns:\n",
    "        GradedJudgment object\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If all retries fail\n",
    "    \"\"\"\n",
    "    \n",
    "    last_error = None\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            if attempt < max_retries - 1:\n",
    "                # Wait before retry\n",
    "                time.sleep(retry_delay_ms / 1000.0)\n",
    "                continue\n",
    "            else:\n",
    "                # All retries exhausted\n",
    "                raise Exception(f\"LLM call failed after {max_retries} attempts. Last error: {str(e)}\")\n",
    "    \n",
    "    # Should not reach here, but just in case\n",
    "    raise Exception(f\"LLM call failed: {str(last_error)}\")\n",
    "\n",
    "\n",
    "def batch_llm_as_judge_graded(\n",
    "    questions: List[str],\n",
    "    gold_answers: List[str],\n",
    "    generated_answers: List[str],\n",
    "    provider: str = \"openai\",\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    temperature: float = 0.0,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay_ms: int = 500\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate multiple answers using graded LLM-as-Judge.\n",
    "    \n",
    "    Args:\n",
    "        questions: List of questions\n",
    "        gold_answers: List of gold answers\n",
    "        generated_answers: List of generated answers\n",
    "        provider: LLM provider\n",
    "        model: Model name\n",
    "        temperature: Generation temperature\n",
    "        max_retries: Retry attempts per call\n",
    "        retry_delay_ms: Delay between retries\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results and statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    if not (len(questions) == len(gold_answers) == len(generated_answers)):\n",
    "        raise ValueError(\n",
    "            f\"Length mismatch: {len(questions)} questions, \"\n",
    "            f\"{len(gold_answers)} gold answers, {len(generated_answers)} generated answers\"\n",
    "        )\n",
    "    \n",
    "    results = []\n",
    "    scores = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    for i, (q, gold, gen) in enumerate(zip(questions, gold_answers, generated_answers)):\n",
    "        print(f\"Evaluating {i+1}/{len(questions)}...\", end=\"\\r\")\n",
    "        \n",
    "        result = llm_as_judge_graded(\n",
    "            question=q,\n",
    "            gold_answer=gold,\n",
    "            generated_answer=gen,\n",
    "            provider=provider,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            max_retries=max_retries,\n",
    "            retry_delay_ms=retry_delay_ms,\n",
    "            return_details=False\n",
    "        )\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        if result.get('success', False):\n",
    "            scores.append(result['score'])\n",
    "        else:\n",
    "            failed_count += 1\n",
    "    \n",
    "    print()  # Clear progress line\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total = len(questions)\n",
    "    success_count = total - failed_count\n",
    "    \n",
    "    if scores:\n",
    "        mean_score = sum(scores) / len(scores)\n",
    "        median_score = sorted(scores)[len(scores) // 2]\n",
    "        \n",
    "        # Score distribution\n",
    "        score_distribution = {i: scores.count(i) for i in range(5)}\n",
    "    else:\n",
    "        mean_score = 0.0\n",
    "        median_score = 0\n",
    "        score_distribution = {i: 0 for i in range(5)}\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'total': total,\n",
    "        'success_count': success_count,\n",
    "        'failed_count': failed_count,\n",
    "        'mean_score': mean_score,\n",
    "        'median_score': median_score,\n",
    "        'score_distribution': score_distribution,\n",
    "        'scores': scores\n",
    "    }\n",
    "\n",
    "\n",
    "def _test_llm_as_judge_graded():\n",
    "    \"\"\"Quick test for graded LLM-as-Judge\"\"\"\n",
    "    \n",
    "    print(\"Testing graded LLM-as-Judge...\")\n",
    "    print(\"NOTE: This requires OpenAI API key to be set\")\n",
    "    print()\n",
    "    \n",
    "    # Test case 1: Good match\n",
    "    print(\"Test 1: Good semantic match\")\n",
    "    result = llm_as_judge_graded(\n",
    "        question=\"Which segment dragged down 3M's overall growth in 2022?\",\n",
    "        gold_answer=\"The consumer segment shrunk by 0.9% organically.\",\n",
    "        generated_answer=\"The Consumer segment has dragged down 3M's overall growth in 2022.\",\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Score: {result['score']}/4\")\n",
    "    print(f\"Facts present: {result['facts_present']}\")\n",
    "    print(f\"Facts missing: {result['facts_missing']}\")\n",
    "    print(f\"Justification: {result['justification']}\")\n",
    "    print()\n",
    "    \n",
    "    # Test case 2: Wrong number\n",
    "    print(\"Test 2: Wrong numeric answer\")\n",
    "    result = llm_as_judge_graded(\n",
    "        question=\"How many times has AES converted inventory in FY2022?\",\n",
    "        gold_answer=\"AES has converted inventory 9.5 times in FY 2022.\",\n",
    "        generated_answer=\"AES Corporation sold its inventory roughly 12 times in FY2022.\",\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Score: {result['score']}/4\")\n",
    "    print(f\"Justification: {result['justification']}\")\n",
    "    print()\n",
    "    \n",
    "    # Test case 3: Refusal\n",
    "    print(\"Test 3: Refusal detection\")\n",
    "    result = llm_as_judge_graded(\n",
    "        question=\"What is the inventory turnover ratio?\",\n",
    "        gold_answer=\"The ratio is 9.5 times.\",\n",
    "        generated_answer=\"I cannot calculate this without specific data.\",\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Score: {result['score']}/4\")\n",
    "    print(f\"Justification: {result['justification']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Graded LLM-as-Judge Module\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    print(\"To test, run: _test_llm_as_judge_graded()\")\n",
    "    print(\"Make sure OPENAI_API_KEY is set in environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a836c0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE TEST SUITE FOR GRADED LLM-AS-JUDGE\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 1: Novel-Generated Examples\n",
      "======================================================================\n",
      "\n",
      "Test: Partial answer - segment correct but missing details\n",
      "Expected score range: 1-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lj/175ptt0d6knb0gg0lg2h4n2h0000gp/T/ipykernel_57483/3816411334.py:153: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  result['raw_response'] = judgment.dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Score: 1/4 (within expected range)\n",
      "  Facts present: 1/4\n",
      "\n",
      "Test: Good answer but missing specific percentage\n",
      "Expected score range: 2-3\n",
      "‚úì Score: 2/4 (within expected range)\n",
      "  Facts present: 2/4\n",
      "\n",
      "Test: Wrong instrument type\n",
      "Expected score range: 0-1\n",
      "‚úì Score: 0/4 (within expected range)\n",
      "  Facts present: 4/3\n",
      "\n",
      "Test: Perfect match with all facts\n",
      "Expected score range: 4-4\n",
      "‚úì Score: 4/4 (within expected range)\n",
      "  Facts present: 4/4\n",
      "\n",
      "Novel-Generated Examples: 4/4 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 2: Domain-Relevant Examples\n",
      "======================================================================\n",
      "\n",
      "Test: Good answer - yes + context, missing specific ratio\n",
      "Expected score range: 2-4\n",
      "‚úì Score: 3/4 (within expected range)\n",
      "  Justification: The generated answer correctly affirms that AMD has a healthy liquidity profile and mentions the qui...\n",
      "\n",
      "Test: Perfect - has all key facts\n",
      "Expected score range: 4-4\n",
      "‚úì Score: 4/4 (within expected range)\n",
      "  Justification: The generated answer captures all essential information: it confirms a healthy liquidity profile, pr...\n",
      "\n",
      "Test: Wrong number - 12 vs 9.5 is significant error\n",
      "Expected score range: 0-2\n",
      "‚úì Score: 1/4 (within expected range)\n",
      "  Justification: The generated answer identifies the company and the fiscal year correctly but provides an incorrect ...\n",
      "\n",
      "Test: Refusal - should score 0\n",
      "Expected score range: 0-0\n",
      "‚úì Score: 0/4 (within expected range)\n",
      "  Justification: The generated answer does not provide any relevant information regarding AES Corporation's inventory...\n",
      "\n",
      "Domain-Relevant Examples: 4/4 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 3: Edge Cases\n",
      "======================================================================\n",
      "\n",
      "Test: Exact match - simple\n",
      "Expected score range: 4-4\n",
      "‚úì Score: 4/4\n",
      "\n",
      "Test: Refusal\n",
      "Expected score range: 0-0\n",
      "‚úì Score: 0/4\n",
      "\n",
      "Test: Data unavailable refusal\n",
      "Expected score range: 0-0\n",
      "‚úì Score: 0/4\n",
      "\n",
      "Test: Semantic equivalence - different words, same meaning\n",
      "Expected score range: 3-4\n",
      "‚úì Score: 4/4\n",
      "\n",
      "Edge Cases: 4/4 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 4: Retry Logic\n",
      "======================================================================\n",
      "Testing with invalid model to trigger retry...\n",
      "‚úì Retry logic works - returns error result after retries\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 5: Batch Processing\n",
      "======================================================================\n",
      "Processing 3 questions...\n",
      "Evaluating 3/3...\n",
      "\n",
      "Results:\n",
      "  Total: 3\n",
      "  Success: 3\n",
      "  Failed: 0\n",
      "  Mean score: 2.67\n",
      "  Score distribution: {0: 1, 1: 0, 2: 0, 3: 0, 4: 2}\n",
      "‚úì Batch processing working correctly\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY\n",
      "======================================================================\n",
      "Total Tests: 14\n",
      "‚úì Passed: 14\n",
      "‚úó Failed: 0\n",
      "Success Rate: 100.0%\n",
      "======================================================================\n",
      "\n",
      "üéâ ALL TESTS PASSED! üéâ\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comprehensive Test Suite for Graded LLM-as-Judge\n",
    "=================================================\n",
    "\n",
    "Tests cover:\n",
    "- Domain-relevant examples\n",
    "- Novel-generated examples\n",
    "- Edge cases (refusals, perfect matches, wrong answers)\n",
    "- Batch processing\n",
    "\n",
    "NOTE: These tests require OPENAI_API_KEY to be set in environment\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def check_api_key():\n",
    "    \"\"\"Check if OpenAI API key is available\"\"\"\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"‚ö†Ô∏è  WARNING: OPENAI_API_KEY not found in environment\")\n",
    "        print(\"   These tests will fail without API access\")\n",
    "        print(\"   Set with: export OPENAI_API_KEY='your-key-here'\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_novel_generated_examples():\n",
    "    \"\"\"Test with novel-generated question examples\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 1: Novel-Generated Examples\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"Skipping tests - no API key\")\n",
    "        return 0, 0\n",
    "    \n",
    "    tests = [\n",
    "        # (question, gold, generated, expected_score_min, expected_score_max, description)\n",
    "        (\n",
    "            \"Which segment dragged down 3M's overall growth in 2022 excluding M&A?\",\n",
    "            \"The consumer segment shrunk by 0.9% organically.\",\n",
    "            \"The Consumer segment.\",\n",
    "            1, 3,\n",
    "            \"Partial answer - segment correct but missing details\"\n",
    "        ),\n",
    "        (\n",
    "            \"Which segment dragged down 3M's overall growth in 2022 excluding M&A?\",\n",
    "            \"The consumer segment shrunk by 0.9% organically.\",\n",
    "            \"The Consumer segment has dragged down 3M's overall growth in 2022.\",\n",
    "            2, 3,\n",
    "            \"Good answer but missing specific percentage\"\n",
    "        ),\n",
    "        (\n",
    "            \"Which derivative had the highest notional value for Verizon in FY 2021?\",\n",
    "            \"Cross currency swaps. Its notional value was $32,502 million.\",\n",
    "            \"Interest rate swaps had the highest notional value among Verizon's derivative instruments in FY 2021.\",\n",
    "            0, 1,\n",
    "            \"Wrong instrument type\"\n",
    "        ),\n",
    "        (\n",
    "            \"Which derivative had the highest notional value for Verizon in FY 2021?\",\n",
    "            \"Cross currency swaps. Its notional value was $32,502 million.\",\n",
    "            \"Cross currency swaps had the highest notional value in FY 2021, at $32,502 million.\",\n",
    "            4, 4,\n",
    "            \"Perfect match with all facts\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    \n",
    "    for question, gold, gen, min_score, max_score, desc in tests:\n",
    "        print(f\"\\nTest: {desc}\")\n",
    "        print(f\"Expected score range: {min_score}-{max_score}\")\n",
    "        \n",
    "        try:\n",
    "            result = llm_as_judge_graded(\n",
    "                question=question,\n",
    "                gold_answer=gold,\n",
    "                generated_answer=gen,\n",
    "                model=\"gpt-4o-mini\"\n",
    "            )\n",
    "            \n",
    "            score = result['score']\n",
    "            \n",
    "            if min_score <= score <= max_score:\n",
    "                passed += 1\n",
    "                print(f\"‚úì Score: {score}/4 (within expected range)\")\n",
    "                print(f\"  Facts present: {len(result['facts_present'])}/{len(result['key_facts_gold'])}\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"‚úó Score: {score}/4 (expected {min_score}-{max_score})\")\n",
    "                print(f\"  Justification: {result['justification']}\")\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"‚úó Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nNovel-Generated Examples: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_domain_relevant_examples():\n",
    "    \"\"\"Test with domain-relevant question examples\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 2: Domain-Relevant Examples\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"Skipping tests - no API key\")\n",
    "        return 0, 0\n",
    "    \n",
    "    tests = [\n",
    "        (\n",
    "            \"Does AMD have a reasonably healthy liquidity profile based on its quick ratio for FY22?\",\n",
    "            \"Yes. The quick ratio is 1.57, calculated as (cash and cash equivalents+Short term investments+Accounts receivable, net+receivables from related parties)/ (current liabilities).\",\n",
    "            \"Yes, AMD has a reasonably healthy liquidity profile based on its quick ratio for FY22.\",\n",
    "            2, 4,\n",
    "            \"Good answer - yes + context, missing specific ratio\"\n",
    "        ),\n",
    "        (\n",
    "            \"Does AMD have a reasonably healthy liquidity profile based on its quick ratio for FY22?\",\n",
    "            \"Yes. The quick ratio is 1.57, calculated as (cash and cash equivalents+Short term investments+Accounts receivable, net+receivables from related parties)/ (current liabilities).\",\n",
    "            \"Yes, AMD has a reasonably healthy liquidity profile based on its quick ratio of approximately 1.57 for FY22.\",\n",
    "            4, 4,\n",
    "            \"Perfect - has all key facts\"\n",
    "        ),\n",
    "        (\n",
    "            \"Roughly how many times has AES Corporation sold its inventory in FY2022?\",\n",
    "            \"AES has converted inventory 9.5 times in FY 2022.\",\n",
    "            \"AES Corporation sold its inventory roughly 12 times in FY2022.\",\n",
    "            0, 2,\n",
    "            \"Wrong number - 12 vs 9.5 is significant error\"\n",
    "        ),\n",
    "        (\n",
    "            \"Roughly how many times has AES Corporation sold its inventory in FY2022?\",\n",
    "            \"AES has converted inventory 9.5 times in FY 2022.\",\n",
    "            \"The inventory turnover ratio for AES Corporation in FY2022 cannot be calculated without specific COGS and average inventory figures.\",\n",
    "            0, 0,\n",
    "            \"Refusal - should score 0\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    \n",
    "    for question, gold, gen, min_score, max_score, desc in tests:\n",
    "        print(f\"\\nTest: {desc}\")\n",
    "        print(f\"Expected score range: {min_score}-{max_score}\")\n",
    "        \n",
    "        try:\n",
    "            result = llm_as_judge_graded(\n",
    "                question=question,\n",
    "                gold_answer=gold,\n",
    "                generated_answer=gen,\n",
    "                model=\"gpt-4o-mini\"\n",
    "            )\n",
    "            \n",
    "            score = result['score']\n",
    "            \n",
    "            if min_score <= score <= max_score:\n",
    "                passed += 1\n",
    "                print(f\"‚úì Score: {score}/4 (within expected range)\")\n",
    "                print(f\"  Justification: {result['justification'][:100]}...\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"‚úó Score: {score}/4 (expected {min_score}-{max_score})\")\n",
    "                print(f\"  Justification: {result['justification']}\")\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"‚úó Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nDomain-Relevant Examples: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_edge_cases():\n",
    "    \"\"\"Test edge cases\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 3: Edge Cases\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"Skipping tests - no API key\")\n",
    "        return 0, 0\n",
    "    \n",
    "    tests = [\n",
    "        (\n",
    "            \"What is the value?\",\n",
    "            \"42\",\n",
    "            \"42\",\n",
    "            4, 4,\n",
    "            \"Exact match - simple\"\n",
    "        ),\n",
    "        (\n",
    "            \"What is the value?\",\n",
    "            \"The answer is 42.\",\n",
    "            \"I don't know\",\n",
    "            0, 0,\n",
    "            \"Refusal\"\n",
    "        ),\n",
    "        (\n",
    "            \"What is the value?\",\n",
    "            \"The answer is 42.\",\n",
    "            \"Data not available.\",\n",
    "            0, 0,\n",
    "            \"Data unavailable refusal\"\n",
    "        ),\n",
    "        (\n",
    "            \"Describe the trend.\",\n",
    "            \"Revenue increased by 25% from $100M to $125M.\",\n",
    "            \"Revenue went up significantly, approximately 25%, from around $100M to $125M.\",\n",
    "            3, 4,\n",
    "            \"Semantic equivalence - different words, same meaning\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    \n",
    "    for question, gold, gen, min_score, max_score, desc in tests:\n",
    "        print(f\"\\nTest: {desc}\")\n",
    "        print(f\"Expected score range: {min_score}-{max_score}\")\n",
    "        \n",
    "        try:\n",
    "            result = llm_as_judge_graded(\n",
    "                question=question,\n",
    "                gold_answer=gold,\n",
    "                generated_answer=gen,\n",
    "                model=\"gpt-4o-mini\"\n",
    "            )\n",
    "            \n",
    "            score = result['score']\n",
    "            \n",
    "            if min_score <= score <= max_score:\n",
    "                passed += 1\n",
    "                print(f\"‚úì Score: {score}/4\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"‚úó Score: {score}/4 (expected {min_score}-{max_score})\")\n",
    "                print(f\"  Justification: {result['justification']}\")\n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"‚úó Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nEdge Cases: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_retry_logic():\n",
    "    \"\"\"Test retry logic with intentional failure\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 4: Retry Logic\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"Testing with invalid model to trigger retry...\")\n",
    "    \n",
    "    try:\n",
    "        result = llm_as_judge_graded(\n",
    "            question=\"Test question\",\n",
    "            gold_answer=\"Test gold\",\n",
    "            generated_answer=\"Test generated\",\n",
    "            model=\"invalid-model-name\",\n",
    "            max_retries=2,\n",
    "            retry_delay_ms=100\n",
    "        )\n",
    "        \n",
    "        # Should return error result\n",
    "        if not result.get('success', True):\n",
    "            print(\"‚úì Retry logic works - returns error result after retries\")\n",
    "            return 1, 0\n",
    "        else:\n",
    "            print(\"‚úó Should have failed but didn't\")\n",
    "            return 0, 1\n",
    "    except Exception as e:\n",
    "        print(f\"‚úì Retry logic works - raised exception after retries: {type(e).__name__}\")\n",
    "        return 1, 0\n",
    "\n",
    "\n",
    "def test_batch_processing():\n",
    "    \"\"\"Test batch evaluation\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 5: Batch Processing\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"Skipping tests - no API key\")\n",
    "        return 0, 0\n",
    "    \n",
    "    questions = [\n",
    "        \"What is the value?\",\n",
    "        \"What is the result?\",\n",
    "        \"What is the answer?\",\n",
    "    ]\n",
    "    \n",
    "    gold_answers = [\n",
    "        \"The value is 42.\",\n",
    "        \"The result is positive.\",\n",
    "        \"The answer is yes.\",\n",
    "    ]\n",
    "    \n",
    "    generated_answers = [\n",
    "        \"42\",\n",
    "        \"Positive result.\",\n",
    "        \"I don't know\",\n",
    "    ]\n",
    "    \n",
    "    print(f\"Processing {len(questions)} questions...\")\n",
    "    \n",
    "    try:\n",
    "        result = batch_llm_as_judge_graded(\n",
    "            questions=questions,\n",
    "            gold_answers=gold_answers,\n",
    "            generated_answers=generated_answers,\n",
    "            model=\"gpt-4o-mini\",\n",
    "            retry_delay_ms=100\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Total: {result['total']}\")\n",
    "        print(f\"  Success: {result['success_count']}\")\n",
    "        print(f\"  Failed: {result['failed_count']}\")\n",
    "        print(f\"  Mean score: {result['mean_score']:.2f}\")\n",
    "        print(f\"  Score distribution: {result['score_distribution']}\")\n",
    "        \n",
    "        if result['success_count'] == len(questions):\n",
    "            print(\"‚úì Batch processing working correctly\")\n",
    "            return 1, 0\n",
    "        else:\n",
    "            print(f\"‚úó Some evaluations failed\")\n",
    "            return 0, 1\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Batch processing failed: {e}\")\n",
    "        return 0, 1\n",
    "\n",
    "\n",
    "def run_all_tests():\n",
    "    \"\"\"Run all test suites\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPREHENSIVE TEST SUITE FOR GRADED LLM-AS-JUDGE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"\\n‚ùå Cannot run tests without OPENAI_API_KEY\")\n",
    "        print(\"Please set your API key and try again\")\n",
    "        return False\n",
    "    \n",
    "    total_passed = 0\n",
    "    total_failed = 0\n",
    "    \n",
    "    test_suites = [\n",
    "        test_novel_generated_examples,\n",
    "        test_domain_relevant_examples,\n",
    "        test_edge_cases,\n",
    "        test_retry_logic,\n",
    "        test_batch_processing,\n",
    "    ]\n",
    "    \n",
    "    for test_func in test_suites:\n",
    "        try:\n",
    "            passed, failed = test_func()\n",
    "            total_passed += passed\n",
    "            total_failed += failed\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Test suite crashed: {e}\")\n",
    "            total_failed += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total Tests: {total_passed + total_failed}\")\n",
    "    print(f\"‚úì Passed: {total_passed}\")\n",
    "    print(f\"‚úó Failed: {total_failed}\")\n",
    "    if total_passed + total_failed > 0:\n",
    "        print(f\"Success Rate: {100 * total_passed / (total_passed + total_failed):.1f}%\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return total_failed == 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Import the module\n",
    "    import sys\n",
    "    sys.path.append('/home/claude')\n",
    "    \n",
    "    success = run_all_tests()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nüéâ ALL TESTS PASSED! üéâ\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  SOME TESTS FAILED - Review output above\")\n",
    "        print(\"Note: LLM scores can vary, some variance is expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bde0f441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 3, 'key_facts_gold': ['AMD', 'higher sales', 'EPYC server processors', 'higher semi-custom product sales', 'inclusion of Xilinx embedded product sales'], 'facts_present': ['AMD', 'revenue change', '64% increase in Data Center segment revenue', '21% increase in Gaming segment revenue', 'significant growth in Embedded segment revenue from Xilinx product sales'], 'facts_missing': ['higher sales of EPYC server processors', 'higher semi-custom product sales'], 'justification': 'The generated answer provides specific percentage increases for the Data Center and Gaming segments, as well as mentioning Xilinx product sales, which aligns with the gold answer. However, it omits the mention of higher sales of EPYC server processors and higher semi-custom product sales, which are key components of the revenue change.', 'success': True, 'raw_response': {'score': 3, 'key_facts_gold': ['AMD', 'higher sales', 'EPYC server processors', 'higher semi-custom product sales', 'inclusion of Xilinx embedded product sales'], 'facts_present': ['AMD', 'revenue change', '64% increase in Data Center segment revenue', '21% increase in Gaming segment revenue', 'significant growth in Embedded segment revenue from Xilinx product sales'], 'facts_missing': ['higher sales of EPYC server processors', 'higher semi-custom product sales'], 'justification': 'The generated answer provides specific percentage increases for the Data Center and Gaming segments, as well as mentioning Xilinx product sales, which aligns with the gold answer. However, it omits the mention of higher sales of EPYC server processors and higher semi-custom product sales, which are key components of the revenue change.'}, 'metadata': {'provider': 'openai', 'model': 'gpt-4o-mini', 'temperature': 0.0, 'question': 'What drove revenue change as of the FY22 for AMD?', 'gold_answer': 'In 2022, AMD reported Higher sales of their EPYC server processors, higher semi-custom product sales, and the inclusion of Xilinx embedded product sales', 'generated_answer': 'Revenue change for AMD in FY22 was driven by a 64% increase in Data Center segment revenue, a 21% increase in Gaming segment revenue, and significant growth in Embedded segment revenue from Xilinx product sales.'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lj/175ptt0d6knb0gg0lg2h4n2h0000gp/T/ipykernel_57483/3816411334.py:153: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  result['raw_response'] = judgment.dict()\n"
     ]
    }
   ],
   "source": [
    "question = \"What drove revenue change as of the FY22 for AMD?\"\n",
    "gold_answer = \"In 2022, AMD reported Higher sales of their EPYC server processors, higher semi-custom product sales, and the inclusion of Xilinx embedded product sales\"\n",
    "generated_answer = \"Revenue change for AMD in FY22 was driven by a 64% increase in Data Center segment revenue, a 21% increase in Gaming segment revenue, and significant growth in Embedded segment revenue from Xilinx product sales.\"\n",
    "\n",
    "result = llm_as_judge_graded(\n",
    "    question=question,\n",
    "    gold_answer=gold_answer,\n",
    "    generated_answer=generated_answer,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_retries=3,\n",
    "    retry_delay_ms=1000\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
