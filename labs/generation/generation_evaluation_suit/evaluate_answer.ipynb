{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c0d7f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e75813d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate Evaluator Module\n",
      "======================================================================\n",
      "\n",
      "To test, run: _test_aggregate_evaluator()\n",
      "Make sure all dependencies are installed and OPENAI_API_KEY is set\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Aggregate Evaluator - Master Router for Financial QA Evaluation\n",
    "================================================================\n",
    "This module provides the master evaluation function that routes to appropriate\n",
    "metrics based on question type and returns comprehensive evaluation results.\n",
    "\n",
    "Routes to:\n",
    "- metrics-generated: numerical_exact_match + llm_as_judge_binary\n",
    "- novel-generated: token_f1 + llm_as_judge_graded  \n",
    "- domain-relevant: llm_as_judge_graded only\n",
    "\n",
    "Author: Financial QA Evaluation System\n",
    "Version: 1.0\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Any, Optional, List\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Import all the metric functions we built in Phase 1 and Phase 2\n",
    "# Note: Adjust these imports based on your actual file structure\n",
    "try:\n",
    "    from numerical_exact_match import numerical_exact_match\n",
    "    from token_f1 import token_f1\n",
    "    from detect_refusal import detect_refusal\n",
    "    from llm_as_judge_binary import llm_as_judge_binary\n",
    "    from llm_as_judge_graded import llm_as_judge_graded\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: Could not import some modules: {e}\")\n",
    "    print(\"Make sure all Phase 1 and Phase 2 modules are in the Python path\")\n",
    "\n",
    "\n",
    "# Default configuration\n",
    "DEFAULT_CONFIG = {\n",
    "    # Numerical matching\n",
    "    'tolerance': 0.01,  # 1% for numerical_exact_match\n",
    "    \n",
    "    # Token F1\n",
    "    'normalize': True,\n",
    "    'remove_stopwords': False,\n",
    "    \n",
    "    # LLM settings\n",
    "    'llm_provider': 'openai',\n",
    "    'llm_model': 'gpt-4o-mini',\n",
    "    'llm_temperature': 0.0,\n",
    "    'llm_max_retries': 3,\n",
    "    'llm_retry_delay_ms': 500,\n",
    "    \n",
    "    # Refusal detection\n",
    "    'pre_check_refusal': True,\n",
    "    \n",
    "    # Return details\n",
    "    'return_details': True\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_answer(\n",
    "    question: str,\n",
    "    question_type: str,\n",
    "    gold_answer: str,\n",
    "    generated_answer: str,\n",
    "    config: Optional[Dict[str, Any]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Master evaluation function that routes to appropriate metrics based on question type.\n",
    "    \n",
    "    This is the main entry point for evaluating financial QA system answers.\n",
    "    It intelligently routes to the right metrics and returns comprehensive results.\n",
    "    \n",
    "    Args:\n",
    "        question: The question being answered\n",
    "        question_type: One of 'metrics-generated', 'novel-generated', 'domain-relevant'\n",
    "        gold_answer: The gold standard answer (ground truth)\n",
    "        generated_answer: The generated answer to evaluate\n",
    "        config: Optional configuration dictionary (uses DEFAULT_CONFIG if None)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - question_type: The type of question\n",
    "            - question: The question text\n",
    "            - gold_answer: The gold answer\n",
    "            - generated_answer: The generated answer\n",
    "            - refusal_check: Results from refusal detection\n",
    "            - metrics: Dictionary of results from each applicable metric\n",
    "            - summary: High-level summary of evaluation\n",
    "    \n",
    "    Question Type Routing:\n",
    "        - metrics-generated: \n",
    "            * numerical_exact_match (rule-based numerical validation)\n",
    "            * llm_as_judge_binary (LLM-based numerical validation)\n",
    "        \n",
    "        - novel-generated:\n",
    "            * token_f1 (token-level overlap scoring)\n",
    "            * llm_as_judge_graded (LLM-based semantic evaluation)\n",
    "        \n",
    "        - domain-relevant:\n",
    "            * llm_as_judge_graded (LLM-based semantic evaluation)\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If question_type is not recognized\n",
    "        Exception: If any metric evaluation fails\n",
    "    \n",
    "    Examples:\n",
    "        >>> # Metrics-generated question\n",
    "        >>> result = evaluate_answer(\n",
    "        ...     question=\"What is the FY2018 capex for 3M?\",\n",
    "        ...     question_type=\"metrics-generated\",\n",
    "        ...     gold_answer=\"$1577.00\",\n",
    "        ...     generated_answer=\"1577 million dollars\"\n",
    "        ... )\n",
    "        >>> print(result['summary']['refusal_detected'])  # False\n",
    "        >>> print(result['metrics']['numerical_exact_match']['match'])  # True\n",
    "        >>> print(result['metrics']['llm_as_judge_binary']['match'])  # True\n",
    "        \n",
    "        >>> # Novel-generated question\n",
    "        >>> result = evaluate_answer(\n",
    "        ...     question=\"Which segment dragged down growth?\",\n",
    "        ...     question_type=\"novel-generated\",\n",
    "        ...     gold_answer=\"The consumer segment shrunk by 0.9% organically.\",\n",
    "        ...     generated_answer=\"The Consumer segment.\"\n",
    "        ... )\n",
    "        >>> print(result['metrics']['token_f1']['f1'])  # Some F1 score\n",
    "        >>> print(result['metrics']['llm_as_judge_graded']['score'])  # 0-4 score\n",
    "        \n",
    "        >>> # Domain-relevant question\n",
    "        >>> result = evaluate_answer(\n",
    "        ...     question=\"Does AMD have healthy liquidity?\",\n",
    "        ...     question_type=\"domain-relevant\",\n",
    "        ...     gold_answer=\"Yes. The quick ratio is 1.57...\",\n",
    "        ...     generated_answer=\"Yes, AMD has healthy liquidity...\"\n",
    "        ... )\n",
    "        >>> print(result['metrics']['llm_as_judge_graded']['score'])  # 0-4 score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Merge config with defaults\n",
    "    cfg = DEFAULT_CONFIG.copy()\n",
    "    if config:\n",
    "        cfg.update(config)\n",
    "    \n",
    "    # Validate question_type\n",
    "    valid_types = ['metrics-generated', 'novel-generated', 'domain-relevant']\n",
    "    if question_type not in valid_types:\n",
    "        raise ValueError(\n",
    "            f\"Invalid question_type: '{question_type}'. \"\n",
    "            f\"Must be one of: {valid_types}\"\n",
    "        )\n",
    "    \n",
    "    # Initialize result structure\n",
    "    result = {\n",
    "        'question_type': question_type,\n",
    "        'question': question,\n",
    "        'gold_answer': gold_answer,\n",
    "        'generated_answer': generated_answer,\n",
    "        'refusal_check': None,\n",
    "        'metrics': {},\n",
    "        'summary': {\n",
    "            'question_type': question_type,\n",
    "            'refusal_detected': False,\n",
    "            'metrics_computed': [],\n",
    "            'evaluation_complete': False,\n",
    "            'errors': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Step 1: Pre-check for refusal (if enabled)\n",
    "    if cfg['pre_check_refusal']:\n",
    "        try:\n",
    "            refusal_result = detect_refusal(generated_answer)\n",
    "            result['refusal_check'] = refusal_result\n",
    "            result['summary']['refusal_detected'] = refusal_result['is_refusal']\n",
    "            \n",
    "            # If it's a clear refusal, we can skip some metrics\n",
    "            # But we still run them for analysis purposes\n",
    "        except Exception as e:\n",
    "            result['summary']['errors'].append(f\"Refusal detection failed: {str(e)}\")\n",
    "            # Continue with evaluation even if refusal check fails\n",
    "    \n",
    "    # Step 2: Route to appropriate metrics based on question type\n",
    "    try:\n",
    "        if question_type == 'metrics-generated':\n",
    "            result['metrics'] = _evaluate_metrics_generated(\n",
    "                question, gold_answer, generated_answer, cfg\n",
    "            )\n",
    "            result['summary']['metrics_computed'] = ['numerical_exact_match', 'llm_as_judge_binary']\n",
    "        \n",
    "        elif question_type == 'novel-generated':\n",
    "            result['metrics'] = _evaluate_novel_generated(\n",
    "                question, gold_answer, generated_answer, cfg\n",
    "            )\n",
    "            result['summary']['metrics_computed'] = ['token_f1', 'llm_as_judge_graded']\n",
    "        \n",
    "        elif question_type == 'domain-relevant':\n",
    "            result['metrics'] = _evaluate_domain_relevant(\n",
    "                question, gold_answer, generated_answer, cfg\n",
    "            )\n",
    "            result['summary']['metrics_computed'] = ['llm_as_judge_graded']\n",
    "        \n",
    "        result['summary']['evaluation_complete'] = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        # If any metric fails, raise exception (as per your requirement - Option B)\n",
    "        result['summary']['evaluation_complete'] = False\n",
    "        result['summary']['errors'].append(f\"Evaluation failed: {str(e)}\")\n",
    "        raise Exception(f\"Evaluation failed for {question_type}: {str(e)}\") from e\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def _evaluate_metrics_generated(\n",
    "    question: str,\n",
    "    gold_answer: str,\n",
    "    generated_answer: str,\n",
    "    config: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate metrics-generated questions using numerical validation methods.\n",
    "    \n",
    "    Uses TWO metrics:\n",
    "    1. numerical_exact_match (rule-based)\n",
    "    2. llm_as_judge_binary (LLM-based)\n",
    "    \n",
    "    Args:\n",
    "        question: Question text\n",
    "        gold_answer: Gold answer\n",
    "        generated_answer: Generated answer\n",
    "        config: Configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results from both metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Metric 1: Numerical Exact Match (rule-based)\n",
    "    try:\n",
    "        nem_result = numerical_exact_match(\n",
    "            gold_answer=gold_answer,\n",
    "            generated_answer=generated_answer,\n",
    "            tolerance=config['tolerance']\n",
    "        )\n",
    "        metrics['numerical_exact_match'] = nem_result\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"numerical_exact_match failed: {str(e)}\") from e\n",
    "    \n",
    "    # Metric 2: LLM-as-Judge Binary (LLM-based)\n",
    "    try:\n",
    "        llm_result = llm_as_judge_binary(\n",
    "            question=question,\n",
    "            gold_answer=gold_answer,\n",
    "            generated_answer=generated_answer,\n",
    "            tolerance=config['tolerance'],\n",
    "            provider=config['llm_provider'],\n",
    "            model=config['llm_model'],\n",
    "            temperature=config['llm_temperature'],\n",
    "            max_retries=config['llm_max_retries'],\n",
    "            retry_delay_ms=config['llm_retry_delay_ms'],\n",
    "            return_details=config['return_details']\n",
    "        )\n",
    "        metrics['llm_as_judge_binary'] = llm_result\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"llm_as_judge_binary failed: {str(e)}\") from e\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def _evaluate_novel_generated(\n",
    "    question: str,\n",
    "    gold_answer: str,\n",
    "    generated_answer: str,\n",
    "    config: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate novel-generated questions using token and semantic methods.\n",
    "    \n",
    "    Uses TWO metrics:\n",
    "    1. token_f1 (token-level overlap)\n",
    "    2. llm_as_judge_graded (semantic evaluation)\n",
    "    \n",
    "    Args:\n",
    "        question: Question text\n",
    "        gold_answer: Gold answer\n",
    "        generated_answer: Generated answer\n",
    "        config: Configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results from both metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Metric 1: Token F1 (token-level overlap)\n",
    "    try:\n",
    "        f1_result = token_f1(\n",
    "            gold_answer=gold_answer,\n",
    "            generated_answer=generated_answer,\n",
    "            normalize=config['normalize'],\n",
    "            remove_stopwords=config['remove_stopwords']\n",
    "        )\n",
    "        metrics['token_f1'] = f1_result\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"token_f1 failed: {str(e)}\") from e\n",
    "    \n",
    "    # Metric 2: LLM-as-Judge Graded (semantic evaluation)\n",
    "    try:\n",
    "        llm_result = llm_as_judge_graded(\n",
    "            question=question,\n",
    "            gold_answer=gold_answer,\n",
    "            generated_answer=generated_answer,\n",
    "            provider=config['llm_provider'],\n",
    "            model=config['llm_model'],\n",
    "            temperature=config['llm_temperature'],\n",
    "            max_retries=config['llm_max_retries'],\n",
    "            retry_delay_ms=config['llm_retry_delay_ms'],\n",
    "            return_details=config['return_details']\n",
    "        )\n",
    "        metrics['llm_as_judge_graded'] = llm_result\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"llm_as_judge_graded failed: {str(e)}\") from e\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def _evaluate_domain_relevant(\n",
    "    question: str,\n",
    "    gold_answer: str,\n",
    "    generated_answer: str,\n",
    "    config: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate domain-relevant questions using semantic evaluation.\n",
    "    \n",
    "    Uses ONE metric:\n",
    "    1. llm_as_judge_graded (semantic evaluation)\n",
    "    \n",
    "    Note: No length-based routing - all domain-relevant questions\n",
    "    use LLM judge regardless of answer length.\n",
    "    \n",
    "    Args:\n",
    "        question: Question text\n",
    "        gold_answer: Gold answer\n",
    "        generated_answer: Generated answer\n",
    "        config: Configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with result from LLM judge\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Metric: LLM-as-Judge Graded (semantic evaluation)\n",
    "    try:\n",
    "        llm_result = llm_as_judge_graded(\n",
    "            question=question,\n",
    "            gold_answer=gold_answer,\n",
    "            generated_answer=generated_answer,\n",
    "            provider=config['llm_provider'],\n",
    "            model=config['llm_model'],\n",
    "            temperature=config['llm_temperature'],\n",
    "            max_retries=config['llm_max_retries'],\n",
    "            retry_delay_ms=config['llm_retry_delay_ms'],\n",
    "            return_details=config['return_details']\n",
    "        )\n",
    "        metrics['llm_as_judge_graded'] = llm_result\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"llm_as_judge_graded failed: {str(e)}\") from e\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_default_config() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get the default configuration dictionary.\n",
    "    \n",
    "    Returns:\n",
    "        Copy of DEFAULT_CONFIG\n",
    "    \"\"\"\n",
    "    return DEFAULT_CONFIG.copy()\n",
    "\n",
    "\n",
    "def validate_config(config: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Validate a configuration dictionary.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration to validate\n",
    "    \n",
    "    Returns:\n",
    "        True if valid\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If configuration is invalid\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check tolerance\n",
    "    if 'tolerance' in config:\n",
    "        if not isinstance(config['tolerance'], (int, float)):\n",
    "            raise ValueError(\"tolerance must be a number\")\n",
    "        if config['tolerance'] <= 0:\n",
    "            raise ValueError(\"tolerance must be positive\")\n",
    "    \n",
    "    # Check LLM model\n",
    "    if 'llm_model' in config:\n",
    "        if not isinstance(config['llm_model'], str):\n",
    "            raise ValueError(\"llm_model must be a string\")\n",
    "    \n",
    "    # Check temperature\n",
    "    if 'llm_temperature' in config:\n",
    "        if not isinstance(config['llm_temperature'], (int, float)):\n",
    "            raise ValueError(\"llm_temperature must be a number\")\n",
    "        if not 0 <= config['llm_temperature'] <= 2:\n",
    "            raise ValueError(\"llm_temperature must be between 0 and 2\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def print_evaluation_summary(result: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Print a human-readable summary of evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        result: Result dictionary from evaluate_answer()\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nQuestion Type: {result['question_type']}\")\n",
    "    print(f\"Question: {result['question'][:80]}...\")\n",
    "    print(f\"Gold Answer: {result['gold_answer'][:80]}...\")\n",
    "    print(f\"Generated Answer: {result['generated_answer'][:80]}...\")\n",
    "    \n",
    "    # Refusal check\n",
    "    if result['refusal_check']:\n",
    "        print(f\"\\nRefusal Detected: {result['summary']['refusal_detected']}\")\n",
    "        if result['summary']['refusal_detected']:\n",
    "            print(f\"  Type: {result['refusal_check']['refusal_type']}\")\n",
    "    \n",
    "    # Metrics\n",
    "    print(f\"\\nMetrics Computed: {', '.join(result['summary']['metrics_computed'])}\")\n",
    "    \n",
    "    for metric_name, metric_result in result['metrics'].items():\n",
    "        print(f\"\\n{metric_name.upper()}:\")\n",
    "        \n",
    "        if metric_name == 'numerical_exact_match':\n",
    "            print(f\"  Match: {metric_result['match']}\")\n",
    "            print(f\"  Category: {metric_result['error_category']}\")\n",
    "            if metric_result.get('relative_error'):\n",
    "                print(f\"  Relative Error: {metric_result['relative_error']:.3f}%\")\n",
    "        \n",
    "        elif metric_name == 'llm_as_judge_binary':\n",
    "            print(f\"  Match: {metric_result['match']}\")\n",
    "            print(f\"  Category: {metric_result['error_category']}\")\n",
    "            if metric_result.get('relative_error'):\n",
    "                print(f\"  Relative Error: {metric_result['relative_error']:.3f}%\")\n",
    "            if metric_result.get('corrected'):\n",
    "                print(f\"  âš ï¸  Auto-corrected by post-processing\")\n",
    "        \n",
    "        elif metric_name == 'token_f1':\n",
    "            print(f\"  F1: {metric_result['f1']:.3f}\")\n",
    "            print(f\"  Precision: {metric_result['precision']:.3f}\")\n",
    "            print(f\"  Recall: {metric_result['recall']:.3f}\")\n",
    "        \n",
    "        elif metric_name == 'llm_as_judge_graded':\n",
    "            print(f\"  Score: {metric_result['score']}/4\")\n",
    "            print(f\"  Facts Present: {len(metric_result['facts_present'])}/{len(metric_result['key_facts_gold'])}\")\n",
    "            print(f\"  Justification: {metric_result['justification'][:100]}...\")\n",
    "    \n",
    "    # Completion status\n",
    "    print(f\"\\nEvaluation Complete: {result['summary']['evaluation_complete']}\")\n",
    "    if result['summary']['errors']:\n",
    "        print(f\"Errors: {result['summary']['errors']}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def _test_aggregate_evaluator():\n",
    "    \"\"\"Quick test for aggregate evaluator\"\"\"\n",
    "    \n",
    "    print(\"Testing Aggregate Evaluator...\")\n",
    "    print(\"NOTE: This requires all Phase 1 and Phase 2 modules + OpenAI API key\")\n",
    "    print()\n",
    "    \n",
    "    # Test 1: Metrics-generated\n",
    "    print(\"Test 1: Metrics-generated question\")\n",
    "    print(\"-\"*70)\n",
    "    result = evaluate_answer(\n",
    "        question=\"What is the FY2018 capital expenditure amount (in USD millions) for 3M?\",\n",
    "        question_type=\"metrics-generated\",\n",
    "        gold_answer=\"$1577.00\",\n",
    "        generated_answer=\"1577 million dollars\"\n",
    "    )\n",
    "    print_evaluation_summary(result)\n",
    "    print()\n",
    "    \n",
    "    # Test 2: Novel-generated\n",
    "    print(\"\\n\\nTest 2: Novel-generated question\")\n",
    "    print(\"-\"*70)\n",
    "    result = evaluate_answer(\n",
    "        question=\"Which segment dragged down 3M's overall growth in 2022?\",\n",
    "        question_type=\"novel-generated\",\n",
    "        gold_answer=\"The consumer segment shrunk by 0.9% organically.\",\n",
    "        generated_answer=\"The Consumer segment has dragged down 3M's overall growth.\"\n",
    "    )\n",
    "    print_evaluation_summary(result)\n",
    "    print()\n",
    "    \n",
    "    # Test 3: Domain-relevant\n",
    "    print(\"\\n\\nTest 3: Domain-relevant question\")\n",
    "    print(\"-\"*70)\n",
    "    result = evaluate_answer(\n",
    "        question=\"Does AMD have a reasonably healthy liquidity profile?\",\n",
    "        question_type=\"domain-relevant\",\n",
    "        gold_answer=\"Yes. The quick ratio is 1.57, calculated as (cash and cash equivalents+Short term investments+Accounts receivable, net+receivables from related parties)/ (current liabilities).\",\n",
    "        generated_answer=\"Yes, AMD has a reasonably healthy liquidity profile based on its quick ratio for FY22.\"\n",
    "    )\n",
    "    print_evaluation_summary(result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Aggregate Evaluator Module\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    print(\"To test, run: _test_aggregate_evaluator()\")\n",
    "    print(\"Make sure all dependencies are installed and OPENAI_API_KEY is set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74da473b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE TEST SUITE FOR AGGREGATE EVALUATOR\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 1: Metrics-Generated Questions\n",
      "======================================================================\n",
      "\n",
      "Test: Exact match with format difference\n",
      "Expected match: True\n",
      "âœ“ At least one metric matches expectation\n",
      "  NEM: True, LLM: True\n",
      "\n",
      "Test: Within tolerance\n",
      "Expected match: True\n",
      "âœ“ At least one metric matches expectation\n",
      "  NEM: True, LLM: True\n",
      "\n",
      "Test: Out of tolerance\n",
      "Expected match: False\n",
      "âœ“ At least one metric matches expectation\n",
      "  NEM: False, LLM: False\n",
      "\n",
      "Test: Refusal\n",
      "Expected match: False\n",
      "âœ“ Refusal detected correctly\n",
      "âœ“ At least one metric matches expectation\n",
      "  NEM: False, LLM: False\n",
      "\n",
      "Metrics-Generated: 4/4 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 2: Novel-Generated Questions\n",
      "======================================================================\n",
      "\n",
      "Test: Partial match - missing details\n",
      "Expected: F1 >= 0.3, Score >= 2\n",
      "âœ“ F1: 0.333 >= 0.3, Score: 2 >= 2\n",
      "\n",
      "Test: Good match with all facts\n",
      "Expected: F1 >= 0.6, Score >= 3\n",
      "âœ“ F1: 0.640 >= 0.6, Score: 4 >= 3\n",
      "\n",
      "Test: Refusal\n",
      "Expected: F1 >= 0.0, Score >= 0\n",
      "âœ“ F1: 0.000 >= 0.0, Score: 0 >= 0\n",
      "\n",
      "Novel-Generated: 3/3 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 3: Domain-Relevant Questions\n",
      "======================================================================\n",
      "\n",
      "Test: Good answer without calculation details\n",
      "Expected score: 3-4\n",
      "âœ“ Score: 3 (within 3-4)\n",
      "\n",
      "Test: Wrong number\n",
      "Expected score: 0-2\n",
      "âœ“ Score: 1 (within 0-2)\n",
      "\n",
      "Test: Refusal\n",
      "Expected score: 0-0\n",
      "âœ“ Score: 0 (within 0-0)\n",
      "\n",
      "Domain-Relevant: 3/3 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 4: Error Handling\n",
      "======================================================================\n",
      "\n",
      "Test: Invalid question type\n",
      "âœ“ Correctly raised ValueError: Invalid question_type: 'invalid-type'. Must be one of: ['metrics-generated', 'novel-generated', 'domain-relevant']\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 5: Configuration Validation\n",
      "======================================================================\n",
      "\n",
      "Test: Valid config\n",
      "âœ“ Default config is valid\n",
      "\n",
      "Test: Invalid tolerance (negative)\n",
      "âœ“ Correctly raised ValueError: tolerance must be positive\n",
      "\n",
      "Test: Invalid temperature (out of range)\n",
      "âœ“ Correctly raised ValueError: llm_temperature must be between 0 and 2\n",
      "\n",
      "Config Validation: 3/3 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 6: Return Structure Validation\n",
      "======================================================================\n",
      "\n",
      "Test: Return structure for metrics-generated\n",
      "âœ“ Return structure is correct\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 7: Custom Configuration\n",
      "======================================================================\n",
      "\n",
      "Test: Custom tolerance\n",
      "âœ“ Strict tolerance working: no match for 0.5% error with 0.1% tolerance\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY\n",
      "======================================================================\n",
      "Total Tests: 16\n",
      "âœ“ Passed: 16\n",
      "âœ— Failed: 0\n",
      "Success Rate: 100.0%\n",
      "======================================================================\n",
      "\n",
      "ðŸŽ‰ ALL TESTS PASSED! ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comprehensive Test Suite for Aggregate Evaluator\n",
    "=================================================\n",
    "Tests the master evaluation function that routes to appropriate metrics.\n",
    "\n",
    "NOTE: These tests require:\n",
    "- All Phase 1 modules (numerical_exact_match, token_f1, detect_refusal)\n",
    "- All Phase 2 modules (llm_as_judge_binary, llm_as_judge_graded)\n",
    "- OpenAI API key set in environment\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('/home/claude')\n",
    "\n",
    "\n",
    "def check_api_key():\n",
    "    \"\"\"Check if OpenAI API key is available\"\"\"\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        print(\"âš ï¸  WARNING: OPENAI_API_KEY not found in environment\")\n",
    "        print(\"   These tests will fail without API access\")\n",
    "        print(\"   Set with: export OPENAI_API_KEY='your-key-here'\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_metrics_generated():\n",
    "    \"\"\"Test evaluation of metrics-generated questions\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 1: Metrics-Generated Questions\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"Skipping tests - no API key\")\n",
    "        return 0, 0\n",
    "    \n",
    "    tests = [\n",
    "        # (question, gold, generated, description, should_match)\n",
    "        (\n",
    "            \"What is the FY2018 capital expenditure amount (in USD millions) for 3M?\",\n",
    "            \"$1577.00\",\n",
    "            \"1577 million dollars\",\n",
    "            \"Exact match with format difference\",\n",
    "            True\n",
    "        ),\n",
    "        (\n",
    "            \"What is the operating margin?\",\n",
    "            \"24.5%\",\n",
    "            \"24.48%\",\n",
    "            \"Within tolerance\",\n",
    "            True\n",
    "        ),\n",
    "        (\n",
    "            \"What is the inventory turnover?\",\n",
    "            \"9.5\",\n",
    "            \"12\",\n",
    "            \"Out of tolerance\",\n",
    "            False\n",
    "        ),\n",
    "        (\n",
    "            \"What is the ratio?\",\n",
    "            \"0.66\",\n",
    "            \"I cannot calculate without data\",\n",
    "            \"Refusal\",\n",
    "            False\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    \n",
    "    for question, gold, gen, desc, expected_match in tests:\n",
    "        print(f\"\\nTest: {desc}\")\n",
    "        print(f\"Expected match: {expected_match}\")\n",
    "        \n",
    "        try:\n",
    "            result = evaluate_answer(\n",
    "                question=question,\n",
    "                question_type=\"metrics-generated\",\n",
    "                gold_answer=gold,\n",
    "                generated_answer=gen\n",
    "            )\n",
    "            \n",
    "            # Check structure\n",
    "            assert 'question_type' in result\n",
    "            assert 'metrics' in result\n",
    "            assert 'summary' in result\n",
    "            assert result['question_type'] == 'metrics-generated'\n",
    "            \n",
    "            # Check metrics exist\n",
    "            assert 'numerical_exact_match' in result['metrics']\n",
    "            assert 'llm_as_judge_binary' in result['metrics']\n",
    "            \n",
    "            # Check refusal detection\n",
    "            if \"cannot\" in gen.lower() or \"don't\" in gen.lower():\n",
    "                if result['summary']['refusal_detected']:\n",
    "                    print(f\"âœ“ Refusal detected correctly\")\n",
    "            \n",
    "            # Check at least one metric agrees with expected\n",
    "            nem_match = result['metrics']['numerical_exact_match']['match']\n",
    "            llm_match = result['metrics']['llm_as_judge_binary']['match']\n",
    "            \n",
    "            if nem_match == expected_match or llm_match == expected_match:\n",
    "                passed += 1\n",
    "                print(f\"âœ“ At least one metric matches expectation\")\n",
    "                print(f\"  NEM: {nem_match}, LLM: {llm_match}\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"âœ— Neither metric matches expectation\")\n",
    "                print(f\"  Expected: {expected_match}\")\n",
    "                print(f\"  NEM: {nem_match}, LLM: {llm_match}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"âœ— Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nMetrics-Generated: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_novel_generated():\n",
    "    \"\"\"Test evaluation of novel-generated questions\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 2: Novel-Generated Questions\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"Skipping tests - no API key\")\n",
    "        return 0, 0\n",
    "    \n",
    "    tests = [\n",
    "        # (question, gold, generated, description, min_f1, min_score)\n",
    "        (\n",
    "            \"Which segment dragged down 3M's overall growth in 2022?\",\n",
    "            \"The consumer segment shrunk by 0.9% organically.\",\n",
    "            \"The Consumer segment has dragged down 3M's overall growth.\",\n",
    "            \"Partial match - missing details\",\n",
    "            0.3,  # Expect some token overlap\n",
    "            2     # Expect score >= 2\n",
    "        ),\n",
    "        (\n",
    "            \"Which derivative had the highest notional value for Verizon in FY 2021?\",\n",
    "            \"Cross currency swaps. Its notional value was $32,502 million.\",\n",
    "            \"Cross currency swaps had the highest notional value in FY 2021, at $32,502 million.\",\n",
    "            \"Good match with all facts\",\n",
    "            0.6,  # Good token overlap\n",
    "            3     # Expect score >= 3\n",
    "        ),\n",
    "        (\n",
    "            \"What is the answer?\",\n",
    "            \"The answer is 42.\",\n",
    "            \"I don't know\",\n",
    "            \"Refusal\",\n",
    "            0.0,  # No overlap\n",
    "            0     # Expect score = 0\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    \n",
    "    for question, gold, gen, desc, min_f1, min_score in tests:\n",
    "        print(f\"\\nTest: {desc}\")\n",
    "        print(f\"Expected: F1 >= {min_f1}, Score >= {min_score}\")\n",
    "        \n",
    "        try:\n",
    "            result = evaluate_answer(\n",
    "                question=question,\n",
    "                question_type=\"novel-generated\",\n",
    "                gold_answer=gold,\n",
    "                generated_answer=gen\n",
    "            )\n",
    "            \n",
    "            # Check structure\n",
    "            assert 'metrics' in result\n",
    "            assert result['question_type'] == 'novel-generated'\n",
    "            \n",
    "            # Check metrics exist\n",
    "            assert 'token_f1' in result['metrics']\n",
    "            assert 'llm_as_judge_graded' in result['metrics']\n",
    "            \n",
    "            # Check values\n",
    "            f1 = result['metrics']['token_f1']['f1']\n",
    "            score = result['metrics']['llm_as_judge_graded']['score']\n",
    "            \n",
    "            if f1 >= min_f1 and score >= min_score:\n",
    "                passed += 1\n",
    "                print(f\"âœ“ F1: {f1:.3f} >= {min_f1}, Score: {score} >= {min_score}\")\n",
    "            else:\n",
    "                # Allow some flexibility\n",
    "                if f1 >= (min_f1 - 0.1) and score >= (min_score - 1):\n",
    "                    passed += 1\n",
    "                    print(f\"âœ“ Close enough: F1: {f1:.3f}, Score: {score}\")\n",
    "                else:\n",
    "                    failed += 1\n",
    "                    print(f\"âœ— F1: {f1:.3f} < {min_f1} or Score: {score} < {min_score}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"âœ— Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nNovel-Generated: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_domain_relevant():\n",
    "    \"\"\"Test evaluation of domain-relevant questions\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 3: Domain-Relevant Questions\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"Skipping tests - no API key\")\n",
    "        return 0, 0\n",
    "    \n",
    "    tests = [\n",
    "        # (question, gold, generated, description, min_score, max_score)\n",
    "        (\n",
    "            \"Does AMD have a reasonably healthy liquidity profile based on its quick ratio for FY22?\",\n",
    "            \"Yes. The quick ratio is 1.57, calculated as (cash and cash equivalents+Short term investments+Accounts receivable, net+receivables from related parties)/ (current liabilities).\",\n",
    "            \"Yes, AMD has a reasonably healthy liquidity profile based on its quick ratio for FY22.\",\n",
    "            \"Good answer without calculation details\",\n",
    "            3, 4\n",
    "        ),\n",
    "        (\n",
    "            \"Roughly how many times has AES Corporation sold its inventory in FY2022?\",\n",
    "            \"AES has converted inventory 9.5 times in FY 2022.\",\n",
    "            \"AES Corporation sold its inventory roughly 12 times in FY2022.\",\n",
    "            \"Wrong number\",\n",
    "            0, 2\n",
    "        ),\n",
    "        (\n",
    "            \"What is the liquidity ratio?\",\n",
    "            \"The ratio is 0.96\",\n",
    "            \"I cannot calculate this\",\n",
    "            \"Refusal\",\n",
    "            0, 0\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    \n",
    "    for question, gold, gen, desc, min_score, max_score in tests:\n",
    "        print(f\"\\nTest: {desc}\")\n",
    "        print(f\"Expected score: {min_score}-{max_score}\")\n",
    "        \n",
    "        try:\n",
    "            result = evaluate_answer(\n",
    "                question=question,\n",
    "                question_type=\"domain-relevant\",\n",
    "                gold_answer=gold,\n",
    "                generated_answer=gen\n",
    "            )\n",
    "            \n",
    "            # Check structure\n",
    "            assert 'metrics' in result\n",
    "            assert result['question_type'] == 'domain-relevant'\n",
    "            \n",
    "            # Check metric exists\n",
    "            assert 'llm_as_judge_graded' in result['metrics']\n",
    "            \n",
    "            # Check value\n",
    "            score = result['metrics']['llm_as_judge_graded']['score']\n",
    "            \n",
    "            if min_score <= score <= max_score:\n",
    "                passed += 1\n",
    "                print(f\"âœ“ Score: {score} (within {min_score}-{max_score})\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"âœ— Score: {score} (expected {min_score}-{max_score})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed += 1\n",
    "            print(f\"âœ— Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nDomain-Relevant: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_invalid_question_type():\n",
    "    \"\"\"Test error handling for invalid question type\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 4: Error Handling\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nTest: Invalid question type\")\n",
    "    \n",
    "    try:\n",
    "        result = evaluate_answer(\n",
    "            question=\"Test question\",\n",
    "            question_type=\"invalid-type\",\n",
    "            gold_answer=\"Answer\",\n",
    "            generated_answer=\"Answer\"\n",
    "        )\n",
    "        print(\"âœ— Should have raised ValueError\")\n",
    "        return 0, 1\n",
    "    except ValueError as e:\n",
    "        if \"Invalid question_type\" in str(e):\n",
    "            print(f\"âœ“ Correctly raised ValueError: {e}\")\n",
    "            return 1, 0\n",
    "        else:\n",
    "            print(f\"âœ— Wrong error message: {e}\")\n",
    "            return 0, 1\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Wrong exception type: {e}\")\n",
    "        return 0, 1\n",
    "\n",
    "\n",
    "def test_config_validation():\n",
    "    \"\"\"Test configuration validation\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 5: Configuration Validation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    \n",
    "    # Test 1: Valid config\n",
    "    print(\"\\nTest: Valid config\")\n",
    "    try:\n",
    "        config = get_default_config()\n",
    "        validate_config(config)\n",
    "        print(\"âœ“ Default config is valid\")\n",
    "        passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error: {e}\")\n",
    "        failed += 1\n",
    "    \n",
    "    # Test 2: Invalid tolerance\n",
    "    print(\"\\nTest: Invalid tolerance (negative)\")\n",
    "    try:\n",
    "        config = {'tolerance': -0.01}\n",
    "        validate_config(config)\n",
    "        print(\"âœ— Should have raised ValueError\")\n",
    "        failed += 1\n",
    "    except ValueError as e:\n",
    "        print(f\"âœ“ Correctly raised ValueError: {e}\")\n",
    "        passed += 1\n",
    "    \n",
    "    # Test 3: Invalid temperature\n",
    "    print(\"\\nTest: Invalid temperature (out of range)\")\n",
    "    try:\n",
    "        config = {'llm_temperature': 3.0}\n",
    "        validate_config(config)\n",
    "        print(\"âœ— Should have raised ValueError\")\n",
    "        failed += 1\n",
    "    except ValueError as e:\n",
    "        print(f\"âœ“ Correctly raised ValueError: {e}\")\n",
    "        passed += 1\n",
    "    \n",
    "    print(f\"\\nConfig Validation: {passed}/3 passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_return_structure():\n",
    "    \"\"\"Test that return structure is correct\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 6: Return Structure Validation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"Skipping tests - no API key\")\n",
    "        return 0, 0\n",
    "    \n",
    "    print(\"\\nTest: Return structure for metrics-generated\")\n",
    "    \n",
    "    try:\n",
    "        result = evaluate_answer(\n",
    "            question=\"What is the value?\",\n",
    "            question_type=\"metrics-generated\",\n",
    "            gold_answer=\"100\",\n",
    "            generated_answer=\"100\"\n",
    "        )\n",
    "        \n",
    "        # Check all required keys\n",
    "        required_keys = ['question_type', 'question', 'gold_answer', 'generated_answer', \n",
    "                        'refusal_check', 'metrics', 'summary']\n",
    "        \n",
    "        for key in required_keys:\n",
    "            assert key in result, f\"Missing key: {key}\"\n",
    "        \n",
    "        # Check metrics structure\n",
    "        assert 'numerical_exact_match' in result['metrics']\n",
    "        assert 'llm_as_judge_binary' in result['metrics']\n",
    "        \n",
    "        # Check summary structure\n",
    "        assert 'question_type' in result['summary']\n",
    "        assert 'refusal_detected' in result['summary']\n",
    "        assert 'metrics_computed' in result['summary']\n",
    "        assert 'evaluation_complete' in result['summary']\n",
    "        \n",
    "        print(\"âœ“ Return structure is correct\")\n",
    "        return 1, 0\n",
    "        \n",
    "    except AssertionError as e:\n",
    "        print(f\"âœ— Structure validation failed: {e}\")\n",
    "        return 0, 1\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error: {e}\")\n",
    "        return 0, 1\n",
    "\n",
    "\n",
    "def test_custom_config():\n",
    "    \"\"\"Test using custom configuration\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 7: Custom Configuration\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"Skipping tests - no API key\")\n",
    "        return 0, 0\n",
    "    \n",
    "    print(\"\\nTest: Custom tolerance\")\n",
    "    \n",
    "    try:\n",
    "        # Strict tolerance\n",
    "        config = {\n",
    "            'tolerance': 0.001,  # 0.1% - very strict\n",
    "            'return_details': False  # Faster\n",
    "        }\n",
    "        \n",
    "        result = evaluate_answer(\n",
    "            question=\"What is the value?\",\n",
    "            question_type=\"metrics-generated\",\n",
    "            gold_answer=\"100\",\n",
    "            generated_answer=\"100.5\",\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        # With 0.1% tolerance, 100.5 vs 100 (0.5% error) should NOT match\n",
    "        nem_match = result['metrics']['numerical_exact_match']['match']\n",
    "        \n",
    "        if not nem_match:\n",
    "            print(f\"âœ“ Strict tolerance working: no match for 0.5% error with 0.1% tolerance\")\n",
    "            return 1, 0\n",
    "        else:\n",
    "            print(f\"âœ— Expected no match with strict tolerance\")\n",
    "            return 0, 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error: {e}\")\n",
    "        return 0, 1\n",
    "\n",
    "\n",
    "def run_all_tests():\n",
    "    \"\"\"Run all test suites\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPREHENSIVE TEST SUITE FOR AGGREGATE EVALUATOR\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not check_api_key():\n",
    "        print(\"\\nâŒ Cannot run tests without OPENAI_API_KEY\")\n",
    "        print(\"Please set your API key and try again\")\n",
    "        return False\n",
    "    \n",
    "    total_passed = 0\n",
    "    total_failed = 0\n",
    "    \n",
    "    test_suites = [\n",
    "        test_metrics_generated,\n",
    "        test_novel_generated,\n",
    "        test_domain_relevant,\n",
    "        test_invalid_question_type,\n",
    "        test_config_validation,\n",
    "        test_return_structure,\n",
    "        test_custom_config,\n",
    "    ]\n",
    "    \n",
    "    for test_func in test_suites:\n",
    "        try:\n",
    "            passed, failed = test_func()\n",
    "            total_passed += passed\n",
    "            total_failed += failed\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Test suite crashed: {e}\")\n",
    "            total_failed += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total Tests: {total_passed + total_failed}\")\n",
    "    print(f\"âœ“ Passed: {total_passed}\")\n",
    "    print(f\"âœ— Failed: {total_failed}\")\n",
    "    if total_passed + total_failed > 0:\n",
    "        print(f\"Success Rate: {100 * total_passed / (total_passed + total_failed):.1f}%\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return total_failed == 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = run_all_tests()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nðŸŽ‰ ALL TESTS PASSED! ðŸŽ‰\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  SOME TESTS FAILED - Review output above\")\n",
    "        print(\"Note: LLM scores can vary, some variance is expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcddc7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXAMPLE 1: Metrics-Generated Question\n",
      "======================================================================\n",
      "Question Type: metrics-generated\n",
      "Refusal Detected: False\n",
      "Metrics Computed: ['numerical_exact_match', 'llm_as_judge_binary']\n",
      "\n",
      "Numerical Exact Match:\n",
      "  Match: True\n",
      "  Category: exact_match\n",
      "\n",
      "LLM as Judge (Binary):\n",
      "  Match: True\n",
      "  Category: exact_match\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 2: Novel-Generated Question\n",
      "======================================================================\n",
      "Token F1:\n",
      "  F1 Score: 0.300\n",
      "  Precision: 0.250\n",
      "  Recall: 0.375\n",
      "  Common Tokens: {'consumer', 'the', 'segment'}\n",
      "  Missing Tokens: {'9', 'shrunk', 'by', 'organically', '0'}\n",
      "\n",
      "LLM as Judge (Graded):\n",
      "  Score: 2/4\n",
      "  Key Facts: 4\n",
      "  Facts Present: 2\n",
      "  Facts Missing: ['0.9%', 'organically']\n",
      "  Justification: The generated answer correctly identifies the consumer segment as the factor affecting 3M's growth, ...\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 3: Domain-Relevant Question\n",
      "======================================================================\n",
      "LLM as Judge (Graded):\n",
      "  Score: 3/4\n",
      "  Justification: The generated answer correctly affirms that AMD has a healthy liquidity profile and mentions the quick ratio, but it omits the specific value of 1.57 and the calculation formula. While the core information is accurate, the missing details prevent it from being a perfect match.\n",
      "\n",
      "======================================================================\n",
      "CUSTOM CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      "Default tolerance: 0.01\n",
      "\n",
      "With strict 0.5% tolerance:\n",
      "  Generated: 24.6% vs Gold: 24.5%\n",
      "  Relative Error: 0.40816326530612823%\n",
      "  Match: True\n",
      "  Category: within_tolerance\n",
      "\n",
      "======================================================================\n",
      "BATCH PROCESSING PATTERN\n",
      "======================================================================\n",
      "\n",
      "Evaluating 1/3: metrics-generated\n",
      "  Complete: True\n",
      "\n",
      "Evaluating 2/3: novel-generated\n",
      "  Complete: True\n",
      "\n",
      "Evaluating 3/3: domain-relevant\n",
      "  Complete: True\n",
      "\n",
      "Batch complete: 3 evaluations\n",
      "\n",
      "======================================================================\n",
      "ERROR HANDLING\n",
      "======================================================================\n",
      "\n",
      "Example 1: Invalid question type\n",
      "âœ“ Caught ValueError: Invalid question_type: 'invalid-type'. Must be one of: ['metrics-generated', 'novel-generated', 'domain-relevant']\n",
      "\n",
      "Example 2: Handling refusals\n",
      "âœ“ Refusal detected:\n",
      "  Type: explicit\n",
      "  Matched Pattern: \\bi\\s+(?:do\\s+not|don't|cannot|can't|could\\s+not|couldn't)\\s+(?:know|have|provide|answer|calculate|determine|find)\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS PATTERNS\n",
      "======================================================================\n",
      "\n",
      "Pattern 1: Rule-based vs LLM comparison\n",
      "  Gold: '$1.577 billion'\n",
      "  Generated: '1577 million'\n",
      "  Rule-based match: True\n",
      "  LLM match: True\n",
      "  Agreement: True\n",
      "\n",
      "Pattern 2: Token F1 vs LLM semantic comparison\n",
      "  Token F1: 0.200\n",
      "  LLM Score: 1/4\n",
      "\n",
      "======================================================================\n",
      "AGGREGATION AND REPORTING\n",
      "======================================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total: 4\n",
      "  Metrics-generated: 2\n",
      "  Novel-generated: 1\n",
      "  Domain-relevant: 1\n",
      "  Refusal rate: 0.0%\n",
      "  Success rate: 100.0%\n",
      "\n",
      "======================================================================\n",
      "PRETTY PRINTING\n",
      "======================================================================\n",
      "======================================================================\n",
      "EVALUATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Question Type: metrics-generated\n",
      "Question: What is the FY2018 capital expenditure?...\n",
      "Gold Answer: $1577.00...\n",
      "Generated Answer: 1577 million dollars...\n",
      "\n",
      "Refusal Detected: False\n",
      "\n",
      "Metrics Computed: numerical_exact_match, llm_as_judge_binary\n",
      "\n",
      "NUMERICAL_EXACT_MATCH:\n",
      "  Match: True\n",
      "  Category: exact_match\n",
      "\n",
      "LLM_AS_JUDGE_BINARY:\n",
      "  Match: True\n",
      "  Category: exact_match\n",
      "\n",
      "Evaluation Complete: True\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "INTEGRATION WITH RAG SYSTEM\n",
      "======================================================================\n",
      "\n",
      "RAG Evaluation:\n",
      "  Question: What is the FY2018 capital expenditure amount (in USD millio...\n",
      "  Type: metrics-generated\n",
      "  Metrics: numerical_exact_match, llm_as_judge_binary\n",
      "  Complete: True\n",
      "\n",
      "======================================================================\n",
      "SUMMARY - Key Takeaways\n",
      "======================================================================\n",
      "\n",
      "The Aggregate Evaluator provides:\n",
      "\n",
      "1. âœ… Automatic routing to appropriate metrics based on question type\n",
      "2. âœ… Comprehensive evaluation with multiple metrics per question\n",
      "3. âœ… Pre-check refusal detection to catch obvious failures\n",
      "4. âœ… Structured, consistent output format\n",
      "5. âœ… Configurable parameters for different use cases\n",
      "6. âœ… Error handling with exceptions for reliability\n",
      "\n",
      "Usage Pattern:\n",
      "    result = evaluate_answer(\n",
      "        question=question,\n",
      "        question_type=question_type,  # metrics/novel/domain-relevant\n",
      "        gold_answer=gold,\n",
      "        generated_answer=generated,\n",
      "        config=config  # optional\n",
      "    )\n",
      "\n",
      "Access Results:\n",
      "    - result['metrics'][metric_name] â†’ Full metric results\n",
      "    - result['summary'] â†’ High-level overview\n",
      "    - result['refusal_check'] â†’ Refusal detection results\n",
      "\n",
      "Best Practices:\n",
      "    - Always handle ValueError for invalid question types\n",
      "    - Check result['summary']['evaluation_complete'] before using results\n",
      "    - Compare metrics (rule-based vs LLM) for insights\n",
      "    - Use custom config for domain-specific tolerances\n",
      "    - Batch process with proper error handling\n",
      "\n",
      "======================================================================\n",
      "Ready to evaluate your FinanceBench dataset! ðŸš€\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Aggregate Evaluator - Usage Examples and Integration Guide\n",
    "==========================================================\n",
    "\n",
    "This document provides comprehensive examples for using the master evaluation\n",
    "function across different question types and use cases.\n",
    "\n",
    "Author: Financial QA Evaluation System\n",
    "Version: 1.0\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# BASIC USAGE\n",
    "# ============================================================================\n",
    "\n",
    "# Example 1: Metrics-Generated Question\n",
    "print(\"=\"*70)\n",
    "print(\"EXAMPLE 1: Metrics-Generated Question\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "result = evaluate_answer(\n",
    "    question=\"What is the FY2018 capital expenditure amount (in USD millions) for 3M?\",\n",
    "    question_type=\"metrics-generated\",\n",
    "    gold_answer=\"$1577.00\",\n",
    "    generated_answer=\"1577 million dollars\"\n",
    ")\n",
    "\n",
    "# Access results\n",
    "print(f\"Question Type: {result['question_type']}\")\n",
    "print(f\"Refusal Detected: {result['summary']['refusal_detected']}\")\n",
    "print(f\"Metrics Computed: {result['summary']['metrics_computed']}\")\n",
    "\n",
    "# Check numerical exact match\n",
    "nem = result['metrics']['numerical_exact_match']\n",
    "print(f\"\\nNumerical Exact Match:\")\n",
    "print(f\"  Match: {nem['match']}\")\n",
    "print(f\"  Category: {nem['error_category']}\")\n",
    "\n",
    "# Check LLM judge\n",
    "llm = result['metrics']['llm_as_judge_binary']\n",
    "print(f\"\\nLLM as Judge (Binary):\")\n",
    "print(f\"  Match: {llm['match']}\")\n",
    "print(f\"  Category: {llm['error_category']}\")\n",
    "if llm.get('corrected'):\n",
    "    print(f\"  âš ï¸  Auto-corrected by post-processing\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# Example 2: Novel-Generated Question\n",
    "print(\"=\"*70)\n",
    "print(\"EXAMPLE 2: Novel-Generated Question\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "result = evaluate_answer(\n",
    "    question=\"Which segment dragged down 3M's overall growth in 2022 excluding M&A?\",\n",
    "    question_type=\"novel-generated\",\n",
    "    gold_answer=\"The consumer segment shrunk by 0.9% organically.\",\n",
    "    generated_answer=\"The Consumer segment has dragged down 3M's overall growth in 2022.\"\n",
    ")\n",
    "\n",
    "# Check Token F1\n",
    "f1 = result['metrics']['token_f1']\n",
    "print(f\"Token F1:\")\n",
    "print(f\"  F1 Score: {f1['f1']:.3f}\")\n",
    "print(f\"  Precision: {f1['precision']:.3f}\")\n",
    "print(f\"  Recall: {f1['recall']:.3f}\")\n",
    "print(f\"  Common Tokens: {f1['common_tokens']}\")\n",
    "print(f\"  Missing Tokens: {f1['missing_tokens']}\")\n",
    "\n",
    "# Check LLM graded\n",
    "llm = result['metrics']['llm_as_judge_graded']\n",
    "print(f\"\\nLLM as Judge (Graded):\")\n",
    "print(f\"  Score: {llm['score']}/4\")\n",
    "print(f\"  Key Facts: {len(llm['key_facts_gold'])}\")\n",
    "print(f\"  Facts Present: {len(llm['facts_present'])}\")\n",
    "print(f\"  Facts Missing: {llm['facts_missing']}\")\n",
    "print(f\"  Justification: {llm['justification'][:100]}...\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# Example 3: Domain-Relevant Question\n",
    "print(\"=\"*70)\n",
    "print(\"EXAMPLE 3: Domain-Relevant Question\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "result = evaluate_answer(\n",
    "    question=\"Does AMD have a reasonably healthy liquidity profile based on its quick ratio for FY22?\",\n",
    "    question_type=\"domain-relevant\",\n",
    "    gold_answer=\"Yes. The quick ratio is 1.57, calculated as (cash and cash equivalents+Short term investments+Accounts receivable, net+receivables from related parties)/ (current liabilities).\",\n",
    "    generated_answer=\"Yes, AMD has a reasonably healthy liquidity profile based on its quick ratio for FY22.\"\n",
    ")\n",
    "\n",
    "# Check LLM graded (only metric for domain-relevant)\n",
    "llm = result['metrics']['llm_as_judge_graded']\n",
    "print(f\"LLM as Judge (Graded):\")\n",
    "print(f\"  Score: {llm['score']}/4\")\n",
    "print(f\"  Justification: {llm['justification']}\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CUSTOM CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CUSTOM CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get default config and modify\n",
    "config = get_default_config()\n",
    "print(f\"\\nDefault tolerance: {config['tolerance']}\")\n",
    "\n",
    "# Make it stricter\n",
    "config['tolerance'] = 0.005  # 0.5% instead of 1%\n",
    "config['return_details'] = False  # Faster, less data\n",
    "\n",
    "result = evaluate_answer(\n",
    "    question=\"What is the margin?\",\n",
    "    question_type=\"metrics-generated\",\n",
    "    gold_answer=\"24.5%\",\n",
    "    generated_answer=\"24.6%\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "nem = result['metrics']['numerical_exact_match']\n",
    "print(f\"\\nWith strict 0.5% tolerance:\")\n",
    "print(f\"  Generated: 24.6% vs Gold: 24.5%\")\n",
    "print(f\"  Relative Error: {nem.get('relative_error', 'N/A')}%\")\n",
    "print(f\"  Match: {nem['match']}\")\n",
    "print(f\"  Category: {nem['error_category']}\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH PROCESSING PATTERN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BATCH PROCESSING PATTERN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simulate evaluating multiple questions\n",
    "questions = [\n",
    "    {\n",
    "        'question': \"What is the FY2018 capex?\",\n",
    "        'question_type': \"metrics-generated\",\n",
    "        'gold_answer': \"$1577.00\",\n",
    "        'generated_answer': \"1577 million\"\n",
    "    },\n",
    "    {\n",
    "        'question': \"Which segment declined?\",\n",
    "        'question_type': \"novel-generated\",\n",
    "        'gold_answer': \"The consumer segment shrunk by 0.9%.\",\n",
    "        'generated_answer': \"Consumer segment.\"\n",
    "    },\n",
    "    {\n",
    "        'question': \"Is liquidity healthy?\",\n",
    "        'question_type': \"domain-relevant\",\n",
    "        'gold_answer': \"Yes. The quick ratio is 1.57.\",\n",
    "        'generated_answer': \"Yes, liquidity is healthy.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "results = []\n",
    "for i, q in enumerate(questions):\n",
    "    print(f\"\\nEvaluating {i+1}/{len(questions)}: {q['question_type']}\")\n",
    "    \n",
    "    result = evaluate_answer(\n",
    "        question=q['question'],\n",
    "        question_type=q['question_type'],\n",
    "        gold_answer=q['gold_answer'],\n",
    "        generated_answer=q['generated_answer']\n",
    "    )\n",
    "    \n",
    "    results.append(result)\n",
    "    print(f\"  Complete: {result['summary']['evaluation_complete']}\")\n",
    "\n",
    "print(f\"\\nBatch complete: {len(results)} evaluations\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ERROR HANDLING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ERROR HANDLING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Example 1: Invalid question type\n",
    "print(\"\\nExample 1: Invalid question type\")\n",
    "try:\n",
    "    result = evaluate_answer(\n",
    "        question=\"Test\",\n",
    "        question_type=\"invalid-type\",\n",
    "        gold_answer=\"Answer\",\n",
    "        generated_answer=\"Answer\"\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"âœ“ Caught ValueError: {e}\")\n",
    "\n",
    "# Example 2: Handling refusals\n",
    "print(\"\\nExample 2: Handling refusals\")\n",
    "result = evaluate_answer(\n",
    "    question=\"What is the ratio?\",\n",
    "    question_type=\"metrics-generated\",\n",
    "    gold_answer=\"0.66\",\n",
    "    generated_answer=\"I cannot calculate this without data\"\n",
    ")\n",
    "\n",
    "if result['summary']['refusal_detected']:\n",
    "    print(f\"âœ“ Refusal detected:\")\n",
    "    print(f\"  Type: {result['refusal_check']['refusal_type']}\")\n",
    "    print(f\"  Matched Pattern: {result['refusal_check'].get('matched_pattern', 'N/A')}\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS PATTERNS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ANALYSIS PATTERNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Pattern 1: Compare rule-based vs LLM for metrics-generated\n",
    "print(\"\\nPattern 1: Rule-based vs LLM comparison\")\n",
    "\n",
    "result = evaluate_answer(\n",
    "    question=\"What is the revenue?\",\n",
    "    question_type=\"metrics-generated\",\n",
    "    gold_answer=\"$1.577 billion\",\n",
    "    generated_answer=\"1577 million\"\n",
    ")\n",
    "\n",
    "nem_match = result['metrics']['numerical_exact_match']['match']\n",
    "llm_match = result['metrics']['llm_as_judge_binary']['match']\n",
    "\n",
    "print(f\"  Gold: '$1.577 billion'\")\n",
    "print(f\"  Generated: '1577 million'\")\n",
    "print(f\"  Rule-based match: {nem_match}\")\n",
    "print(f\"  LLM match: {llm_match}\")\n",
    "print(f\"  Agreement: {nem_match == llm_match}\")\n",
    "\n",
    "if nem_match != llm_match:\n",
    "    print(f\"  âš ï¸  Disagreement detected - investigate!\")\n",
    "    print(f\"  LLM Justification: {result['metrics']['llm_as_judge_binary']['justification']}\")\n",
    "\n",
    "\n",
    "# Pattern 2: Token F1 vs LLM for novel-generated\n",
    "print(\"\\nPattern 2: Token F1 vs LLM semantic comparison\")\n",
    "\n",
    "result = evaluate_answer(\n",
    "    question=\"What happened to the segment?\",\n",
    "    question_type=\"novel-generated\",\n",
    "    gold_answer=\"The consumer segment shrunk by 0.9% organically.\",\n",
    "    generated_answer=\"Consumer declined.\"\n",
    ")\n",
    "\n",
    "f1_score = result['metrics']['token_f1']['f1']\n",
    "llm_score = result['metrics']['llm_as_judge_graded']['score']\n",
    "\n",
    "print(f\"  Token F1: {f1_score:.3f}\")\n",
    "print(f\"  LLM Score: {llm_score}/4\")\n",
    "\n",
    "if f1_score < 0.3 and llm_score >= 2:\n",
    "    print(f\"  ðŸ’¡ Low token overlap but decent LLM score - semantic equivalence detected\")\n",
    "elif f1_score > 0.5 and llm_score < 2:\n",
    "    print(f\"  âš ï¸  High token overlap but low LLM score - investigate!\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# AGGREGATION AND REPORTING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AGGREGATION AND REPORTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simulate evaluating a dataset\n",
    "results = []\n",
    "\n",
    "# Mix of question types\n",
    "test_data = [\n",
    "    (\"metrics-generated\", \"$1577\", \"1577\", True),\n",
    "    (\"metrics-generated\", \"24.5%\", \"30%\", False),\n",
    "    (\"novel-generated\", \"Consumer segment declined\", \"Consumer declined\", True),\n",
    "    (\"domain-relevant\", \"Yes, healthy liquidity\", \"Yes, liquidity is good\", True),\n",
    "]\n",
    "\n",
    "for qtype, gold, gen, _ in test_data:\n",
    "    result = evaluate_answer(\n",
    "        question=\"Sample question\",\n",
    "        question_type=qtype,\n",
    "        gold_answer=gold,\n",
    "        generated_answer=gen\n",
    "    )\n",
    "    results.append(result)\n",
    "\n",
    "# Aggregate statistics\n",
    "metrics_generated_count = sum(1 for r in results if r['question_type'] == 'metrics-generated')\n",
    "novel_generated_count = sum(1 for r in results if r['question_type'] == 'novel-generated')\n",
    "domain_relevant_count = sum(1 for r in results if r['question_type'] == 'domain-relevant')\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Total: {len(results)}\")\n",
    "print(f\"  Metrics-generated: {metrics_generated_count}\")\n",
    "print(f\"  Novel-generated: {novel_generated_count}\")\n",
    "print(f\"  Domain-relevant: {domain_relevant_count}\")\n",
    "\n",
    "# Refusal rate\n",
    "refusal_count = sum(1 for r in results if r['summary']['refusal_detected'])\n",
    "print(f\"  Refusal rate: {100*refusal_count/len(results):.1f}%\")\n",
    "\n",
    "# Success rate\n",
    "success_count = sum(1 for r in results if r['summary']['evaluation_complete'])\n",
    "print(f\"  Success rate: {100*success_count/len(results):.1f}%\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PRETTY PRINTING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PRETTY PRINTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "result = evaluate_answer(\n",
    "    question=\"What is the FY2018 capital expenditure?\",\n",
    "    question_type=\"metrics-generated\",\n",
    "    gold_answer=\"$1577.00\",\n",
    "    generated_answer=\"1577 million dollars\"\n",
    ")\n",
    "\n",
    "# Use built-in pretty printer\n",
    "print_evaluation_summary(result)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INTEGRATION WITH RAG SYSTEM\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTEGRATION WITH RAG SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def evaluate_rag_output(question_data, generated_answer):\n",
    "    \"\"\"\n",
    "    Example function showing how to integrate with your RAG system.\n",
    "    \n",
    "    Args:\n",
    "        question_data: Dict with 'question', 'question_type', 'answer' (gold)\n",
    "        generated_answer: String from your RAG system\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation result\n",
    "    \"\"\"\n",
    "    \n",
    "    result = evaluate_answer(\n",
    "        question=question_data['question'],\n",
    "        question_type=question_data['question_type'],\n",
    "        gold_answer=question_data['answer'],\n",
    "        generated_answer=generated_answer\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "financebench_question = {\n",
    "    'question': \"What is the FY2018 capital expenditure amount (in USD millions) for 3M?\",\n",
    "    'question_type': \"metrics-generated\",\n",
    "    'answer': \"$1577.00\"\n",
    "}\n",
    "\n",
    "rag_output = \"The FY2018 capital expenditure for 3M was 1577 million dollars.\"\n",
    "\n",
    "result = evaluate_rag_output(financebench_question, rag_output)\n",
    "\n",
    "print(f\"\\nRAG Evaluation:\")\n",
    "print(f\"  Question: {result['question'][:60]}...\")\n",
    "print(f\"  Type: {result['question_type']}\")\n",
    "print(f\"  Metrics: {', '.join(result['summary']['metrics_computed'])}\")\n",
    "print(f\"  Complete: {result['summary']['evaluation_complete']}\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SUMMARY - Key Takeaways\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "The Aggregate Evaluator provides:\n",
    "\n",
    "1. âœ… Automatic routing to appropriate metrics based on question type\n",
    "2. âœ… Comprehensive evaluation with multiple metrics per question\n",
    "3. âœ… Pre-check refusal detection to catch obvious failures\n",
    "4. âœ… Structured, consistent output format\n",
    "5. âœ… Configurable parameters for different use cases\n",
    "6. âœ… Error handling with exceptions for reliability\n",
    "\n",
    "Usage Pattern:\n",
    "    result = evaluate_answer(\n",
    "        question=question,\n",
    "        question_type=question_type,  # metrics/novel/domain-relevant\n",
    "        gold_answer=gold,\n",
    "        generated_answer=generated,\n",
    "        config=config  # optional\n",
    "    )\n",
    "\n",
    "Access Results:\n",
    "    - result['metrics'][metric_name] â†’ Full metric results\n",
    "    - result['summary'] â†’ High-level overview\n",
    "    - result['refusal_check'] â†’ Refusal detection results\n",
    "\n",
    "Best Practices:\n",
    "    - Always handle ValueError for invalid question types\n",
    "    - Check result['summary']['evaluation_complete'] before using results\n",
    "    - Compare metrics (rule-based vs LLM) for insights\n",
    "    - Use custom config for domain-specific tolerances\n",
    "    - Batch process with proper error handling\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Ready to evaluate your FinanceBench dataset! ðŸš€\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
