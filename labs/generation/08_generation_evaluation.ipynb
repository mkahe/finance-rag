{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "421f9779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Successfully imported evaluation functions\n",
      "\n",
      "======================================================================\n",
      "STEP 1: PROJECT SETUP & CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "CONFIGURATION VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "üìÅ Paths:\n",
      "   Generation Path: ../../generation_set/closedbook_oracle_sets\n",
      "   Evaluation Path: ../../evaluation_results/generation\n",
      "\n",
      "‚öôÔ∏è  Evaluation Configuration:\n",
      "   Tolerance: 0.05 (5.0%)\n",
      "   LLM Model: gpt-4o-mini\n",
      "   Temperature: 0.0\n",
      "   Return Details: True\n",
      "\n",
      "‚è±Ô∏è  Rate Limiting:\n",
      "   Call Delay: 0.5s between queries\n",
      "   Retry Delay: 10s after API error\n",
      "   Max Retries: 3 attempts\n",
      "\n",
      "üìã Evaluation Configurations:\n",
      "   Total files to process: 9\n",
      "\n",
      "   Configurations:\n",
      "   [1/9] closed_book  | metrics-generated\n",
      "   [2/9] closed_book  | novel-generated\n",
      "   [3/9] closed_book  | domain-relevant\n",
      "   [4/9] rag          | metrics-generated\n",
      "   [5/9] rag          | novel-generated\n",
      "   [6/9] rag          | domain-relevant\n",
      "   [7/9] oracle       | metrics-generated\n",
      "   [8/9] oracle       | novel-generated\n",
      "   [9/9] oracle       | domain-relevant\n",
      "\n",
      "======================================================================\n",
      "‚úì Configuration loaded successfully!\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Step 1 Complete!\n",
      "\n",
      "Next steps:\n",
      "  1. Update GENERATION_PATH to your actual directory\n",
      "  2. Update EVALUATION_PATH to your actual directory\n",
      "  3. Verify the 9 EVALUATION_CONFIGS match your file naming\n",
      "  4. Run this script again to confirm configuration\n",
      "  5. Report back for approval to proceed to Step 2\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FinanceBench Generation Evaluation Pipeline\n",
    "===========================================\n",
    "\n",
    "This script evaluates generated answers from RAG experiments using multiple metrics\n",
    "based on question types (metrics-generated, novel-generated, domain-relevant).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# Progress tracking\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ============================================================================\n",
    "# Load Environment Variables\n",
    "# ============================================================================\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTANT: Update these paths to match your directory structure\n",
    "# ============================================================================\n",
    "\n",
    "# Directory containing generated answer JSON files\n",
    "GENERATION_PATH = \"../../generation_set/closedbook_oracle_sets\"\n",
    "\n",
    "# Directory where evaluation result JSON files will be saved\n",
    "EVALUATION_PATH = \"../../evaluation_results/generation\"\n",
    "\n",
    "# ============================================================================\n",
    "# Evaluation Configuration (Hardcoded - modify as needed)\n",
    "# ============================================================================\n",
    "\n",
    "EVALUATION_CONFIG = {\n",
    "    'tolerance': 0.05,              # 5% tolerance for numerical matching\n",
    "    'llm_model': 'gpt-4o-mini',     # Model for LLM-as-judge evaluation\n",
    "    'temperature': 0.0,              # Temperature for LLM calls\n",
    "    'return_details': True           # Return full evaluation details\n",
    "}\n",
    "\n",
    "# API Rate Limiting\n",
    "CALL_DELAY = 0.5        # Seconds to wait between each query evaluation\n",
    "RETRY_DELAY = 10        # Seconds to wait before retrying after API error\n",
    "MAX_RETRIES = 3         # Maximum number of retry attempts\n",
    "\n",
    "# ============================================================================\n",
    "# Evaluation File Configurations\n",
    "# ============================================================================\n",
    "# Define all 9 experimental configurations to process\n",
    "\n",
    "EVALUATION_CONFIGS = [\n",
    "    # Closed-book experiments (3 question types)\n",
    "    {\n",
    "        'mode': 'closed_book',\n",
    "        'question_type': 'metrics-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'metrics_closed_basic'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'closed_book',\n",
    "        'question_type': 'novel-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'novel_closed_basic'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'closed_book',\n",
    "        'question_type': 'domain-relevant',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'domain_closed_basic'\n",
    "    },\n",
    "    \n",
    "    # RAG experiments (3 question types)\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'metrics-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'metrics_rag_basic'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'novel-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'novel_rag_basic'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'domain-relevant',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'domain_rag_basic'\n",
    "    },\n",
    "    \n",
    "    # Oracle experiments (3 question types)\n",
    "    {\n",
    "        'mode': 'oracle',\n",
    "        'question_type': 'metrics-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'metrics_rag_basic'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'oracle',\n",
    "        'question_type': 'novel-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'novel_rag_basic'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'oracle',\n",
    "        'question_type': 'domain-relevant',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'domain_rag_basic'\n",
    "    },\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# Import Evaluation Functions\n",
    "# ============================================================================\n",
    "\n",
    "# Track whether evaluation functions are available\n",
    "EVAL_FUNCTIONS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    # Import your evaluation suite functions\n",
    "    from generation_evaluation_suit.evaluate_answer import evaluate_answer\n",
    "    from generation_evaluation_suit.detect_refusal import detect_refusal\n",
    "    from generation_evaluation_suit.numerical_exact_match import numerical_exact_match\n",
    "    from generation_evaluation_suit.token_f1 import token_f1\n",
    "    from generation_evaluation_suit.llm_as_judge_binary import llm_as_judge_binary\n",
    "    from generation_evaluation_suit.llm_as_judge_graded import llm_as_judge_graded\n",
    "    \n",
    "    EVAL_FUNCTIONS_AVAILABLE = True\n",
    "    print(\"‚úì Successfully imported evaluation functions\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Note: Evaluation functions not yet imported\")\n",
    "    print(f\"   (This is fine for Step 1 configuration testing)\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    print(f\"\\n   When ready to run full evaluation, ensure these modules are available:\")\n",
    "    print(\"     - evaluate_answer.py\")\n",
    "    print(\"     - detect_refusal.py\")\n",
    "    print(\"     - numerical_exact_match.py\")\n",
    "    print(\"     - token_f1.py\")\n",
    "    print(\"     - llm_as_judge_binary.py\")\n",
    "    print(\"     - llm_as_judge_graded.py\")\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration Verification\n",
    "# ============================================================================\n",
    "\n",
    "def verify_configuration():\n",
    "    \"\"\"Verify that all configuration is set up correctly.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CONFIGURATION VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check paths\n",
    "    print(f\"\\nüìÅ Paths:\")\n",
    "    print(f\"   Generation Path: {GENERATION_PATH}\")\n",
    "    print(f\"   Evaluation Path: {EVALUATION_PATH}\")\n",
    "    \n",
    "    if GENERATION_PATH == \"/path/to/generated_answers\":\n",
    "        print(f\"   ‚ö†Ô∏è  WARNING: GENERATION_PATH is still set to placeholder!\")\n",
    "        print(f\"   ‚ö†Ô∏è  Please update GENERATION_PATH to your actual directory\")\n",
    "    \n",
    "    if EVALUATION_PATH == \"/path/to/evaluation_results\":\n",
    "        print(f\"   ‚ö†Ô∏è  WARNING: EVALUATION_PATH is still set to placeholder!\")\n",
    "        print(f\"   ‚ö†Ô∏è  Please update EVALUATION_PATH to your actual directory\")\n",
    "    \n",
    "    # Check evaluation config\n",
    "    print(f\"\\n‚öôÔ∏è  Evaluation Configuration:\")\n",
    "    print(f\"   Tolerance: {EVALUATION_CONFIG['tolerance']} ({EVALUATION_CONFIG['tolerance']*100}%)\")\n",
    "    print(f\"   LLM Model: {EVALUATION_CONFIG['llm_model']}\")\n",
    "    print(f\"   Temperature: {EVALUATION_CONFIG['temperature']}\")\n",
    "    print(f\"   Return Details: {EVALUATION_CONFIG['return_details']}\")\n",
    "    \n",
    "    # Check rate limiting\n",
    "    print(f\"\\n‚è±Ô∏è  Rate Limiting:\")\n",
    "    print(f\"   Call Delay: {CALL_DELAY}s between queries\")\n",
    "    print(f\"   Retry Delay: {RETRY_DELAY}s after API error\")\n",
    "    print(f\"   Max Retries: {MAX_RETRIES} attempts\")\n",
    "    \n",
    "    # Check configurations\n",
    "    print(f\"\\nüìã Evaluation Configurations:\")\n",
    "    print(f\"   Total files to process: {len(EVALUATION_CONFIGS)}\")\n",
    "    print(f\"\\n   Configurations:\")\n",
    "    \n",
    "    for i, config in enumerate(EVALUATION_CONFIGS, 1):\n",
    "        mode = config['mode']\n",
    "        qtype = config['question_type']\n",
    "        print(f\"   [{i}/9] {mode:12s} | {qtype}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úì Configuration loaded successfully!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Main Test Function\n",
    "# ============================================================================\n",
    "\n",
    "def test_step1():\n",
    "    \"\"\"Test Step 1: Verify imports and configuration.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 1: PROJECT SETUP & CONFIGURATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Verify configuration\n",
    "    verify_configuration()\n",
    "    \n",
    "    print(\"\\n‚úÖ Step 1 Complete!\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  1. Update GENERATION_PATH to your actual directory\")\n",
    "    print(\"  2. Update EVALUATION_PATH to your actual directory\")\n",
    "    print(\"  3. Verify the 9 EVALUATION_CONFIGS match your file naming\")\n",
    "    print(\"  4. Run this script again to confirm configuration\")\n",
    "    print(\"  5. Report back for approval to proceed to Step 2\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Entry Point\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_step1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a97812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: FILE PATH MANAGEMENT\n",
      "======================================================================\n",
      "\n",
      "üß™ Testing with configuration:\n",
      "   Mode: closed_book\n",
      "   Question Type: metrics-generated\n",
      "   Provider: openai\n",
      "   Model: gpt-4o-mini\n",
      "   Temperature: 0.0\n",
      "   Template: metrics_closed_basic\n",
      "\n",
      "üìù Testing filename generation:\n",
      "   Generation: closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "   Evaluation: evaluation_closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "\n",
      "üìÅ Testing filepath generation:\n",
      "   Generation: ../../generation_set/closedbook_oracle_sets/closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "   Evaluation: ../../evaluation_results/generation/evaluation_closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "\n",
      "üîç Checking file existence:\n",
      "   Generation file exists: True\n",
      "   Evaluation file exists: True\n",
      "   Generation file size: 402.86 KB\n",
      "   Evaluation file size: 176.38 KB\n",
      "\n",
      "======================================================================\n",
      "üìã Testing all 9 configurations:\n",
      "======================================================================\n",
      "\n",
      "[1/9] closed_book  | metrics-generated\n",
      "   Generation file: closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úì\n",
      "   ‚îî‚îÄ Size: 402.86 KB\n",
      "   Evaluation file: evaluation_closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úì\n",
      "   ‚îî‚îÄ Size: 176.38 KB\n",
      "\n",
      "[2/9] closed_book  | novel-generated\n",
      "   Generation file: closed_book_novel-generated_openai_gpt-4o-mini_0.0_novel_closed_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úì\n",
      "   ‚îî‚îÄ Size: 324.01 KB\n",
      "   Evaluation file: evaluation_closed_book_novel-generated_openai_gpt-4o-mini_0.0_novel_closed_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úì\n",
      "   ‚îî‚îÄ Size: 264.1 KB\n",
      "\n",
      "[3/9] closed_book  | domain-relevant\n",
      "   Generation file: closed_book_domain-relevant_openai_gpt-4o-mini_0.0_domain_closed_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úì\n",
      "   ‚îî‚îÄ Size: 338.87 KB\n",
      "   Evaluation file: evaluation_closed_book_domain-relevant_openai_gpt-4o-mini_0.0_domain_closed_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úì\n",
      "   ‚îî‚îÄ Size: 209.22 KB\n",
      "\n",
      "[4/9] rag          | metrics-generated\n",
      "   Generation file: rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úì\n",
      "   ‚îî‚îÄ Size: 389.51 KB\n",
      "   Evaluation file: evaluation_rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úó\n",
      "\n",
      "[5/9] rag          | novel-generated\n",
      "   Generation file: rag_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úó\n",
      "   Evaluation file: evaluation_rag_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úó\n",
      "\n",
      "[6/9] rag          | domain-relevant\n",
      "   Generation file: rag_domain-relevant_openai_gpt-4o-mini_0.0_domain_rag_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úó\n",
      "   Evaluation file: evaluation_rag_domain-relevant_openai_gpt-4o-mini_0.0_domain_rag_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úó\n",
      "\n",
      "[7/9] oracle       | metrics-generated\n",
      "   Generation file: oracle_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úì\n",
      "   ‚îî‚îÄ Size: 386.78 KB\n",
      "   Evaluation file: evaluation_oracle_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úì\n",
      "   ‚îî‚îÄ Size: 171.54 KB\n",
      "\n",
      "[8/9] oracle       | novel-generated\n",
      "   Generation file: oracle_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úì\n",
      "   ‚îî‚îÄ Size: 314.31 KB\n",
      "   Evaluation file: evaluation_oracle_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úì\n",
      "   ‚îî‚îÄ Size: 260.84 KB\n",
      "\n",
      "[9/9] oracle       | domain-relevant\n",
      "   Generation file: oracle_domain-relevant_openai_gpt-4o-mini_0.0_domain_rag_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úì\n",
      "   ‚îî‚îÄ Size: 336.46 KB\n",
      "   Evaluation file: evaluation_oracle_domain-relevant_openai_gpt-4o-mini_0.0_domain_rag_basic.json\n",
      "   ‚îî‚îÄ Exists: ‚úì\n",
      "   ‚îî‚îÄ Size: 207.53 KB\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üìä Summary:\n",
      "   Total configurations: 9\n",
      "   Generation files found: 7/9\n",
      "   Evaluation files found: 6/9\n",
      "\n",
      "‚ö†Ô∏è  WARNING: Only 7/9 generation files found\n",
      "   Some configurations are missing generation files\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Step 2 Complete!\n",
      "======================================================================\n",
      "\n",
      "Step 2 Functions Implemented:\n",
      "  ‚úì get_generation_filename(config)\n",
      "  ‚úì get_evaluation_filename(config)\n",
      "  ‚úì get_generation_filepath(config)\n",
      "  ‚úì get_evaluation_filepath(config)\n",
      "  ‚úì check_file_exists(filepath)\n",
      "  ‚úì get_file_info(filepath)\n",
      "\n",
      "Next steps:\n",
      "  1. Verify filenames match your actual files\n",
      "  2. Confirm generation files are detected correctly\n",
      "  3. Report back for approval to proceed to Step 3\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: File Path Management Functions\n",
    "# ============================================================================\n",
    "\n",
    "def get_generation_filename(config: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Construct generation file name from config.\n",
    "    \n",
    "    Format: {mode}_{question_type}_{provider}_{model}_{temperature}_{template_alias}.json\n",
    "    \n",
    "    Args:\n",
    "        config: Dictionary with keys: mode, question_type, provider, model, \n",
    "                temperature, template_alias\n",
    "    \n",
    "    Returns:\n",
    "        Filename string (e.g., 'closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json')\n",
    "    \n",
    "    Example:\n",
    "        >>> config = {\n",
    "        ...     'mode': 'closed_book',\n",
    "        ...     'question_type': 'metrics-generated',\n",
    "        ...     'provider': 'openai',\n",
    "        ...     'model': 'gpt-4o-mini',\n",
    "        ...     'temperature': '0.0',\n",
    "        ...     'template_alias': 'metrics_closed_basic'\n",
    "        ... }\n",
    "        >>> get_generation_filename(config)\n",
    "        'closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json'\n",
    "    \"\"\"\n",
    "    filename = (\n",
    "        f\"{config['mode']}_\"\n",
    "        f\"{config['question_type']}_\"\n",
    "        f\"{config['provider']}_\"\n",
    "        f\"{config['model']}_\"\n",
    "        f\"{config['temperature']}_\"\n",
    "        f\"{config['template_alias']}.json\"\n",
    "    )\n",
    "    return filename\n",
    "\n",
    "\n",
    "def get_evaluation_filename(config: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Construct evaluation file name from config.\n",
    "    \n",
    "    Format: evaluation_{mode}_{question_type}_{provider}_{model}_{temperature}_{template_alias}.json\n",
    "    \n",
    "    Args:\n",
    "        config: Dictionary with keys: mode, question_type, provider, model, \n",
    "                temperature, template_alias\n",
    "    \n",
    "    Returns:\n",
    "        Filename string (e.g., 'evaluation_closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json')\n",
    "    \n",
    "    Example:\n",
    "        >>> config = {\n",
    "        ...     'mode': 'closed_book',\n",
    "        ...     'question_type': 'metrics-generated',\n",
    "        ...     'provider': 'openai',\n",
    "        ...     'model': 'gpt-4o-mini',\n",
    "        ...     'temperature': '0.0',\n",
    "        ...     'template_alias': 'metrics_closed_basic'\n",
    "        ... }\n",
    "        >>> get_evaluation_filename(config)\n",
    "        'evaluation_closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json'\n",
    "    \"\"\"\n",
    "    filename = (\n",
    "        f\"evaluation_\"\n",
    "        f\"{config['mode']}_\"\n",
    "        f\"{config['question_type']}_\"\n",
    "        f\"{config['provider']}_\"\n",
    "        f\"{config['model']}_\"\n",
    "        f\"{config['temperature']}_\"\n",
    "        f\"{config['template_alias']}.json\"\n",
    "    )\n",
    "    return filename\n",
    "\n",
    "\n",
    "def get_generation_filepath(config: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Get full path to generation file.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Full path to generation JSON file\n",
    "    \n",
    "    Example:\n",
    "        >>> config = {'mode': 'closed_book', ...}\n",
    "        >>> get_generation_filepath(config)\n",
    "        '/path/to/generated_answers/closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json'\n",
    "    \"\"\"\n",
    "    filename = get_generation_filename(config)\n",
    "    filepath = os.path.join(GENERATION_PATH, filename)\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def get_evaluation_filepath(config: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Get full path to evaluation file.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Full path to evaluation JSON file\n",
    "    \n",
    "    Example:\n",
    "        >>> config = {'mode': 'closed_book', ...}\n",
    "        >>> get_evaluation_filepath(config)\n",
    "        '/path/to/evaluation_results/evaluation_closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json'\n",
    "    \"\"\"\n",
    "    filename = get_evaluation_filename(config)\n",
    "    filepath = os.path.join(EVALUATION_PATH, filename)\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def check_file_exists(filepath: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if file exists at given path.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Full path to file\n",
    "    \n",
    "    Returns:\n",
    "        True if file exists, False otherwise\n",
    "    \n",
    "    Example:\n",
    "        >>> check_file_exists('/path/to/file.json')\n",
    "        True\n",
    "    \"\"\"\n",
    "    return os.path.exists(filepath) and os.path.isfile(filepath)\n",
    "\n",
    "\n",
    "def get_file_info(filepath: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get information about a file.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Full path to file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with file information:\n",
    "            - exists: bool\n",
    "            - size_bytes: int (if exists)\n",
    "            - size_kb: float (if exists)\n",
    "            - size_mb: float (if exists)\n",
    "    \n",
    "    Example:\n",
    "        >>> get_file_info('/path/to/file.json')\n",
    "        {'exists': True, 'size_bytes': 1024, 'size_kb': 1.0, 'size_mb': 0.001}\n",
    "    \"\"\"\n",
    "    info = {'exists': False}\n",
    "    \n",
    "    if check_file_exists(filepath):\n",
    "        size_bytes = os.path.getsize(filepath)\n",
    "        info['exists'] = True\n",
    "        info['size_bytes'] = size_bytes\n",
    "        info['size_kb'] = round(size_bytes / 1024, 2)\n",
    "        info['size_mb'] = round(size_bytes / (1024 * 1024), 3)\n",
    "    \n",
    "    return info\n",
    "\n",
    "# ============================================================================\n",
    "def test_step2():\n",
    "    \"\"\"Test Step 2: File path management functions.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 2: FILE PATH MANAGEMENT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Test with first configuration (closed_book metrics-generated)\n",
    "    test_config = EVALUATION_CONFIGS[0]\n",
    "    \n",
    "    print(\"\\nüß™ Testing with configuration:\")\n",
    "    print(f\"   Mode: {test_config['mode']}\")\n",
    "    print(f\"   Question Type: {test_config['question_type']}\")\n",
    "    print(f\"   Provider: {test_config['provider']}\")\n",
    "    print(f\"   Model: {test_config['model']}\")\n",
    "    print(f\"   Temperature: {test_config['temperature']}\")\n",
    "    print(f\"   Template: {test_config['template_alias']}\")\n",
    "    \n",
    "    # Test filename generation\n",
    "    print(\"\\nüìù Testing filename generation:\")\n",
    "    gen_filename = get_generation_filename(test_config)\n",
    "    eval_filename = get_evaluation_filename(test_config)\n",
    "    print(f\"   Generation: {gen_filename}\")\n",
    "    print(f\"   Evaluation: {eval_filename}\")\n",
    "    \n",
    "    # Test filepath generation\n",
    "    print(\"\\nüìÅ Testing filepath generation:\")\n",
    "    gen_filepath = get_generation_filepath(test_config)\n",
    "    eval_filepath = get_evaluation_filepath(test_config)\n",
    "    print(f\"   Generation: {gen_filepath}\")\n",
    "    print(f\"   Evaluation: {eval_filepath}\")\n",
    "    \n",
    "    # Test file existence\n",
    "    print(\"\\nüîç Checking file existence:\")\n",
    "    gen_exists = check_file_exists(gen_filepath)\n",
    "    eval_exists = check_file_exists(eval_filepath)\n",
    "    print(f\"   Generation file exists: {gen_exists}\")\n",
    "    print(f\"   Evaluation file exists: {eval_exists}\")\n",
    "    \n",
    "    # Get file info if exists\n",
    "    if gen_exists:\n",
    "        gen_info = get_file_info(gen_filepath)\n",
    "        print(f\"   Generation file size: {gen_info['size_kb']} KB\")\n",
    "    \n",
    "    if eval_exists:\n",
    "        eval_info = get_file_info(eval_filepath)\n",
    "        print(f\"   Evaluation file size: {eval_info['size_kb']} KB\")\n",
    "    \n",
    "    # Test all 9 configurations\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã Testing all 9 configurations:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for i, config in enumerate(EVALUATION_CONFIGS, 1):\n",
    "        gen_filepath = get_generation_filepath(config)\n",
    "        eval_filepath = get_evaluation_filepath(config)\n",
    "        gen_exists = check_file_exists(gen_filepath)\n",
    "        eval_exists = check_file_exists(eval_filepath)\n",
    "        \n",
    "        mode = config['mode']\n",
    "        qtype = config['question_type']\n",
    "        \n",
    "        print(f\"\\n[{i}/9] {mode:12s} | {qtype}\")\n",
    "        print(f\"   Generation file: {os.path.basename(gen_filepath)}\")\n",
    "        print(f\"   ‚îî‚îÄ Exists: {'‚úì' if gen_exists else '‚úó'}\")\n",
    "        \n",
    "        if gen_exists:\n",
    "            info = get_file_info(gen_filepath)\n",
    "            print(f\"   ‚îî‚îÄ Size: {info['size_kb']} KB\")\n",
    "        \n",
    "        print(f\"   Evaluation file: {os.path.basename(eval_filepath)}\")\n",
    "        print(f\"   ‚îî‚îÄ Exists: {'‚úì' if eval_exists else '‚úó'}\")\n",
    "        \n",
    "        if eval_exists:\n",
    "            info = get_file_info(eval_filepath)\n",
    "            print(f\"   ‚îî‚îÄ Size: {info['size_kb']} KB\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    gen_count = sum(1 for c in EVALUATION_CONFIGS if check_file_exists(get_generation_filepath(c)))\n",
    "    eval_count = sum(1 for c in EVALUATION_CONFIGS if check_file_exists(get_evaluation_filepath(c)))\n",
    "    \n",
    "    print(\"\\nüìä Summary:\")\n",
    "    print(f\"   Total configurations: {len(EVALUATION_CONFIGS)}\")\n",
    "    print(f\"   Generation files found: {gen_count}/{len(EVALUATION_CONFIGS)}\")\n",
    "    print(f\"   Evaluation files found: {eval_count}/{len(EVALUATION_CONFIGS)}\")\n",
    "    \n",
    "    if gen_count == 0:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: No generation files found!\")\n",
    "        print(\"   Please check:\")\n",
    "        print(f\"   1. GENERATION_PATH is correct: {GENERATION_PATH}\")\n",
    "        print(\"   2. Files exist in that directory\")\n",
    "        print(\"   3. Filenames match the expected pattern\")\n",
    "    elif gen_count < len(EVALUATION_CONFIGS):\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Only {gen_count}/{len(EVALUATION_CONFIGS)} generation files found\")\n",
    "        print(\"   Some configurations are missing generation files\")\n",
    "    else:\n",
    "        print(\"\\n‚úì All generation files found!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Step 2 Complete!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nStep 2 Functions Implemented:\")\n",
    "    print(\"  ‚úì get_generation_filename(config)\")\n",
    "    print(\"  ‚úì get_evaluation_filename(config)\")\n",
    "    print(\"  ‚úì get_generation_filepath(config)\")\n",
    "    print(\"  ‚úì get_evaluation_filepath(config)\")\n",
    "    print(\"  ‚úì check_file_exists(filepath)\")\n",
    "    print(\"  ‚úì get_file_info(filepath)\")\n",
    "    \n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  1. Verify filenames match your actual files\")\n",
    "    print(\"  2. Confirm generation files are detected correctly\")\n",
    "    print(\"  3. Report back for approval to proceed to Step 3\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "test_step2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bcee211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: JSON FILE READER\n",
      "======================================================================\n",
      "\n",
      "üß™ Testing with: closed_book - metrics-generated\n",
      "   File: closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "\n",
      "üìñ Reading file...\n",
      "   ‚úì File read successfully!\n",
      "\n",
      "üìã Metadata:\n",
      "   Mode: closed_book\n",
      "   Question Type: metrics-generated\n",
      "   Provider: openai\n",
      "   Model: gpt-4o-mini\n",
      "   Temperature: 0.0\n",
      "   Total Questions: 50\n",
      "\n",
      "üìù Queries:\n",
      "   Count: 50\n",
      "\n",
      "   First Query:\n",
      "   ‚îú‚îÄ ID: financebench_id_03029\n",
      "   ‚îú‚îÄ Question: What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a r...\n",
      "   ‚îú‚îÄ Gold Answer: $1577.00...\n",
      "   ‚îî‚îÄ Generated Answer: $1,700 million...\n",
      "\n",
      "‚úÖ Validation Passed:\n",
      "   ‚úì All required top-level fields present\n",
      "   ‚úì All required metadata fields present\n",
      "   ‚úì Queries is a list with 50 items\n",
      "   ‚úì All queries have required fields\n",
      "   ‚úì All answers are strings\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üß™ Testing error handling with non-existent file...\n",
      "   ‚úì Correctly raised FileNotFoundError\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Step 3 Complete!\n",
      "======================================================================\n",
      "\n",
      "Step 3 Function Implemented:\n",
      "  ‚úì read_generation_file(filepath)\n",
      "\n",
      "Features:\n",
      "  ‚úì File existence check\n",
      "  ‚úì JSON parsing\n",
      "  ‚úì Structure validation\n",
      "  ‚úì Required fields validation\n",
      "  ‚úì Data type validation\n",
      "  ‚úì Comprehensive error messages\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: JSON File Reader\n",
    "# ============================================================================\n",
    "\n",
    "def read_generation_file(filepath: str) -> dict:\n",
    "    \"\"\"\n",
    "    Read generation JSON file and return parsed data.\n",
    "    Validates structure and required fields.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Full path to generation JSON file\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with parsed JSON data\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If file doesn't exist\n",
    "        json.JSONDecodeError: If file is not valid JSON\n",
    "        ValueError: If required fields are missing\n",
    "    \n",
    "    Example:\n",
    "        >>> data = read_generation_file('/path/to/closed_book_metrics-generated_...')\n",
    "        >>> print(f\"Mode: {data['metadata']['mode']}\")\n",
    "        >>> print(f\"Questions: {len(data['queries'])}\")\n",
    "    \"\"\"\n",
    "    # Check file exists\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Generation file not found: {filepath}\\n\"\n",
    "            f\"Please check:\\n\"\n",
    "            f\"  1. GENERATION_PATH is correct\\n\"\n",
    "            f\"  2. File exists in the directory\\n\"\n",
    "            f\"  3. Filename matches the expected pattern\"\n",
    "        )\n",
    "    \n",
    "    # Read and parse JSON\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise json.JSONDecodeError(\n",
    "            f\"Failed to parse JSON file: {filepath}\\n\"\n",
    "            f\"Error: {str(e)}\\n\"\n",
    "            f\"The file may be corrupted or not valid JSON.\",\n",
    "            e.doc, e.pos\n",
    "        )\n",
    "    \n",
    "    # Validate required top-level fields\n",
    "    required_top_fields = ['metadata', 'queries']\n",
    "    missing_top = [field for field in required_top_fields if field not in data]\n",
    "    if missing_top:\n",
    "        raise ValueError(\n",
    "            f\"Generation file missing required top-level fields: {missing_top}\\n\"\n",
    "            f\"File: {filepath}\\n\"\n",
    "            f\"Required fields: {required_top_fields}\"\n",
    "        )\n",
    "    \n",
    "    # Validate metadata fields\n",
    "    required_metadata_fields = ['mode', 'question_type', 'provider', 'model', \n",
    "                                 'temperature', 'total_questions']\n",
    "    metadata = data.get('metadata', {})\n",
    "    missing_metadata = [field for field in required_metadata_fields \n",
    "                        if field not in metadata]\n",
    "    if missing_metadata:\n",
    "        raise ValueError(\n",
    "            f\"Generation file metadata missing required fields: {missing_metadata}\\n\"\n",
    "            f\"File: {filepath}\\n\"\n",
    "            f\"Required metadata fields: {required_metadata_fields}\"\n",
    "        )\n",
    "    \n",
    "    # Validate queries is a list\n",
    "    queries = data.get('queries', [])\n",
    "    if not isinstance(queries, list):\n",
    "        raise ValueError(\n",
    "            f\"Field 'queries' must be a list, got {type(queries).__name__}\\n\"\n",
    "            f\"File: {filepath}\"\n",
    "        )\n",
    "    \n",
    "    # Validate queries is not empty\n",
    "    if len(queries) == 0:\n",
    "        raise ValueError(\n",
    "            f\"Field 'queries' is empty (no questions to evaluate)\\n\"\n",
    "            f\"File: {filepath}\"\n",
    "        )\n",
    "    \n",
    "    # Validate each query has required fields\n",
    "    required_query_fields = ['financebench_id', 'question', 'answer', \n",
    "                             'generated_answer', 'question_type']\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        missing_query = [field for field in required_query_fields \n",
    "                         if field not in query]\n",
    "        if missing_query:\n",
    "            raise ValueError(\n",
    "                f\"Query #{i+1} missing required fields: {missing_query}\\n\"\n",
    "                f\"File: {filepath}\\n\"\n",
    "                f\"Query ID: {query.get('financebench_id', 'UNKNOWN')}\\n\"\n",
    "                f\"Required query fields: {required_query_fields}\"\n",
    "            )\n",
    "        \n",
    "        # Validate that answer and generated_answer are strings\n",
    "        if not isinstance(query.get('answer'), str):\n",
    "            raise ValueError(\n",
    "                f\"Query #{i+1} 'answer' must be a string\\n\"\n",
    "                f\"File: {filepath}\\n\"\n",
    "                f\"Query ID: {query.get('financebench_id', 'UNKNOWN')}\"\n",
    "            )\n",
    "        \n",
    "        if not isinstance(query.get('generated_answer'), str):\n",
    "            raise ValueError(\n",
    "                f\"Query #{i+1} 'generated_answer' must be a string\\n\"\n",
    "                f\"File: {filepath}\\n\"\n",
    "                f\"Query ID: {query.get('financebench_id', 'UNKNOWN')}\"\n",
    "            )\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3 Test Function\n",
    "# ============================================================================\n",
    "\n",
    "def test_step3():\n",
    "    \"\"\"Test Step 3: JSON file reading and validation.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 3: JSON FILE READER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Test with first configuration\n",
    "    test_config = EVALUATION_CONFIGS[0]\n",
    "    gen_filepath = get_generation_filepath(test_config)\n",
    "    \n",
    "    print(f\"\\nüß™ Testing with: {test_config['mode']} - {test_config['question_type']}\")\n",
    "    print(f\"   File: {os.path.basename(gen_filepath)}\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not check_file_exists(gen_filepath):\n",
    "        print(f\"\\n‚ö†Ô∏è  File not found: {gen_filepath}\")\n",
    "        print(\"   Cannot test JSON reading without a file.\")\n",
    "        print(\"\\n   To test Step 3:\")\n",
    "        print(\"   1. Ensure GENERATION_PATH points to your actual directory\")\n",
    "        print(\"   2. Ensure at least one generation file exists\")\n",
    "        print(\"   3. Run this test again\")\n",
    "        return\n",
    "    \n",
    "    # Try to read the file\n",
    "    print(f\"\\nüìñ Reading file...\")\n",
    "    try:\n",
    "        data = read_generation_file(gen_filepath)\n",
    "        print(f\"   ‚úì File read successfully!\")\n",
    "        \n",
    "        # Display metadata\n",
    "        metadata = data['metadata']\n",
    "        print(f\"\\nüìã Metadata:\")\n",
    "        print(f\"   Mode: {metadata['mode']}\")\n",
    "        print(f\"   Question Type: {metadata['question_type']}\")\n",
    "        print(f\"   Provider: {metadata['provider']}\")\n",
    "        print(f\"   Model: {metadata['model']}\")\n",
    "        print(f\"   Temperature: {metadata['temperature']}\")\n",
    "        print(f\"   Total Questions: {metadata['total_questions']}\")\n",
    "        \n",
    "        # Display queries info\n",
    "        queries = data['queries']\n",
    "        print(f\"\\nüìù Queries:\")\n",
    "        print(f\"   Count: {len(queries)}\")\n",
    "        \n",
    "        # Show first query\n",
    "        if len(queries) > 0:\n",
    "            first_query = queries[0]\n",
    "            print(f\"\\n   First Query:\")\n",
    "            print(f\"   ‚îú‚îÄ ID: {first_query['financebench_id']}\")\n",
    "            print(f\"   ‚îú‚îÄ Question: {first_query['question'][:80]}...\")\n",
    "            print(f\"   ‚îú‚îÄ Gold Answer: {first_query['answer'][:60]}...\")\n",
    "            print(f\"   ‚îî‚îÄ Generated Answer: {first_query['generated_answer'][:60]}...\")\n",
    "        \n",
    "        # Validation summary\n",
    "        print(f\"\\n‚úÖ Validation Passed:\")\n",
    "        print(f\"   ‚úì All required top-level fields present\")\n",
    "        print(f\"   ‚úì All required metadata fields present\")\n",
    "        print(f\"   ‚úì Queries is a list with {len(queries)} items\")\n",
    "        print(f\"   ‚úì All queries have required fields\")\n",
    "        print(f\"   ‚úì All answers are strings\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n‚úó Error: File not found\")\n",
    "        print(f\"   {str(e)}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"\\n‚úó Error: Invalid JSON\")\n",
    "        print(f\"   {str(e)}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"\\n‚úó Error: Validation failed\")\n",
    "        print(f\"   {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Unexpected error: {type(e).__name__}\")\n",
    "        print(f\"   {str(e)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    # Test with non-existent file\n",
    "    print(\"\\nüß™ Testing error handling with non-existent file...\")\n",
    "    fake_path = \"/path/to/nonexistent/file.json\"\n",
    "    try:\n",
    "        read_generation_file(fake_path)\n",
    "        print(\"   ‚úó Should have raised FileNotFoundError!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"   ‚úì Correctly raised FileNotFoundError\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Step 3 Complete!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nStep 3 Function Implemented:\")\n",
    "    print(\"  ‚úì read_generation_file(filepath)\")\n",
    "    print(\"\\nFeatures:\")\n",
    "    print(\"  ‚úì File existence check\")\n",
    "    print(\"  ‚úì JSON parsing\")\n",
    "    print(\"  ‚úì Structure validation\")\n",
    "    print(\"  ‚úì Required fields validation\")\n",
    "    print(\"  ‚úì Data type validation\")\n",
    "    print(\"  ‚úì Comprehensive error messages\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "# Test Step 3\n",
    "test_step3()  # Add this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86984e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4: SINGLE QUERY EVALUATOR\n",
      "======================================================================\n",
      "\n",
      "üß™ Testing with: closed_book - metrics-generated\n",
      "   File: closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "   ‚úì Loaded 50 queries\n",
      "\n",
      "üìù Testing evaluation on 2 queries...\n",
      "\n",
      "   Query 1/2: financebench_id_03029\n",
      "   Question: What is the FY2018 capital expenditure amount (in USD millio...\n",
      "   ‚úì Evaluation successful!\n",
      "      Retries needed: 0\n",
      "      Refusal detected: False\n",
      "      Metrics computed: numerical_exact_match, llm_as_judge_binary\n",
      "      NEM match: False\n",
      "      LLM binary match: False\n",
      "\n",
      "   Query 2/2: financebench_id_04672\n",
      "   Question: Assume that you are a public equities analyst. Answer the fo...\n",
      "   ‚úì Evaluation successful!\n",
      "      Retries needed: 0\n",
      "      Refusal detected: False\n",
      "      Metrics computed: numerical_exact_match, llm_as_judge_binary\n",
      "      NEM match: False\n",
      "      LLM binary match: False\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Step 4 Complete!\n",
      "======================================================================\n",
      "\n",
      "Step 4 Function Implemented:\n",
      "  ‚úì evaluate_single_query(query, question_type, eval_config)\n",
      "\n",
      "Features:\n",
      "  ‚úì Calls evaluate_answer() from your suite\n",
      "  ‚úì Retry logic for API errors\n",
      "  ‚úì Max retries: 3\n",
      "  ‚úì Retry delay: 10s\n",
      "  ‚úì Comprehensive error messages\n",
      "  ‚úì Returns structured result with evaluation details\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: Single Query Evaluator\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_single_query(\n",
    "    query: Dict[str, Any],\n",
    "    question_type: str,\n",
    "    eval_config: Dict[str, Any],\n",
    "    retry_delay: int = 10,\n",
    "    max_retries: int = 3\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a single query using evaluate_answer() with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        query: Query dictionary with keys: financebench_id, question, answer, \n",
    "               generated_answer, question_type\n",
    "        question_type: Question type for routing ('metrics-generated', \n",
    "                      'novel-generated', 'domain-relevant')\n",
    "        eval_config: Evaluation configuration dictionary\n",
    "        retry_delay: Seconds to wait before retrying after API error (default: 10)\n",
    "        max_retries: Maximum number of retry attempts (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - financebench_id: Query ID\n",
    "            - question: The question\n",
    "            - gold_answer: Gold standard answer\n",
    "            - generated_answer: Generated answer\n",
    "            - question_type: Question type\n",
    "            - evaluation: Full evaluation result from evaluate_answer()\n",
    "            - evaluation_success: Boolean indicating if evaluation succeeded\n",
    "            - retry_count: Number of retries needed\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If evaluation fails after max_retries attempts\n",
    "    \n",
    "    Example:\n",
    "        >>> query = {\n",
    "        ...     'financebench_id': 'test_001',\n",
    "        ...     'question': 'What is the revenue?',\n",
    "        ...     'answer': '$100 million',\n",
    "        ...     'generated_answer': '$100M',\n",
    "        ...     'question_type': 'metrics-generated'\n",
    "        ... }\n",
    "        >>> result = evaluate_single_query(query, 'metrics-generated', EVALUATION_CONFIG)\n",
    "        >>> print(result['evaluation']['summary']['metrics_computed'])\n",
    "    \"\"\"\n",
    "    # Extract required fields\n",
    "    financebench_id = query['financebench_id']\n",
    "    question = query['question']\n",
    "    gold_answer = query['answer']\n",
    "    generated_answer = query['generated_answer']\n",
    "    \n",
    "    # Attempt evaluation with retry logic\n",
    "    retry_count = 0\n",
    "    last_error = None\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Call evaluate_answer from your evaluation suite\n",
    "            evaluation_result = evaluate_answer(\n",
    "                question=question,\n",
    "                question_type=question_type,\n",
    "                gold_answer=gold_answer,\n",
    "                generated_answer=generated_answer,\n",
    "                config=eval_config\n",
    "            )\n",
    "            \n",
    "            # Success! Return the result\n",
    "            return {\n",
    "                'financebench_id': financebench_id,\n",
    "                'question': question,\n",
    "                'gold_answer': gold_answer,\n",
    "                'generated_answer': generated_answer,\n",
    "                'question_type': question_type,\n",
    "                'evaluation': evaluation_result,\n",
    "                'evaluation_success': True,\n",
    "                'retry_count': retry_count\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            retry_count += 1\n",
    "            last_error = e\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Check if it's an API error that should be retried\n",
    "            is_api_error = any(keyword in error_msg.lower() \n",
    "                              for keyword in ['rate limit', 'api', 'timeout', \n",
    "                                            'connection', 'openai'])\n",
    "            \n",
    "            if is_api_error and attempt < max_retries - 1:\n",
    "                # Log the retry\n",
    "                print(f\"   ‚ö†Ô∏è  API error on attempt {attempt + 1}/{max_retries}\")\n",
    "                print(f\"      Error: {error_msg[:100]}...\")\n",
    "                print(f\"      Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                # Either not an API error, or we've exhausted retries\n",
    "                break\n",
    "    \n",
    "    # If we get here, all retries failed\n",
    "    error_message = (\n",
    "        f\"Evaluation failed for query {financebench_id} after {max_retries} attempts.\\n\"\n",
    "        f\"Question: {question[:100]}...\\n\"\n",
    "        f\"Last error: {str(last_error)}\"\n",
    "    )\n",
    "    \n",
    "    raise Exception(error_message)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4 Test Function\n",
    "# ============================================================================\n",
    "\n",
    "def test_step4():\n",
    "    \"\"\"Test Step 4: Single query evaluation with retry logic.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 4: SINGLE QUERY EVALUATOR\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check if evaluation functions are available\n",
    "    if not EVAL_FUNCTIONS_AVAILABLE:\n",
    "        print(\"\\n‚ö†Ô∏è  Evaluation functions not available\")\n",
    "        print(\"   Step 4 requires the evaluation suite modules:\")\n",
    "        print(\"   - evaluate_answer.py\")\n",
    "        print(\"   - detect_refusal.py\")\n",
    "        print(\"   - numerical_exact_match.py\")\n",
    "        print(\"   - token_f1.py\")\n",
    "        print(\"   - llm_as_judge_binary.py\")\n",
    "        print(\"   - llm_as_judge_graded.py\")\n",
    "        print(\"\\n   Cannot test Step 4 without these modules.\")\n",
    "        return\n",
    "    \n",
    "    # Find a generation file to test with\n",
    "    test_config = None\n",
    "    test_filepath = None\n",
    "    \n",
    "    for config in EVALUATION_CONFIGS:\n",
    "        filepath = get_generation_filepath(config)\n",
    "        if check_file_exists(filepath):\n",
    "            test_config = config\n",
    "            test_filepath = filepath\n",
    "            break\n",
    "    \n",
    "    if test_config is None:\n",
    "        print(\"\\n‚ö†Ô∏è  No generation files found\")\n",
    "        print(f\"   Checked in: {GENERATION_PATH}\")\n",
    "        print(\"   Cannot test Step 4 without a generation file.\")\n",
    "        return\n",
    "    \n",
    "    # Read the file\n",
    "    print(f\"\\nüß™ Testing with: {test_config['mode']} - {test_config['question_type']}\")\n",
    "    print(f\"   File: {os.path.basename(test_filepath)}\")\n",
    "    \n",
    "    try:\n",
    "        data = read_generation_file(test_filepath)\n",
    "        queries = data['queries']\n",
    "        question_type = data['metadata']['question_type']\n",
    "        \n",
    "        print(f\"   ‚úì Loaded {len(queries)} queries\")\n",
    "        \n",
    "        # Test with first 2 queries\n",
    "        num_test_queries = min(2, len(queries))\n",
    "        print(f\"\\nüìù Testing evaluation on {num_test_queries} queries...\")\n",
    "        \n",
    "        for i in range(num_test_queries):\n",
    "            query = queries[i]\n",
    "            query_id = query['financebench_id']\n",
    "            \n",
    "            print(f\"\\n   Query {i+1}/{num_test_queries}: {query_id}\")\n",
    "            print(f\"   Question: {query['question'][:60]}...\")\n",
    "            \n",
    "            try:\n",
    "                result = evaluate_single_query(\n",
    "                    query=query,\n",
    "                    question_type=question_type,\n",
    "                    eval_config=EVALUATION_CONFIG,\n",
    "                    retry_delay=RETRY_DELAY,\n",
    "                    max_retries=MAX_RETRIES\n",
    "                )\n",
    "                \n",
    "                print(f\"   ‚úì Evaluation successful!\")\n",
    "                print(f\"      Retries needed: {result['retry_count']}\")\n",
    "                \n",
    "                # Show evaluation summary\n",
    "                eval_summary = result['evaluation']['summary']\n",
    "                print(f\"      Refusal detected: {eval_summary['refusal_detected']}\")\n",
    "                print(f\"      Metrics computed: {', '.join(eval_summary['metrics_computed'])}\")\n",
    "                \n",
    "                # Show some metric results based on question type\n",
    "                metrics = result['evaluation']['metrics']\n",
    "                \n",
    "                if question_type == 'metrics-generated':\n",
    "                    nem = metrics.get('numerical_exact_match', {})\n",
    "                    llm_bin = metrics.get('llm_as_judge_binary', {})\n",
    "                    print(f\"      NEM match: {nem.get('match', 'N/A')}\")\n",
    "                    print(f\"      LLM binary match: {llm_bin.get('match', 'N/A')}\")\n",
    "                    \n",
    "                elif question_type == 'novel-generated':\n",
    "                    f1 = metrics.get('token_f1', {})\n",
    "                    llm_grade = metrics.get('llm_as_judge_graded', {})\n",
    "                    print(f\"      Token F1: {f1.get('f1', 'N/A'):.3f}\")\n",
    "                    print(f\"      LLM grade: {llm_grade.get('score', 'N/A')}/4\")\n",
    "                    \n",
    "                elif question_type == 'domain-relevant':\n",
    "                    llm_grade = metrics.get('llm_as_judge_graded', {})\n",
    "                    print(f\"      LLM grade: {llm_grade.get('score', 'N/A')}/4\")\n",
    "                \n",
    "                # Add delay between queries (simulating batch processing)\n",
    "                if i < num_test_queries - 1:\n",
    "                    time.sleep(CALL_DELAY)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚úó Evaluation failed: {str(e)[:100]}...\")\n",
    "                raise  # Re-raise to stop the process\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ Step 4 Complete!\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nStep 4 Function Implemented:\")\n",
    "        print(\"  ‚úì evaluate_single_query(query, question_type, eval_config)\")\n",
    "        print(\"\\nFeatures:\")\n",
    "        print(\"  ‚úì Calls evaluate_answer() from your suite\")\n",
    "        print(\"  ‚úì Retry logic for API errors\")\n",
    "        print(f\"  ‚úì Max retries: {MAX_RETRIES}\")\n",
    "        print(f\"  ‚úì Retry delay: {RETRY_DELAY}s\")\n",
    "        print(\"  ‚úì Comprehensive error messages\")\n",
    "        print(\"  ‚úì Returns structured result with evaluation details\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error during testing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# # Test Step 4\n",
    "test_step4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96229fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 5: BATCH FILE EVALUATOR\n",
      "======================================================================\n",
      "\n",
      "üß™ Testing with: closed_book - metrics-generated\n",
      "   Generation file: closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "   Evaluation file: evaluation_closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "\n",
      "‚ö†Ô∏è  This will evaluate ALL queries in the file!\n",
      "   Estimated cost: ~$0.03 for 50 queries\n",
      "   Estimated time: ~1-2 minutes\n",
      "   Cancelled. Skipping Step 5 test.\n",
      "\n",
      "   To test Step 5:\n",
      "   1. Ensure you're ready to evaluate a full file\n",
      "   2. Run test_step5() again and type 'yes'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: Batch File Evaluator\n",
    "# ============================================================================\n",
    "\n",
    "def make_json_serializable(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert non-serializable objects to JSON-serializable types.\n",
    "    \n",
    "    Args:\n",
    "        obj: Object to convert (dict, list, set, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        JSON-serializable version of the object\n",
    "    \"\"\"\n",
    "    if isinstance(obj, set):\n",
    "        return list(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: make_json_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [make_json_serializable(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def evaluate_generation_file(\n",
    "    generation_filepath: str,\n",
    "    evaluation_filepath: str,\n",
    "    eval_config: Dict[str, Any],\n",
    "    call_delay: float = 0.5\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process entire generation file and save evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        generation_filepath: Full path to generation JSON file\n",
    "        evaluation_filepath: Full path where evaluation results will be saved\n",
    "        eval_config: Evaluation configuration dictionary\n",
    "        call_delay: Seconds to wait between query evaluations (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing summary statistics:\n",
    "            - total_queries: Total number of queries evaluated\n",
    "            - successful_evaluations: Number of successful evaluations\n",
    "            - failed_evaluations: Number of failed evaluations\n",
    "            - total_time_seconds: Total processing time\n",
    "            - average_time_per_query: Average time per query\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If any query evaluation fails (stops entire process)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read generation file\n",
    "    print(f\"\\nüìñ Reading generation file...\")\n",
    "    data = read_generation_file(generation_filepath)\n",
    "    \n",
    "    metadata = data['metadata']\n",
    "    queries = data['queries']\n",
    "    question_type = metadata['question_type']\n",
    "    \n",
    "    print(f\"   ‚úì Loaded {len(queries)} queries\")\n",
    "    print(f\"   Mode: {metadata['mode']}\")\n",
    "    print(f\"   Question Type: {question_type}\")\n",
    "    \n",
    "    # Initialize results structure\n",
    "    evaluation_results = {\n",
    "        'metadata': {\n",
    "            'original_file': os.path.basename(generation_filepath),\n",
    "            'mode': metadata['mode'],\n",
    "            'question_type': metadata['question_type'],\n",
    "            'provider': metadata['provider'],\n",
    "            'model': metadata['model'],\n",
    "            'temperature': metadata['temperature'],\n",
    "            'template_alias': metadata.get('template_alias', 'N/A'),\n",
    "            'evaluated_at': datetime.now().isoformat(),\n",
    "            'evaluation_config': eval_config.copy(),\n",
    "            'total_questions': len(queries),\n",
    "            'total_evaluated': 0,\n",
    "            'evaluation_success': True\n",
    "        },\n",
    "        'results': []\n",
    "    }\n",
    "    \n",
    "    # Process queries with progress bar\n",
    "    print(f\"\\nüîÑ Evaluating {len(queries)} queries...\")\n",
    "    \n",
    "    successful_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    # Create progress bar\n",
    "    progress_bar = tqdm(\n",
    "        queries,\n",
    "        desc=f\"Processing {metadata['mode']}_{question_type}\",\n",
    "        unit=\"query\",\n",
    "        ncols=100\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        for i, query in enumerate(progress_bar):\n",
    "            query_id = query['financebench_id']\n",
    "            \n",
    "            # Update progress bar description with current query\n",
    "            progress_bar.set_postfix_str(f\"ID: {query_id}\")\n",
    "            \n",
    "            try:\n",
    "                # Evaluate single query\n",
    "                result = evaluate_single_query(\n",
    "                    query=query,\n",
    "                    question_type=question_type,\n",
    "                    eval_config=eval_config,\n",
    "                    retry_delay=RETRY_DELAY,\n",
    "                    max_retries=MAX_RETRIES\n",
    "                )\n",
    "                \n",
    "                # Add to results\n",
    "                evaluation_results['results'].append(result)\n",
    "                successful_count += 1\n",
    "                \n",
    "                # Delay before next query (except for last query)\n",
    "                if i < len(queries) - 1:\n",
    "                    time.sleep(call_delay)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Evaluation failed - stop the entire process\n",
    "                failed_count += 1\n",
    "                progress_bar.close()\n",
    "                \n",
    "                error_msg = (\n",
    "                    f\"\\n{'='*70}\\n\"\n",
    "                    f\"‚ùå EVALUATION FAILED - STOPPING PROCESS\\n\"\n",
    "                    f\"{'='*70}\\n\"\n",
    "                    f\"Query ID: {query_id}\\n\"\n",
    "                    f\"Query #{i+1}/{len(queries)}\\n\"\n",
    "                    f\"Question: {query['question'][:100]}...\\n\"\n",
    "                    f\"Error: {str(e)}\\n\"\n",
    "                    f\"{'='*70}\\n\"\n",
    "                )\n",
    "                print(error_msg)\n",
    "                raise\n",
    "        \n",
    "        progress_bar.close()\n",
    "        \n",
    "        # Update metadata with final counts\n",
    "        evaluation_results['metadata']['total_evaluated'] = successful_count\n",
    "        evaluation_results['metadata']['evaluation_success'] = (failed_count == 0)\n",
    "        \n",
    "        # Calculate timing\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        avg_time = total_time / len(queries) if len(queries) > 0 else 0\n",
    "        \n",
    "        # Save evaluation results\n",
    "        print(f\"\\nüíæ Saving evaluation results...\")\n",
    "        print(f\"   Output: {evaluation_filepath}\")\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(os.path.dirname(evaluation_filepath), exist_ok=True)\n",
    "        \n",
    "        # Convert sets to lists for JSON serialization\n",
    "        evaluation_results_serializable = make_json_serializable(evaluation_results)\n",
    "        \n",
    "        with open(evaluation_filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(evaluation_results_serializable, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        file_info = get_file_info(evaluation_filepath)\n",
    "        print(f\"   ‚úì Saved successfully ({file_info['size_kb']} KB)\")\n",
    "        \n",
    "        # Return summary\n",
    "        summary = {\n",
    "            'total_queries': len(queries),\n",
    "            'successful_evaluations': successful_count,\n",
    "            'failed_evaluations': failed_count,\n",
    "            'total_time_seconds': round(total_time, 2),\n",
    "            'average_time_per_query': round(avg_time, 2)\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Make sure progress bar is closed\n",
    "        if 'progress_bar' in locals():\n",
    "            progress_bar.close()\n",
    "        raise\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5 Test Function\n",
    "# ============================================================================\n",
    "\n",
    "def test_step5():\n",
    "    \"\"\"Test Step 5: Batch file evaluation.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 5: BATCH FILE EVALUATOR\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check if evaluation functions are available\n",
    "    if not EVAL_FUNCTIONS_AVAILABLE:\n",
    "        print(\"\\n‚ö†Ô∏è  Evaluation functions not available\")\n",
    "        print(\"   Cannot test Step 5 without evaluation suite modules.\")\n",
    "        return\n",
    "    \n",
    "    # Find a generation file to test with\n",
    "    test_config = None\n",
    "    test_gen_filepath = None\n",
    "    \n",
    "    for config in EVALUATION_CONFIGS:\n",
    "        gen_filepath = get_generation_filepath(config)\n",
    "        eval_filepath = get_evaluation_filepath(config)\n",
    "        \n",
    "        # Use a file that hasn't been evaluated yet (or use first available)\n",
    "        if check_file_exists(gen_filepath):\n",
    "            test_config = config\n",
    "            test_gen_filepath = gen_filepath\n",
    "            break\n",
    "    \n",
    "    if test_config is None:\n",
    "        print(\"\\n‚ö†Ô∏è  No generation files found\")\n",
    "        print(f\"   Checked in: {GENERATION_PATH}\")\n",
    "        print(\"   Cannot test Step 5 without a generation file.\")\n",
    "        return\n",
    "    \n",
    "    test_eval_filepath = get_evaluation_filepath(test_config)\n",
    "    \n",
    "    print(f\"\\nüß™ Testing with: {test_config['mode']} - {test_config['question_type']}\")\n",
    "    print(f\"   Generation file: {os.path.basename(test_gen_filepath)}\")\n",
    "    print(f\"   Evaluation file: {os.path.basename(test_eval_filepath)}\")\n",
    "    \n",
    "    # Ask for confirmation (since this will process all 50 queries and cost money)\n",
    "    print(f\"\\n‚ö†Ô∏è  This will evaluate ALL queries in the file!\")\n",
    "    print(f\"   Estimated cost: ~$0.03 for 50 queries\")\n",
    "    print(f\"   Estimated time: ~1-2 minutes\")\n",
    "    \n",
    "    response = input(\"\\n   Proceed with full evaluation? (yes/no): \")\n",
    "    if response.lower() != 'yes':\n",
    "        print(\"   Cancelled. Skipping Step 5 test.\")\n",
    "        print(\"\\n   To test Step 5:\")\n",
    "        print(\"   1. Ensure you're ready to evaluate a full file\")\n",
    "        print(\"   2. Run test_step5() again and type 'yes'\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Run batch evaluation\n",
    "        summary = evaluate_generation_file(\n",
    "            generation_filepath=test_gen_filepath,\n",
    "            evaluation_filepath=test_eval_filepath,\n",
    "            eval_config=EVALUATION_CONFIG,\n",
    "            call_delay=CALL_DELAY\n",
    "        )\n",
    "        \n",
    "        # Display summary\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìä EVALUATION SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"   Total queries: {summary['total_queries']}\")\n",
    "        print(f\"   Successful: {summary['successful_evaluations']}\")\n",
    "        print(f\"   Failed: {summary['failed_evaluations']}\")\n",
    "        print(f\"   Total time: {summary['total_time_seconds']}s\")\n",
    "        print(f\"   Avg time per query: {summary['average_time_per_query']}s\")\n",
    "        \n",
    "        # Verify evaluation file was created\n",
    "        if check_file_exists(test_eval_filepath):\n",
    "            file_info = get_file_info(test_eval_filepath)\n",
    "            print(f\"\\n‚úì Evaluation file created: {file_info['size_kb']} KB\")\n",
    "            \n",
    "            # Read and show sample results\n",
    "            with open(test_eval_filepath, 'r', encoding='utf-8') as f:\n",
    "                eval_data = json.load(f)\n",
    "            \n",
    "            print(f\"\\nüìã Evaluation File Contents:\")\n",
    "            print(f\"   Metadata fields: {len(eval_data['metadata'])}\")\n",
    "            print(f\"   Results count: {len(eval_data['results'])}\")\n",
    "            \n",
    "            if len(eval_data['results']) > 0:\n",
    "                first_result = eval_data['results'][0]\n",
    "                print(f\"\\n   First result structure:\")\n",
    "                print(f\"   ‚îú‚îÄ financebench_id: {first_result['financebench_id']}\")\n",
    "                print(f\"   ‚îú‚îÄ question_type: {first_result['question_type']}\")\n",
    "                print(f\"   ‚îú‚îÄ evaluation_success: {first_result['evaluation_success']}\")\n",
    "                print(f\"   ‚îî‚îÄ evaluation metrics: {list(first_result['evaluation']['metrics'].keys())}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ Step 5 Complete!\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nStep 5 Function Implemented:\")\n",
    "        print(\"  ‚úì evaluate_generation_file(gen_path, eval_path, config)\")\n",
    "        print(\"\\nFeatures:\")\n",
    "        print(\"  ‚úì Reads generation file\")\n",
    "        print(\"  ‚úì Processes all queries with progress bar\")\n",
    "        print(\"  ‚úì Delays between queries\")\n",
    "        print(\"  ‚úì Stops on first error\")\n",
    "        print(\"  ‚úì Saves structured evaluation results\")\n",
    "        print(\"  ‚úì Returns summary statistics\")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Batch evaluation failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Test Step 5\n",
    "test_step5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfe8c5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 6: MAIN ORCHESTRATOR (DRY RUN)\n",
      "======================================================================\n",
      "\n",
      "‚ö†Ô∏è  This is a DRY RUN - showing what would happen without processing\n",
      "\n",
      "üìã Configuration: 9 files\n",
      "\n",
      "[1/9] closed_book  | metrics-generated\n",
      "   Action: SKIP (already evaluated)\n",
      "\n",
      "[2/9] closed_book  | novel-generated\n",
      "   Action: SKIP (already evaluated)\n",
      "\n",
      "[3/9] closed_book  | domain-relevant\n",
      "   Action: SKIP (already evaluated)\n",
      "\n",
      "[4/9] rag          | metrics-generated\n",
      "   Action: PROCESS\n",
      "\n",
      "[5/9] rag          | novel-generated\n",
      "   Action: FAIL (generation file not found)\n",
      "\n",
      "[6/9] rag          | domain-relevant\n",
      "   Action: FAIL (generation file not found)\n",
      "\n",
      "[7/9] oracle       | metrics-generated\n",
      "   Action: SKIP (already evaluated)\n",
      "\n",
      "[8/9] oracle       | novel-generated\n",
      "   Action: SKIP (already evaluated)\n",
      "\n",
      "[9/9] oracle       | domain-relevant\n",
      "   Action: SKIP (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "DRY RUN SUMMARY:\n",
      "======================================================================\n",
      "   Would skip: 6\n",
      "   Would process: 1\n",
      "   Would fail: 2\n",
      "\n",
      "   Estimated time: ~2 minutes\n",
      "   Estimated cost: ~$0.03\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Step 6 Complete!\n",
      "======================================================================\n",
      "\n",
      "Step 6 Function Implemented:\n",
      "  ‚úì main() - Full orchestration\n",
      "\n",
      "Features:\n",
      "  ‚úì Loops through all 9 configurations\n",
      "  ‚úì Skip logic for existing evaluation files\n",
      "  ‚úì Error handling for missing generation files\n",
      "  ‚úì Progress tracking for each file\n",
      "  ‚úì Comprehensive summary report\n",
      "  ‚úì Total time and cost tracking\n",
      "\n",
      "To run full evaluation:\n",
      "  main()  # This will process all files!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: Main Orchestrator\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main orchestration function to process all evaluation configurations.\n",
    "    \n",
    "    Features:\n",
    "    - Checks if evaluation file already exists (skip if yes)\n",
    "    - Checks if generation file exists (error if no)\n",
    "    - Processes files that need evaluation\n",
    "    - Provides comprehensive summary report\n",
    "    - Tracks timing for each file and overall\n",
    "    \n",
    "    Process:\n",
    "    1. Loop through all EVALUATION_CONFIGS\n",
    "    2. For each config:\n",
    "       - Check if evaluation file exists ‚Üí skip\n",
    "       - Check if generation file exists ‚Üí error if missing\n",
    "       - Process file if needed\n",
    "    3. Display summary report\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If generation file is missing or evaluation fails\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GENERATION ANSWER EVALUATION - MAIN ORCHESTRATOR\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check if evaluation functions are available\n",
    "    if not EVAL_FUNCTIONS_AVAILABLE:\n",
    "        print(\"\\n‚ùå ERROR: Evaluation functions not available\")\n",
    "        print(\"\\n   Required modules:\")\n",
    "        print(\"   - evaluate_answer.py\")\n",
    "        print(\"   - detect_refusal.py\")\n",
    "        print(\"   - numerical_exact_match.py\")\n",
    "        print(\"   - token_f1.py\")\n",
    "        print(\"   - llm_as_judge_binary.py\")\n",
    "        print(\"   - llm_as_judge_graded.py\")\n",
    "        print(\"\\n   Please ensure these modules are in your Python path.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìã Configuration: {len(EVALUATION_CONFIGS)} files to check\")\n",
    "    print(f\"üìÅ Generation path: {GENERATION_PATH}\")\n",
    "    print(f\"üìÅ Evaluation path: {EVALUATION_PATH}\")\n",
    "    print(f\"\\n‚öôÔ∏è  Evaluation settings:\")\n",
    "    print(f\"   Tolerance: {EVALUATION_CONFIG['tolerance']} ({EVALUATION_CONFIG['tolerance']*100}%)\")\n",
    "    print(f\"   LLM Model: {EVALUATION_CONFIG['llm_model']}\")\n",
    "    print(f\"   Call delay: {CALL_DELAY}s\")\n",
    "    print(f\"   Retry delay: {RETRY_DELAY}s\")\n",
    "    print(f\"   Max retries: {MAX_RETRIES}\")\n",
    "    \n",
    "    # Track statistics\n",
    "    total_files = len(EVALUATION_CONFIGS)\n",
    "    skipped_files = 0\n",
    "    processed_files = 0\n",
    "    failed_files = 0\n",
    "    \n",
    "    skipped_list = []\n",
    "    processed_list = []\n",
    "    failed_list = []\n",
    "    \n",
    "    overall_start_time = time.time()\n",
    "    \n",
    "    # Process each configuration\n",
    "    for i, config in enumerate(EVALUATION_CONFIGS, 1):\n",
    "        mode = config['mode']\n",
    "        question_type = config['question_type']\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"[{i}/{total_files}] {mode} | {question_type}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Get file paths\n",
    "        gen_filepath = get_generation_filepath(config)\n",
    "        eval_filepath = get_evaluation_filepath(config)\n",
    "        \n",
    "        gen_filename = os.path.basename(gen_filepath)\n",
    "        eval_filename = os.path.basename(eval_filepath)\n",
    "        \n",
    "        print(f\"Generation: {gen_filename}\")\n",
    "        print(f\"Evaluation: {eval_filename}\")\n",
    "        \n",
    "        # Check if evaluation file already exists\n",
    "        if check_file_exists(eval_filepath):\n",
    "            file_info = get_file_info(eval_filepath)\n",
    "            print(f\"\\n‚úì Evaluation file already exists ({file_info['size_kb']} KB)\")\n",
    "            print(\"   ‚Üí SKIPPED (already evaluated)\")\n",
    "            skipped_files += 1\n",
    "            skipped_list.append({\n",
    "                'mode': mode,\n",
    "                'question_type': question_type,\n",
    "                'reason': 'already_exists'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Check if generation file exists\n",
    "        if not check_file_exists(gen_filepath):\n",
    "            print(f\"\\n‚ùå ERROR: Generation file not found!\")\n",
    "            print(f\"   Path: {gen_filepath}\")\n",
    "            print(\"\\n   Please check:\")\n",
    "            print(\"   1. GENERATION_PATH is correct\")\n",
    "            print(\"   2. File exists in the directory\")\n",
    "            print(\"   3. Filename matches the expected pattern\")\n",
    "            failed_files += 1\n",
    "            failed_list.append({\n",
    "                'mode': mode,\n",
    "                'question_type': question_type,\n",
    "                'error': 'generation_file_not_found'\n",
    "            })\n",
    "            raise FileNotFoundError(f\"Generation file not found: {gen_filepath}\")\n",
    "        \n",
    "        # File needs processing\n",
    "        gen_info = get_file_info(gen_filepath)\n",
    "        print(f\"\\n‚Üí Processing... (generation file: {gen_info['size_kb']} KB)\")\n",
    "        \n",
    "        try:\n",
    "            file_start_time = time.time()\n",
    "            \n",
    "            # Process the file\n",
    "            summary = evaluate_generation_file(\n",
    "                generation_filepath=gen_filepath,\n",
    "                evaluation_filepath=eval_filepath,\n",
    "                eval_config=EVALUATION_CONFIG,\n",
    "                call_delay=CALL_DELAY\n",
    "            )\n",
    "            \n",
    "            file_end_time = time.time()\n",
    "            file_time = file_end_time - file_start_time\n",
    "            \n",
    "            # Success\n",
    "            print(f\"\\n‚úì Evaluation completed successfully!\")\n",
    "            print(f\"   Time: {summary['total_time_seconds']}s\")\n",
    "            print(f\"   Queries: {summary['successful_evaluations']}/{summary['total_queries']}\")\n",
    "            \n",
    "            processed_files += 1\n",
    "            processed_list.append({\n",
    "                'mode': mode,\n",
    "                'question_type': question_type,\n",
    "                'queries': summary['total_queries'],\n",
    "                'time_seconds': summary['total_time_seconds']\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå ERROR: Evaluation failed!\")\n",
    "            print(f\"   Error: {str(e)[:200]}...\")\n",
    "            failed_files += 1\n",
    "            failed_list.append({\n",
    "                'mode': mode,\n",
    "                'question_type': question_type,\n",
    "                'error': str(e)[:100]\n",
    "            })\n",
    "            raise  # Stop the entire process\n",
    "    \n",
    "    # Calculate overall timing\n",
    "    overall_end_time = time.time()\n",
    "    overall_time = overall_end_time - overall_start_time\n",
    "    \n",
    "    # Display summary report\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä FINAL SUMMARY REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüìà Overall Statistics:\")\n",
    "    print(f\"   Total files: {total_files}\")\n",
    "    print(f\"   Skipped: {skipped_files}\")\n",
    "    print(f\"   Processed: {processed_files}\")\n",
    "    print(f\"   Failed: {failed_files}\")\n",
    "    print(f\"   Total time: {overall_time:.2f}s ({overall_time/60:.1f} minutes)\")\n",
    "    \n",
    "    if processed_files > 0:\n",
    "        avg_time = overall_time / processed_files\n",
    "        print(f\"   Avg time per file: {avg_time:.2f}s\")\n",
    "    \n",
    "    # Show skipped files\n",
    "    if skipped_list:\n",
    "        print(f\"\\n‚è≠Ô∏è  Skipped Files ({len(skipped_list)}):\")\n",
    "        for item in skipped_list:\n",
    "            print(f\"   - {item['mode']:12s} | {item['question_type']}\")\n",
    "    \n",
    "    # Show processed files\n",
    "    if processed_list:\n",
    "        print(f\"\\n‚úÖ Processed Files ({len(processed_list)}):\")\n",
    "        total_queries = 0\n",
    "        for item in processed_list:\n",
    "            print(f\"   - {item['mode']:12s} | {item['question_type']:20s} | \"\n",
    "                  f\"{item['queries']} queries | {item['time_seconds']:.1f}s\")\n",
    "            total_queries += item['queries']\n",
    "        print(f\"\\n   Total queries evaluated: {total_queries}\")\n",
    "    \n",
    "    # Show failed files\n",
    "    if failed_list:\n",
    "        print(f\"\\n‚ùå Failed Files ({len(failed_list)}):\")\n",
    "        for item in failed_list:\n",
    "            print(f\"   - {item['mode']:12s} | {item['question_type']}\")\n",
    "            print(f\"     Error: {item['error']}\")\n",
    "    \n",
    "    # Final status\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    if failed_files > 0:\n",
    "        print(\"‚ùå EVALUATION INCOMPLETE - Some files failed\")\n",
    "    elif processed_files == 0 and skipped_files == total_files:\n",
    "        print(\"‚úÖ ALL FILES ALREADY EVALUATED\")\n",
    "    elif processed_files > 0:\n",
    "        print(\"‚úÖ EVALUATION COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Show output location\n",
    "    if processed_files > 0:\n",
    "        print(f\"\\nüìÅ Evaluation results saved to: {EVALUATION_PATH}\")\n",
    "        print(\"\\nGenerated files:\")\n",
    "        for item in processed_list:\n",
    "            eval_filename = get_evaluation_filename({\n",
    "                'mode': item['mode'],\n",
    "                'question_type': item['question_type'],\n",
    "                'provider': 'openai',\n",
    "                'model': 'gpt-4o-mini',\n",
    "                'temperature': '0.0',\n",
    "                'template_alias': 'metrics_closed_basic'  # simplified for display\n",
    "            })\n",
    "            print(f\"   - {eval_filename}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6 Test Function\n",
    "# ============================================================================\n",
    "\n",
    "def test_step6():\n",
    "    \"\"\"Test Step 6: Main orchestrator (dry run).\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 6: MAIN ORCHESTRATOR (DRY RUN)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è  This is a DRY RUN - showing what would happen without processing\")\n",
    "    \n",
    "    # Simulate what main() would do\n",
    "    print(f\"\\nüìã Configuration: {len(EVALUATION_CONFIGS)} files\")\n",
    "    \n",
    "    would_skip = 0\n",
    "    would_process = 0\n",
    "    would_fail = 0\n",
    "    \n",
    "    for i, config in enumerate(EVALUATION_CONFIGS, 1):\n",
    "        mode = config['mode']\n",
    "        question_type = config['question_type']\n",
    "        \n",
    "        gen_filepath = get_generation_filepath(config)\n",
    "        eval_filepath = get_evaluation_filepath(config)\n",
    "        \n",
    "        gen_exists = check_file_exists(gen_filepath)\n",
    "        eval_exists = check_file_exists(eval_filepath)\n",
    "        \n",
    "        print(f\"\\n[{i}/9] {mode:12s} | {question_type}\")\n",
    "        \n",
    "        if eval_exists:\n",
    "            print(f\"   Action: SKIP (already evaluated)\")\n",
    "            would_skip += 1\n",
    "        elif not gen_exists:\n",
    "            print(f\"   Action: FAIL (generation file not found)\")\n",
    "            would_fail += 1\n",
    "        else:\n",
    "            print(f\"   Action: PROCESS\")\n",
    "            would_process += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DRY RUN SUMMARY:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"   Would skip: {would_skip}\")\n",
    "    print(f\"   Would process: {would_process}\")\n",
    "    print(f\"   Would fail: {would_fail}\")\n",
    "    \n",
    "    if would_process > 0:\n",
    "        est_time = would_process * 1.5  # ~1.5 min per file\n",
    "        est_cost = would_process * 0.03  # ~$0.03 per file\n",
    "        print(f\"\\n   Estimated time: ~{est_time:.0f} minutes\")\n",
    "        print(f\"   Estimated cost: ~${est_cost:.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Step 6 Complete!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nStep 6 Function Implemented:\")\n",
    "    print(\"  ‚úì main() - Full orchestration\")\n",
    "    print(\"\\nFeatures:\")\n",
    "    print(\"  ‚úì Loops through all 9 configurations\")\n",
    "    print(\"  ‚úì Skip logic for existing evaluation files\")\n",
    "    print(\"  ‚úì Error handling for missing generation files\")\n",
    "    print(\"  ‚úì Progress tracking for each file\")\n",
    "    print(\"  ‚úì Comprehensive summary report\")\n",
    "    print(\"  ‚úì Total time and cost tracking\")\n",
    "    print(\"\\nTo run full evaluation:\")\n",
    "    print(\"  main()  # This will process all files!\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Test Step 6\n",
    "test_step6()  # Dry run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a865b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 7: ERROR HANDLING & VALIDATION\n",
      "======================================================================\n",
      "\n",
      "üß™ Test 1: Path Validation\n",
      "   ‚úì Paths are valid\n",
      "\n",
      "üß™ Test 2: API Key Validation\n",
      "   ‚úì API key is valid\n",
      "\n",
      "üß™ Test 3: Processing Plan\n",
      "   Files to skip: 6\n",
      "   Files to process: 1\n",
      "   Missing generation: 2\n",
      "   Estimated time: ~1.4 min\n",
      "   Estimated cost: ~$0.03\n",
      "\n",
      "======================================================================\n",
      "üìã PROCESSING PLAN\n",
      "======================================================================\n",
      "\n",
      "Total configurations: 9\n",
      "  ‚úì Already evaluated: 6\n",
      "  ‚Üí To process: 1\n",
      "  ‚úó Missing generation files: 2\n",
      "\n",
      "‚è≠Ô∏è  Files to skip (6):\n",
      "   - closed_book  | metrics-generated\n",
      "   - closed_book  | novel-generated\n",
      "   - closed_book  | domain-relevant\n",
      "   - oracle       | metrics-generated\n",
      "   - oracle       | novel-generated\n",
      "   - oracle       | domain-relevant\n",
      "\n",
      "‚ö†Ô∏è  Missing generation files (2):\n",
      "   - rag          | novel-generated\n",
      "     Expected: ../../generation_set/closedbook_oracle_sets/rag_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic.json\n",
      "   - rag          | domain-relevant\n",
      "     Expected: ../../generation_set/closedbook_oracle_sets/rag_domain-relevant_openai_gpt-4o-mini_0.0_domain_rag_basic.json\n",
      "\n",
      "üîÑ Files to process (1):\n",
      "   - rag          | metrics-generated\n",
      "\n",
      "üìä Estimates:\n",
      "   Total queries: 50\n",
      "   Estimated time: ~1.4 minutes\n",
      "   Estimated cost: ~$0.03\n",
      "\n",
      "   Breakdown by question type:\n",
      "   - metrics-generated   :  50 queries | ~$0.03 | ~1.4 min\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Step 7 Complete!\n",
      "======================================================================\n",
      "\n",
      "Step 7 Functions Implemented:\n",
      "  ‚úì validate_paths()\n",
      "  ‚úì validate_openai_api_key()\n",
      "  ‚úì estimate_cost_and_time()\n",
      "  ‚úì get_processing_plan()\n",
      "  ‚úì print_processing_plan()\n",
      "  ‚úì main_with_validation() - Enhanced main with safety checks\n",
      "\n",
      "Features:\n",
      "  ‚úì Path validation\n",
      "  ‚úì API key validation\n",
      "  ‚úì Cost and time estimation\n",
      "  ‚úì Processing plan preview\n",
      "  ‚úì User confirmation before processing\n",
      "  ‚úì Better error messages\n",
      "  ‚úì Keyboard interrupt handling\n",
      "\n",
      "Recommended usage:\n",
      "  main_with_validation()  # Use this instead of main()\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: Error Handling & Edge Cases\n",
    "# ============================================================================\n",
    "\n",
    "def validate_paths():\n",
    "    \"\"\"\n",
    "    Validate that required paths are set and directories exist.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (is_valid, error_messages)\n",
    "    \n",
    "    Example:\n",
    "        >>> is_valid, errors = validate_paths()\n",
    "        >>> if not is_valid:\n",
    "        ...     print(\"\\\\n\".join(errors))\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    # Check GENERATION_PATH\n",
    "    if GENERATION_PATH == \"/path/to/generated_answers\":\n",
    "        errors.append(\"GENERATION_PATH is still set to placeholder. Please update it.\")\n",
    "    elif not os.path.exists(GENERATION_PATH):\n",
    "        errors.append(f\"GENERATION_PATH does not exist: {GENERATION_PATH}\")\n",
    "    elif not os.path.isdir(GENERATION_PATH):\n",
    "        errors.append(f\"GENERATION_PATH is not a directory: {GENERATION_PATH}\")\n",
    "    \n",
    "    # Check EVALUATION_PATH\n",
    "    if EVALUATION_PATH == \"/path/to/evaluation_results\":\n",
    "        errors.append(\"EVALUATION_PATH is still set to placeholder. Please update it.\")\n",
    "    else:\n",
    "        # Try to create evaluation directory if it doesn't exist\n",
    "        try:\n",
    "            os.makedirs(EVALUATION_PATH, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            errors.append(f\"Cannot create EVALUATION_PATH: {EVALUATION_PATH}. Error: {e}\")\n",
    "    \n",
    "    return (len(errors) == 0, errors)\n",
    "\n",
    "\n",
    "def validate_openai_api_key():\n",
    "    \"\"\"\n",
    "    Validate that OpenAI API key is available.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (is_valid, error_message)\n",
    "    \n",
    "    Example:\n",
    "        >>> is_valid, error = validate_openai_api_key()\n",
    "        >>> if not is_valid:\n",
    "        ...     print(error)\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    \n",
    "    if not api_key:\n",
    "        return (False, \"OPENAI_API_KEY environment variable not set. \"\n",
    "                      \"LLM-as-judge evaluation will fail.\")\n",
    "    \n",
    "    if len(api_key) < 20:  # Basic sanity check\n",
    "        return (False, f\"OPENAI_API_KEY seems invalid (too short: {len(api_key)} chars)\")\n",
    "    \n",
    "    return (True, None)\n",
    "\n",
    "\n",
    "def estimate_cost_and_time(configs_to_process: List[Dict[str, str]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Estimate cost and time for processing given configurations.\n",
    "    \n",
    "    Args:\n",
    "        configs_to_process: List of configuration dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with estimates:\n",
    "            - total_queries: Total queries to evaluate\n",
    "            - estimated_time_minutes: Estimated time in minutes\n",
    "            - estimated_cost_usd: Estimated cost in USD\n",
    "            - breakdown_by_type: Cost/time breakdown by question type\n",
    "    \n",
    "    Example:\n",
    "        >>> estimate = estimate_cost_and_time(EVALUATION_CONFIGS)\n",
    "        >>> print(f\"Time: ~{estimate['estimated_time_minutes']} min\")\n",
    "        >>> print(f\"Cost: ~${estimate['estimated_cost_usd']:.2f}\")\n",
    "    \"\"\"\n",
    "    # Typical values (adjust based on your experience)\n",
    "    QUERIES_PER_FILE = 50\n",
    "    TIME_PER_QUERY_SECONDS = 1.7  # Average from testing\n",
    "    \n",
    "    # Cost per query by question type (approximate)\n",
    "    COST_PER_QUERY = {\n",
    "        'metrics-generated': 0.0006,   # NEM + LLM-binary (1 LLM call)\n",
    "        'novel-generated': 0.0006,      # Token-F1 + LLM-graded (1 LLM call)\n",
    "        'domain-relevant': 0.0006       # LLM-graded (1 LLM call)\n",
    "    }\n",
    "    \n",
    "    total_queries = len(configs_to_process) * QUERIES_PER_FILE\n",
    "    total_time_seconds = total_queries * TIME_PER_QUERY_SECONDS\n",
    "    \n",
    "    # Calculate cost by question type\n",
    "    breakdown = {}\n",
    "    total_cost = 0\n",
    "    \n",
    "    for config in configs_to_process:\n",
    "        qtype = config['question_type']\n",
    "        if qtype not in breakdown:\n",
    "            breakdown[qtype] = {\n",
    "                'count': 0,\n",
    "                'queries': 0,\n",
    "                'cost': 0,\n",
    "                'time_minutes': 0\n",
    "            }\n",
    "        \n",
    "        breakdown[qtype]['count'] += 1\n",
    "        breakdown[qtype]['queries'] += QUERIES_PER_FILE\n",
    "        breakdown[qtype]['cost'] += QUERIES_PER_FILE * COST_PER_QUERY[qtype]\n",
    "        breakdown[qtype]['time_minutes'] += (QUERIES_PER_FILE * TIME_PER_QUERY_SECONDS) / 60\n",
    "        \n",
    "        total_cost += QUERIES_PER_FILE * COST_PER_QUERY[qtype]\n",
    "    \n",
    "    return {\n",
    "        'total_queries': total_queries,\n",
    "        'estimated_time_minutes': round(total_time_seconds / 60, 1),\n",
    "        'estimated_cost_usd': round(total_cost, 2),\n",
    "        'breakdown_by_type': breakdown\n",
    "    }\n",
    "\n",
    "\n",
    "def get_processing_plan() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze which files need processing and create execution plan.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - to_skip: List of configs that already have evaluation files\n",
    "            - to_process: List of configs that need evaluation\n",
    "            - missing_generation: List of configs missing generation files\n",
    "            - estimate: Cost and time estimate for processing\n",
    "    \n",
    "    Example:\n",
    "        >>> plan = get_processing_plan()\n",
    "        >>> print(f\"Will process: {len(plan['to_process'])} files\")\n",
    "        >>> print(f\"Will skip: {len(plan['to_skip'])} files\")\n",
    "    \"\"\"\n",
    "    to_skip = []\n",
    "    to_process = []\n",
    "    missing_generation = []\n",
    "    \n",
    "    for config in EVALUATION_CONFIGS:\n",
    "        gen_filepath = get_generation_filepath(config)\n",
    "        eval_filepath = get_evaluation_filepath(config)\n",
    "        \n",
    "        gen_exists = check_file_exists(gen_filepath)\n",
    "        eval_exists = check_file_exists(eval_filepath)\n",
    "        \n",
    "        if eval_exists:\n",
    "            to_skip.append(config)\n",
    "        elif not gen_exists:\n",
    "            missing_generation.append(config)\n",
    "        else:\n",
    "            to_process.append(config)\n",
    "    \n",
    "    # Get cost/time estimate for files to process\n",
    "    estimate = estimate_cost_and_time(to_process) if to_process else None\n",
    "    \n",
    "    return {\n",
    "        'to_skip': to_skip,\n",
    "        'to_process': to_process,\n",
    "        'missing_generation': missing_generation,\n",
    "        'estimate': estimate\n",
    "    }\n",
    "\n",
    "\n",
    "def print_processing_plan(plan: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Print a detailed processing plan before execution.\n",
    "    \n",
    "    Args:\n",
    "        plan: Plan dictionary from get_processing_plan()\n",
    "    \n",
    "    Example:\n",
    "        >>> plan = get_processing_plan()\n",
    "        >>> print_processing_plan(plan)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã PROCESSING PLAN\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total = len(EVALUATION_CONFIGS)\n",
    "    skip_count = len(plan['to_skip'])\n",
    "    process_count = len(plan['to_process'])\n",
    "    missing_count = len(plan['missing_generation'])\n",
    "    \n",
    "    print(f\"\\nTotal configurations: {total}\")\n",
    "    print(f\"  ‚úì Already evaluated: {skip_count}\")\n",
    "    print(f\"  ‚Üí To process: {process_count}\")\n",
    "    print(f\"  ‚úó Missing generation files: {missing_count}\")\n",
    "    \n",
    "    if plan['to_skip']:\n",
    "        print(f\"\\n‚è≠Ô∏è  Files to skip ({skip_count}):\")\n",
    "        for config in plan['to_skip']:\n",
    "            print(f\"   - {config['mode']:12s} | {config['question_type']}\")\n",
    "    \n",
    "    if plan['missing_generation']:\n",
    "        print(f\"\\n‚ö†Ô∏è  Missing generation files ({missing_count}):\")\n",
    "        for config in plan['missing_generation']:\n",
    "            print(f\"   - {config['mode']:12s} | {config['question_type']}\")\n",
    "            gen_filepath = get_generation_filepath(config)\n",
    "            print(f\"     Expected: {gen_filepath}\")\n",
    "    \n",
    "    if plan['to_process']:\n",
    "        print(f\"\\nüîÑ Files to process ({process_count}):\")\n",
    "        for config in plan['to_process']:\n",
    "            print(f\"   - {config['mode']:12s} | {config['question_type']}\")\n",
    "        \n",
    "        if plan['estimate']:\n",
    "            est = plan['estimate']\n",
    "            print(f\"\\nüìä Estimates:\")\n",
    "            print(f\"   Total queries: {est['total_queries']}\")\n",
    "            print(f\"   Estimated time: ~{est['estimated_time_minutes']} minutes\")\n",
    "            print(f\"   Estimated cost: ~${est['estimated_cost_usd']:.2f}\")\n",
    "            \n",
    "            print(f\"\\n   Breakdown by question type:\")\n",
    "            for qtype, data in est['breakdown_by_type'].items():\n",
    "                print(f\"   - {qtype:20s}: {data['queries']:3d} queries | \"\n",
    "                      f\"~${data['cost']:.2f} | ~{data['time_minutes']:.1f} min\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "def main_with_validation():\n",
    "    \"\"\"\n",
    "    Enhanced main function with comprehensive validation and user confirmation.\n",
    "    \n",
    "    This is the recommended entry point for running evaluations.\n",
    "    It includes:\n",
    "    - Path validation\n",
    "    - API key validation\n",
    "    - Processing plan display\n",
    "    - User confirmation before processing\n",
    "    - Better error messages\n",
    "    \n",
    "    Example:\n",
    "        >>> main_with_validation()  # Interactive with confirmations\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GENERATION ANSWER EVALUATION - ENHANCED\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Validate paths\n",
    "    print(\"\\nüîç Step 1: Validating paths...\")\n",
    "    paths_valid, path_errors = validate_paths()\n",
    "    if not paths_valid:\n",
    "        print(\"\\n‚ùå Path validation failed:\")\n",
    "        for error in path_errors:\n",
    "            print(f\"   - {error}\")\n",
    "        print(\"\\nPlease fix the path issues and try again.\")\n",
    "        return\n",
    "    print(\"   ‚úì Paths validated\")\n",
    "    \n",
    "    # Step 2: Validate API key\n",
    "    print(\"\\nüîç Step 2: Validating OpenAI API key...\")\n",
    "    api_valid, api_error = validate_openai_api_key()\n",
    "    if not api_valid:\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: {api_error}\")\n",
    "        print(\"   Evaluation will likely fail without a valid API key.\")\n",
    "        response = input(\"\\n   Continue anyway? (yes/no): \")\n",
    "        if response.lower() != 'yes':\n",
    "            print(\"   Cancelled.\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"   ‚úì API key found\")\n",
    "    \n",
    "    # Step 3: Check evaluation functions\n",
    "    print(\"\\nüîç Step 3: Checking evaluation functions...\")\n",
    "    if not EVAL_FUNCTIONS_AVAILABLE:\n",
    "        print(\"\\n‚ùå ERROR: Evaluation functions not available\")\n",
    "        print(\"   Required modules:\")\n",
    "        print(\"   - evaluate_answer.py\")\n",
    "        print(\"   - detect_refusal.py\")\n",
    "        print(\"   - numerical_exact_match.py\")\n",
    "        print(\"   - token_f1.py\")\n",
    "        print(\"   - llm_as_judge_binary.py\")\n",
    "        print(\"   - llm_as_judge_graded.py\")\n",
    "        return\n",
    "    print(\"   ‚úì Evaluation functions available\")\n",
    "    \n",
    "    # Step 4: Create processing plan\n",
    "    print(\"\\nüîç Step 4: Creating processing plan...\")\n",
    "    plan = get_processing_plan()\n",
    "    print_processing_plan(plan)\n",
    "    \n",
    "    # Step 5: Check if anything to process\n",
    "    if not plan['to_process']:\n",
    "        if plan['to_skip']:\n",
    "            print(\"\\n‚úÖ All files already evaluated! Nothing to do.\")\n",
    "        elif plan['missing_generation']:\n",
    "            print(\"\\n‚ùå No files to process. All generation files are missing.\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  No files to process.\")\n",
    "        return\n",
    "    \n",
    "    # Step 6: User confirmation\n",
    "    if plan['estimate']:\n",
    "        print(\"\\n‚ö†Ô∏è  CONFIRMATION REQUIRED\")\n",
    "        print(f\"   This will evaluate {plan['estimate']['total_queries']} queries\")\n",
    "        print(f\"   Estimated time: ~{plan['estimate']['estimated_time_minutes']} minutes\")\n",
    "        print(f\"   Estimated cost: ~${plan['estimate']['estimated_cost_usd']:.2f}\")\n",
    "        \n",
    "        response = input(\"\\n   Proceed with evaluation? (yes/no): \")\n",
    "        if response.lower() != 'yes':\n",
    "            print(\"\\n   Cancelled by user.\")\n",
    "            return\n",
    "    \n",
    "    # Step 7: Run evaluation\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ STARTING EVALUATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        main()  # Call the original main function\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö†Ô∏è  Evaluation interrupted by user (Ctrl+C)\")\n",
    "        print(\"   Partial results may have been saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n‚ùå Evaluation failed with error:\")\n",
    "        print(f\"   {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7 Test Function\n",
    "# ============================================================================\n",
    "\n",
    "def test_step7():\n",
    "    \"\"\"Test Step 7: Error handling and validation.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 7: ERROR HANDLING & VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Test path validation\n",
    "    print(\"\\nüß™ Test 1: Path Validation\")\n",
    "    paths_valid, path_errors = validate_paths()\n",
    "    if paths_valid:\n",
    "        print(\"   ‚úì Paths are valid\")\n",
    "    else:\n",
    "        print(\"   ‚úó Path validation failed:\")\n",
    "        for error in path_errors:\n",
    "            print(f\"     - {error}\")\n",
    "    \n",
    "    # Test API key validation\n",
    "    print(\"\\nüß™ Test 2: API Key Validation\")\n",
    "    api_valid, api_error = validate_openai_api_key()\n",
    "    if api_valid:\n",
    "        print(\"   ‚úì API key is valid\")\n",
    "    else:\n",
    "        print(f\"   ‚úó {api_error}\")\n",
    "    \n",
    "    # Test processing plan\n",
    "    print(\"\\nüß™ Test 3: Processing Plan\")\n",
    "    plan = get_processing_plan()\n",
    "    print(f\"   Files to skip: {len(plan['to_skip'])}\")\n",
    "    print(f\"   Files to process: {len(plan['to_process'])}\")\n",
    "    print(f\"   Missing generation: {len(plan['missing_generation'])}\")\n",
    "    \n",
    "    if plan['estimate']:\n",
    "        est = plan['estimate']\n",
    "        print(f\"   Estimated time: ~{est['estimated_time_minutes']} min\")\n",
    "        print(f\"   Estimated cost: ~${est['estimated_cost_usd']:.2f}\")\n",
    "    \n",
    "    # Show full processing plan\n",
    "    print_processing_plan(plan)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Step 7 Complete!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nStep 7 Functions Implemented:\")\n",
    "    print(\"  ‚úì validate_paths()\")\n",
    "    print(\"  ‚úì validate_openai_api_key()\")\n",
    "    print(\"  ‚úì estimate_cost_and_time()\")\n",
    "    print(\"  ‚úì get_processing_plan()\")\n",
    "    print(\"  ‚úì print_processing_plan()\")\n",
    "    print(\"  ‚úì main_with_validation() - Enhanced main with safety checks\")\n",
    "    print(\"\\nFeatures:\")\n",
    "    print(\"  ‚úì Path validation\")\n",
    "    print(\"  ‚úì API key validation\")\n",
    "    print(\"  ‚úì Cost and time estimation\")\n",
    "    print(\"  ‚úì Processing plan preview\")\n",
    "    print(\"  ‚úì User confirmation before processing\")\n",
    "    print(\"  ‚úì Better error messages\")\n",
    "    print(\"  ‚úì Keyboard interrupt handling\")\n",
    "    print(\"\\nRecommended usage:\")\n",
    "    print(\"  main_with_validation()  # Use this instead of main()\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "\n",
    "test_step7()\n",
    "    \n",
    "# Recommended: Use enhanced main with validation\n",
    "# main_with_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc55ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GENERATION ANSWER EVALUATION - ENHANCED\n",
      "======================================================================\n",
      "\n",
      "üîç Step 1: Validating paths...\n",
      "   ‚úì Paths validated\n",
      "\n",
      "üîç Step 2: Validating OpenAI API key...\n",
      "   ‚úì API key found\n",
      "\n",
      "üîç Step 3: Checking evaluation functions...\n",
      "   ‚úì Evaluation functions available\n",
      "\n",
      "üîç Step 4: Creating processing plan...\n",
      "\n",
      "======================================================================\n",
      "üìã PROCESSING PLAN\n",
      "======================================================================\n",
      "\n",
      "Total configurations: 20\n",
      "  ‚úì Already evaluated: 16\n",
      "  ‚Üí To process: 4\n",
      "  ‚úó Missing generation files: 0\n",
      "\n",
      "‚è≠Ô∏è  Files to skip (16):\n",
      "   - closed_book  | metrics-generated\n",
      "   - closed_book  | metrics-generated\n",
      "   - closed_book  | novel-generated\n",
      "   - closed_book  | domain-relevant\n",
      "   - rag          | metrics-generated\n",
      "   - rag          | metrics-generated\n",
      "   - rag          | metrics-generated\n",
      "   - rag          | metrics-generated\n",
      "   - rag          | metrics-generated\n",
      "   - rag          | metrics-generated\n",
      "   - rag          | metrics-generated\n",
      "   - rag          | metrics-generated\n",
      "   - oracle       | metrics-generated\n",
      "   - oracle       | metrics-generated\n",
      "   - oracle       | novel-generated\n",
      "   - oracle       | domain-relevant\n",
      "\n",
      "üîÑ Files to process (4):\n",
      "   - rag          | novel-generated\n",
      "   - rag          | novel-generated\n",
      "   - rag          | novel-generated\n",
      "   - rag          | novel-generated\n",
      "\n",
      "üìä Estimates:\n",
      "   Total queries: 200\n",
      "   Estimated time: ~5.7 minutes\n",
      "   Estimated cost: ~$0.12\n",
      "\n",
      "   Breakdown by question type:\n",
      "   - novel-generated     : 200 queries | ~$0.12 | ~5.7 min\n",
      "\n",
      "======================================================================\n",
      "\n",
      "‚ö†Ô∏è  CONFIRMATION REQUIRED\n",
      "   This will evaluate 200 queries\n",
      "   Estimated time: ~5.7 minutes\n",
      "   Estimated cost: ~$0.12\n",
      "\n",
      "======================================================================\n",
      "üöÄ STARTING EVALUATION\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "GENERATION ANSWER EVALUATION - MAIN ORCHESTRATOR\n",
      "======================================================================\n",
      "\n",
      "üìã Configuration: 20 files to check\n",
      "üìÅ Generation path: ../../generation_set/closedbook_oracle_sets\n",
      "üìÅ Evaluation path: ../../evaluation_results/generation\n",
      "\n",
      "‚öôÔ∏è  Evaluation settings:\n",
      "   Tolerance: 0.05 (5.0%)\n",
      "   LLM Model: gpt-4o-mini\n",
      "   Call delay: 0.5s\n",
      "   Retry delay: 10s\n",
      "   Max retries: 3\n",
      "\n",
      "======================================================================\n",
      "[1/20] closed_book | metrics-generated\n",
      "======================================================================\n",
      "Generation: closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "Evaluation: evaluation_closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "\n",
      "‚úì Evaluation file already exists (176.38 KB)\n",
      "   ‚Üí SKIPPED (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "[2/20] closed_book | metrics-generated\n",
      "======================================================================\n",
      "Generation: closed_book_metrics-generated_openai_gpt-4o_0.0_metrics_closed_basic.json\n",
      "Evaluation: evaluation_closed_book_metrics-generated_openai_gpt-4o_0.0_metrics_closed_basic.json\n",
      "\n",
      "‚úì Evaluation file already exists (177.21 KB)\n",
      "   ‚Üí SKIPPED (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "[3/20] closed_book | novel-generated\n",
      "======================================================================\n",
      "Generation: closed_book_novel-generated_openai_gpt-4o-mini_0.0_novel_closed_basic.json\n",
      "Evaluation: evaluation_closed_book_novel-generated_openai_gpt-4o-mini_0.0_novel_closed_basic.json\n",
      "\n",
      "‚úì Evaluation file already exists (264.1 KB)\n",
      "   ‚Üí SKIPPED (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "[4/20] closed_book | domain-relevant\n",
      "======================================================================\n",
      "Generation: closed_book_domain-relevant_openai_gpt-4o-mini_0.0_domain_closed_basic.json\n",
      "Evaluation: evaluation_closed_book_domain-relevant_openai_gpt-4o-mini_0.0_domain_closed_basic.json\n",
      "\n",
      "‚úì Evaluation file already exists (209.22 KB)\n",
      "   ‚Üí SKIPPED (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "[5/20] rag | metrics-generated\n",
      "======================================================================\n",
      "Generation: rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic_global_chunk512_baseline_k20.json\n",
      "Evaluation: evaluation_rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic_global_chunk512_baseline_k20.json\n",
      "\n",
      "‚úì Evaluation file already exists (171.18 KB)\n",
      "   ‚Üí SKIPPED (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "[6/20] rag | metrics-generated\n",
      "======================================================================\n",
      "Generation: rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic_single_chunk512_baseline_k20.json\n",
      "Evaluation: evaluation_rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic_single_chunk512_baseline_k20.json\n",
      "\n",
      "‚úì Evaluation file already exists (171.31 KB)\n",
      "   ‚Üí SKIPPED (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "[7/20] rag | metrics-generated\n",
      "======================================================================\n",
      "Generation: rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic_global_chunk512_rerank_k20.json\n",
      "Evaluation: evaluation_rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic_global_chunk512_rerank_k20.json\n",
      "\n",
      "‚úì Evaluation file already exists (170.96 KB)\n",
      "   ‚Üí SKIPPED (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "[8/20] rag | metrics-generated\n",
      "======================================================================\n",
      "Generation: rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic_single_chunk512_rerank_k20.json\n",
      "Evaluation: evaluation_rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic_single_chunk512_rerank_k20.json\n",
      "\n",
      "‚úì Evaluation file already exists (171.7 KB)\n",
      "   ‚Üí SKIPPED (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "[9/20] rag | metrics-generated\n",
      "======================================================================\n",
      "Generation: rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic_global_chunk1024_rerank_k20.json\n",
      "Evaluation: evaluation_rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic_global_chunk1024_rerank_k20.json\n",
      "\n",
      "‚úì Evaluation file already exists (170.5 KB)\n",
      "   ‚Üí SKIPPED (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "[10/20] rag | metrics-generated\n",
      "======================================================================\n",
      "Generation: rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic_single_chunk1024_rerank_k20.json\n",
      "Evaluation: evaluation_rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic_single_chunk1024_rerank_k20.json\n",
      "\n",
      "‚úì Evaluation file already exists (171.64 KB)\n",
      "   ‚Üí SKIPPED (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "[11/20] rag | metrics-generated\n",
      "======================================================================\n",
      "Generation: rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic_global_chunk2048_rerank_k20.json\n",
      "Evaluation: evaluation_rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic_global_chunk2048_rerank_k20.json\n",
      "\n",
      "‚úì Evaluation file already exists (171.17 KB)\n",
      "   ‚Üí SKIPPED (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "[12/20] rag | metrics-generated\n",
      "======================================================================\n",
      "Generation: rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic_single_chunk2048_rerank_k20.json\n",
      "Evaluation: evaluation_rag_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic_single_chunk2048_rerank_k20.json\n",
      "\n",
      "‚úì Evaluation file already exists (171.96 KB)\n",
      "   ‚Üí SKIPPED (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "[13/20] rag | novel-generated\n",
      "======================================================================\n",
      "Generation: rag_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic_global_chunk512_baseline_k20.json\n",
      "Evaluation: evaluation_rag_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic_global_chunk512_baseline_k20.json\n",
      "\n",
      "‚Üí Processing... (generation file: 326.03 KB)\n",
      "\n",
      "üìñ Reading generation file...\n",
      "   ‚úì Loaded 50 queries\n",
      "   Mode: rag\n",
      "   Question Type: novel-generated\n",
      "\n",
      "üîÑ Evaluating 50 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rag_novel-generated: 100%|‚ñà| 50/50 [03:25<00:00,  4.11s/query, ID: financebench_id_02024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving evaluation results...\n",
      "   Output: ../../evaluation_results/generation/evaluation_rag_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic_global_chunk512_baseline_k20.json\n",
      "   ‚úì Saved successfully (273.19 KB)\n",
      "\n",
      "‚úì Evaluation completed successfully!\n",
      "   Time: 205.75s\n",
      "   Queries: 50/50\n",
      "\n",
      "======================================================================\n",
      "[14/20] rag | novel-generated\n",
      "======================================================================\n",
      "Generation: rag_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic_single_chunk512_baseline_k20.json\n",
      "Evaluation: evaluation_rag_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic_single_chunk512_baseline_k20.json\n",
      "\n",
      "‚Üí Processing... (generation file: 324.68 KB)\n",
      "\n",
      "üìñ Reading generation file...\n",
      "   ‚úì Loaded 50 queries\n",
      "   Mode: rag\n",
      "   Question Type: novel-generated\n",
      "\n",
      "üîÑ Evaluating 50 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rag_novel-generated: 100%|‚ñà| 50/50 [02:56<00:00,  3.54s/query, ID: financebench_id_02024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving evaluation results...\n",
      "   Output: ../../evaluation_results/generation/evaluation_rag_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic_single_chunk512_baseline_k20.json\n",
      "   ‚úì Saved successfully (269.73 KB)\n",
      "\n",
      "‚úì Evaluation completed successfully!\n",
      "   Time: 176.83s\n",
      "   Queries: 50/50\n",
      "\n",
      "======================================================================\n",
      "[15/20] rag | novel-generated\n",
      "======================================================================\n",
      "Generation: rag_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic_global_chunk512_rerank_k20.json\n",
      "Evaluation: evaluation_rag_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic_global_chunk512_rerank_k20.json\n",
      "\n",
      "‚Üí Processing... (generation file: 325.55 KB)\n",
      "\n",
      "üìñ Reading generation file...\n",
      "   ‚úì Loaded 50 queries\n",
      "   Mode: rag\n",
      "   Question Type: novel-generated\n",
      "\n",
      "üîÑ Evaluating 50 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rag_novel-generated: 100%|‚ñà| 50/50 [03:06<00:00,  3.74s/query, ID: financebench_id_02024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving evaluation results...\n",
      "   Output: ../../evaluation_results/generation/evaluation_rag_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic_global_chunk512_rerank_k20.json\n",
      "   ‚úì Saved successfully (274.69 KB)\n",
      "\n",
      "‚úì Evaluation completed successfully!\n",
      "   Time: 186.97s\n",
      "   Queries: 50/50\n",
      "\n",
      "======================================================================\n",
      "[16/20] rag | novel-generated\n",
      "======================================================================\n",
      "Generation: rag_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic_single_chunk512_rerank_k20.json\n",
      "Evaluation: evaluation_rag_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic_single_chunk512_rerank_k20.json\n",
      "\n",
      "‚Üí Processing... (generation file: 325.46 KB)\n",
      "\n",
      "üìñ Reading generation file...\n",
      "   ‚úì Loaded 50 queries\n",
      "   Mode: rag\n",
      "   Question Type: novel-generated\n",
      "\n",
      "üîÑ Evaluating 50 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rag_novel-generated: 100%|‚ñà| 50/50 [03:08<00:00,  3.77s/query, ID: financebench_id_02024]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saving evaluation results...\n",
      "   Output: ../../evaluation_results/generation/evaluation_rag_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic_single_chunk512_rerank_k20.json\n",
      "   ‚úì Saved successfully (272.95 KB)\n",
      "\n",
      "‚úì Evaluation completed successfully!\n",
      "   Time: 188.49s\n",
      "   Queries: 50/50\n",
      "\n",
      "======================================================================\n",
      "[17/20] oracle | metrics-generated\n",
      "======================================================================\n",
      "Generation: oracle_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic.json\n",
      "Evaluation: evaluation_oracle_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic.json\n",
      "\n",
      "‚úì Evaluation file already exists (171.54 KB)\n",
      "   ‚Üí SKIPPED (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "[18/20] oracle | metrics-generated\n",
      "======================================================================\n",
      "Generation: oracle_metrics-generated_openai_gpt-4o_0.0_metrics_rag_basic.json\n",
      "Evaluation: evaluation_oracle_metrics-generated_openai_gpt-4o_0.0_metrics_rag_basic.json\n",
      "\n",
      "‚úì Evaluation file already exists (171.01 KB)\n",
      "   ‚Üí SKIPPED (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "[19/20] oracle | novel-generated\n",
      "======================================================================\n",
      "Generation: oracle_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic.json\n",
      "Evaluation: evaluation_oracle_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic.json\n",
      "\n",
      "‚úì Evaluation file already exists (260.84 KB)\n",
      "   ‚Üí SKIPPED (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "[20/20] oracle | domain-relevant\n",
      "======================================================================\n",
      "Generation: oracle_domain-relevant_openai_gpt-4o-mini_0.0_domain_rag_basic.json\n",
      "Evaluation: evaluation_oracle_domain-relevant_openai_gpt-4o-mini_0.0_domain_rag_basic.json\n",
      "\n",
      "‚úì Evaluation file already exists (207.53 KB)\n",
      "   ‚Üí SKIPPED (already evaluated)\n",
      "\n",
      "======================================================================\n",
      "üìä FINAL SUMMARY REPORT\n",
      "======================================================================\n",
      "\n",
      "üìà Overall Statistics:\n",
      "   Total files: 20\n",
      "   Skipped: 16\n",
      "   Processed: 4\n",
      "   Failed: 0\n",
      "   Total time: 758.09s (12.6 minutes)\n",
      "   Avg time per file: 189.52s\n",
      "\n",
      "‚è≠Ô∏è  Skipped Files (16):\n",
      "   - closed_book  | metrics-generated\n",
      "   - closed_book  | metrics-generated\n",
      "   - closed_book  | novel-generated\n",
      "   - closed_book  | domain-relevant\n",
      "   - rag          | metrics-generated\n",
      "   - rag          | metrics-generated\n",
      "   - rag          | metrics-generated\n",
      "   - rag          | metrics-generated\n",
      "   - rag          | metrics-generated\n",
      "   - rag          | metrics-generated\n",
      "   - rag          | metrics-generated\n",
      "   - rag          | metrics-generated\n",
      "   - oracle       | metrics-generated\n",
      "   - oracle       | metrics-generated\n",
      "   - oracle       | novel-generated\n",
      "   - oracle       | domain-relevant\n",
      "\n",
      "‚úÖ Processed Files (4):\n",
      "   - rag          | novel-generated      | 50 queries | 205.8s\n",
      "   - rag          | novel-generated      | 50 queries | 176.8s\n",
      "   - rag          | novel-generated      | 50 queries | 187.0s\n",
      "   - rag          | novel-generated      | 50 queries | 188.5s\n",
      "\n",
      "   Total queries evaluated: 200\n",
      "\n",
      "======================================================================\n",
      "‚úÖ EVALUATION COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "üìÅ Evaluation results saved to: ../../evaluation_results/generation\n",
      "\n",
      "Generated files:\n",
      "   - evaluation_rag_novel-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "   - evaluation_rag_novel-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "   - evaluation_rag_novel-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "   - evaluation_rag_novel-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RUN MAIN ORCHESTRATOR WITH ALL CONFIGURATIONS\n",
    "# ============================================================================\n",
    "\n",
    "EVALUATION_CONFIGS = [\n",
    "    # Closed-book experiments (3 question types)\n",
    "    {\n",
    "        'mode': 'closed_book',\n",
    "        'question_type': 'metrics-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'metrics_closed_basic'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'closed_book',\n",
    "        'question_type': 'metrics-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'metrics_closed_basic'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'closed_book',\n",
    "        'question_type': 'novel-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'novel_closed_basic'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'closed_book',\n",
    "        'question_type': 'domain-relevant',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'domain_closed_basic'\n",
    "    },\n",
    "    \n",
    "    # RAG experiments (3 question types)\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'metrics-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'metrics_rag_basic_global_chunk512_baseline_k20'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'metrics-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'metrics_rag_basic_single_chunk512_baseline_k20'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'metrics-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'metrics_rag_basic_global_chunk512_rerank_k20'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'metrics-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'metrics_rag_basic_single_chunk512_rerank_k20'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'metrics-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'metrics_rag_basic_global_chunk1024_rerank_k20'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'metrics-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'metrics_rag_basic_single_chunk1024_rerank_k20'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'metrics-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'metrics_rag_basic_global_chunk2048_rerank_k20'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'metrics-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'metrics_rag_basic_single_chunk2048_rerank_k20'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'novel-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'novel_rag_basic_global_chunk512_baseline_k20'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'novel-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'novel_rag_basic_single_chunk512_baseline_k20'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'novel-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'novel_rag_basic_global_chunk512_rerank_k20'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'novel-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'novel_rag_basic_single_chunk512_rerank_k20'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'novel-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'novel_rag_basic_global_chunk1024_rerank_k20'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'novel-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'novel_rag_basic_single_chunk1024_rerank_k20'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'novel-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'novel_rag_basic_global_chunk2048_rerank_k20'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'rag',\n",
    "        'question_type': 'novel-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'novel_rag_basic_single_chunk2048_rerank_k20'\n",
    "    },\n",
    "    # {\n",
    "    #     'mode': 'rag',\n",
    "    #     'question_type': 'novel-generated',\n",
    "    #     'provider': 'openai',\n",
    "    #     'model': 'gpt-4o-mini',\n",
    "    #     'temperature': '0.0',\n",
    "    #     'template_alias': 'novel_rag_basic_global_chunk1024_rerank_k20'\n",
    "    # },\n",
    "    # {\n",
    "    #     'mode': 'rag',\n",
    "    #     'question_type': 'novel-generated',\n",
    "    #     'provider': 'openai',\n",
    "    #     'model': 'gpt-4o-mini',\n",
    "    #     'temperature': '0.0',\n",
    "    #     'template_alias': 'novel_rag_basic_single_chunk1024_rerank_k20'\n",
    "    # },\n",
    "    # {\n",
    "    #     'mode': 'rag',\n",
    "    #     'question_type': 'domain-relevant',\n",
    "    #     'provider': 'openai',\n",
    "    #     'model': 'gpt-4o-mini',\n",
    "    #     'temperature': '0.0',\n",
    "    #     'template_alias': 'domain_rag_basic'\n",
    "    # },\n",
    "    \n",
    "    # Oracle experiments (3 question types)\n",
    "    {\n",
    "        'mode': 'oracle',\n",
    "        'question_type': 'metrics-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'metrics_rag_basic'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'oracle',\n",
    "        'question_type': 'metrics-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'metrics_rag_basic'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'oracle',\n",
    "        'question_type': 'novel-generated',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'novel_rag_basic'\n",
    "    },\n",
    "    {\n",
    "        'mode': 'oracle',\n",
    "        'question_type': 'domain-relevant',\n",
    "        'provider': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'temperature': '0.0',\n",
    "        'template_alias': 'domain_rag_basic'\n",
    "    },\n",
    "]\n",
    "\n",
    "# To actually run the full evaluation, uncomment the line below:\n",
    "# main() # It will process all files!\n",
    "main_with_validation()  # It will process all files with validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
