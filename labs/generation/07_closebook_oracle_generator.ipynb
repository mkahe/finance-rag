{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9dd7183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehrdad/projects/finance-rag/finance-rag/labs/generation/gen_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "‚úÖ Output directory ready: ../../generation_set/closedbook_oracle_sets\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: SETUP & DEPENDENCIES\n",
    "# ============================================================================\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# LangChain imports for LLM providers\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# LangChain utilities for prompts and parsing\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "# Structured output imports (NEW) - using standard Pydantic\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Dataset and utility imports\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "OUTPUT_DIR = \"../../generation_set/closedbook_oracle_sets\"\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"‚úÖ Output directory ready: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b482c18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Structured output schema defined!\n",
      "   Schema fields: ['reasoning', 'final_answer']\n",
      "\n",
      "üìã Sample instance:\n",
      "   Reasoning: Based on the financial statements, revenue was $100M in Q1 and $120M in Q2. Grow...\n",
      "   Final Answer: 20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lj/175ptt0d6knb0gg0lg2h4n2h0000gp/T/ipykernel_70065/4147351160.py:18: PydanticDeprecatedSince20: The `__fields__` attribute is deprecated, use the `model_fields` class property instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  print(f\"   Schema fields: {list(FinanceAnswer.__fields__.keys())}\")\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STRUCTURED OUTPUT SCHEMA\n",
    "# ============================================================================\n",
    "\n",
    "class FinanceAnswer(BaseModel):\n",
    "    \"\"\"\n",
    "    Structured output schema for financial question answering.\n",
    "    Separates reasoning from final answer for better evaluation.\n",
    "    \"\"\"\n",
    "    reasoning: str = Field(\n",
    "        description=\"Step-by-step reasoning and analysis leading to the answer\"\n",
    "    )\n",
    "    final_answer: str = Field(\n",
    "        description=\"The final answer only, concise and precise\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Structured output schema defined!\")\n",
    "print(f\"   Schema fields: {list(FinanceAnswer.__fields__.keys())}\")\n",
    "\n",
    "# Test: Create a sample instance to verify schema\n",
    "test_instance = FinanceAnswer(\n",
    "    reasoning=\"Based on the financial statements, revenue was $100M in Q1 and $120M in Q2. Growth = ($120M - $100M) / $100M = 20%\",\n",
    "    final_answer=\"20%\"\n",
    ")\n",
    "print(f\"\\nüìã Sample instance:\")\n",
    "print(f\"   Reasoning: {test_instance.reasoning[:80]}...\")\n",
    "print(f\"   Final Answer: {test_instance.final_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc4bc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dataset: PatronusAI/financebench (train split)\n",
      "   Output directory: ../../generation_set/closedbook_oracle_sets\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: CONFIGURATION VARIABLES\n",
    "# ============================================================================\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Dataset Configuration\n",
    "DATASET_NAME = \"PatronusAI/financebench\"\n",
    "DATASET_SPLIT = \"train\"  # 150 questions in train split\n",
    "\n",
    "# API Configuration - Load from environment\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "\n",
    "# Rate Limiting Configuration\n",
    "CALL_DELAY = 1  # Seconds between each LLM call (rate limiting)\n",
    "RETRY_DELAY = 30  # Seconds to wait before retry after failure\n",
    "MAX_RETRIES = 3  # Maximum number of retry attempts\n",
    "\n",
    "# Global Generation Parameters\n",
    "TEMPERATURE = 0.0  # Deterministic generation for reproducibility\n",
    "\n",
    "print(f\"   Dataset: {DATASET_NAME} ({DATASET_SPLIT} split)\")\n",
    "print(f\"   Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab142ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Template variables restructured successfully!\n",
      "   METRICS_GENERATED_TEMPLATES: ['closed_book', 'rag']\n",
      "   DOMAIN_RELEVANT_TEMPLATES: ['closed_book', 'rag']\n",
      "   NOVEL_GENERATED_TEMPLATES: ['closed_book', 'rag']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PROMPT TEMPLATES - ORGANIZED BY QUESTION TYPE (STRUCTURED OUTPUT)\n",
    "# ============================================================================\n",
    "\n",
    "# Templates for METRICS-GENERATED questions\n",
    "# These questions ask for specific numerical values from financial statements\n",
    "METRICS_GENERATED_TEMPLATES = {\n",
    "    \"closed_book\": {\n",
    "        \"basic\": {\n",
    "            \"alias\": \"metrics_closed_basic\",\n",
    "            \"template\": \"\"\"You are a financial expert. Answer the following question about a specific financial metric using only your knowledge.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide your step-by-step reasoning, then give the final numerical answer (e.g., $X million, X%, etc.).\"\"\"\n",
    "        },\n",
    "        \"cot\": {\n",
    "            \"alias\": \"metrics_closed_cot\",\n",
    "            \"template\": \"\"\"You are a financial expert. Answer the following question about a specific financial metric using only your knowledge.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Think through this systematically:\n",
    "1. Identify what metric is being asked\n",
    "2. Recall the relevant information\n",
    "3. Provide the numerical answer\n",
    "\n",
    "Show your reasoning and provide the final answer.\"\"\"\n",
    "        }\n",
    "    },\n",
    "    \"rag\": {\n",
    "        \"basic\": {\n",
    "            \"alias\": \"metrics_rag_basic\",\n",
    "            \"template\": \"\"\"You are a financial expert. Extract the specific numerical metric from the provided financial document evidence.\n",
    "\n",
    "Evidence:\n",
    "{evidence}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Analyze the evidence and provide:\n",
    "1. Your reasoning: Explain which parts of the evidence contain the answer\n",
    "2. Final answer: Provide only the numerical value exactly as shown in the evidence (e.g., $X million, X%, etc.)\"\"\"\n",
    "        },\n",
    "        \"cot\": {\n",
    "            \"alias\": \"metrics_rag_cot\",\n",
    "            \"template\": \"\"\"You are a financial expert. Extract the specific numerical metric from the provided financial document evidence.\n",
    "\n",
    "Evidence:\n",
    "{evidence}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Solve this systematically:\n",
    "1. Locate the relevant section in the evidence\n",
    "2. Identify the exact metric requested\n",
    "3. Extract the value in the correct format\n",
    "\n",
    "Provide your complete reasoning and the final numerical answer.\"\"\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Templates for DOMAIN-RELEVANT questions\n",
    "# These questions test understanding of financial concepts and domain knowledge\n",
    "DOMAIN_RELEVANT_TEMPLATES = {\n",
    "    \"closed_book\": {\n",
    "        \"basic\": {\n",
    "            \"alias\": \"domain_closed_basic\",\n",
    "            \"template\": \"\"\"You are a financial expert. Answer the following question about financial concepts and business operations using your knowledge.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide your reasoning and a clear, accurate final answer.\"\"\"\n",
    "        },\n",
    "        \"cot\": {\n",
    "            \"alias\": \"domain_closed_cot\",\n",
    "            \"template\": \"\"\"You are a financial expert. Answer the following question about financial concepts and business operations using your knowledge.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Think through this step by step:\n",
    "1. Understand the financial concept being asked\n",
    "2. Apply relevant domain knowledge\n",
    "3. Formulate the answer\n",
    "\n",
    "Provide your reasoning process and the final answer.\"\"\"\n",
    "        }\n",
    "    },\n",
    "    \"rag\": {\n",
    "        \"basic\": {\n",
    "            \"alias\": \"domain_rag_basic\",\n",
    "            \"template\": \"\"\"You are a financial expert. Answer the question based on the provided evidence from financial documents.\n",
    "\n",
    "Evidence:\n",
    "{evidence}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide:\n",
    "1. Your reasoning: Analyze the relevant information from the evidence\n",
    "2. Final answer: A clear, concise answer based strictly on the evidence\"\"\"\n",
    "        },\n",
    "        \"cot\": {\n",
    "            \"alias\": \"domain_rag_cot\",\n",
    "            \"template\": \"\"\"You are a financial expert. Answer the question using the provided evidence from financial documents.\n",
    "\n",
    "Evidence:\n",
    "{evidence}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Analyze this carefully:\n",
    "1. Identify relevant domain information in the evidence\n",
    "2. Connect it to the question being asked\n",
    "3. Formulate a precise answer\n",
    "\n",
    "Provide your complete reasoning and the final answer.\"\"\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Templates for NOVEL-GENERATED questions\n",
    "# These questions require synthesis and reasoning beyond simple extraction\n",
    "NOVEL_GENERATED_TEMPLATES = {\n",
    "    \"closed_book\": {\n",
    "        \"basic\": {\n",
    "            \"alias\": \"novel_closed_basic\",\n",
    "            \"template\": \"\"\"You are a financial expert. Answer the following question that requires analysis and synthesis using your knowledge.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide your analytical reasoning and a well-supported final answer.\"\"\"\n",
    "        },\n",
    "        \"cot\": {\n",
    "            \"alias\": \"novel_closed_cot\",\n",
    "            \"template\": \"\"\"You are a financial expert. Answer the following question that requires analysis and synthesis using your knowledge.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Work through this systematically:\n",
    "1. Break down what the question is asking\n",
    "2. Identify the information and analysis needed\n",
    "3. Synthesize an answer with clear reasoning\n",
    "\n",
    "Provide your complete reasoning process and the final answer.\"\"\"\n",
    "        }\n",
    "    },\n",
    "    \"rag\": {\n",
    "        \"basic\": {\n",
    "            \"alias\": \"novel_rag_basic\",\n",
    "            \"template\": \"\"\"You are a financial expert. Answer the question by analyzing and synthesizing information from the provided financial document evidence.\n",
    "\n",
    "Evidence:\n",
    "{evidence}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide:\n",
    "1. Your reasoning: Analyze and synthesize the relevant information from the evidence\n",
    "2. Final answer: A clear, reasoned answer based on your analysis\"\"\"\n",
    "        },\n",
    "        \"cot\": {\n",
    "            \"alias\": \"novel_rag_cot\",\n",
    "            \"template\": \"\"\"You are a financial expert. Answer the question by analyzing and synthesizing information from the provided financial document evidence.\n",
    "\n",
    "Evidence:\n",
    "{evidence}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Approach this analytically:\n",
    "1. Review all relevant information in the evidence\n",
    "2. Identify connections and patterns\n",
    "3. Synthesize a comprehensive answer\n",
    "\n",
    "Provide your complete analytical reasoning and the final answer.\"\"\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Template variables restructured successfully!\")\n",
    "print(f\"   METRICS_GENERATED_TEMPLATES: {list(METRICS_GENERATED_TEMPLATES.keys())}\")\n",
    "print(f\"   DOMAIN_RELEVANT_TEMPLATES: {list(DOMAIN_RELEVANT_TEMPLATES.keys())}\")\n",
    "print(f\"   NOVEL_GENERATED_TEMPLATES: {list(NOVEL_GENERATED_TEMPLATES.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d8f29c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: PatronusAI/financebench (split: train)...\n",
      "‚úÖ Dataset loaded successfully!\n",
      "   Total questions: 150\n",
      "   Dataset features: dict_keys(['financebench_id', 'company', 'doc_name', 'question_type', 'question_reasoning', 'domain_question_num', 'question', 'answer', 'justification', 'dataset_subset_label', 'evidence', 'gics_sector', 'doc_type', 'doc_period', 'doc_link'])\n",
      "\n",
      "================================================================================\n",
      "SAMPLE ENTRY:\n",
      "================================================================================\n",
      "ID: financebench_id_03029\n",
      "Company: 3M\n",
      "Question Type: metrics-generated\n",
      "Question: What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the quest...\n",
      "Answer: $1577.00\n",
      "Number of evidence pieces: 1\n",
      "First evidence preview: Table of Contents \n",
      "3M Company and Subsidiaries\n",
      "Consolidated Statement of Cash Flow s\n",
      "Years ended December 31\n",
      " \n",
      "(Millions)\n",
      " \n",
      "2018\n",
      " \n",
      "2017\n",
      " \n",
      "2016\n",
      " \n",
      "Cash ...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: LOAD FINANCEBENCH DATASET\n",
    "# ============================================================================\n",
    "\n",
    "# Load dataset from HuggingFace\n",
    "print(f\"Loading dataset: {DATASET_NAME} (split: {DATASET_SPLIT})...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=DATASET_SPLIT)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"   Total questions: {len(dataset)}\")\n",
    "print(f\"   Dataset features: {dataset.features.keys()}\")\n",
    "\n",
    "# Display a sample entry to verify structure\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE ENTRY:\")\n",
    "print(\"=\"*80)\n",
    "sample = dataset[0]\n",
    "print(f\"ID: {sample['financebench_id']}\")\n",
    "print(f\"Company: {sample['company']}\")\n",
    "print(f\"Question Type: {sample['question_type']}\")\n",
    "print(f\"Question: {sample['question'][:100]}...\")\n",
    "print(f\"Answer: {sample['answer']}\")\n",
    "print(f\"Number of evidence pieces: {len(sample['evidence'])}\")\n",
    "if len(sample['evidence']) > 0:\n",
    "    print(f\"First evidence preview: {sample['evidence'][0]['evidence_text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "827fb07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangChain prompt templates created successfully!\n",
      "   Question types: ['metrics-generated', 'domain-relevant', 'novel-generated']\n",
      "   Modes per type: ['closed_book', 'rag']\n",
      "   Variants per mode: ['basic', 'cot']\n",
      "\n",
      "================================================================================\n",
      "SAMPLE TEMPLATES:\n",
      "================================================================================\n",
      "\n",
      "1. METRICS-GENERATED / CLOSED-BOOK / BASIC:\n",
      "--------------------------------------------------------------------------------\n",
      "System: You are a financial expert assistant.\n",
      "Human: You are a financial expert. Answer the following question about a specific financial metric using only your knowledge.\n",
      "\n",
      "Question: What is the FY2020 revenue of Apple in USD millions?\n",
      "\n",
      "Provide your step-by-step reasoning, then give the final numerical answer (e.g., $X million, X%, etc.).\n",
      "\n",
      "2. DOMAIN-RELEVANT / RAG / BASIC:\n",
      "--------------------------------------------------------------------------------\n",
      "System: You are a financial expert assistant.\n",
      "Human: You are a financial expert. Answer the question based on the provided evidence from financial documents.\n",
      "\n",
      "Evidence:\n",
      "Risk Factors: The company faces competition from other tech firms...\n",
      "\n",
      "Question: What are the main risk factors for the company?\n",
      "\n",
      "Provide:\n",
      "1. Your reasoning: Analyze the relevant information from the evidence\n",
      "2. Final answer: A clear, concise answer based strictly on the evidence\n",
      "\n",
      "3. NOVEL-GENERATED / RAG / COT:\n",
      "--------------------------------------------------------------------------------\n",
      "System: You are a financial expert assistant.\n",
      "Human: You are a financial expert. Answer the question by analyzing and synthesizing information from the provided financial document evidence.\n",
      "\n",
      "Evidence:\n",
      "2019 debt: $50M. 2020 debt: $75M...\n",
      "\n",
      "Question: How has the company's debt structure changed?\n",
      "\n",
      "Approach this analytically:\n",
      "1. Review all relevant information in the evidence\n",
      "2. Identify connections and patterns\n",
      "3. Synthesize a comprehensive answer\n",
      "\n",
      "Provide your complete analytical reasoning and the final answer.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: LANGCHAIN PROMPT TEMPLATES\n",
    "# ============================================================================\n",
    "\n",
    "def create_langchain_templates(template_dict):\n",
    "    \"\"\"\n",
    "    Convert template dictionary to LangChain ChatPromptTemplate objects.\n",
    "    \n",
    "    Args:\n",
    "        template_dict: Dictionary with structure {mode: {variant: {alias, template}}}\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with LangChain prompt objects added\n",
    "    \"\"\"\n",
    "    langchain_dict = {}\n",
    "    \n",
    "    for mode, variants in template_dict.items():\n",
    "        langchain_dict[mode] = {}\n",
    "        for variant, template_info in variants.items():\n",
    "            langchain_dict[mode][variant] = {\n",
    "                \"alias\": template_info[\"alias\"],\n",
    "                \"template\": template_info[\"template\"],\n",
    "                \"langchain_prompt\": ChatPromptTemplate.from_messages([\n",
    "                    (\"system\", \"You are a financial expert assistant.\"),\n",
    "                    (\"human\", template_info[\"template\"])\n",
    "                ])\n",
    "            }\n",
    "    \n",
    "    return langchain_dict\n",
    "\n",
    "# Convert all template dictionaries to LangChain format\n",
    "METRICS_GENERATED_LANGCHAIN = create_langchain_templates(METRICS_GENERATED_TEMPLATES)\n",
    "DOMAIN_RELEVANT_LANGCHAIN = create_langchain_templates(DOMAIN_RELEVANT_TEMPLATES)\n",
    "NOVEL_GENERATED_LANGCHAIN = create_langchain_templates(NOVEL_GENERATED_TEMPLATES)\n",
    "\n",
    "# Create a master dictionary for easy access by question type\n",
    "LANGCHAIN_TEMPLATES_BY_QUESTION_TYPE = {\n",
    "    \"metrics-generated\": METRICS_GENERATED_LANGCHAIN,\n",
    "    \"domain-relevant\": DOMAIN_RELEVANT_LANGCHAIN,\n",
    "    \"novel-generated\": NOVEL_GENERATED_LANGCHAIN\n",
    "}\n",
    "\n",
    "print(\"‚úÖ LangChain prompt templates created successfully!\")\n",
    "print(f\"   Question types: {list(LANGCHAIN_TEMPLATES_BY_QUESTION_TYPE.keys())}\")\n",
    "print(f\"   Modes per type: {list(METRICS_GENERATED_LANGCHAIN.keys())}\")\n",
    "print(f\"   Variants per mode: {list(METRICS_GENERATED_LANGCHAIN['closed_book'].keys())}\")\n",
    "\n",
    "# Test: Display sample templates\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE TEMPLATES:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test metrics-generated closed-book basic\n",
    "print(\"\\n1. METRICS-GENERATED / CLOSED-BOOK / BASIC:\")\n",
    "print(\"-\" * 80)\n",
    "test_prompt = METRICS_GENERATED_LANGCHAIN[\"closed_book\"][\"basic\"][\"langchain_prompt\"]\n",
    "print(test_prompt.format(question=\"What is the FY2020 revenue of Apple in USD millions?\"))\n",
    "\n",
    "# Test domain-relevant rag basic\n",
    "print(\"\\n2. DOMAIN-RELEVANT / RAG / BASIC:\")\n",
    "print(\"-\" * 80)\n",
    "test_prompt = DOMAIN_RELEVANT_LANGCHAIN[\"rag\"][\"basic\"][\"langchain_prompt\"]\n",
    "print(test_prompt.format(\n",
    "    question=\"What are the main risk factors for the company?\",\n",
    "    evidence=\"Risk Factors: The company faces competition from other tech firms...\"\n",
    "))\n",
    "\n",
    "# Test novel-generated rag cot\n",
    "print(\"\\n3. NOVEL-GENERATED / RAG / COT:\")\n",
    "print(\"-\" * 80)\n",
    "test_prompt = NOVEL_GENERATED_LANGCHAIN[\"rag\"][\"cot\"][\"langchain_prompt\"]\n",
    "print(test_prompt.format(\n",
    "    question=\"How has the company's debt structure changed?\",\n",
    "    evidence=\"2019 debt: $50M. 2020 debt: $75M...\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff1e9924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialization function created successfully!\n",
      "   ‚úÖ Test initialization successful: OpenAI GPT-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: LLM MODEL INITIALIZATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def get_llm(provider: str, model: str, temperature: float):\n",
    "    \"\"\"\n",
    "    Initialize and return a LangChain LLM based on provider.\n",
    "    \n",
    "    Args:\n",
    "        provider: One of 'openai', 'anthropic', 'ollama'\n",
    "        model: Model name (e.g., 'gpt-4o', 'claude-sonnet-4', 'llama3.1:8b')\n",
    "        temperature: Temperature for generation (0.0 for deterministic)\n",
    "    \n",
    "    Returns:\n",
    "        LangChain chat model instance\n",
    "    \"\"\"\n",
    "    if provider == \"openai\":\n",
    "        if not OPENAI_API_KEY:\n",
    "            raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "        return ChatOpenAI(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            api_key=OPENAI_API_KEY\n",
    "        )\n",
    "    \n",
    "    elif provider == \"anthropic\":\n",
    "        if not ANTHROPIC_API_KEY:\n",
    "            raise ValueError(\"ANTHROPIC_API_KEY not found in environment variables\")\n",
    "        return ChatAnthropic(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            api_key=ANTHROPIC_API_KEY\n",
    "        )\n",
    "    \n",
    "    elif provider == \"ollama\":\n",
    "        return ChatOllama(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            base_url=OLLAMA_BASE_URL\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}. Supported: openai, anthropic, ollama\")\n",
    "\n",
    "print(\"‚úÖ LLM initialization function created successfully!\")\n",
    "\n",
    "# Test: Initialize a sample LLM (if API key available)\n",
    "try:\n",
    "    test_llm = get_llm(\"openai\", \"gpt-4o-mini\", 0.0)\n",
    "    print(f\"   ‚úÖ Test initialization successful: OpenAI GPT-4o-mini\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Could not initialize test LLM: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e9efb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions created successfully!\n",
      "\n",
      "================================================================================\n",
      "TESTING HELPER FUNCTIONS:\n",
      "================================================================================\n",
      "Sanitize 'gpt-4o' ‚Üí 'gpt-4o'\n",
      "Sanitize 'llama3.1:8b' ‚Üí 'llama3-1_8b'\n",
      "\n",
      "Generated filename (closed-book): closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "Expected: closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "\n",
      "Generated filename (oracle): oracle_domain-relevant_openai_gpt-4o_0.0_domain_rag_cot.json\n",
      "Expected: oracle_domain-relevant_openai_gpt-4o_0.0_domain_rag_cot.json\n",
      "\n",
      "File exists: True\n",
      "\n",
      "Formatted evidence (first 200 chars):\n",
      "Table of Contents \n",
      "3M Company and Subsidiaries\n",
      "Consolidated Statement of Cash Flow s\n",
      "Years ended December 31\n",
      " \n",
      "(Millions)\n",
      " \n",
      "2018\n",
      " \n",
      "2017\n",
      " \n",
      "2016\n",
      " \n",
      "Cash Flows from Operating Activities\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Net ...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def sanitize_model_name(model_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize model name for use in filenames.\n",
    "    Replace special characters with underscores or hyphens.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Original model name (e.g., 'llama3.1:8b', 'gpt-4o')\n",
    "    \n",
    "    Returns:\n",
    "        Sanitized model name (e.g., 'llama3-1_8b', 'gpt-4o')\n",
    "    \"\"\"\n",
    "    # Replace : with _ and . with -\n",
    "    sanitized = model_name.replace(\":\", \"_\").replace(\".\", \"-\")\n",
    "    return sanitized\n",
    "\n",
    "\n",
    "def generate_filename(config: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Generate output filename based on configuration.\n",
    "    Format: {mode}_{question_type}_{provider}_{model}_{temperature}_{template_alias}.json\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Filename string\n",
    "    \"\"\"\n",
    "    mode = config[\"mode\"]\n",
    "    provider = config[\"provider\"]\n",
    "    model = sanitize_model_name(config[\"model\"])\n",
    "    temperature = config[\"temperature\"]\n",
    "    question_type = config[\"question_type\"]\n",
    "    template_key = config[\"template_key\"]\n",
    "    \n",
    "    # Get template alias based on question_type and mode\n",
    "    # Map mode: \"oracle\" uses \"rag\" templates\n",
    "    template_mode = \"rag\" if mode == \"oracle\" else mode\n",
    "    \n",
    "    template_dict = LANGCHAIN_TEMPLATES_BY_QUESTION_TYPE[question_type][template_mode][template_key]\n",
    "    template_alias = template_dict[\"alias\"]\n",
    "    \n",
    "    filename = f\"{mode}_{question_type}_{provider}_{model}_{temperature}_{template_alias}.json\"\n",
    "    return filename\n",
    "\n",
    "\n",
    "def check_file_exists(config: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if output file for this configuration already exists.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "    \n",
    "    Returns:\n",
    "        True if file exists, False otherwise\n",
    "    \"\"\"\n",
    "    filename = generate_filename(config)\n",
    "    filepath = Path(OUTPUT_DIR) / filename\n",
    "    return filepath.exists()\n",
    "\n",
    "\n",
    "def format_evidence(evidence_list: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Format evidence pieces for oracle/RAG mode prompts.\n",
    "    Handles single or multiple evidence pieces with clear separation.\n",
    "    \n",
    "    Args:\n",
    "        evidence_list: List of evidence dictionaries from FinanceBench\n",
    "    \n",
    "    Returns:\n",
    "        Formatted evidence string\n",
    "    \"\"\"\n",
    "    if not evidence_list or len(evidence_list) == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    # Single evidence piece\n",
    "    if len(evidence_list) == 1:\n",
    "        return evidence_list[0][\"evidence_text\"]\n",
    "    \n",
    "    # Multiple evidence pieces - format with clear separation\n",
    "    formatted_parts = []\n",
    "    for idx, evidence in enumerate(evidence_list, 1):\n",
    "        formatted_parts.append(f\"Evidence {idx}:\\n{evidence['evidence_text']}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted_parts)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions created successfully!\")\n",
    "\n",
    "# Test helper functions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING HELPER FUNCTIONS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test 1: Sanitize model name\n",
    "test_model_1 = \"gpt-4o\"\n",
    "test_model_2 = \"llama3.1:8b\"\n",
    "print(f\"Sanitize 'gpt-4o' ‚Üí '{sanitize_model_name(test_model_1)}'\")\n",
    "print(f\"Sanitize 'llama3.1:8b' ‚Üí '{sanitize_model_name(test_model_2)}'\")\n",
    "\n",
    "# Test 2: Generate filename with question_type (closed-book)\n",
    "test_config = {\n",
    "    \"mode\": \"closed_book\",\n",
    "    \"provider\": \"openai\",\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"temperature\": 0.0,\n",
    "    \"question_type\": \"metrics-generated\",\n",
    "    \"template_key\": \"basic\"\n",
    "}\n",
    "test_filename = generate_filename(test_config)\n",
    "print(f\"\\nGenerated filename (closed-book): {test_filename}\")\n",
    "print(f\"Expected: closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\")\n",
    "\n",
    "# Test 3: Generate filename for oracle mode (uses \"rag\" templates)\n",
    "test_config_oracle = {\n",
    "    \"mode\": \"oracle\",\n",
    "    \"provider\": \"openai\",\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"temperature\": 0.0,\n",
    "    \"question_type\": \"domain-relevant\",\n",
    "    \"template_key\": \"cot\"\n",
    "}\n",
    "test_filename_oracle = generate_filename(test_config_oracle)\n",
    "print(f\"\\nGenerated filename (oracle): {test_filename_oracle}\")\n",
    "print(f\"Expected: oracle_domain-relevant_openai_gpt-4o_0.0_domain_rag_cot.json\")\n",
    "\n",
    "# Test 4: Check file exists\n",
    "file_exists = check_file_exists(test_config)\n",
    "print(f\"\\nFile exists: {file_exists}\")\n",
    "\n",
    "# Test 5: Format evidence (using sample from dataset)\n",
    "sample_evidence = dataset[0][\"evidence\"]\n",
    "formatted_evidence = format_evidence(sample_evidence)\n",
    "print(f\"\\nFormatted evidence (first 200 chars):\\n{formatted_evidence[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "196e699c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM caller with retry logic created successfully!\n",
      "\n",
      "================================================================================\n",
      "TESTING LLM CALLER:\n",
      "================================================================================\n",
      "‚úÖ Test successful!\n",
      "   Prompt: What is 2+2? Answer with just the number.\n",
      "   Answer: 4\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: LLM CALLER WITH RETRY LOGIC\n",
    "# ============================================================================\n",
    "\n",
    "def call_llm_with_retry(\n",
    "    llm,\n",
    "    prompt: str,\n",
    "    max_retries: int = MAX_RETRIES,\n",
    "    retry_delay: int = RETRY_DELAY,\n",
    "    call_delay: float = CALL_DELAY\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Call LLM with retry logic and rate limiting.\n",
    "    \n",
    "    Args:\n",
    "        llm: LangChain LLM instance\n",
    "        prompt: Formatted prompt string\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        retry_delay: Seconds to wait before retry after failure\n",
    "        call_delay: Seconds to wait between successful calls (rate limiting)\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer text\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If all retries fail, stops execution\n",
    "    \"\"\"\n",
    "    last_error = None\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Invoke the LLM\n",
    "            response = llm.invoke(prompt)\n",
    "            \n",
    "            # Extract text content from response\n",
    "            if hasattr(response, 'content'):\n",
    "                answer = response.content\n",
    "            else:\n",
    "                answer = str(response)\n",
    "            \n",
    "            # Rate limiting: wait before next call\n",
    "            time.sleep(call_delay)\n",
    "            \n",
    "            return answer.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            print(f\"   ‚ö†Ô∏è  Attempt {attempt + 1}/{max_retries} failed: {str(e)}\")\n",
    "            \n",
    "            # If not the last attempt, wait before retry\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"   ‚è≥ Waiting {retry_delay} seconds before retry...\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                # Last attempt failed - stop execution\n",
    "                print(f\"   ‚ùå All {max_retries} attempts failed!\")\n",
    "                raise Exception(f\"LLM call failed after {max_retries} attempts: {str(last_error)}\")\n",
    "    \n",
    "    # Should not reach here, but just in case\n",
    "    raise Exception(f\"LLM call failed: {str(last_error)}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ LLM caller with retry logic created successfully!\")\n",
    "\n",
    "# Test: Call LLM with a simple prompt (if API key available)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING LLM CALLER:\")\n",
    "print(\"=\"*80)\n",
    "try:\n",
    "    test_llm = get_llm(\"openai\", \"gpt-4o-mini\", 0.0)\n",
    "    test_prompt = \"What is 2+2? Answer with just the number.\"\n",
    "    test_answer = call_llm_with_retry(test_llm, test_prompt)\n",
    "    print(f\"‚úÖ Test successful!\")\n",
    "    print(f\"   Prompt: {test_prompt}\")\n",
    "    print(f\"   Answer: {test_answer}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not test LLM caller: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13d8c7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Query processor with hybrid structured output created successfully!\n",
      "\n",
      "================================================================================\n",
      "TESTING QUERY PROCESSOR WITH STRUCTURED OUTPUT:\n",
      "================================================================================\n",
      "Processing query: financebench_id_03029\n",
      "Question type: metrics-generated\n",
      "Question: What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the quest...\n",
      "Provider: openai (using structured output)\n",
      "\n",
      "‚úÖ Test successful!\n",
      "   Ground truth: $1577.00\n",
      "   Reasoning (first 200 chars): To determine the FY2018 capital expenditure amount for 3M, we need to refer to the cash flow statement for that fiscal year. Capital expenditures (CapEx) are typically listed under the investing activ...\n",
      "   Final answer: $1,700 million\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: QUERY PROCESSOR WITH HYBRID STRUCTURED OUTPUT\n",
    "# ============================================================================\n",
    "\n",
    "def process_query(\n",
    "    query_data: Dict[str, Any],\n",
    "    config: Dict[str, Any],\n",
    "    llm,\n",
    "    prompt_template: ChatPromptTemplate\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single query and generate a structured answer.\n",
    "    \n",
    "    Hybrid Approach:\n",
    "    - OpenAI/Anthropic: Use native structured output (with_structured_output)\n",
    "    - Ollama: Use JsonOutputParser as fallback\n",
    "    \n",
    "    Args:\n",
    "        query_data: Single query from FinanceBench dataset\n",
    "        config: Configuration dictionary\n",
    "        llm: LangChain LLM instance\n",
    "        prompt_template: LangChain ChatPromptTemplate\n",
    "    \n",
    "    Returns:\n",
    "        Query data with 'reasoning' and 'generated_answer' fields added\n",
    "    \"\"\"\n",
    "    mode = config[\"mode\"]\n",
    "    provider = config[\"provider\"]\n",
    "    question = query_data[\"question\"]\n",
    "    \n",
    "    # Prepare variables based on mode\n",
    "    if mode == \"closed_book\":\n",
    "        # Closed-book: only question\n",
    "        variables = {\"question\": question}\n",
    "    else:  # oracle mode\n",
    "        # Oracle: question + evidence from FinanceBench annotations\n",
    "        evidence = query_data.get(\"evidence\", [])\n",
    "        formatted_evidence = format_evidence(evidence)\n",
    "        variables = {\n",
    "            \"question\": question,\n",
    "            \"evidence\": formatted_evidence\n",
    "        }\n",
    "    \n",
    "    # Build chain based on provider (HYBRID APPROACH)\n",
    "    if provider in [\"openai\", \"anthropic\"]:\n",
    "        # Use native structured output for OpenAI/Anthropic\n",
    "        # This uses the model's function calling API for reliable JSON\n",
    "        structured_llm = llm.with_structured_output(FinanceAnswer)\n",
    "        chain = prompt_template | structured_llm\n",
    "    else:\n",
    "        # Use JsonOutputParser for Ollama and other providers\n",
    "        # This adds JSON format instructions to the prompt automatically\n",
    "        parser = JsonOutputParser(pydantic_object=FinanceAnswer)\n",
    "        chain = prompt_template | llm | parser\n",
    "    \n",
    "    # Invoke the chain\n",
    "    try:\n",
    "        result = chain.invoke(variables)\n",
    "        \n",
    "        # Add delay for rate limiting\n",
    "        time.sleep(CALL_DELAY)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # If error occurs, re-raise to stop execution\n",
    "        raise Exception(f\"Failed to process query {query_data.get('financebench_id', 'unknown')}: {str(e)}\")\n",
    "    \n",
    "    # Normalize output (handle both Pydantic object and dict)\n",
    "    if hasattr(result, 'final_answer'):\n",
    "        # Pydantic object from with_structured_output()\n",
    "        reasoning = result.reasoning\n",
    "        final_answer = result.final_answer\n",
    "    elif isinstance(result, dict):\n",
    "        # Dictionary from JsonOutputParser\n",
    "        reasoning = result.get(\"reasoning\", \"\")\n",
    "        final_answer = result.get(\"final_answer\", \"\")\n",
    "    else:\n",
    "        # Fallback for unexpected format (shouldn't happen)\n",
    "        reasoning = str(result)\n",
    "        final_answer = str(result)\n",
    "    \n",
    "    # Add structured output to query data\n",
    "    result_query = query_data.copy()\n",
    "    result_query[\"reasoning\"] = reasoning.strip()\n",
    "    result_query[\"generated_answer\"] = final_answer.strip()\n",
    "    \n",
    "    return result_query\n",
    "\n",
    "\n",
    "print(\"‚úÖ Query processor with hybrid structured output created successfully!\")\n",
    "\n",
    "# Test: Process a single query\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING QUERY PROCESSOR WITH STRUCTURED OUTPUT:\")\n",
    "print(\"=\"*80)\n",
    "try:\n",
    "    # Get test configuration and LLM\n",
    "    test_config = {\n",
    "        \"mode\": \"closed_book\",\n",
    "        \"provider\": \"openai\",\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"question_type\": \"metrics-generated\",\n",
    "        \"template_key\": \"basic\",\n",
    "        \"temperature\": 0.0\n",
    "    }\n",
    "    test_llm = get_llm(test_config[\"provider\"], test_config[\"model\"], test_config[\"temperature\"])\n",
    "    \n",
    "    # Get the appropriate template\n",
    "    template_mode = \"closed_book\"\n",
    "    test_prompt_template = LANGCHAIN_TEMPLATES_BY_QUESTION_TYPE[\n",
    "        test_config[\"question_type\"]\n",
    "    ][template_mode][test_config[\"template_key\"]][\"langchain_prompt\"]\n",
    "    \n",
    "    # Find a metrics-generated query from dataset\n",
    "    test_query = None\n",
    "    for query in dataset:\n",
    "        if query[\"question_type\"] == \"metrics-generated\":\n",
    "            test_query = query\n",
    "            break\n",
    "    \n",
    "    if test_query:\n",
    "        print(f\"Processing query: {test_query['financebench_id']}\")\n",
    "        print(f\"Question type: {test_query['question_type']}\")\n",
    "        print(f\"Question: {test_query['question'][:100]}...\")\n",
    "        print(f\"Provider: {test_config['provider']} (using {'structured output' if test_config['provider'] in ['openai', 'anthropic'] else 'JsonOutputParser'})\")\n",
    "        \n",
    "        result = process_query(test_query, test_config, test_llm, test_prompt_template)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Test successful!\")\n",
    "        print(f\"   Ground truth: {result['answer']}\")\n",
    "        print(f\"   Reasoning (first 200 chars): {result['reasoning'][:200]}...\")\n",
    "        print(f\"   Final answer: {result['generated_answer']}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No metrics-generated query found in dataset\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not test query processor: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "697ffe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metadata builder function created successfully!\n",
      "\n",
      "================================================================================\n",
      "TESTING METADATA BUILDER:\n",
      "================================================================================\n",
      "{\n",
      "  \"mode\": \"closed_book\",\n",
      "  \"provider\": \"openai\",\n",
      "  \"model\": \"gpt-4o-mini\",\n",
      "  \"temperature\": 0.0,\n",
      "  \"question_type\": \"metrics-generated\",\n",
      "  \"prompt_template\": \"You are a financial expert. Answer the following question about a specific financial metric using only your knowledge.\\n\\nQuestion: {question}\\n\\nProvide your step-by-step reasoning, then give the final numerical answer (e.g., $X million, X%, etc.).\",\n",
      "  \"template_alias\": \"metrics_closed_basic\",\n",
      "  \"structured_output\": true,\n",
      "  \"output_schema\": {\n",
      "    \"reasoning\": \"Step-by-step reasoning and analysis\",\n",
      "    \"final_answer\": \"The final answer only (concise and precise)\"\n",
      "  },\n",
      "  \"structured_output_method\": \"with_structured_output\",\n",
      "  \"dataset\": \"PatronusAI/financebench\",\n",
      "  \"dataset_split\": \"train\",\n",
      "  \"total_questions\": 50,\n",
      "  \"total_dataset_questions\": 150,\n",
      "  \"generated_at\": \"2025-11-08T21:17:23.384269Z\",\n",
      "  \"call_delay\": 1,\n",
      "  \"max_retries\": 3\n",
      "}\n",
      "\n",
      "Note: This configuration will process 50 out of 150 total questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lj/175ptt0d6knb0gg0lg2h4n2h0000gp/T/ipykernel_70065/2525182371.py:41: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 9: METADATA BUILDER\n",
    "# ============================================================================\n",
    "\n",
    "def build_metadata(\n",
    "    config: Dict[str, Any],\n",
    "    template_info: Dict[str, Any],\n",
    "    dataset_info: Any,\n",
    "    filtered_count: int\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build metadata object for output JSON file.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "        template_info: Template information (alias and template text)\n",
    "        dataset_info: Dataset object from HuggingFace\n",
    "        filtered_count: Number of queries matching the question_type filter\n",
    "    \n",
    "    Returns:\n",
    "        Metadata dictionary\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"mode\": config[\"mode\"],\n",
    "        \"provider\": config[\"provider\"],\n",
    "        \"model\": config[\"model\"],\n",
    "        \"temperature\": config[\"temperature\"],\n",
    "        \"question_type\": config[\"question_type\"],\n",
    "        \"prompt_template\": template_info[\"template\"],\n",
    "        \"template_alias\": template_info[\"alias\"],\n",
    "        \"structured_output\": True,\n",
    "        \"output_schema\": {\n",
    "            \"reasoning\": \"Step-by-step reasoning and analysis\",\n",
    "            \"final_answer\": \"The final answer only (concise and precise)\"\n",
    "        },\n",
    "        \"structured_output_method\": \"with_structured_output\" if config[\"provider\"] in [\"openai\", \"anthropic\"] else \"JsonOutputParser\",\n",
    "        \"dataset\": DATASET_NAME,\n",
    "        \"dataset_split\": DATASET_SPLIT,\n",
    "        \"total_questions\": filtered_count,  # Number of questions for this question_type\n",
    "        \"total_dataset_questions\": len(dataset_info),  # Total questions in dataset (150)\n",
    "        \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"call_delay\": CALL_DELAY,\n",
    "        \"max_retries\": MAX_RETRIES\n",
    "    }\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "\n",
    "print(\"‚úÖ Metadata builder function created successfully!\")\n",
    "\n",
    "# Test: Build sample metadata\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING METADATA BUILDER:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a sample test configuration\n",
    "test_config = {\n",
    "    \"mode\": \"closed_book\",\n",
    "    \"provider\": \"openai\",\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"temperature\": 0.0,\n",
    "    \"question_type\": \"metrics-generated\",\n",
    "    \"template_key\": \"basic\"\n",
    "}\n",
    "\n",
    "# Get the appropriate template info\n",
    "template_mode = \"closed_book\"\n",
    "test_template_info = LANGCHAIN_TEMPLATES_BY_QUESTION_TYPE[\n",
    "    test_config[\"question_type\"]\n",
    "][template_mode][test_config[\"template_key\"]]\n",
    "\n",
    "# Count how many queries match this question_type\n",
    "filtered_queries = [q for q in dataset if q[\"question_type\"] == test_config[\"question_type\"]]\n",
    "filtered_count = len(filtered_queries)\n",
    "\n",
    "# Build metadata\n",
    "test_metadata = build_metadata(test_config, test_template_info, dataset, filtered_count)\n",
    "\n",
    "print(json.dumps(test_metadata, indent=2))\n",
    "print(f\"\\nNote: This configuration will process {filtered_count} out of {len(dataset)} total questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "162e0bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Main execution loop function created successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 10: MAIN EXECUTION LOOP\n",
    "# ============================================================================\n",
    "\n",
    "def generate_answers_for_config(config: Dict[str, Any], dataset) -> None:\n",
    "    \"\"\"\n",
    "    Process all queries for a given configuration and save results to JSON.\n",
    "    Only processes queries matching the configuration's question_type.\n",
    "    Uses hybrid structured output approach.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "        dataset: FinanceBench dataset\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If any query fails after retries, stops execution\n",
    "    \"\"\"\n",
    "    # Check if output file already exists\n",
    "    if check_file_exists(config):\n",
    "        filename = generate_filename(config)\n",
    "        print(f\"‚è≠Ô∏è  Skipping {filename} (already exists)\")\n",
    "        return\n",
    "    \n",
    "    # Get mode, question_type, and template information\n",
    "    mode = config[\"mode\"]\n",
    "    question_type = config[\"question_type\"]\n",
    "    template_key = config[\"template_key\"]\n",
    "    \n",
    "    # Map mode: \"oracle\" uses \"rag\" templates\n",
    "    template_mode = \"rag\" if mode == \"oracle\" else mode\n",
    "    \n",
    "    # Get appropriate template based on question_type and mode\n",
    "    template_info = LANGCHAIN_TEMPLATES_BY_QUESTION_TYPE[question_type][template_mode][template_key]\n",
    "    prompt_template = template_info[\"langchain_prompt\"]\n",
    "    \n",
    "    # Filter dataset to only include queries matching this question_type\n",
    "    filtered_dataset = [q for q in dataset if q[\"question_type\"] == question_type]\n",
    "    \n",
    "    if len(filtered_dataset) == 0:\n",
    "        print(f\"‚ö†Ô∏è  No queries found for question_type: {question_type}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize LLM\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üöÄ Starting generation for: {generate_filename(config)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"   Mode: {mode}\")\n",
    "    print(f\"   Question Type: {question_type}\")\n",
    "    print(f\"   Provider: {config['provider']}\")\n",
    "    print(f\"   Model: {config['model']}\")\n",
    "    print(f\"   Temperature: {config['temperature']}\")\n",
    "    print(f\"   Template: {template_info['alias']}\")\n",
    "    print(f\"   Structured Output Method: {'with_structured_output' if config['provider'] in ['openai', 'anthropic'] else 'JsonOutputParser'}\")\n",
    "    print(f\"   Filtered queries: {len(filtered_dataset)} out of {len(dataset)}\")\n",
    "    \n",
    "    try:\n",
    "        llm = get_llm(config[\"provider\"], config[\"model\"], config[\"temperature\"])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to initialize LLM: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Build metadata with filtered count\n",
    "    metadata = build_metadata(config, template_info, dataset, len(filtered_dataset))\n",
    "    \n",
    "    # Initialize results structure\n",
    "    results = {\n",
    "        \"metadata\": metadata,\n",
    "        \"queries\": []\n",
    "    }\n",
    "    \n",
    "    # Process filtered queries with progress bar\n",
    "    print(f\"\\nüìä Processing {len(filtered_dataset)} queries for {question_type}...\")\n",
    "    \n",
    "    for idx, query in enumerate(tqdm(filtered_dataset, desc=\"Generating answers\")):\n",
    "        try:\n",
    "            # Process query with structured output\n",
    "            result = process_query(query, config, llm, prompt_template)\n",
    "            \n",
    "            # Keep only required fields in specific order (including reasoning)\n",
    "            query_result = {\n",
    "                \"financebench_id\": result[\"financebench_id\"],\n",
    "                \"question_type\": result[\"question_type\"],\n",
    "                \"question_reasoning\": result[\"question_reasoning\"],\n",
    "                \"question\": result[\"question\"],\n",
    "                \"doc_name\": result[\"doc_name\"],\n",
    "                \"company\": result[\"company\"],\n",
    "                \"answer\": result[\"answer\"],\n",
    "                \"reasoning\": result[\"reasoning\"],  # NEW FIELD - structured output\n",
    "                \"generated_answer\": result[\"generated_answer\"],  # Now contains only final answer\n",
    "                \"evidence\": result[\"evidence\"]\n",
    "            }\n",
    "            \n",
    "            results[\"queries\"].append(query_result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error processing query {idx + 1}/{len(filtered_dataset)}: {e}\")\n",
    "            print(f\"   Query ID: {query.get('financebench_id', 'unknown')}\")\n",
    "            raise  # Stop execution on error\n",
    "    \n",
    "    # Save results to JSON file\n",
    "    filename = generate_filename(config)\n",
    "    filepath = Path(OUTPUT_DIR) / filename\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully generated and saved: {filename}\")\n",
    "    print(f\"   Total queries processed: {len(results['queries'])}\")\n",
    "    print(f\"   File size: {filepath.stat().st_size / 1024:.2f} KB\")\n",
    "    print(f\"   Output includes structured reasoning + final answers\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Main execution loop function created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4fb3624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Run all configurations function created successfully!\n",
      "\n",
      "================================================================================\n",
      "‚ö†Ô∏è  READY TO EXECUTE\n",
      "================================================================================\n",
      "To start generating answers, define your CONFIGURATIONS and run:\n",
      "   run_all_configurations(CONFIGURATIONS)\n",
      "\n",
      "This will process all configurations and may take significant time.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 11: RUN ALL CONFIGURATIONS\n",
    "# ============================================================================\n",
    "\n",
    "def run_all_configurations(configurations: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Execute all configurations in the provided configurations list.\n",
    "    Generates JSON files for each configuration.\n",
    "    \n",
    "    Args:\n",
    "        configurations: List of configuration dictionaries to process\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"üéØ STARTING BATCH GENERATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total configurations: {len(configurations)}\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    print(f\"Dataset: {DATASET_NAME} ({len(dataset)} questions)\")\n",
    "    \n",
    "    # Show breakdown by question type\n",
    "    question_type_counts = {}\n",
    "    for q in dataset:\n",
    "        qt = q[\"question_type\"]\n",
    "        question_type_counts[qt] = question_type_counts.get(qt, 0) + 1\n",
    "    \n",
    "    print(f\"\\nQuestion type distribution:\")\n",
    "    for qt, count in sorted(question_type_counts.items()):\n",
    "        print(f\"  ‚Ä¢ {qt}: {count} questions\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Track statistics\n",
    "    generated_count = 0\n",
    "    skipped_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    # Process each configuration\n",
    "    for idx, config in enumerate(configurations, 1):\n",
    "        print(f\"\\n[{idx}/{len(configurations)}] Processing configuration...\")\n",
    "        print(f\"   Mode: {config['mode']}, Question Type: {config['question_type']}, Model: {config['model']}\")\n",
    "        \n",
    "        try:\n",
    "            # Check if file exists before processing\n",
    "            if check_file_exists(config):\n",
    "                skipped_count += 1\n",
    "            else:\n",
    "                generate_answers_for_config(config, dataset)\n",
    "                generated_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed_count += 1\n",
    "            print(f\"\\n‚ùå FATAL ERROR: Configuration failed!\")\n",
    "            print(f\"   Config: {config}\")\n",
    "            print(f\"   Error: {e}\")\n",
    "            print(f\"\\n‚õî Stopping execution due to error.\")\n",
    "            break\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä GENERATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úÖ Generated: {generated_count}\")\n",
    "    print(f\"‚è≠Ô∏è  Skipped (already exists): {skipped_count}\")\n",
    "    print(f\"‚ùå Failed: {failed_count}\")\n",
    "    print(f\"üìÅ Output directory: {OUTPUT_DIR}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if failed_count == 0:\n",
    "        print(\"üéâ All configurations completed successfully!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Some configurations failed. Check errors above.\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Run all configurations function created successfully!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ö†Ô∏è  READY TO EXECUTE\")\n",
    "print(\"=\"*80)\n",
    "print(\"To start generating answers, define your CONFIGURATIONS and run:\")\n",
    "print(\"   run_all_configurations(CONFIGURATIONS)\")\n",
    "print(\"\\nThis will process all configurations and may take significant time.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00867e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lj/175ptt0d6knb0gg0lg2h4n2h0000gp/T/ipykernel_70065/2525182371.py:41: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"generated_at\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìã CONFIGURATION STATISTICS\n",
      "================================================================================\n",
      "Total configurations: 2\n",
      "  ‚Ä¢ Closed-book: 1\n",
      "  ‚Ä¢ Oracle: 1\n",
      "\n",
      "By question type:\n",
      "  ‚Ä¢ metrics-generated: 2\n",
      "\n",
      "By provider:\n",
      "  ‚Ä¢ openai: 2\n",
      "\n",
      "================================================================================\n",
      "üìÇ FILE STATUS CHECK\n",
      "================================================================================\n",
      "Existing files: 1\n",
      "To be generated: 1\n",
      "\n",
      "‚úÖ Already exist (1):\n",
      "  [1] closed_book_metrics-generated_openai_gpt-4o_0.0_metrics_closed_basic.json\n",
      "\n",
      "üîÑ Will generate (1):\n",
      "  [2] oracle_metrics-generated_openai_gpt-4o_0.0_metrics_rag_basic.json\n",
      "      Mode: oracle | Question Type: metrics-generated | Provider: openai | Model: gpt-4o\n",
      "\n",
      "‚è±Ô∏è  Estimated time: ~0.8 minutes\n",
      "   (Based on 50 total filtered queries √ó 1s delay)\n",
      "================================================================================\n",
      "\n",
      "üöÄ Ready to start generation!\n",
      "   This will process all configurations listed above.\n",
      "================================================================================\n",
      "================================================================================\n",
      "üéØ STARTING BATCH GENERATION\n",
      "================================================================================\n",
      "Total configurations: 2\n",
      "Output directory: ../../generation_set/closedbook_oracle_sets\n",
      "Dataset: PatronusAI/financebench (150 questions)\n",
      "\n",
      "Question type distribution:\n",
      "  ‚Ä¢ domain-relevant: 50 questions\n",
      "  ‚Ä¢ metrics-generated: 50 questions\n",
      "  ‚Ä¢ novel-generated: 50 questions\n",
      "================================================================================\n",
      "\n",
      "[1/2] Processing configuration...\n",
      "   Mode: closed_book, Question Type: metrics-generated, Model: gpt-4o\n",
      "\n",
      "[2/2] Processing configuration...\n",
      "   Mode: oracle, Question Type: metrics-generated, Model: gpt-4o\n",
      "\n",
      "================================================================================\n",
      "üöÄ Starting generation for: oracle_metrics-generated_openai_gpt-4o_0.0_metrics_rag_basic.json\n",
      "================================================================================\n",
      "   Mode: oracle\n",
      "   Question Type: metrics-generated\n",
      "   Provider: openai\n",
      "   Model: gpt-4o\n",
      "   Temperature: 0.0\n",
      "   Template: metrics_rag_basic\n",
      "   Structured Output Method: with_structured_output\n",
      "   Filtered queries: 50 out of 150\n",
      "\n",
      "üìä Processing 50 queries for metrics-generated...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [02:55<00:00,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Successfully generated and saved: oracle_metrics-generated_openai_gpt-4o_0.0_metrics_rag_basic.json\n",
      "   Total queries processed: 50\n",
      "   File size: 389.98 KB\n",
      "   Output includes structured reasoning + final answers\n",
      "\n",
      "================================================================================\n",
      "üìä GENERATION SUMMARY\n",
      "================================================================================\n",
      "‚úÖ Generated: 1\n",
      "‚è≠Ô∏è  Skipped (already exists): 1\n",
      "‚ùå Failed: 0\n",
      "üìÅ Output directory: ../../generation_set/closedbook_oracle_sets\n",
      "================================================================================\n",
      "üéâ All configurations completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXECUTION CONFIGURATIONS & BATCH GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "provider = \"openai\"  # Change as needed: \"openai\", \"anthropic\", \"ollama\"\n",
    "model = \"gpt-4o\"  # Change as needed\n",
    "\n",
    "# Execution Configurations\n",
    "# Each configuration will generate a separate output JSON file\n",
    "CONFIGURATIONS = [\n",
    "    # Closed-Book Configurations - Metrics Generated\n",
    "    {\n",
    "        \"mode\": \"closed_book\",\n",
    "        \"provider\": provider,\n",
    "        \"model\": model,\n",
    "        \"question_type\": \"metrics-generated\",\n",
    "        \"template_key\": \"basic\",\n",
    "        \"temperature\": 0.0\n",
    "    },\n",
    "    \n",
    "    # # Closed-Book Configurations - Domain Relevant\n",
    "    # {\n",
    "    #     \"mode\": \"closed_book\",\n",
    "    #     \"provider\": \"openai\",\n",
    "    #     \"model\": \"gpt-4o-mini\",\n",
    "    #     \"question_type\": \"domain-relevant\",\n",
    "    #     \"template_key\": \"basic\",\n",
    "    #     \"temperature\": 0.0\n",
    "    # },\n",
    "    \n",
    "    # # Closed-Book Configurations - Novel Generated\n",
    "    # {\n",
    "    #     \"mode\": \"closed_book\",\n",
    "    #     \"provider\": \"openai\",\n",
    "    #     \"model\": \"gpt-4o-mini\",\n",
    "    #     \"question_type\": \"novel-generated\",\n",
    "    #     \"template_key\": \"basic\",\n",
    "    #     \"temperature\": 0.0\n",
    "    # },\n",
    "    \n",
    "    # Oracle Configurations - Metrics Generated\n",
    "    {\n",
    "        \"mode\": \"oracle\",\n",
    "        \"provider\": provider,\n",
    "        \"model\": model,\n",
    "        \"question_type\": \"metrics-generated\",\n",
    "        \"template_key\": \"basic\",\n",
    "        \"temperature\": 0.0\n",
    "    },\n",
    "    \n",
    "    # # Oracle Configurations - Domain Relevant\n",
    "    # {\n",
    "    #     \"mode\": \"oracle\",\n",
    "    #     \"provider\": \"openai\",\n",
    "    #     \"model\": \"gpt-4o-mini\",\n",
    "    #     \"question_type\": \"domain-relevant\",\n",
    "    #     \"template_key\": \"basic\",\n",
    "    #     \"temperature\": 0.0\n",
    "    # },\n",
    "    \n",
    "    # # Oracle Configurations - Novel Generated\n",
    "    # {\n",
    "    #     \"mode\": \"oracle\",\n",
    "    #     \"provider\": \"openai\",\n",
    "    #     \"model\": \"gpt-4o-mini\",\n",
    "    #     \"question_type\": \"novel-generated\",\n",
    "    #     \"template_key\": \"basic\",\n",
    "    #     \"temperature\": 0.0\n",
    "    # },\n",
    "]\n",
    "\n",
    "# Display configuration statistics\n",
    "print(\"=\"*80)\n",
    "print(\"üìã CONFIGURATION STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Count by mode\n",
    "closed_book_count = sum(1 for c in CONFIGURATIONS if c[\"mode\"] == \"closed_book\")\n",
    "oracle_count = sum(1 for c in CONFIGURATIONS if c[\"mode\"] == \"oracle\")\n",
    "\n",
    "print(f\"Total configurations: {len(CONFIGURATIONS)}\")\n",
    "print(f\"  ‚Ä¢ Closed-book: {closed_book_count}\")\n",
    "print(f\"  ‚Ä¢ Oracle: {oracle_count}\")\n",
    "\n",
    "# Count by question type\n",
    "question_types = {}\n",
    "for config in CONFIGURATIONS:\n",
    "    qt = config[\"question_type\"]\n",
    "    question_types[qt] = question_types.get(qt, 0) + 1\n",
    "\n",
    "print(f\"\\nBy question type:\")\n",
    "for qt, count in sorted(question_types.items()):\n",
    "    print(f\"  ‚Ä¢ {qt}: {count}\")\n",
    "\n",
    "# Count by provider\n",
    "providers = {}\n",
    "for config in CONFIGURATIONS:\n",
    "    provider = config[\"provider\"]\n",
    "    providers[provider] = providers.get(provider, 0) + 1\n",
    "\n",
    "print(f\"\\nBy provider:\")\n",
    "for provider, count in sorted(providers.items()):\n",
    "    print(f\"  ‚Ä¢ {provider}: {count}\")\n",
    "\n",
    "# Check which files already exist and which need to be generated\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìÇ FILE STATUS CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "existing_files = []\n",
    "to_generate = []\n",
    "\n",
    "for idx, config in enumerate(CONFIGURATIONS, 1):\n",
    "    filename = generate_filename(config)\n",
    "    exists = check_file_exists(config)\n",
    "    \n",
    "    if exists:\n",
    "        existing_files.append((idx, filename, config))\n",
    "    else:\n",
    "        to_generate.append((idx, filename, config))\n",
    "\n",
    "print(f\"Existing files: {len(existing_files)}\")\n",
    "print(f\"To be generated: {len(to_generate)}\")\n",
    "\n",
    "# Show existing files\n",
    "if existing_files:\n",
    "    print(f\"\\n‚úÖ Already exist ({len(existing_files)}):\")\n",
    "    for idx, filename, config in existing_files:\n",
    "        print(f\"  [{idx}] {filename}\")\n",
    "\n",
    "# Show files to be generated\n",
    "if to_generate:\n",
    "    print(f\"\\nüîÑ Will generate ({len(to_generate)}):\")\n",
    "    for idx, filename, config in to_generate:\n",
    "        print(f\"  [{idx}] {filename}\")\n",
    "        print(f\"      Mode: {config['mode']} | Question Type: {config['question_type']} | Provider: {config['provider']} | Model: {config['model']}\")\n",
    "\n",
    "# Estimate time (considering filtered queries)\n",
    "if to_generate:\n",
    "    # Calculate total queries to process (accounting for question_type filtering)\n",
    "    total_queries_to_process = 0\n",
    "    for _, _, config in to_generate:\n",
    "        filtered_count = sum(1 for q in dataset if q[\"question_type\"] == config[\"question_type\"])\n",
    "        total_queries_to_process += filtered_count\n",
    "    \n",
    "    estimated_time_minutes = (total_queries_to_process * CALL_DELAY) / 60\n",
    "    print(f\"\\n‚è±Ô∏è  Estimated time: ~{estimated_time_minutes:.1f} minutes\")\n",
    "    print(f\"   (Based on {total_queries_to_process} total filtered queries √ó {CALL_DELAY}s delay)\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Confirmation before starting\n",
    "if to_generate:\n",
    "    print(\"\\nüöÄ Ready to start generation!\")\n",
    "    print(\"   This will process all configurations listed above.\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Start the batch generation process\n",
    "    run_all_configurations(CONFIGURATIONS)\n",
    "else:\n",
    "    print(\"\\n‚úÖ All files already exist. Nothing to generate.\")\n",
    "    print(\"   Delete files from output directory if you want to regenerate.\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0780609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç VERIFYING GENERATED FILES\n",
      "================================================================================\n",
      "Output directory: ../../generation_set/closedbook_oracle_sets\n",
      "Total JSON files found: 7\n",
      "\n",
      "üìÅ Generated files:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "book:\n",
      "  1. closed_book_domain-relevant_openai_gpt-4o-mini_0.0_domain_closed_basic.json (338.87 KB)\n",
      "  2. closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json (402.86 KB)\n",
      "  3. closed_book_metrics-generated_openai_gpt-4o_0.0_metrics_closed_basic.json (414.28 KB)\n",
      "  4. closed_book_novel-generated_openai_gpt-4o-mini_0.0_novel_closed_basic.json (324.01 KB)\n",
      "\n",
      "domain-relevant:\n",
      "  1. oracle_domain-relevant_openai_gpt-4o-mini_0.0_domain_rag_basic.json (336.46 KB)\n",
      "\n",
      "metrics-generated:\n",
      "  1. oracle_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic.json (386.78 KB)\n",
      "\n",
      "novel-generated:\n",
      "  1. oracle_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic.json (314.31 KB)\n",
      "\n",
      "================================================================================\n",
      "üìÑ SAMPLE FILE INSPECTION\n",
      "================================================================================\n",
      "Inspecting: closed_book_domain-relevant_openai_gpt-4o-mini_0.0_domain_closed_basic.json\n",
      "\n",
      "METADATA:\n",
      "--------------------------------------------------------------------------------\n",
      "  mode: closed_book\n",
      "  provider: openai\n",
      "  model: gpt-4o-mini\n",
      "  temperature: 0.0\n",
      "  question_type: domain-relevant\n",
      "  prompt_template: You are a financial expert. Answer the following question about financial concepts and business oper...\n",
      "  template_alias: domain_closed_basic\n",
      "  structured_output: True\n",
      "  output_schema:\n",
      "    - reasoning: Step-by-step reasoning and analysis\n",
      "    - final_answer: The final answer only (concise and precise)\n",
      "  structured_output_method: with_structured_output\n",
      "  dataset: PatronusAI/financebench\n",
      "  dataset_split: train\n",
      "  total_questions: 50\n",
      "  total_dataset_questions: 150\n",
      "  generated_at: 2025-10-28T09:21:55.548558Z\n",
      "  call_delay: 1\n",
      "  max_retries: 3\n",
      "\n",
      "QUERIES:\n",
      "--------------------------------------------------------------------------------\n",
      "  Total queries: 50\n",
      "  Question type filter: domain-relevant\n",
      "  Structured output: True\n",
      "  Method: with_structured_output\n",
      "\n",
      "  First query example:\n",
      "    ID: financebench_id_00499\n",
      "    Question Type: domain-relevant\n",
      "    Company: 3M\n",
      "    Question: Is 3M a capital-intensive business based on FY2022 data?...\n",
      "    Ground truth: No, the company is managing its CAPEX and Fixed Assets pretty efficiently, which is evident from below key metrics:\n",
      "CAPEX/Revenue Ratio: 5.1%\n",
      "Fixed assets/Total Assets: 20%\n",
      "Return on Assets= 12.4%\n",
      "    Reasoning (first 150 chars): To determine if 3M is a capital-intensive business, we need to analyze its financial data from FY2022, particularly focusing on its capital expenditur...\n",
      "    Generated answer: No, 3M is not a capital-intensive business based on FY2022 data.\n",
      "    Evidence pieces: 3\n",
      "\n",
      "================================================================================\n",
      "‚úÖ STRUCTURE VALIDATION\n",
      "================================================================================\n",
      "  ‚úÖ closed_book_novel-generated_openai_gpt-4o-mini_0.0_novel_closed_basic.json\n",
      "      Question Type: novel-generated, Queries: 50, Method: with_structured_output\n",
      "  ‚úÖ closed_book_metrics-generated_openai_gpt-4o_0.0_metrics_closed_basic.json\n",
      "      Question Type: metrics-generated, Queries: 50, Method: with_structured_output\n",
      "  ‚úÖ oracle_domain-relevant_openai_gpt-4o-mini_0.0_domain_rag_basic.json\n",
      "      Question Type: domain-relevant, Queries: 50, Method: with_structured_output\n",
      "  ‚úÖ closed_book_metrics-generated_openai_gpt-4o-mini_0.0_metrics_closed_basic.json\n",
      "      Question Type: metrics-generated, Queries: 50, Method: with_structured_output\n",
      "  ‚úÖ oracle_metrics-generated_openai_gpt-4o-mini_0.0_metrics_rag_basic.json\n",
      "      Question Type: metrics-generated, Queries: 50, Method: with_structured_output\n",
      "  ‚úÖ closed_book_domain-relevant_openai_gpt-4o-mini_0.0_domain_closed_basic.json\n",
      "      Question Type: domain-relevant, Queries: 50, Method: with_structured_output\n",
      "  ‚úÖ oracle_novel-generated_openai_gpt-4o-mini_0.0_novel_rag_basic.json\n",
      "      Question Type: novel-generated, Queries: 50, Method: with_structured_output\n",
      "\n",
      "================================================================================\n",
      "üéâ All files validated successfully!\n",
      "   Total files: 7\n",
      "   Total queries across all files: 350\n",
      "   All files use structured output with reasoning + final_answer\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 13: VERIFICATION & TESTING\n",
    "# ============================================================================\n",
    "\n",
    "def verify_generated_files():\n",
    "    \"\"\"\n",
    "    Verify all generated JSON files and display summary.\n",
    "    Includes validation for structured output fields.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"üîç VERIFYING GENERATED FILES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get all JSON files in output directory\n",
    "    output_path = Path(OUTPUT_DIR)\n",
    "    json_files = list(output_path.glob(\"*.json\"))\n",
    "    \n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    print(f\"Total JSON files found: {len(json_files)}\\n\")\n",
    "    \n",
    "    if len(json_files) == 0:\n",
    "        print(\"‚ö†Ô∏è  No JSON files found. Run run_all_configurations() first.\")\n",
    "        return\n",
    "    \n",
    "    # List all files grouped by question type\n",
    "    print(\"üìÅ Generated files:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    files_by_question_type = {}\n",
    "    for file in json_files:\n",
    "        # Extract question type from filename (format: mode_question-type_provider_...)\n",
    "        parts = file.stem.split(\"_\")\n",
    "        if len(parts) >= 2:\n",
    "            question_type = parts[1]\n",
    "            if question_type not in files_by_question_type:\n",
    "                files_by_question_type[question_type] = []\n",
    "            files_by_question_type[question_type].append(file)\n",
    "    \n",
    "    for qt in sorted(files_by_question_type.keys()):\n",
    "        print(f\"\\n{qt}:\")\n",
    "        for idx, file in enumerate(sorted(files_by_question_type[qt]), 1):\n",
    "            file_size = file.stat().st_size / 1024  # KB\n",
    "            print(f\"  {idx}. {file.name} ({file_size:.2f} KB)\")\n",
    "    \n",
    "    # Load and inspect a sample file\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìÑ SAMPLE FILE INSPECTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sample_file = sorted(json_files)[0]\n",
    "    print(f\"Inspecting: {sample_file.name}\\n\")\n",
    "    \n",
    "    with open(sample_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Display metadata\n",
    "    print(\"METADATA:\")\n",
    "    print(\"-\" * 80)\n",
    "    for key, value in data[\"metadata\"].items():\n",
    "        if key == \"prompt_template\":\n",
    "            # Truncate long template\n",
    "            print(f\"  {key}: {value[:100]}...\")\n",
    "        elif key == \"output_schema\":\n",
    "            # Show schema on separate lines\n",
    "            print(f\"  {key}:\")\n",
    "            for field, desc in value.items():\n",
    "                print(f\"    - {field}: {desc}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Display queries info\n",
    "    print(f\"\\nQUERIES:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Total queries: {len(data['queries'])}\")\n",
    "    print(f\"  Question type filter: {data['metadata'].get('question_type', 'N/A')}\")\n",
    "    print(f\"  Structured output: {data['metadata'].get('structured_output', False)}\")\n",
    "    print(f\"  Method: {data['metadata'].get('structured_output_method', 'N/A')}\")\n",
    "    \n",
    "    # Show first query as example\n",
    "    if len(data[\"queries\"]) > 0:\n",
    "        print(f\"\\n  First query example:\")\n",
    "        first_query = data[\"queries\"][0]\n",
    "        print(f\"    ID: {first_query['financebench_id']}\")\n",
    "        print(f\"    Question Type: {first_query['question_type']}\")\n",
    "        print(f\"    Company: {first_query['company']}\")\n",
    "        print(f\"    Question: {first_query['question'][:100]}...\")\n",
    "        print(f\"    Ground truth: {first_query['answer']}\")\n",
    "        print(f\"    Reasoning (first 150 chars): {first_query['reasoning'][:150]}...\")\n",
    "        print(f\"    Generated answer: {first_query['generated_answer']}\")\n",
    "        print(f\"    Evidence pieces: {len(first_query['evidence'])}\")\n",
    "    \n",
    "    # Verify all files have correct structure\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ STRUCTURE VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_valid = True\n",
    "    validation_summary = []\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Check required top-level fields\n",
    "            assert \"metadata\" in data, \"Missing 'metadata' field\"\n",
    "            assert \"queries\" in data, \"Missing 'queries' field\"\n",
    "            \n",
    "            # Check metadata fields (including structured output fields)\n",
    "            required_metadata = [\n",
    "                \"mode\", \"provider\", \"model\", \"temperature\", \"question_type\",\n",
    "                \"prompt_template\", \"template_alias\", \n",
    "                \"structured_output\", \"output_schema\", \"structured_output_method\",  # NEW\n",
    "                \"dataset\", \"total_questions\", \"total_dataset_questions\", \n",
    "                \"generated_at\", \"call_delay\", \"max_retries\"\n",
    "            ]\n",
    "            for field in required_metadata:\n",
    "                assert field in data[\"metadata\"], f\"Missing metadata field: {field}\"\n",
    "            \n",
    "            # Validate structured output metadata\n",
    "            assert data[\"metadata\"][\"structured_output\"] == True, \\\n",
    "                \"structured_output should be True\"\n",
    "            assert \"reasoning\" in data[\"metadata\"][\"output_schema\"], \\\n",
    "                \"output_schema missing 'reasoning' field\"\n",
    "            assert \"final_answer\" in data[\"metadata\"][\"output_schema\"], \\\n",
    "                \"output_schema missing 'final_answer' field\"\n",
    "            \n",
    "            # Validate question_type consistency\n",
    "            expected_question_type = data[\"metadata\"][\"question_type\"]\n",
    "            actual_queries = len(data[\"queries\"])\n",
    "            expected_queries = data[\"metadata\"][\"total_questions\"]\n",
    "            \n",
    "            assert actual_queries == expected_queries, \\\n",
    "                f\"Query count mismatch: expected {expected_queries}, got {actual_queries}\"\n",
    "            \n",
    "            # Verify all queries match the question_type filter\n",
    "            for query in data[\"queries\"]:\n",
    "                assert query[\"question_type\"] == expected_question_type, \\\n",
    "                    f\"Query {query['financebench_id']} has type {query['question_type']}, expected {expected_question_type}\"\n",
    "            \n",
    "            # Check query fields (including new reasoning field)\n",
    "            if len(data[\"queries\"]) > 0:\n",
    "                required_query_fields = [\n",
    "                    \"financebench_id\", \"question_type\", \"question_reasoning\",\n",
    "                    \"question\", \"doc_name\", \"company\", \"answer\", \n",
    "                    \"reasoning\",  # NEW FIELD\n",
    "                    \"generated_answer\", \"evidence\"\n",
    "                ]\n",
    "                for field in required_query_fields:\n",
    "                    assert field in data[\"queries\"][0], f\"Missing query field: {field}\"\n",
    "                \n",
    "                # Validate that reasoning and generated_answer are not empty\n",
    "                first_query = data[\"queries\"][0]\n",
    "                assert len(first_query[\"reasoning\"].strip()) > 0, \\\n",
    "                    \"Reasoning field is empty\"\n",
    "                assert len(first_query[\"generated_answer\"].strip()) > 0, \\\n",
    "                    \"Generated answer field is empty\"\n",
    "            \n",
    "            validation_summary.append({\n",
    "                \"file\": json_file.name,\n",
    "                \"status\": \"‚úÖ\",\n",
    "                \"question_type\": expected_question_type,\n",
    "                \"queries\": actual_queries,\n",
    "                \"method\": data[\"metadata\"][\"structured_output_method\"]\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            validation_summary.append({\n",
    "                \"file\": json_file.name,\n",
    "                \"status\": \"‚ùå\",\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            all_valid = False\n",
    "    \n",
    "    # Display validation results\n",
    "    for item in validation_summary:\n",
    "        if item[\"status\"] == \"‚úÖ\":\n",
    "            print(f\"  {item['status']} {item['file']}\")\n",
    "            print(f\"      Question Type: {item['question_type']}, Queries: {item['queries']}, Method: {item['method']}\")\n",
    "        else:\n",
    "            print(f\"  {item['status']} {item['file']}: {item['error']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    if all_valid:\n",
    "        print(\"üéâ All files validated successfully!\")\n",
    "        print(f\"   Total files: {len(json_files)}\")\n",
    "        total_queries = sum(item['queries'] for item in validation_summary if 'queries' in item)\n",
    "        print(f\"   Total queries across all files: {total_queries}\")\n",
    "        print(f\"   All files use structured output with reasoning + final_answer\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Some files have validation errors.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "# Run verification\n",
    "verify_generated_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
