{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c34ef090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running quick tests for token_f1()...\n",
      "âœ“ Novel-generated: partial overlap                   F1=0.300 P=0.250 R=0.375\n",
      "âœ“ Novel-generated: good overlap                      F1=0.545 P=0.375 R=1.000\n",
      "âœ“ Exact match                                        F1=1.000 P=1.000 R=1.000\n",
      "âœ“ Domain-relevant: close match with extra word       F1=0.923 P=0.857 R=1.000\n",
      "âœ“ Domain-relevant: some overlap, different numbers   F1=0.400 P=0.400 R=0.400\n",
      "\n",
      "Results: 5 passed, 0 failed out of 5 tests\n",
      "\n",
      "âœ… All quick tests passed!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Token F1 Calculator for Financial QA Evaluation\n",
    "================================================\n",
    "\n",
    "This module provides token-level F1 score calculation for evaluating\n",
    "text-based answers (novel-generated and domain-relevant questions).\n",
    "\n",
    "Features:\n",
    "- Token-level precision, recall, and F1\n",
    "- Text normalization (lowercase, punctuation removal, etc.)\n",
    "- Optional stopword removal\n",
    "- Detailed token analysis (common, missing, extra)\n",
    "\n",
    "Author: Financial QA Evaluation System\n",
    "Version: 1.0\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import string\n",
    "from typing import Dict, Any, List, Set, Optional\n",
    "\n",
    "\n",
    "# Standard English stopwords (common words that don't carry much meaning)\n",
    "ENGLISH_STOPWORDS = {\n",
    "    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',\n",
    "    'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',\n",
    "    'to', 'was', 'were', 'will', 'with', 'the', 'this', 'but', 'they',\n",
    "    'have', 'had', 'what', 'when', 'where', 'who', 'which', 'why', 'how'\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_text(text: str, remove_punctuation: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text for token comparison.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        remove_punctuation: Whether to remove punctuation\n",
    "    \n",
    "    Returns:\n",
    "        Normalized text\n",
    "    \n",
    "    Examples:\n",
    "        >>> normalize_text(\"The Consumer Segment!\")\n",
    "        'the consumer segment'\n",
    "        \n",
    "        >>> normalize_text(\"3M's revenue\")\n",
    "        '3ms revenue'\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation if requested\n",
    "    if remove_punctuation:\n",
    "        # Keep alphanumeric and spaces, remove everything else\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize(text: str, normalize: bool = True, remove_stopwords: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize text into words.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        normalize: Whether to normalize text first\n",
    "        remove_stopwords: Whether to remove stopwords\n",
    "    \n",
    "    Returns:\n",
    "        List of tokens\n",
    "    \n",
    "    Examples:\n",
    "        >>> tokenize(\"The consumer segment shrunk by 0.9% organically.\")\n",
    "        ['the', 'consumer', 'segment', 'shrunk', 'by', '0', '9', 'organically']\n",
    "        \n",
    "        >>> tokenize(\"The consumer segment shrunk by 0.9% organically.\", remove_stopwords=True)\n",
    "        ['consumer', 'segment', 'shrunk', '0', '9', 'organically']\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        text = normalize_text(text, remove_punctuation=True)\n",
    "    \n",
    "    # Split on whitespace\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords if requested\n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in ENGLISH_STOPWORDS]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def token_f1(\n",
    "    gold_answer: str,\n",
    "    generated_answer: str,\n",
    "    normalize: bool = True,\n",
    "    remove_stopwords: bool = False,\n",
    "    return_details: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculate token-level F1 score between gold and generated answers.\n",
    "    \n",
    "    This metric is used for:\n",
    "    - novel-generated questions (always)\n",
    "    - domain-relevant questions with short/medium answers\n",
    "    \n",
    "    Args:\n",
    "        gold_answer: The gold standard answer\n",
    "        generated_answer: The generated answer to evaluate\n",
    "        normalize: Whether to normalize text (lowercase, remove punctuation)\n",
    "        remove_stopwords: Whether to remove common stopwords\n",
    "        return_details: If True, return full details; if False, return only scores\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - f1: float - F1 score (0-1)\n",
    "            - precision: float - Precision (0-1)\n",
    "            - recall: float - Recall (0-1)\n",
    "            - gold_tokens: List[str] - Tokens from gold answer\n",
    "            - gen_tokens: List[str] - Tokens from generated answer\n",
    "            - common_tokens: Set[str] - Tokens present in both\n",
    "            - missing_tokens: Set[str] - Tokens in gold but not in generated\n",
    "            - extra_tokens: Set[str] - Tokens in generated but not in gold\n",
    "            - gold_token_count: int - Number of gold tokens\n",
    "            - gen_token_count: int - Number of generated tokens\n",
    "            - common_token_count: int - Number of common tokens\n",
    "    \n",
    "    Examples:\n",
    "        >>> token_f1(\n",
    "        ...     \"The consumer segment shrunk by 0.9% organically.\",\n",
    "        ...     \"The Consumer segment has dragged down 3M's overall growth in 2022.\"\n",
    "        ... )\n",
    "        {'f1': 0.4, 'precision': 0.25, 'recall': 1.0, ...}\n",
    "        \n",
    "        >>> token_f1(\n",
    "        ...     \"Cross currency swaps. Its notional value was $32,502 million.\",\n",
    "        ...     \"Cross currency swaps had the highest notional value in FY 2021, at $32,502 million.\"\n",
    "        ... )\n",
    "        {'f1': 0.73, 'precision': 0.73, 'recall': 0.73, ...}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize both answers\n",
    "    gold_tokens = tokenize(gold_answer, normalize=normalize, remove_stopwords=remove_stopwords)\n",
    "    gen_tokens = tokenize(generated_answer, normalize=normalize, remove_stopwords=remove_stopwords)\n",
    "    \n",
    "    # Convert to sets for comparison (but keep lists for counts)\n",
    "    gold_set = set(gold_tokens)\n",
    "    gen_set = set(gen_tokens)\n",
    "    \n",
    "    # Special case: both empty\n",
    "    if not gold_set and not gen_set:\n",
    "        return {\n",
    "            'f1': 1.0,\n",
    "            'precision': 1.0,\n",
    "            'recall': 1.0,\n",
    "            'gold_tokens': gold_tokens,\n",
    "            'gen_tokens': gen_tokens,\n",
    "            'common_tokens': set(),\n",
    "            'missing_tokens': set(),\n",
    "            'extra_tokens': set(),\n",
    "            'gold_token_count': 0,\n",
    "            'gen_token_count': 0,\n",
    "            'common_token_count': 0,\n",
    "            'gold_unique_count': 0,\n",
    "            'gen_unique_count': 0,\n",
    "        } if return_details else {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}\n",
    "    \n",
    "    # Calculate overlaps\n",
    "    common = gold_set & gen_set\n",
    "    missing = gold_set - gen_set\n",
    "    extra = gen_set - gold_set\n",
    "    \n",
    "    # Calculate metrics\n",
    "    common_count = len(common)\n",
    "    gold_count = len(gold_set)\n",
    "    gen_count = len(gen_set)\n",
    "    \n",
    "    # Precision: what fraction of generated tokens are correct\n",
    "    precision = common_count / gen_count if gen_count > 0 else 0.0\n",
    "    \n",
    "    # Recall: what fraction of gold tokens are captured\n",
    "    recall = common_count / gold_count if gold_count > 0 else 0.0\n",
    "    \n",
    "    # F1: harmonic mean of precision and recall\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    \n",
    "    result = {\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }\n",
    "    \n",
    "    if return_details:\n",
    "        result.update({\n",
    "            'gold_tokens': gold_tokens,\n",
    "            'gen_tokens': gen_tokens,\n",
    "            'common_tokens': common,\n",
    "            'missing_tokens': missing,\n",
    "            'extra_tokens': extra,\n",
    "            'gold_token_count': len(gold_tokens),\n",
    "            'gen_token_count': len(gen_tokens),\n",
    "            'common_token_count': common_count,\n",
    "            'gold_unique_count': gold_count,\n",
    "            'gen_unique_count': gen_count,\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def batch_token_f1(\n",
    "    gold_answers: List[str],\n",
    "    generated_answers: List[str],\n",
    "    normalize: bool = True,\n",
    "    remove_stopwords: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculate token F1 for multiple answer pairs.\n",
    "    \n",
    "    Args:\n",
    "        gold_answers: List of gold standard answers\n",
    "        generated_answers: List of generated answers (same length)\n",
    "        normalize: Whether to normalize text\n",
    "        remove_stopwords: Whether to remove stopwords\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - results: List of individual results\n",
    "            - mean_f1: Mean F1 score\n",
    "            - mean_precision: Mean precision\n",
    "            - mean_recall: Mean recall\n",
    "            - median_f1: Median F1 score\n",
    "            - scores: List of F1 scores for distribution analysis\n",
    "    \n",
    "    Example:\n",
    "        >>> gold = [\n",
    "        ...     \"The consumer segment shrunk by 0.9% organically.\",\n",
    "        ...     \"Cross currency swaps. Its notional value was $32,502 million.\"\n",
    "        ... ]\n",
    "        >>> generated = [\n",
    "        ...     \"The Consumer segment.\",\n",
    "        ...     \"Cross currency swaps had the highest notional value at $32,502 million.\"\n",
    "        ... ]\n",
    "        >>> results = batch_token_f1(gold, generated)\n",
    "        >>> print(f\"Mean F1: {results['mean_f1']:.2f}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(gold_answers) != len(generated_answers):\n",
    "        raise ValueError(\n",
    "            f\"Length mismatch: {len(gold_answers)} gold vs {len(generated_answers)} generated\"\n",
    "        )\n",
    "    \n",
    "    results = []\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    \n",
    "    for gold, gen in zip(gold_answers, generated_answers):\n",
    "        result = token_f1(\n",
    "            gold, gen, \n",
    "            normalize=normalize, \n",
    "            remove_stopwords=remove_stopwords,\n",
    "            return_details=True\n",
    "        )\n",
    "        results.append(result)\n",
    "        f1_scores.append(result['f1'])\n",
    "        precision_scores.append(result['precision'])\n",
    "        recall_scores.append(result['recall'])\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0\n",
    "    mean_precision = sum(precision_scores) / len(precision_scores) if precision_scores else 0.0\n",
    "    mean_recall = sum(recall_scores) / len(recall_scores) if recall_scores else 0.0\n",
    "    \n",
    "    # Median\n",
    "    sorted_f1 = sorted(f1_scores)\n",
    "    median_f1 = sorted_f1[len(sorted_f1)//2] if sorted_f1 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'total': len(gold_answers),\n",
    "        'mean_f1': mean_f1,\n",
    "        'mean_precision': mean_precision,\n",
    "        'mean_recall': mean_recall,\n",
    "        'median_f1': median_f1,\n",
    "        'f1_scores': f1_scores,\n",
    "        'precision_scores': precision_scores,\n",
    "        'recall_scores': recall_scores,\n",
    "    }\n",
    "\n",
    "\n",
    "def token_overlap_ratio(\n",
    "    gold_answer: str,\n",
    "    generated_answer: str,\n",
    "    normalize: bool = True\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Simple token overlap ratio (Jaccard similarity).\n",
    "    Alternative simpler metric to F1.\n",
    "    \n",
    "    Args:\n",
    "        gold_answer: Gold standard answer\n",
    "        generated_answer: Generated answer\n",
    "        normalize: Whether to normalize text\n",
    "    \n",
    "    Returns:\n",
    "        Overlap ratio (0-1): |intersection| / |union|\n",
    "    \n",
    "    Example:\n",
    "        >>> token_overlap_ratio(\"consumer segment\", \"consumer segment growth\")\n",
    "        0.67  # 2 common tokens out of 3 total unique tokens\n",
    "    \"\"\"\n",
    "    gold_tokens = set(tokenize(gold_answer, normalize=normalize, remove_stopwords=False))\n",
    "    gen_tokens = set(tokenize(generated_answer, normalize=normalize, remove_stopwords=False))\n",
    "    \n",
    "    if not gold_tokens and not gen_tokens:\n",
    "        return 1.0  # Both empty\n",
    "    \n",
    "    intersection = gold_tokens & gen_tokens\n",
    "    union = gold_tokens | gen_tokens\n",
    "    \n",
    "    return len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "\n",
    "def _test_token_f1():\n",
    "    \"\"\"Quick sanity tests for token_f1\"\"\"\n",
    "    \n",
    "    print(\"Running quick tests for token_f1()...\")\n",
    "    \n",
    "    test_cases = [\n",
    "        # (gold, generated, expected_f1_range, description)\n",
    "        (\n",
    "            \"The consumer segment shrunk by 0.9% organically.\",\n",
    "            \"The Consumer segment has dragged down 3M's overall growth in 2022.\",\n",
    "            (0.2, 0.5),\n",
    "            \"Novel-generated: partial overlap\"\n",
    "        ),\n",
    "        (\n",
    "            \"Cross currency swaps.\",\n",
    "            \"Cross currency swaps had the highest notional value.\",\n",
    "            (0.4, 0.7),\n",
    "            \"Novel-generated: good overlap\"\n",
    "        ),\n",
    "        (\n",
    "            \"Yes. It decreased.\",\n",
    "            \"Yes. It decreased.\",\n",
    "            (0.95, 1.0),\n",
    "            \"Exact match\"\n",
    "        ),\n",
    "        (\n",
    "            \"The quick ratio is 1.57\",\n",
    "            \"The quick ratio is approximately 1.57\",\n",
    "            (0.8, 1.0),\n",
    "            \"Domain-relevant: close match with extra word\"\n",
    "        ),\n",
    "        (\n",
    "            \"AES has converted inventory 9.5 times in FY 2022.\",\n",
    "            \"AES Corporation sold its inventory roughly 12 times in FY2022.\",\n",
    "            (0.3, 0.6),\n",
    "            \"Domain-relevant: some overlap, different numbers\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for gold, gen, (min_f1, max_f1), desc in test_cases:\n",
    "        result = token_f1(gold, gen)\n",
    "        f1 = result['f1']\n",
    "        \n",
    "        if min_f1 <= f1 <= max_f1:\n",
    "            passed += 1\n",
    "            print(f\"âœ“ {desc:50} F1={f1:.3f} P={result['precision']:.3f} R={result['recall']:.3f}\")\n",
    "        else:\n",
    "            failed += 1\n",
    "            print(f\"âœ— {desc:50}\")\n",
    "            print(f\"  Expected F1 in [{min_f1}, {max_f1}], got {f1:.3f}\")\n",
    "            print(f\"  Common tokens: {result['common_tokens']}\")\n",
    "            print(f\"  Missing: {result['missing_tokens']}\")\n",
    "    \n",
    "    print(f\"\\nResults: {passed} passed, {failed} failed out of {len(test_cases)} tests\")\n",
    "    return failed == 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = _test_token_f1()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nâœ… All quick tests passed!\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  Some tests failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0329f870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE TEST SUITE FOR token_f1()\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 1: Basic Token F1\n",
      "======================================================================\n",
      "âœ“ Partial match - 2/3 overlap                        F1=0.800\n",
      "âœ“ Exact match                                        F1=1.000\n",
      "âœ“ No overlap                                         F1=0.000\n",
      "âœ“ 50% overlap                                        F1=0.500\n",
      "âœ“ Repeated tokens collapse to unique                 F1=1.000\n",
      "\n",
      "Basic Token F1: 5/5 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 2: Text Normalization\n",
      "======================================================================\n",
      "âœ“ Case normalization                                 F1=1.000\n",
      "âœ“ Simple match                                       F1=1.000\n",
      "âœ“ Punctuation removal                                F1=1.000\n",
      "âœ“ Period removal                                     F1=1.000\n",
      "âœ“ Same numbers                                       F1=1.000\n",
      "\n",
      "Normalization: 5/5 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 3: Stopword Removal\n",
      "======================================================================\n",
      "Gold: 'The consumer segment shrunk by the margin'\n",
      "Generated: 'consumer segment shrunk margin'\n",
      "\n",
      "With stopwords:\n",
      "  F1: 0.800\n",
      "  Common tokens: {'consumer', 'shrunk', 'margin', 'segment'}\n",
      "\n",
      "Without stopwords:\n",
      "  F1: 1.000\n",
      "  Common tokens: {'consumer', 'shrunk', 'margin', 'segment'}\n",
      "\n",
      "âœ“ Stopword removal increases F1 as expected\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 4: Novel-Generated Examples from FinanceBench\n",
      "======================================================================\n",
      "\n",
      "Example 1a: closedbook - partial answer\n",
      "  F1=0.545, P=1.000, R=0.375\n",
      "  Common tokens: 3/8\n",
      "\n",
      "Example 1b: oracle - expanded answer\n",
      "  F1=0.300, P=0.250, R=0.375\n",
      "  Common tokens: 3/8\n",
      "\n",
      "Example 2a: closedbook - wrong answer\n",
      "  F1=0.231, P=0.188, R=0.300\n",
      "  Common tokens: 3/10\n",
      "\n",
      "Example 2b: oracle - correct with context\n",
      "  F1=0.640, P=0.533, R=0.800\n",
      "  Common tokens: 8/10\n",
      "\n",
      "Example 3a: closedbook - approximate answer\n",
      "  F1=0.000, P=0.000, R=0.000\n",
      "  Common tokens: 0/13\n",
      "\n",
      "Example 3b: oracle - total only\n",
      "  F1=0.125, P=0.333, R=0.077\n",
      "  Common tokens: 1/13\n",
      "\n",
      "Novel-Generated Examples: 6/6 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 5: Domain-Relevant Examples from FinanceBench\n",
      "======================================================================\n",
      "\n",
      "Example 1a: closedbook - refusal\n",
      "  F1=0.214, P=0.167, R=0.300\n",
      "  Common tokens: 3/10\n",
      "\n",
      "Example 1b: oracle - wrong number but relevant\n",
      "  F1=0.229, P=0.160, R=0.400\n",
      "  Common tokens: 4/10\n",
      "\n",
      "Example 2a: closedbook - refusal\n",
      "  F1=0.211, P=0.222, R=0.200\n",
      "  Common tokens: 4/20\n",
      "\n",
      "Example 2b: oracle - simplified correct\n",
      "  F1=0.400, P=0.600, R=0.300\n",
      "  Common tokens: 6/20\n",
      "\n",
      "Example 3a: closedbook - yes but no details\n",
      "  F1=0.154, P=0.200, R=0.125\n",
      "  Common tokens: 3/24\n",
      "\n",
      "Example 3b: oracle - yes with ratio\n",
      "  F1=0.233, P=0.263, R=0.208\n",
      "  Common tokens: 5/24\n",
      "\n",
      "Domain-Relevant Examples: 6/6 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 6: Precision vs Recall Trade-off\n",
      "======================================================================\n",
      "\n",
      "High recall, high precision\n",
      "  Generated: 'consumer segment growth revenue'\n",
      "  F1=1.000, P=1.000, R=1.000\n",
      "\n",
      "High precision, low recall\n",
      "  Generated: 'consumer segment'\n",
      "  F1=0.667, P=1.000, R=0.500\n",
      "\n",
      "Low precision, high recall\n",
      "  Generated: 'consumer segment growth revenue extra words here'\n",
      "  F1=0.727, P=0.571, R=1.000\n",
      "\n",
      "Low precision, low recall\n",
      "  Generated: 'completely different words'\n",
      "  F1=0.000, P=0.000, R=0.000\n",
      "\n",
      "Precision vs Recall: 4/4 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 7: Edge Cases\n",
      "======================================================================\n",
      "âœ“ Both empty                               F1=1.000\n",
      "âœ“ Generated empty                          F1=0.000\n",
      "âœ“ Gold empty                               F1=0.000\n",
      "âœ“ Repeated tokens                          F1=1.000\n",
      "âœ“ Simple case match                        F1=1.000\n",
      "âœ“ Simple case with punctuation             F1=1.000\n",
      "\n",
      "Edge Cases: 6/6 passed\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 8: Batch Evaluation\n",
      "======================================================================\n",
      "Total: 3\n",
      "Mean F1: 0.758\n",
      "Mean Precision: 0.889\n",
      "Mean Recall: 0.725\n",
      "Median F1: 0.727\n",
      "\n",
      "Individual F1 scores: ['0.545', '0.727', '1.000']\n",
      "\n",
      "âœ“ Batch evaluation working correctly\n",
      "\n",
      "======================================================================\n",
      "TEST SUITE 9: Token Overlap Ratio (Jaccard)\n",
      "======================================================================\n",
      "Partial overlap                          Overlap=0.667\n",
      "Exact match                              Overlap=1.000\n",
      "No overlap                               Overlap=0.000\n",
      "\n",
      "Token Overlap Ratio: 3/3 passed\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY\n",
      "======================================================================\n",
      "Total Tests: 37\n",
      "âœ“ Passed: 37\n",
      "âœ— Failed: 0\n",
      "Success Rate: 100.0%\n",
      "======================================================================\n",
      "\n",
      "ðŸŽ‰ ALL TESTS PASSED! ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comprehensive Test Suite for token_f1()\n",
    "========================================\n",
    "\n",
    "Tests cover:\n",
    "- Novel-generated question examples\n",
    "- Domain-relevant question examples\n",
    "- Normalization behavior\n",
    "- Stopword removal\n",
    "- Edge cases\n",
    "- Real FinanceBench examples\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def test_basic_token_f1():\n",
    "    \"\"\"Test basic F1 calculation\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 1: Basic Token F1\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    tests = [\n",
    "        # (gold, generated, description, expected_f1_min, expected_f1_max)\n",
    "        (\"cat dog bird\", \"cat dog\", \"Partial match - 2/3 overlap\", 0.7, 0.9),\n",
    "        (\"cat dog\", \"cat dog\", \"Exact match\", 0.99, 1.0),\n",
    "        (\"cat dog\", \"bird fish\", \"No overlap\", 0.0, 0.0),\n",
    "        (\"the cat\", \"the dog\", \"50% overlap\", 0.4, 0.7),\n",
    "        (\"abc\", \"abc abc abc\", \"Repeated tokens collapse to unique\", 0.99, 1.0),  # Sets collapse duplicates\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    for gold, gen, desc, min_f1, max_f1 in tests:\n",
    "        result = token_f1(gold, gen)\n",
    "        f1 = result['f1']\n",
    "        \n",
    "        if min_f1 <= f1 <= max_f1:\n",
    "            passed += 1\n",
    "            print(f\"âœ“ {desc:50} F1={f1:.3f}\")\n",
    "        else:\n",
    "            failed += 1\n",
    "            print(f\"âœ— {desc:50}\")\n",
    "            print(f\"  Expected F1 in [{min_f1:.3f}, {max_f1:.3f}], got {f1:.3f}\")\n",
    "            print(f\"  P={result['precision']:.3f}, R={result['recall']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nBasic Token F1: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_normalization():\n",
    "    \"\"\"Test text normalization\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 2: Text Normalization\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    tests = [\n",
    "        # (gold, generated, description, should_match)\n",
    "        (\"Consumer Segment\", \"consumer segment\", \"Case normalization\", True),\n",
    "        (\"revenue growth\", \"revenue growth\", \"Simple match\", True),\n",
    "        (\"shrunk by 0.9%\", \"shrunk by 0 9\", \"Punctuation removal\", True),\n",
    "        (\"Yes.\", \"Yes\", \"Period removal\", True),\n",
    "        (\"32502 million\", \"32502 million\", \"Same numbers\", True),  # Already normalized\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    for gold, gen, desc, should_match in tests:\n",
    "        result = token_f1(gold, gen, normalize=True)\n",
    "        f1 = result['f1']\n",
    "        \n",
    "        # Should match means F1 should be high (>0.9)\n",
    "        matches = f1 > 0.9\n",
    "        \n",
    "        if matches == should_match:\n",
    "            passed += 1\n",
    "            print(f\"âœ“ {desc:50} F1={f1:.3f}\")\n",
    "        else:\n",
    "            failed += 1\n",
    "            print(f\"âœ— {desc:50}\")\n",
    "            print(f\"  Expected match={should_match}, F1={f1:.3f}\")\n",
    "            print(f\"  Common: {result['common_tokens']}\")\n",
    "    \n",
    "    print(f\"\\nNormalization: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_stopword_removal():\n",
    "    \"\"\"Test stopword removal functionality\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 3: Stopword Removal\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Test with and without stopwords\n",
    "    gold = \"The consumer segment shrunk by the margin\"\n",
    "    gen = \"consumer segment shrunk margin\"\n",
    "    \n",
    "    result_with_stopwords = token_f1(gold, gen, remove_stopwords=False)\n",
    "    result_without_stopwords = token_f1(gold, gen, remove_stopwords=True)\n",
    "    \n",
    "    print(f\"Gold: '{gold}'\")\n",
    "    print(f\"Generated: '{gen}'\")\n",
    "    print(f\"\\nWith stopwords:\")\n",
    "    print(f\"  F1: {result_with_stopwords['f1']:.3f}\")\n",
    "    print(f\"  Common tokens: {result_with_stopwords['common_tokens']}\")\n",
    "    print(f\"\\nWithout stopwords:\")\n",
    "    print(f\"  F1: {result_without_stopwords['f1']:.3f}\")\n",
    "    print(f\"  Common tokens: {result_without_stopwords['common_tokens']}\")\n",
    "    \n",
    "    # Without stopwords should have higher F1 since we ignore 'the', 'by'\n",
    "    if result_without_stopwords['f1'] >= result_with_stopwords['f1']:\n",
    "        print(f\"\\nâœ“ Stopword removal increases F1 as expected\")\n",
    "        return 1, 0\n",
    "    else:\n",
    "        print(f\"\\nâœ— Stopword removal did not increase F1\")\n",
    "        return 0, 1\n",
    "\n",
    "\n",
    "def test_novel_generated_examples():\n",
    "    \"\"\"Test real novel-generated question examples\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 4: Novel-Generated Examples from FinanceBench\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    tests = [\n",
    "        # Example 1\n",
    "        (\n",
    "            \"The consumer segment shrunk by 0.9% organically.\",\n",
    "            \"The Consumer segment.\",\n",
    "            \"Example 1a: closedbook - partial answer\"\n",
    "        ),\n",
    "        (\n",
    "            \"The consumer segment shrunk by 0.9% organically.\",\n",
    "            \"The Consumer segment has dragged down 3M's overall growth in 2022.\",\n",
    "            \"Example 1b: oracle - expanded answer\"\n",
    "        ),\n",
    "        \n",
    "        # Example 2\n",
    "        (\n",
    "            \"Cross currency swaps. Its notional value was $32,502 million.\",\n",
    "            \"Interest rate swaps had the highest notional value among Verizon's derivative instruments in FY 2021.\",\n",
    "            \"Example 2a: closedbook - wrong answer\"\n",
    "        ),\n",
    "        (\n",
    "            \"Cross currency swaps. Its notional value was $32,502 million.\",\n",
    "            \"Cross currency swaps had the highest notional value in FY 2021, at $32,502 million.\",\n",
    "            \"Example 2b: oracle - correct with context\"\n",
    "        ),\n",
    "        \n",
    "        # Example 3\n",
    "        (\n",
    "            \"The estimated pension benefits were $1097 million, and the estimated health care and life insurance benefits were $862 million.\",\n",
    "            \"Approximately $1.5 billion.\",\n",
    "            \"Example 3a: closedbook - approximate answer\"\n",
    "        ),\n",
    "        (\n",
    "            \"The estimated pension benefits were $1097 million, and the estimated health care and life insurance benefits were $862 million.\",\n",
    "            \"$1,959 million\",\n",
    "            \"Example 3b: oracle - total only\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    for gold, gen, desc in tests:\n",
    "        result = token_f1(gold, gen)\n",
    "        \n",
    "        # Just report the scores - we're not checking specific ranges\n",
    "        print(f\"\\n{desc}\")\n",
    "        print(f\"  F1={result['f1']:.3f}, P={result['precision']:.3f}, R={result['recall']:.3f}\")\n",
    "        print(f\"  Common tokens: {len(result['common_tokens'])}/{result['gold_unique_count']}\")\n",
    "        \n",
    "        # Basic sanity check: F1 should be between 0 and 1\n",
    "        if 0 <= result['f1'] <= 1:\n",
    "            passed += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "            print(f\"  âœ— Invalid F1 score: {result['f1']}\")\n",
    "    \n",
    "    print(f\"\\nNovel-Generated Examples: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_domain_relevant_examples():\n",
    "    \"\"\"Test real domain-relevant question examples\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 5: Domain-Relevant Examples from FinanceBench\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    tests = [\n",
    "        # Example 1\n",
    "        (\n",
    "            \"AES has converted inventory 9.5 times in FY 2022.\",\n",
    "            \"The inventory turnover ratio for AES Corporation in FY2022 cannot be calculated without specific COGS and average inventory figures.\",\n",
    "            \"Example 1a: closedbook - refusal\"\n",
    "        ),\n",
    "        (\n",
    "            \"AES has converted inventory 9.5 times in FY 2022.\",\n",
    "            \"AES Corporation sold its inventory roughly 12 times in FY2022; however, conventional inventory management may not be meaningful due to the nature of its business in the energy sector.\",\n",
    "            \"Example 1b: oracle - wrong number but relevant\"\n",
    "        ),\n",
    "        \n",
    "        # Example 2\n",
    "        (\n",
    "            \"The quick ratio has slightly improved from 0.67 times to 0.69 times between FY 2023 and FY 2022.(3.4% jump)\",\n",
    "            \"The quick ratio's improvement or decline cannot be determined without specific financial data for FY2022 and FY2023.\",\n",
    "            \"Example 2a: closedbook - refusal\"\n",
    "        ),\n",
    "        (\n",
    "            \"The quick ratio has slightly improved from 0.67 times to 0.69 times between FY 2023 and FY 2022.(3.4% jump)\",\n",
    "            \"AMCOR's quick ratio has improved from FY2022 to FY2023.\",\n",
    "            \"Example 2b: oracle - simplified correct\"\n",
    "        ),\n",
    "        \n",
    "        # Example 3\n",
    "        (\n",
    "            \"Yes. The quick ratio is 1.57, calculated as (cash and cash equivalents+Short term investments+Accounts receivable, net+receivables from related parties)/ (current liabilities).\",\n",
    "            \"Yes, AMD has a reasonably healthy liquidity profile based on its quick ratio for FY22.\",\n",
    "            \"Example 3a: closedbook - yes but no details\"\n",
    "        ),\n",
    "        (\n",
    "            \"Yes. The quick ratio is 1.57, calculated as (cash and cash equivalents+Short term investments+Accounts receivable, net+receivables from related parties)/ (current liabilities).\",\n",
    "            \"Yes, AMD has a reasonably healthy liquidity profile based on its quick ratio of approximately 1.57 for FY22.\",\n",
    "            \"Example 3b: oracle - yes with ratio\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    for gold, gen, desc in tests:\n",
    "        result = token_f1(gold, gen)\n",
    "        \n",
    "        print(f\"\\n{desc}\")\n",
    "        print(f\"  F1={result['f1']:.3f}, P={result['precision']:.3f}, R={result['recall']:.3f}\")\n",
    "        print(f\"  Common tokens: {len(result['common_tokens'])}/{result['gold_unique_count']}\")\n",
    "        \n",
    "        # Basic sanity check\n",
    "        if 0 <= result['f1'] <= 1:\n",
    "            passed += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "            print(f\"  âœ— Invalid F1 score: {result['f1']}\")\n",
    "    \n",
    "    print(f\"\\nDomain-Relevant Examples: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_precision_recall_trade_off():\n",
    "    \"\"\"Test understanding of precision vs recall\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 6: Precision vs Recall Trade-off\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    gold = \"consumer segment growth revenue\"\n",
    "    \n",
    "    tests = [\n",
    "        (\"consumer segment growth revenue\", \"High recall, high precision\"),\n",
    "        (\"consumer segment\", \"High precision, low recall\"),\n",
    "        (\"consumer segment growth revenue extra words here\", \"Low precision, high recall\"),\n",
    "        (\"completely different words\", \"Low precision, low recall\"),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    for gen, desc in tests:\n",
    "        result = token_f1(gold, gen)\n",
    "        \n",
    "        print(f\"\\n{desc}\")\n",
    "        print(f\"  Generated: '{gen}'\")\n",
    "        print(f\"  F1={result['f1']:.3f}, P={result['precision']:.3f}, R={result['recall']:.3f}\")\n",
    "        \n",
    "        # Just validate that metrics are in valid range\n",
    "        if 0 <= result['precision'] <= 1 and 0 <= result['recall'] <= 1:\n",
    "            passed += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "            print(f\"  âœ— Invalid metrics\")\n",
    "    \n",
    "    print(f\"\\nPrecision vs Recall: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_edge_cases():\n",
    "    \"\"\"Test edge cases\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 7: Edge Cases\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    tests = [\n",
    "        (\"\", \"\", \"Both empty\", 1.0),\n",
    "        (\"test\", \"\", \"Generated empty\", 0.0),\n",
    "        (\"\", \"test\", \"Gold empty\", 0.0),\n",
    "        (\"a\", \"a a a a\", \"Repeated tokens\", None),  # Just check valid range\n",
    "        (\"yes\", \"Yes\", \"Simple case match\", 1.0),\n",
    "        (\"no\", \"No.\", \"Simple case with punctuation\", 1.0),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    for gold, gen, desc, expected_f1 in tests:\n",
    "        result = token_f1(gold, gen)\n",
    "        f1 = result['f1']\n",
    "        \n",
    "        if expected_f1 is not None:\n",
    "            # Check exact or approximate match\n",
    "            if abs(f1 - expected_f1) < 0.01:\n",
    "                passed += 1\n",
    "                print(f\"âœ“ {desc:40} F1={f1:.3f}\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"âœ— {desc:40}\")\n",
    "                print(f\"  Expected F1={expected_f1:.3f}, got {f1:.3f}\")\n",
    "        else:\n",
    "            # Just check valid range\n",
    "            if 0 <= f1 <= 1:\n",
    "                passed += 1\n",
    "                print(f\"âœ“ {desc:40} F1={f1:.3f}\")\n",
    "            else:\n",
    "                failed += 1\n",
    "                print(f\"âœ— {desc:40} F1={f1:.3f} out of range\")\n",
    "    \n",
    "    print(f\"\\nEdge Cases: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def test_batch_evaluation():\n",
    "    \"\"\"Test batch evaluation\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 8: Batch Evaluation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    gold_answers = [\n",
    "        \"The consumer segment shrunk by 0.9% organically.\",\n",
    "        \"Cross currency swaps. Its notional value was $32,502 million.\",\n",
    "        \"Yes. It decreased.\",\n",
    "    ]\n",
    "    \n",
    "    generated_answers = [\n",
    "        \"The Consumer segment.\",\n",
    "        \"Cross currency swaps had the highest notional value at $32,502 million.\",\n",
    "        \"Yes. It decreased.\",\n",
    "    ]\n",
    "    \n",
    "    result = batch_token_f1(gold_answers, generated_answers)\n",
    "    \n",
    "    print(f\"Total: {result['total']}\")\n",
    "    print(f\"Mean F1: {result['mean_f1']:.3f}\")\n",
    "    print(f\"Mean Precision: {result['mean_precision']:.3f}\")\n",
    "    print(f\"Mean Recall: {result['mean_recall']:.3f}\")\n",
    "    print(f\"Median F1: {result['median_f1']:.3f}\")\n",
    "    print(f\"\\nIndividual F1 scores: {[f'{s:.3f}' for s in result['f1_scores']]}\")\n",
    "    \n",
    "    # Validate\n",
    "    if (0 <= result['mean_f1'] <= 1 and \n",
    "        len(result['results']) == 3 and\n",
    "        len(result['f1_scores']) == 3):\n",
    "        print(f\"\\nâœ“ Batch evaluation working correctly\")\n",
    "        return 1, 0\n",
    "    else:\n",
    "        print(f\"\\nâœ— Batch evaluation failed validation\")\n",
    "        return 0, 1\n",
    "\n",
    "\n",
    "def test_token_overlap_ratio():\n",
    "    \"\"\"Test simpler overlap ratio metric\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SUITE 9: Token Overlap Ratio (Jaccard)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    tests = [\n",
    "        (\"consumer segment\", \"consumer segment growth\", \"Partial overlap\"),\n",
    "        (\"consumer segment\", \"consumer segment\", \"Exact match\"),\n",
    "        (\"consumer segment\", \"growth revenue\", \"No overlap\"),\n",
    "    ]\n",
    "    \n",
    "    passed, failed = 0, 0\n",
    "    for gold, gen, desc in tests:\n",
    "        ratio = token_overlap_ratio(gold, gen)\n",
    "        \n",
    "        print(f\"{desc:40} Overlap={ratio:.3f}\")\n",
    "        \n",
    "        if 0 <= ratio <= 1:\n",
    "            passed += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "            print(f\"  âœ— Invalid ratio: {ratio}\")\n",
    "    \n",
    "    print(f\"\\nToken Overlap Ratio: {passed}/{len(tests)} passed\")\n",
    "    return passed, failed\n",
    "\n",
    "\n",
    "def run_all_tests():\n",
    "    \"\"\"Run all test suites\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPREHENSIVE TEST SUITE FOR token_f1()\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_passed = 0\n",
    "    total_failed = 0\n",
    "    \n",
    "    test_suites = [\n",
    "        test_basic_token_f1,\n",
    "        test_normalization,\n",
    "        test_stopword_removal,\n",
    "        test_novel_generated_examples,\n",
    "        test_domain_relevant_examples,\n",
    "        test_precision_recall_trade_off,\n",
    "        test_edge_cases,\n",
    "        test_batch_evaluation,\n",
    "        test_token_overlap_ratio,\n",
    "    ]\n",
    "    \n",
    "    for test_func in test_suites:\n",
    "        passed, failed = test_func()\n",
    "        total_passed += passed\n",
    "        total_failed += failed\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total Tests: {total_passed + total_failed}\")\n",
    "    print(f\"âœ“ Passed: {total_passed}\")\n",
    "    print(f\"âœ— Failed: {total_failed}\")\n",
    "    print(f\"Success Rate: {100 * total_passed / (total_passed + total_failed):.1f}%\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return total_failed == 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = run_all_tests()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nðŸŽ‰ ALL TESTS PASSED! ðŸŽ‰\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  SOME TESTS FAILED - Review output above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e508e59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.400\n",
      "Precision: 0.429\n",
      "Recall: 0.375\n",
      "Common tokens: {'segment', 'the', 'consumer'}\n",
      "Missing tokens: {'organically', 'by', '0', 'shrunk', '9'}\n",
      "Mean F1: 0.636\n",
      "Mean Precision: 0.833\n",
      "Mean Recall: 0.588\n"
     ]
    }
   ],
   "source": [
    "# Single comparison\n",
    "result = token_f1(\n",
    "    gold_answer=\"The consumer segment shrunk by 0.9% organically.\",\n",
    "    generated_answer=\"The Consumer segment has dragged down growth.\",\n",
    "    normalize=True,\n",
    "    remove_stopwords=False\n",
    ")\n",
    "\n",
    "print(f\"F1: {result['f1']:.3f}\")\n",
    "print(f\"Precision: {result['precision']:.3f}\")\n",
    "print(f\"Recall: {result['recall']:.3f}\")\n",
    "print(f\"Common tokens: {result['common_tokens']}\")\n",
    "print(f\"Missing tokens: {result['missing_tokens']}\")\n",
    "\n",
    "# Batch evaluation\n",
    "results = batch_token_f1(\n",
    "    gold_answers=[\n",
    "        \"The consumer segment shrunk by 0.9% organically.\",\n",
    "        \"Cross currency swaps. Its notional value was $32,502 million.\"\n",
    "    ],\n",
    "    generated_answers=[\n",
    "        \"The Consumer segment.\",\n",
    "        \"Cross currency swaps had the highest notional value at $32,502 million.\"\n",
    "    ],\n",
    "    normalize=True,\n",
    "    remove_stopwords=False\n",
    ")\n",
    "\n",
    "print(f\"Mean F1: {results['mean_f1']:.3f}\")\n",
    "print(f\"Mean Precision: {results['mean_precision']:.3f}\")\n",
    "print(f\"Mean Recall: {results['mean_recall']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
