{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a8ab50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n",
      "✓ API keys loaded successfully\n",
      "✓ Configuration parameters set\n",
      "  - Vector DB base directory: ../../vector_databases\n",
      "  - Collection prefix: financebench_docs_chunk_\n",
      "  - Output directory: ../../retrieval_set\n",
      "\n",
      "  Evaluation modes: ['global', 'single']\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 1 COMPLETE: Imports and Configuration\n",
      "================================================================================\n",
      "\n",
      "Next steps:\n",
      "  2. Load dataset\n",
      "  3. Initialize reranker models\n",
      "  4. Implement retrieval and re-ranking functions\n",
      "  5. Implement main pipeline\n",
      "  6. Run batch evaluation\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Simplified Re-Ranking Pipeline for Answer Generation\n",
    "# This notebook implements re-ranking for RAG systems and outputs simplified JSON files\n",
    "# with FULL chunk text for use in answer generation pipelines.\n",
    "# \n",
    "# **Workflow:**\n",
    "# 1. Retrieve top k_retrieve chunks from existing vector stores\n",
    "# 2. Apply re-ranking to get top k_rerank chunks\n",
    "# 3. Save results with full chunk text (no truncation, no evaluation metrics)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Imports and Configuration\n",
    "\n",
    "# %%\n",
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.schema import Document\n",
    "from langchain_voyageai import VoyageAIRerank\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Sentence transformers for cross-encoders\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Hugging Face datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "\n",
    "# %%\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API Keys\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "VOYAGE_API_KEY = os.getenv(\"VOYAGE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Verify API keys\n",
    "assert VOYAGE_API_KEY is not None, \"VOYAGE_API_KEY not found in environment\"\n",
    "print(\"✓ API keys loaded successfully\")\n",
    "\n",
    "# %%\n",
    "# Configuration Parameters\n",
    "# Paths\n",
    "VECTOR_DB_BASE_DIR = \"../../vector_databases\"  # Base directory for vector databases\n",
    "COLLECTION_PREFIX = \"financebench_docs_chunk_\"  # Collection name prefix\n",
    "OUTPUT_DIR = \"../../retrieval_set\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✓ Configuration parameters set\")\n",
    "print(f\"  - Vector DB base directory: {VECTOR_DB_BASE_DIR}\")\n",
    "print(f\"  - Collection prefix: {COLLECTION_PREFIX}\")\n",
    "print(f\"  - Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Evaluation modes\n",
    "modes = ['global', 'single']\n",
    "print(f\"\\n  Evaluation modes: {modes}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 1 COMPLETE: Imports and Configuration\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  2. Load dataset\")\n",
    "print(\"  3. Initialize reranker models\")\n",
    "print(\"  4. Implement retrieval and re-ranking functions\")\n",
    "print(\"  5. Implement main pipeline\")\n",
    "print(\"  6. Run batch evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6cc75b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FinanceBench dataset...\n",
      "✓ Loaded 150 questions from FinanceBench\n",
      "\n",
      "Dataset fields:\n",
      "  Available columns: ['financebench_id', 'company', 'doc_name', 'question_type', 'question_reasoning', 'domain_question_num', 'question', 'answer', 'justification', 'dataset_subset_label', 'evidence', 'gics_sector', 'doc_type', 'doc_period', 'doc_link']\n",
      "\n",
      "Sample query from dataset:\n",
      "  financebench_id: financebench_id_03029\n",
      "  question_type: metrics-generated\n",
      "  question_reasoning: Information extraction\n",
      "  question: What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the quest...\n",
      "  doc_name: 3M_2018_10K\n",
      "  answer: $1577.00\n",
      "  evidence items: 1\n",
      "\n",
      "Verifying required fields...\n",
      "  ✓ financebench_id\n",
      "  ✓ question_type\n",
      "  ✓ question_reasoning\n",
      "  ✓ question\n",
      "  ✓ doc_name\n",
      "  ✓ answer\n",
      "  ✓ evidence\n",
      "\n",
      "✓ All required fields present\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 2 COMPLETE: Dataset Loaded\n",
      "================================================================================\n",
      "\n",
      "Dataset statistics:\n",
      "  Total queries: 150\n",
      "  Fields available: 15\n",
      "\n",
      "Next step: Initialize reranker models\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 2. Load Dataset\n",
    "\n",
    "# %%\n",
    "# Load FinanceBench dataset\n",
    "print(\"Loading FinanceBench dataset...\")\n",
    "dataset = load_dataset(\"PatronusAI/financebench\", split=\"train\")\n",
    "print(f\"✓ Loaded {len(dataset)} questions from FinanceBench\")\n",
    "\n",
    "# %%\n",
    "# Display dataset structure\n",
    "print(\"\\nDataset fields:\")\n",
    "print(f\"  Available columns: {dataset.column_names}\")\n",
    "\n",
    "# %%\n",
    "# Display sample query to verify structure\n",
    "print(\"\\nSample query from dataset:\")\n",
    "sample_query = dataset[0]\n",
    "print(f\"  financebench_id: {sample_query['financebench_id']}\")\n",
    "print(f\"  question_type: {sample_query['question_type']}\")\n",
    "print(f\"  question_reasoning: {sample_query['question_reasoning']}\")\n",
    "print(f\"  question: {sample_query['question'][:100]}...\")\n",
    "print(f\"  doc_name: {sample_query['doc_name']}\")\n",
    "print(f\"  answer: {sample_query['answer']}\")\n",
    "print(f\"  evidence items: {len(sample_query['evidence'])}\")\n",
    "\n",
    "# %%\n",
    "# Verify all required fields exist\n",
    "required_fields = [\n",
    "    'financebench_id', \n",
    "    'question_type', \n",
    "    'question_reasoning', \n",
    "    'question', \n",
    "    'doc_name',\n",
    "    'answer',\n",
    "    'evidence'\n",
    "]\n",
    "\n",
    "print(\"\\nVerifying required fields...\")\n",
    "for field in required_fields:\n",
    "    if field in sample_query:\n",
    "        print(f\"  ✓ {field}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {field} - MISSING!\")\n",
    "        raise ValueError(f\"Required field '{field}' not found in dataset\")\n",
    "\n",
    "print(\"\\n✓ All required fields present\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 2 COMPLETE: Dataset Loaded\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"  Total queries: {len(dataset)}\")\n",
    "print(f\"  Fields available: {len(dataset.column_names)}\")\n",
    "print(f\"\\nNext step: Initialize reranker models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc010e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Defined 6 configurations\n",
      "\n",
      "Unique reranker models to initialize: 3\n",
      "  - BAAI/bge-reranker-large\n",
      "  - cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "  - voyage-rerank-2.5\n",
      "\n",
      "================================================================================\n",
      "Initializing reranker models...\n",
      "================================================================================\n",
      "\n",
      "Loading: BAAI/bge-reranker-large\n",
      "  ✓ Successfully loaded Hugging Face model\n",
      "    Max sequence length: 512\n",
      "\n",
      "Loading: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "  ✓ Successfully loaded Hugging Face model\n",
      "    Max sequence length: 512\n",
      "\n",
      "Loading: voyage-rerank-2.5\n",
      "  ✓ Voyage reranker marked as API-based (will initialize per-query)\n",
      "\n",
      "================================================================================\n",
      "✓ Initialized 3 reranker models\n",
      "================================================================================\n",
      "\n",
      "Loaded reranker models:\n",
      "  ✓ BAAI/bge-reranker-large (Local model)\n",
      "  ✓ cross-encoder/ms-marco-MiniLM-L-12-v2 (Local model)\n",
      "  ✓ voyage-rerank-2.5 (API-based)\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 3 COMPLETE: Reranker Models Initialized\n",
      "================================================================================\n",
      "\n",
      "Ready rerankers:\n",
      "  - BAAI/bge-reranker-large\n",
      "  - cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "  - voyage-rerank-2.5\n",
      "\n",
      "Next step: Implement helper functions\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 3. Initialize Reranker Models\n",
    "\n",
    "# %%\n",
    "# Define configurations\n",
    "configurations = [\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-3-large',\n",
    "        'chunk_sizes': [512],\n",
    "        'k_retrieve': 80,\n",
    "        'k_rerank': 20,\n",
    "        'reranker_models': [\n",
    "            'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "            'BAAI/bge-reranker-large',\n",
    "            'voyage-rerank-2.5'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'provider': 'ollama',\n",
    "        'model': 'nomic-embed-text',\n",
    "        'chunk_sizes': [512],\n",
    "        'k_retrieve': 80,\n",
    "        'k_rerank': 20,\n",
    "        'reranker_models': [\n",
    "            'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "            'BAAI/bge-reranker-large',\n",
    "            'voyage-rerank-2.5'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-3-large',\n",
    "        'chunk_sizes': [1024],\n",
    "        'k_retrieve': 80,\n",
    "        'k_rerank': 20,\n",
    "        'reranker_models': [\n",
    "            'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "            'BAAI/bge-reranker-large',\n",
    "            'voyage-rerank-2.5'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'provider': 'ollama',\n",
    "        'model': 'nomic-embed-text',\n",
    "        'chunk_sizes': [1024],\n",
    "        'k_retrieve': 80,\n",
    "        'k_rerank': 20,\n",
    "        'reranker_models': [\n",
    "            'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "            'BAAI/bge-reranker-large',\n",
    "            'voyage-rerank-2.5'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-3-large',\n",
    "        'chunk_sizes': [2048],\n",
    "        'k_retrieve': 80,\n",
    "        'k_rerank': 10,\n",
    "        'reranker_models': [\n",
    "            'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "            'BAAI/bge-reranker-large',\n",
    "            'voyage-rerank-2.5'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'provider': 'ollama',\n",
    "        'model': 'nomic-embed-text',\n",
    "        'chunk_sizes': [2048],\n",
    "        'k_retrieve': 80,\n",
    "        'k_rerank': 10,\n",
    "        'reranker_models': [\n",
    "            'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "            'BAAI/bge-reranker-large',\n",
    "            'voyage-rerank-2.5'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"✓ Defined {len(configurations)} configurations\")\n",
    "\n",
    "# %%\n",
    "def get_all_unique_rerankers(configurations: List[Dict]) -> List[str]:\n",
    "    \"\"\"Extract all unique reranker models from configurations.\"\"\"\n",
    "    rerankers = set()\n",
    "    for config in configurations:\n",
    "        for reranker in config['reranker_models']:\n",
    "            rerankers.add(reranker)\n",
    "    return sorted(list(rerankers))\n",
    "\n",
    "# Get all unique rerankers\n",
    "unique_rerankers = get_all_unique_rerankers(configurations)\n",
    "print(f\"\\nUnique reranker models to initialize: {len(unique_rerankers)}\")\n",
    "for reranker in unique_rerankers:\n",
    "    print(f\"  - {reranker}\")\n",
    "\n",
    "# %%\n",
    "# Initialize reranker models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Initializing reranker models...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "reranker_instances = {}\n",
    "\n",
    "for reranker_name in unique_rerankers:\n",
    "    print(f\"\\nLoading: {reranker_name}\")\n",
    "    \n",
    "    if reranker_name == 'voyage-rerank-2.5':\n",
    "        # Voyage reranker will be initialized per-query (API-based)\n",
    "        reranker_instances[reranker_name] = 'api'\n",
    "        print(f\"  ✓ Voyage reranker marked as API-based (will initialize per-query)\")\n",
    "        \n",
    "    elif reranker_name.startswith('cross-encoder/') or reranker_name.startswith('BAAI/'):\n",
    "        # Load Hugging Face cross-encoder models\n",
    "        try:\n",
    "            model = CrossEncoder(reranker_name)\n",
    "            reranker_instances[reranker_name] = model\n",
    "            print(f\"  ✓ Successfully loaded Hugging Face model\")\n",
    "            print(f\"    Max sequence length: {model.max_length}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Failed to load {reranker_name}: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown reranker type: {reranker_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"✓ Initialized {len(reranker_instances)} reranker models\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# %%\n",
    "# Display loaded models\n",
    "print(\"\\nLoaded reranker models:\")\n",
    "for name, instance in reranker_instances.items():\n",
    "    if instance == 'api':\n",
    "        print(f\"  ✓ {name} (API-based)\")\n",
    "    else:\n",
    "        print(f\"  ✓ {name} (Local model)\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 3 COMPLETE: Reranker Models Initialized\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nReady rerankers:\")\n",
    "for reranker_name in reranker_instances.keys():\n",
    "    print(f\"  - {reranker_name}\")\n",
    "print(f\"\\nNext step: Implement helper functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff44deb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ extract_doc_name_from_path() defined\n",
      "✓ extract_metadata_from_retrieved_doc() defined\n",
      "✓ simplify_reranker_name() defined\n",
      "✓ get_output_filename() defined\n",
      "✓ check_if_results_exist() defined\n",
      "✓ save_results() defined\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 4 COMPLETE: Helper Functions Implemented\n",
      "================================================================================\n",
      "\n",
      "Implemented functions:\n",
      "  ✓ extract_doc_name_from_path() - Extract doc name from file path\n",
      "  ✓ extract_metadata_from_retrieved_doc() - Extract metadata from retrieved docs\n",
      "  ✓ simplify_reranker_name() - Simplify reranker names\n",
      "  ✓ get_output_filename() - Generate output filenames\n",
      "  ✓ check_if_results_exist() - Check if results already exist\n",
      "  ✓ save_results() - Save results to JSON\n",
      "\n",
      "Next step: Implement retrieval functions\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 4. Helper Functions\n",
    "\n",
    "# %%\n",
    "def extract_doc_name_from_path(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract document name from file path.\n",
    "    \n",
    "    Example:\n",
    "        \"../../documents/3M_2018_10K.pdf\" → \"3M_2018_10K\"\n",
    "    \n",
    "    Args:\n",
    "        file_path: Full path to document\n",
    "        \n",
    "    Returns:\n",
    "        Document name without extension\n",
    "    \"\"\"\n",
    "    return Path(file_path).stem\n",
    "\n",
    "print(\"✓ extract_doc_name_from_path() defined\")\n",
    "\n",
    "# %%\n",
    "def extract_metadata_from_retrieved_doc(doc) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract metadata from a retrieved LangChain document.\n",
    "    \n",
    "    ChromaDB metadata structure:\n",
    "        - file_path: Full path to PDF\n",
    "        - source: Page number (as integer or string, 0-indexed)\n",
    "    \n",
    "    Args:\n",
    "        doc: LangChain Document object from vectorstore.similarity_search()\n",
    "        \n",
    "    Returns:\n",
    "        Dict with:\n",
    "            - doc_name: Document name (e.g., \"3M_2018_10K\")\n",
    "            - page_number: Page number (integer, 1-indexed)\n",
    "            - chunk_text: Full chunk text\n",
    "    \"\"\"\n",
    "    metadata = doc.metadata\n",
    "    \n",
    "    # Extract document name from file_path\n",
    "    file_path = metadata.get('file_path', '')\n",
    "    doc_name = extract_doc_name_from_path(file_path) if file_path else ''\n",
    "    \n",
    "    # Extract page number from 'source'\n",
    "    # FinanceBench uses 0-indexed pages, but we convert to 1-indexed\n",
    "    page_source = metadata.get('source', -1)\n",
    "    \n",
    "    # Handle both string and integer page numbers\n",
    "    if isinstance(page_source, str):\n",
    "        try:\n",
    "            page_number = int(page_source) + 1  # Convert to 1-indexed\n",
    "        except ValueError:\n",
    "            page_number = -1\n",
    "    elif isinstance(page_source, int):\n",
    "        page_number = page_source + 1  # Convert to 1-indexed\n",
    "    else:\n",
    "        page_number = -1\n",
    "    \n",
    "    return {\n",
    "        'doc_name': doc_name,\n",
    "        'page_number': page_number,\n",
    "        'chunk_text': doc.page_content\n",
    "    }\n",
    "\n",
    "print(\"✓ extract_metadata_from_retrieved_doc() defined\")\n",
    "\n",
    "# %%\n",
    "def simplify_reranker_name(reranker_model: str) -> str:\n",
    "    \"\"\"\n",
    "    Simplify reranker model name for display and filenames.\n",
    "    \n",
    "    Examples:\n",
    "        'cross-encoder/ms-marco-MiniLM-L-12-v2' -> 'ms-marco-MiniLM-L-12-v2'\n",
    "        'BAAI/bge-reranker-large' -> 'bge-reranker-large'\n",
    "        'voyage-rerank-2.5' -> 'voyage-rerank-2.5'\n",
    "    \"\"\"\n",
    "    if '/' in reranker_model:\n",
    "        return reranker_model.split('/')[-1]\n",
    "    return reranker_model\n",
    "\n",
    "print(\"✓ simplify_reranker_name() defined\")\n",
    "\n",
    "# %%\n",
    "def get_output_filename(\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k_retrieve: int,\n",
    "    k_rerank: int,\n",
    "    mode: str,\n",
    "    reranker_model: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate output filename for results.\n",
    "    \n",
    "    Format: {provider}_{model}_chunk{chunk_size}_k{k_retrieve}_{mode}_rerank_k{k_rerank}-{reranker}.json\n",
    "    \n",
    "    Example: voyage_voyage-3-large_chunk512_k80_single_rerank_k20-voyage-rerank-2.5.json\n",
    "    \"\"\"\n",
    "    reranker_simple = simplify_reranker_name(reranker_model)\n",
    "    filename = f\"{provider}_{model}_chunk{chunk_size}_k{k_retrieve}_{mode}_rerank_k{k_rerank}-{reranker_simple}.json\"\n",
    "    return filename\n",
    "\n",
    "print(\"✓ get_output_filename() defined\")\n",
    "\n",
    "# %%\n",
    "def check_if_results_exist(\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k_retrieve: int,\n",
    "    k_rerank: int,\n",
    "    mode: str,\n",
    "    reranker_model: str,\n",
    "    output_dir: str\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Check if results file already exists.\n",
    "    \n",
    "    Returns:\n",
    "        True if file exists, False otherwise\n",
    "    \"\"\"\n",
    "    filename = get_output_filename(provider, model, chunk_size, k_retrieve, k_rerank, mode, reranker_model)\n",
    "    filepath = Path(output_dir) / filename\n",
    "    return filepath.exists()\n",
    "\n",
    "print(\"✓ check_if_results_exist() defined\")\n",
    "\n",
    "# %%\n",
    "def save_results(\n",
    "    results: Dict,\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k_retrieve: int,\n",
    "    k_rerank: int,\n",
    "    mode: str,\n",
    "    reranker_model: str,\n",
    "    output_dir: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Save results to JSON file.\n",
    "    \n",
    "    Args:\n",
    "        results: Results dictionary to save\n",
    "        provider: Embedding provider\n",
    "        model: Embedding model\n",
    "        chunk_size: Chunk size\n",
    "        k_retrieve: Number of documents retrieved\n",
    "        k_rerank: Number of documents kept after reranking\n",
    "        mode: 'global' or 'single'\n",
    "        reranker_model: Reranker model name\n",
    "        output_dir: Output directory\n",
    "        \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    filename = get_output_filename(provider, model, chunk_size, k_retrieve, k_rerank, mode, reranker_model)\n",
    "    filepath = Path(output_dir) / filename\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return str(filepath)\n",
    "\n",
    "print(\"✓ save_results() defined\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 4 COMPLETE: Helper Functions Implemented\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nImplemented functions:\")\n",
    "print(\"  ✓ extract_doc_name_from_path() - Extract doc name from file path\")\n",
    "print(\"  ✓ extract_metadata_from_retrieved_doc() - Extract metadata from retrieved docs\")\n",
    "print(\"  ✓ simplify_reranker_name() - Simplify reranker names\")\n",
    "print(\"  ✓ get_output_filename() - Generate output filenames\")\n",
    "print(\"  ✓ check_if_results_exist() - Check if results already exist\")\n",
    "print(\"  ✓ save_results() - Save results to JSON\")\n",
    "print(\"\\nNext step: Implement retrieval functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab142408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ get_embedding_model() defined\n",
      "✓ load_vector_store() defined\n",
      "✓ retrieve_documents() defined\n",
      "\n",
      "================================================================================\n",
      "Testing retrieval functions...\n",
      "================================================================================\n",
      "\n",
      "Test parameters:\n",
      "  Provider: voyage\n",
      "  Model: voyage-3-large\n",
      "  Chunk size: 512\n",
      "  k_retrieve: 80\n",
      "\n",
      "Loading vector store...\n",
      "  ✓ Loaded collection 'financebench_docs_chunk_512' from ../../vector_databases/voyage_voyage-3-large\n",
      "    Documents: 28635\n",
      "\n",
      "Test query: What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the quest...\n",
      "Target doc: 3M_2018_10K\n",
      "\n",
      "Retrieving top 80 documents (global mode)...\n",
      "  ✓ Retrieved 80 documents\n",
      "  Top result: 3M_2018_10K (page 48, score: 0.5490)\n",
      "  Content length: 1848 characters\n",
      "\n",
      "Retrieving top 80 documents (single mode)...\n",
      "  ✓ Retrieved 80 documents\n",
      "  Top result: 3M_2018_10K (page 48, score: 0.5490)\n",
      "  Content length: 1848 characters\n",
      "\n",
      "✓ Retrieval test successful!\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 5 COMPLETE: Retrieval Functions Implemented\n",
      "================================================================================\n",
      "\n",
      "Implemented functions:\n",
      "  ✓ get_embedding_model() - Initialize embedding models\n",
      "  ✓ load_vector_store() - Load ChromaDB collections\n",
      "  ✓ retrieve_documents() - Retrieve top-k documents with filtering\n",
      "\n",
      "Next step: Implement re-ranking functions\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Retrieval Functions\n",
    "\n",
    "# %%\n",
    "def get_embedding_model(provider: str, model: str):\n",
    "    \"\"\"\n",
    "    Initialize embedding model based on provider and model name.\n",
    "    \n",
    "    Args:\n",
    "        provider: 'voyage', 'openai', or 'ollama'\n",
    "        model: Model name\n",
    "        \n",
    "    Returns:\n",
    "        Embedding model instance\n",
    "    \"\"\"\n",
    "    if provider == 'voyage':\n",
    "        return VoyageAIEmbeddings(\n",
    "            model=model,\n",
    "            voyage_api_key=VOYAGE_API_KEY\n",
    "        )\n",
    "    elif provider == 'openai':\n",
    "        return OpenAIEmbeddings(\n",
    "            model=model,\n",
    "            openai_api_key=OPENAI_API_KEY\n",
    "        )\n",
    "    elif provider == 'ollama':\n",
    "        return OllamaEmbeddings(\n",
    "            model=model,\n",
    "            base_url=OLLAMA_BASE_URL\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "print(\"✓ get_embedding_model() defined\")\n",
    "\n",
    "# %%\n",
    "def load_vector_store(provider: str, model: str, chunk_size: int, base_dir: str, collection_prefix: str = \"financebench_docs_chunk_\"):\n",
    "    \"\"\"\n",
    "    Load existing ChromaDB vector store.\n",
    "    \n",
    "    Matches the directory structure from baseline:\n",
    "    {base_dir}/{provider}_{model}/financebench_docs_chunk_{chunk_size}/\n",
    "    \n",
    "    Args:\n",
    "        provider: Embedding provider\n",
    "        model: Embedding model name\n",
    "        chunk_size: Chunk size used\n",
    "        base_dir: Base directory for vector databases\n",
    "        collection_prefix: Prefix for collection names\n",
    "        \n",
    "    Returns:\n",
    "        Chroma vector store instance\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If collection doesn't exist\n",
    "    \"\"\"\n",
    "    # Construct paths matching baseline structure\n",
    "    model_id = f\"{provider}_{model.replace('/', '_')}\"\n",
    "    db_path = os.path.join(base_dir, model_id)\n",
    "    collection_name = f\"{collection_prefix}{chunk_size}\"\n",
    "    \n",
    "    embedding_model = get_embedding_model(provider, model)\n",
    "    \n",
    "    try:\n",
    "        vectorstore = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=embedding_model,\n",
    "            persist_directory=db_path\n",
    "        )\n",
    "        \n",
    "        # Verify collection exists by checking count\n",
    "        count = vectorstore._collection.count()\n",
    "        if count == 0:\n",
    "            raise ValueError(f\"Collection '{collection_name}' is empty\")\n",
    "        \n",
    "        print(f\"  ✓ Loaded collection '{collection_name}' from {db_path}\")\n",
    "        print(f\"    Documents: {count}\")\n",
    "        return vectorstore\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load collection '{collection_name}' from {db_path}: {e}\")\n",
    "\n",
    "print(\"✓ load_vector_store() defined\")\n",
    "\n",
    "# %%\n",
    "def retrieve_documents(\n",
    "    vectorstore,\n",
    "    query: str,\n",
    "    k: int,\n",
    "    mode: str,\n",
    "    doc_name: str = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve top k documents from vector store.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: ChromaDB vector store\n",
    "        query: Search query\n",
    "        k: Number of documents to retrieve\n",
    "        mode: 'global' or 'single'\n",
    "        doc_name: Document name (required for 'single' mode)\n",
    "        \n",
    "    Returns:\n",
    "        List of retrieved documents with metadata\n",
    "        Format: [{'doc_name': str, 'page_number': int, 'content': str, 'rank': int, 'score': float}, ...]\n",
    "    \"\"\"\n",
    "    if mode == 'single':\n",
    "        if doc_name is None:\n",
    "            raise ValueError(\"doc_name required for single-document mode\")\n",
    "        \n",
    "        # Retrieve more documents and filter in Python\n",
    "        # Retrieve 3x to ensure we get enough from the target document\n",
    "        results = vectorstore.similarity_search_with_score(query, k=k * 3)\n",
    "        \n",
    "        # Filter to only documents matching the doc_name\n",
    "        filtered_results = []\n",
    "        for doc, score in results:\n",
    "            metadata = extract_metadata_from_retrieved_doc(doc)\n",
    "            if metadata['doc_name'] == doc_name:\n",
    "                filtered_results.append((doc, score))\n",
    "                if len(filtered_results) >= k:\n",
    "                    break\n",
    "        \n",
    "        results = filtered_results[:k]\n",
    "    else:  # global mode\n",
    "        results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    # Format results using metadata extraction\n",
    "    retrieved_docs = []\n",
    "    for rank, (doc, score) in enumerate(results, start=1):\n",
    "        metadata = extract_metadata_from_retrieved_doc(doc)\n",
    "        \n",
    "        retrieved_docs.append({\n",
    "            'doc_name': metadata['doc_name'],\n",
    "            'page_number': metadata['page_number'],\n",
    "            'content': metadata['chunk_text'],  # FULL text\n",
    "            'rank': rank,\n",
    "            'score': float(score)\n",
    "        })\n",
    "    \n",
    "    return retrieved_docs\n",
    "\n",
    "print(\"✓ retrieve_documents() defined\")\n",
    "\n",
    "# %%\n",
    "# Test retrieval with a sample configuration\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing retrieval functions...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use first configuration for testing\n",
    "test_config = configurations[0]\n",
    "test_provider = test_config['provider']\n",
    "test_model = test_config['model']\n",
    "test_chunk_size = test_config['chunk_sizes'][0]\n",
    "test_k_retrieve = test_config['k_retrieve']\n",
    "\n",
    "print(f\"\\nTest parameters:\")\n",
    "print(f\"  Provider: {test_provider}\")\n",
    "print(f\"  Model: {test_model}\")\n",
    "print(f\"  Chunk size: {test_chunk_size}\")\n",
    "print(f\"  k_retrieve: {test_k_retrieve}\")\n",
    "\n",
    "try:\n",
    "    # Load vector store\n",
    "    print(f\"\\nLoading vector store...\")\n",
    "    test_vectorstore = load_vector_store(\n",
    "        test_provider,\n",
    "        test_model,\n",
    "        test_chunk_size,\n",
    "        VECTOR_DB_BASE_DIR,\n",
    "        COLLECTION_PREFIX\n",
    "    )\n",
    "    \n",
    "    # Test retrieval with first query\n",
    "    test_query = dataset[0]\n",
    "    print(f\"\\nTest query: {test_query['question'][:100]}...\")\n",
    "    print(f\"Target doc: {test_query['doc_name']}\")\n",
    "    \n",
    "    # Test global mode\n",
    "    print(f\"\\nRetrieving top {test_k_retrieve} documents (global mode)...\")\n",
    "    retrieved_global = retrieve_documents(\n",
    "        test_vectorstore,\n",
    "        test_query['question'],\n",
    "        k=test_k_retrieve,\n",
    "        mode='global'\n",
    "    )\n",
    "    print(f\"  ✓ Retrieved {len(retrieved_global)} documents\")\n",
    "    print(f\"  Top result: {retrieved_global[0]['doc_name']} (page {retrieved_global[0]['page_number']}, score: {retrieved_global[0]['score']:.4f})\")\n",
    "    print(f\"  Content length: {len(retrieved_global[0]['content'])} characters\")\n",
    "    \n",
    "    # Test single mode\n",
    "    print(f\"\\nRetrieving top {test_k_retrieve} documents (single mode)...\")\n",
    "    retrieved_single = retrieve_documents(\n",
    "        test_vectorstore,\n",
    "        test_query['question'],\n",
    "        k=test_k_retrieve,\n",
    "        mode='single',\n",
    "        doc_name=test_query['doc_name']\n",
    "    )\n",
    "    print(f\"  ✓ Retrieved {len(retrieved_single)} documents\")\n",
    "    print(f\"  Top result: {retrieved_single[0]['doc_name']} (page {retrieved_single[0]['page_number']}, score: {retrieved_single[0]['score']:.4f})\")\n",
    "    print(f\"  Content length: {len(retrieved_single[0]['content'])} characters\")\n",
    "    \n",
    "    print(\"\\n✓ Retrieval test successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Retrieval test failed: {e}\")\n",
    "    print(\"\\nThis is expected if vector stores haven't been created yet.\")\n",
    "    print(\"Make sure you have run the baseline evaluation first to create vector stores.\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 5 COMPLETE: Retrieval Functions Implemented\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nImplemented functions:\")\n",
    "print(\"  ✓ get_embedding_model() - Initialize embedding models\")\n",
    "print(\"  ✓ load_vector_store() - Load ChromaDB collections\")\n",
    "print(\"  ✓ retrieve_documents() - Retrieve top-k documents with filtering\")\n",
    "print(\"\\nNext step: Implement re-ranking functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16863a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ rerank_with_voyage() defined with rate limit handling\n",
      "✓ rerank_with_cross_encoder() defined\n",
      "✓ rerank_documents() defined\n",
      "\n",
      "================================================================================\n",
      "Testing re-ranking functions...\n",
      "================================================================================\n",
      "\n",
      "Sample query: What was the company's revenue in 2023?\n",
      "Sample documents: 3\n",
      "\n",
      "Testing with cross-encoder/ms-marco-MiniLM-L-12-v2...\n",
      "  ✓ Re-ranking successful!\n",
      "\n",
      "  Initial order:\n",
      "    Rank 1: Page 1 (score: 0.950)\n",
      "    Rank 2: Page 2 (score: 0.850)\n",
      "    Rank 3: Page 3 (score: 0.800)\n",
      "\n",
      "  Re-ranked order:\n",
      "    Rank 1: Page 3 (initial rank: 3, rerank score: -7.339)\n",
      "    Rank 2: Page 1 (initial rank: 1, rerank score: -10.861)\n",
      "    Rank 3: Page 2 (initial rank: 2, rerank score: -11.263)\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 6 COMPLETE: Re-ranking Functions Implemented\n",
      "================================================================================\n",
      "\n",
      "Implemented functions:\n",
      "  ✓ rerank_with_voyage() - Voyage API re-ranking with rate limiting\n",
      "  ✓ rerank_with_cross_encoder() - HuggingFace cross-encoder re-ranking\n",
      "  ✓ rerank_documents() - Universal re-ranking router\n",
      "\n",
      "Next step: Implement main pipeline (simplified, no metrics)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 6. Re-ranking Functions\n",
    "\n",
    "# %%\n",
    "import time\n",
    "\n",
    "def rerank_with_voyage(\n",
    "    query: str,\n",
    "    retrieved_docs: List[Dict],\n",
    "    reranker_model: str,\n",
    "    top_k: int,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay: int = 60\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Re-rank documents using Voyage AI reranker API with rate limit handling.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        retrieved_docs: List from retrieval (with 'content', 'rank', 'score')\n",
    "        reranker_model: Voyage model name (e.g., 'voyage-rerank-2.5')\n",
    "        top_k: Number of documents to return after re-ranking\n",
    "        max_retries: Maximum number of retries on rate limit\n",
    "        retry_delay: Seconds to wait between retries\n",
    "        \n",
    "    Returns:\n",
    "        Re-ranked list with {'doc_name', 'page_number', 'rank', 'initial_rank', 'initial_score', 'rerank_score', 'content'}\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document\n",
    "    from langchain_voyageai import VoyageAIRerank\n",
    "    \n",
    "    # Convert to LangChain documents\n",
    "    lc_docs = [\n",
    "        Document(\n",
    "            page_content=doc['content'],\n",
    "            metadata={\n",
    "                'doc_name': doc['doc_name'], \n",
    "                'page_number': doc['page_number'],\n",
    "                'initial_rank': doc['rank'],\n",
    "                'initial_score': doc['score']\n",
    "            }\n",
    "        )\n",
    "        for doc in retrieved_docs\n",
    "    ]\n",
    "    \n",
    "    # Initialize Voyage reranker\n",
    "    # Extract model name (e.g., \"rerank-2.5\" from \"voyage-rerank-2.5\")\n",
    "    model_name = reranker_model.replace('voyage-', '')\n",
    "    \n",
    "    reranker = VoyageAIRerank(\n",
    "        model=model_name,\n",
    "        voyage_api_key=VOYAGE_API_KEY,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    # Retry logic for rate limiting\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Rerank documents\n",
    "            reranked_docs = reranker.compress_documents(lc_docs, query)\n",
    "            break  # Success, exit retry loop\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Check if it's a rate limit error\n",
    "            if \"rate limit\" in error_msg.lower() or \"tpm\" in error_msg.lower():\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"\\n⚠️  Rate limit hit. Waiting {retry_delay} seconds before retry {attempt + 1}/{max_retries}...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    print(f\"\\n❌ Rate limit exceeded after {max_retries} attempts\")\n",
    "                    raise\n",
    "            else:\n",
    "                # Non-rate-limit error, raise immediately\n",
    "                raise\n",
    "    \n",
    "    # Convert back to our format\n",
    "    results = []\n",
    "    for rank, doc in enumerate(reranked_docs, start=1):\n",
    "        result = {\n",
    "            'doc_name': doc.metadata['doc_name'],\n",
    "            'page_number': doc.metadata['page_number'],\n",
    "            'content': doc.page_content,  # FULL text\n",
    "            'rank': rank,\n",
    "            'initial_rank': doc.metadata['initial_rank'],\n",
    "            'initial_score': doc.metadata['initial_score'],\n",
    "            'rerank_score': doc.metadata.get('relevance_score', 0.0)\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ rerank_with_voyage() defined with rate limit handling\")\n",
    "\n",
    "# %%\n",
    "def rerank_with_cross_encoder(\n",
    "    query: str,\n",
    "    retrieved_docs: List[Dict],\n",
    "    cross_encoder_model: CrossEncoder,\n",
    "    top_k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Re-rank documents using Hugging Face cross-encoder model.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        retrieved_docs: List from retrieval (with 'content', 'rank', 'score')\n",
    "        cross_encoder_model: Loaded CrossEncoder model instance\n",
    "        top_k: Number of documents to return after re-ranking\n",
    "        \n",
    "    Returns:\n",
    "        Re-ranked list with {'doc_name', 'page_number', 'rank', 'initial_rank', 'initial_score', 'rerank_score', 'content'}\n",
    "    \"\"\"\n",
    "    # Prepare query-document pairs\n",
    "    pairs = [[query, doc['content']] for doc in retrieved_docs]\n",
    "    \n",
    "    # Get relevance scores from cross-encoder\n",
    "    scores = cross_encoder_model.predict(pairs)\n",
    "    \n",
    "    # Combine scores with documents\n",
    "    docs_with_scores = []\n",
    "    for doc, score in zip(retrieved_docs, scores):\n",
    "        docs_with_scores.append({\n",
    "            'doc_name': doc['doc_name'],\n",
    "            'page_number': doc['page_number'],\n",
    "            'content': doc['content'],  # FULL text\n",
    "            'initial_rank': doc['rank'],\n",
    "            'initial_score': doc['score'],\n",
    "            'rerank_score': float(score)\n",
    "        })\n",
    "    \n",
    "    # Sort by rerank score (descending) and take top_k\n",
    "    docs_with_scores.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
    "    top_docs = docs_with_scores[:top_k]\n",
    "    \n",
    "    # Assign new ranks\n",
    "    results = []\n",
    "    for rank, doc in enumerate(top_docs, start=1):\n",
    "        doc['rank'] = rank\n",
    "        results.append(doc)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ rerank_with_cross_encoder() defined\")\n",
    "\n",
    "# %%\n",
    "def rerank_documents(\n",
    "    query: str,\n",
    "    retrieved_docs: List[Dict],\n",
    "    reranker_model: str,\n",
    "    reranker_instance: Any,\n",
    "    top_k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Universal re-ranking function that routes to appropriate reranker.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        retrieved_docs: List from retrieval\n",
    "        reranker_model: Model name/identifier\n",
    "        reranker_instance: Loaded model instance or 'api' for Voyage\n",
    "        top_k: Number of documents to return after re-ranking\n",
    "        \n",
    "    Returns:\n",
    "        Re-ranked document list\n",
    "    \"\"\"\n",
    "    if reranker_model == 'voyage-rerank-2.5':\n",
    "        # Use Voyage API\n",
    "        return rerank_with_voyage(query, retrieved_docs, reranker_model, top_k)\n",
    "    \n",
    "    elif isinstance(reranker_instance, CrossEncoder):\n",
    "        # Use Hugging Face cross-encoder\n",
    "        return rerank_with_cross_encoder(query, retrieved_docs, reranker_instance, top_k)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown reranker type for model: {reranker_model}\")\n",
    "\n",
    "print(\"✓ rerank_documents() defined\")\n",
    "\n",
    "# %%\n",
    "# Test re-ranking with sample data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing re-ranking functions...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create sample retrieved documents for testing\n",
    "sample_retrieved_docs = [\n",
    "    {\n",
    "        'doc_name': 'test_doc.pdf',\n",
    "        'page_number': 1,\n",
    "        'content': 'This is a highly relevant document about financial reporting and annual statements.',\n",
    "        'rank': 1,\n",
    "        'score': 0.95\n",
    "    },\n",
    "    {\n",
    "        'doc_name': 'test_doc.pdf',\n",
    "        'page_number': 2,\n",
    "        'content': 'This document discusses unrelated topics like weather and sports.',\n",
    "        'rank': 2,\n",
    "        'score': 0.85\n",
    "    },\n",
    "    {\n",
    "        'doc_name': 'test_doc.pdf',\n",
    "        'page_number': 3,\n",
    "        'content': 'Annual financial statements and revenue details for the fiscal year.',\n",
    "        'rank': 3,\n",
    "        'score': 0.80\n",
    "    }\n",
    "]\n",
    "\n",
    "sample_query = \"What was the company's revenue in 2023?\"\n",
    "\n",
    "print(f\"\\nSample query: {sample_query}\")\n",
    "print(f\"Sample documents: {len(sample_retrieved_docs)}\")\n",
    "\n",
    "# Test with cross-encoder (if loaded)\n",
    "try:\n",
    "    test_reranker_name = 'cross-encoder/ms-marco-MiniLM-L-12-v2'\n",
    "    if test_reranker_name in reranker_instances:\n",
    "        print(f\"\\nTesting with {test_reranker_name}...\")\n",
    "        test_reranker = reranker_instances[test_reranker_name]\n",
    "        \n",
    "        reranked = rerank_with_cross_encoder(\n",
    "            sample_query,\n",
    "            sample_retrieved_docs,\n",
    "            test_reranker,\n",
    "            top_k=3\n",
    "        )\n",
    "        \n",
    "        print(f\"  ✓ Re-ranking successful!\")\n",
    "        print(f\"\\n  Initial order:\")\n",
    "        for doc in sample_retrieved_docs:\n",
    "            print(f\"    Rank {doc['rank']}: Page {doc['page_number']} (score: {doc['score']:.3f})\")\n",
    "        \n",
    "        print(f\"\\n  Re-ranked order:\")\n",
    "        for doc in reranked:\n",
    "            print(f\"    Rank {doc['rank']}: Page {doc['page_number']} (initial rank: {doc['initial_rank']}, rerank score: {doc['rerank_score']:.3f})\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  Reranker {test_reranker_name} not loaded. Skipping test.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Re-ranking test failed: {e}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 6 COMPLETE: Re-ranking Functions Implemented\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nImplemented functions:\")\n",
    "print(\"  ✓ rerank_with_voyage() - Voyage API re-ranking with rate limiting\")\n",
    "print(\"  ✓ rerank_with_cross_encoder() - HuggingFace cross-encoder re-ranking\")\n",
    "print(\"  ✓ rerank_documents() - Universal re-ranking router\")\n",
    "print(\"\\nNext step: Implement main pipeline (simplified, no metrics)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac773023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ evaluate_configuration() defined\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 7 COMPLETE: Main Pipeline Implemented\n",
      "================================================================================\n",
      "\n",
      "Implemented functions:\n",
      "  ✓ evaluate_configuration() - Complete re-ranking pipeline (simplified)\n",
      "\n",
      "Key features:\n",
      "  - No evaluation metrics (MRR, recall, etc.)\n",
      "  - Full chunk text (no truncation)\n",
      "  - FinanceBench-compatible output format\n",
      "  - Evidence array with rank and rerank_score\n",
      "\n",
      "Next step: Batch execution\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 7. Main Pipeline (Simplified - No Metrics)\n",
    "\n",
    "# %%\n",
    "def evaluate_configuration(\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k_retrieve: int,\n",
    "    k_rerank: int,\n",
    "    reranker_model: str,\n",
    "    mode: str,\n",
    "    dataset,\n",
    "    reranker_instance,\n",
    "    vector_db_base_dir: str,\n",
    "    collection_prefix: str,\n",
    "    output_dir: str\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run re-ranking pipeline for a single configuration.\n",
    "    Simplified version - no evaluation metrics, just retrieve → rerank → save.\n",
    "    \n",
    "    Args:\n",
    "        provider: Embedding provider\n",
    "        model: Embedding model\n",
    "        chunk_size: Chunk size\n",
    "        k_retrieve: Number of documents to retrieve\n",
    "        k_rerank: Number of documents to keep after reranking\n",
    "        reranker_model: Reranker model name\n",
    "        mode: 'global' or 'single'\n",
    "        dataset: FinanceBench dataset\n",
    "        reranker_instance: Loaded reranker model instance\n",
    "        vector_db_base_dir: Base directory for vector databases\n",
    "        collection_prefix: Collection name prefix\n",
    "        output_dir: Output directory for results\n",
    "        \n",
    "    Returns:\n",
    "        Summary statistics dict\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing: {provider}/{model} | chunk={chunk_size} | k_retrieve={k_retrieve} | k_rerank={k_rerank} | {mode} | reranker={simplify_reranker_name(reranker_model)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Check if results already exist\n",
    "    if check_if_results_exist(provider, model, chunk_size, k_retrieve, k_rerank, mode, reranker_model, output_dir):\n",
    "        print(\"⚠️  Results already exist. Skipping...\")\n",
    "        return {'status': 'skipped'}\n",
    "    \n",
    "    # Load vector store\n",
    "    try:\n",
    "        print(\"\\n1. Loading vector store...\")\n",
    "        vectorstore = load_vector_store(provider, model, chunk_size, vector_db_base_dir, collection_prefix)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load vector store: {e}\")\n",
    "        return {'status': 'failed', 'error': str(e)}\n",
    "    \n",
    "    # Initialize results storage\n",
    "    query_results = []\n",
    "    \n",
    "    # Process each query\n",
    "    print(f\"\\n2. Processing {len(dataset)} queries...\")\n",
    "    for idx, query_item in enumerate(tqdm(dataset, desc=\"Queries\")):\n",
    "        question = query_item['question']\n",
    "        doc_name = query_item['doc_name']\n",
    "        \n",
    "        # Add small delay to avoid rate limits (only for API-based rerankers)\n",
    "        if reranker_model == 'voyage-rerank-2.5':\n",
    "            time.sleep(0.5)  # 500ms delay between queries\n",
    "        \n",
    "        # Step 1: Retrieve top k_retrieve documents\n",
    "        try:\n",
    "            retrieved_k_retrieve = retrieve_documents(\n",
    "                vectorstore,\n",
    "                question,\n",
    "                k=k_retrieve,\n",
    "                mode=mode,\n",
    "                doc_name=doc_name if mode == 'single' else None\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️  Query {idx} retrieval failed: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Step 2: Rerank ALL k_retrieve documents\n",
    "        try:\n",
    "            reranked_all = rerank_documents(\n",
    "                question,\n",
    "                retrieved_k_retrieve,\n",
    "                reranker_model,\n",
    "                reranker_instance,\n",
    "                k_retrieve  # Rerank all documents\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️  Query {idx} reranking failed: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Step 3: Get top k_rerank from reranked results\n",
    "        reranked_top_k = reranked_all[:k_rerank]\n",
    "        \n",
    "        # Step 4: Format evidence array matching FinanceBench structure\n",
    "        evidence = []\n",
    "        for doc in reranked_top_k:\n",
    "            evidence.append({\n",
    "                'evidence_text': doc['content'],  # FULL text, no truncation\n",
    "                'doc_name': doc['doc_name'],\n",
    "                'evidence_page_num': doc['page_number'],\n",
    "                'rank': doc['rank'],\n",
    "                'rerank_score': round(doc['rerank_score'], 4)\n",
    "            })\n",
    "        \n",
    "        # Step 5: Store query result matching FinanceBench structure\n",
    "        query_result = {\n",
    "            'financebench_id': query_item['financebench_id'],\n",
    "            'question_type': query_item['question_type'],\n",
    "            'question_reasoning': query_item['question_reasoning'],\n",
    "            'question': question,\n",
    "            'doc_name': doc_name,\n",
    "            'evidence': evidence\n",
    "        }\n",
    "        query_results.append(query_result)\n",
    "    \n",
    "    # Check if any queries were processed\n",
    "    num_queries = len(query_results)\n",
    "    if num_queries == 0:\n",
    "        print(\"\\n❌ No queries were successfully processed\")\n",
    "        return {'status': 'failed', 'error': 'No queries processed'}\n",
    "    \n",
    "    # Prepare final results\n",
    "    results = {\n",
    "        'configuration': {\n",
    "            'provider': provider,\n",
    "            'model': model,\n",
    "            'chunk_size': chunk_size,\n",
    "            'k_retrieve': k_retrieve,\n",
    "            'k_rerank': k_rerank,\n",
    "            'reranker_model': reranker_model,\n",
    "            'mode': mode\n",
    "        },\n",
    "        'queries': query_results\n",
    "    }\n",
    "    \n",
    "    # Save results\n",
    "    print(f\"\\n3. Saving results...\")\n",
    "    save_path = save_results(\n",
    "        results,\n",
    "        provider,\n",
    "        model,\n",
    "        chunk_size,\n",
    "        k_retrieve,\n",
    "        k_rerank,\n",
    "        mode,\n",
    "        reranker_model,\n",
    "        output_dir\n",
    "    )\n",
    "    print(f\"✓ Results saved to: {save_path}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nTotal queries processed: {num_queries}\")\n",
    "    print(f\"Evidence chunks per query: {k_rerank}\")\n",
    "    print(f\"Output file: {os.path.basename(save_path)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return {\n",
    "        'status': 'completed',\n",
    "        'num_queries': num_queries,\n",
    "        'output_path': save_path\n",
    "    }\n",
    "\n",
    "print(\"✓ evaluate_configuration() defined\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 7 COMPLETE: Main Pipeline Implemented\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nImplemented functions:\")\n",
    "print(\"  ✓ evaluate_configuration() - Complete re-ranking pipeline (simplified)\")\n",
    "print(\"\\nKey features:\")\n",
    "print(\"  - No evaluation metrics (MRR, recall, etc.)\")\n",
    "print(\"  - Full chunk text (no truncation)\")\n",
    "print(\"  - FinanceBench-compatible output format\")\n",
    "print(\"  - Evidence array with rank and rerank_score\")\n",
    "print(\"\\nNext step: Batch execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2705276a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ run_batch_evaluation() defined\n",
      "\n",
      "================================================================================\n",
      "EVALUATION PLAN\n",
      "================================================================================\n",
      "\n",
      "Dataset: FinanceBench (150 queries)\n",
      "\n",
      "Evaluation Settings:\n",
      "  Modes: ['global', 'single']\n",
      "\n",
      "Configurations to evaluate:\n",
      "\n",
      "  1. voyage/voyage-3-large\n",
      "     Chunk sizes: [512]\n",
      "     k_retrieve: 80, k_rerank: 20\n",
      "     Reranker models: 3\n",
      "       - cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "       - BAAI/bge-reranker-large\n",
      "       - voyage-rerank-2.5\n",
      "     Total runs: 6\n",
      "     Sample output files:\n",
      "       - voyage_voyage-3-large_chunk512_k80_global_rerank_k20-ms-marco-MiniLM-L-12-v2.json [TO CREATE]\n",
      "       - voyage_voyage-3-large_chunk512_k80_single_rerank_k20-ms-marco-MiniLM-L-12-v2.json [TO CREATE]\n",
      "       - voyage_voyage-3-large_chunk512_k80_global_rerank_k20-bge-reranker-large.json [TO CREATE]\n",
      "       - voyage_voyage-3-large_chunk512_k80_single_rerank_k20-bge-reranker-large.json [TO CREATE]\n",
      "       ... (1 more rerankers)\n",
      "\n",
      "  2. ollama/nomic-embed-text\n",
      "     Chunk sizes: [512]\n",
      "     k_retrieve: 80, k_rerank: 20\n",
      "     Reranker models: 3\n",
      "       - cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "       - BAAI/bge-reranker-large\n",
      "       - voyage-rerank-2.5\n",
      "     Total runs: 6\n",
      "     Sample output files:\n",
      "       - ollama_nomic-embed-text_chunk512_k80_global_rerank_k20-ms-marco-MiniLM-L-12-v2.json [TO CREATE]\n",
      "       - ollama_nomic-embed-text_chunk512_k80_single_rerank_k20-ms-marco-MiniLM-L-12-v2.json [TO CREATE]\n",
      "       - ollama_nomic-embed-text_chunk512_k80_global_rerank_k20-bge-reranker-large.json [TO CREATE]\n",
      "       - ollama_nomic-embed-text_chunk512_k80_single_rerank_k20-bge-reranker-large.json [TO CREATE]\n",
      "       ... (1 more rerankers)\n",
      "\n",
      "  3. voyage/voyage-3-large\n",
      "     Chunk sizes: [1024]\n",
      "     k_retrieve: 80, k_rerank: 20\n",
      "     Reranker models: 3\n",
      "       - cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "       - BAAI/bge-reranker-large\n",
      "       - voyage-rerank-2.5\n",
      "     Total runs: 6\n",
      "     Sample output files:\n",
      "       - voyage_voyage-3-large_chunk1024_k80_global_rerank_k20-ms-marco-MiniLM-L-12-v2.json [TO CREATE]\n",
      "       - voyage_voyage-3-large_chunk1024_k80_single_rerank_k20-ms-marco-MiniLM-L-12-v2.json [TO CREATE]\n",
      "       - voyage_voyage-3-large_chunk1024_k80_global_rerank_k20-bge-reranker-large.json [TO CREATE]\n",
      "       - voyage_voyage-3-large_chunk1024_k80_single_rerank_k20-bge-reranker-large.json [TO CREATE]\n",
      "       ... (1 more rerankers)\n",
      "\n",
      "  4. ollama/nomic-embed-text\n",
      "     Chunk sizes: [1024]\n",
      "     k_retrieve: 80, k_rerank: 20\n",
      "     Reranker models: 3\n",
      "       - cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "       - BAAI/bge-reranker-large\n",
      "       - voyage-rerank-2.5\n",
      "     Total runs: 6\n",
      "     Sample output files:\n",
      "       - ollama_nomic-embed-text_chunk1024_k80_global_rerank_k20-ms-marco-MiniLM-L-12-v2.json [TO CREATE]\n",
      "       - ollama_nomic-embed-text_chunk1024_k80_single_rerank_k20-ms-marco-MiniLM-L-12-v2.json [TO CREATE]\n",
      "       - ollama_nomic-embed-text_chunk1024_k80_global_rerank_k20-bge-reranker-large.json [TO CREATE]\n",
      "       - ollama_nomic-embed-text_chunk1024_k80_single_rerank_k20-bge-reranker-large.json [TO CREATE]\n",
      "       ... (1 more rerankers)\n",
      "\n",
      "  5. voyage/voyage-3-large\n",
      "     Chunk sizes: [2048]\n",
      "     k_retrieve: 80, k_rerank: 10\n",
      "     Reranker models: 3\n",
      "       - cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "       - BAAI/bge-reranker-large\n",
      "       - voyage-rerank-2.5\n",
      "     Total runs: 6\n",
      "     Sample output files:\n",
      "       - voyage_voyage-3-large_chunk2048_k80_global_rerank_k10-ms-marco-MiniLM-L-12-v2.json [TO CREATE]\n",
      "       - voyage_voyage-3-large_chunk2048_k80_single_rerank_k10-ms-marco-MiniLM-L-12-v2.json [TO CREATE]\n",
      "       - voyage_voyage-3-large_chunk2048_k80_global_rerank_k10-bge-reranker-large.json [TO CREATE]\n",
      "       - voyage_voyage-3-large_chunk2048_k80_single_rerank_k10-bge-reranker-large.json [TO CREATE]\n",
      "       ... (1 more rerankers)\n",
      "\n",
      "  6. ollama/nomic-embed-text\n",
      "     Chunk sizes: [2048]\n",
      "     k_retrieve: 80, k_rerank: 10\n",
      "     Reranker models: 3\n",
      "       - cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "       - BAAI/bge-reranker-large\n",
      "       - voyage-rerank-2.5\n",
      "     Total runs: 6\n",
      "     Sample output files:\n",
      "       - ollama_nomic-embed-text_chunk2048_k80_global_rerank_k10-ms-marco-MiniLM-L-12-v2.json [TO CREATE]\n",
      "       - ollama_nomic-embed-text_chunk2048_k80_single_rerank_k10-ms-marco-MiniLM-L-12-v2.json [TO CREATE]\n",
      "       - ollama_nomic-embed-text_chunk2048_k80_global_rerank_k10-bge-reranker-large.json [TO CREATE]\n",
      "       - ollama_nomic-embed-text_chunk2048_k80_single_rerank_k10-bge-reranker-large.json [TO CREATE]\n",
      "       ... (1 more rerankers)\n",
      "\n",
      "================================================================================\n",
      "Total evaluation runs: 36\n",
      "Output directory: ../../retrieval_set\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 8 COMPLETE: Batch Evaluation Ready\n",
      "================================================================================\n",
      "\n",
      "To run the evaluation:\n",
      "  1. Review the evaluation plan above\n",
      "  2. Run the next cell to start batch evaluation\n",
      "  3. Monitor progress (this may take time depending on configurations)\n",
      "\n",
      "Note: You can interrupt and resume anytime - existing results are skipped\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 8. Batch Evaluation Execution\n",
    "\n",
    "# %%\n",
    "def run_batch_evaluation(\n",
    "    configurations: List[Dict],\n",
    "    modes: List[str],\n",
    "    dataset,\n",
    "    reranker_instances: Dict,\n",
    "    vector_db_base_dir: str,\n",
    "    collection_prefix: str,\n",
    "    output_dir: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Run re-ranking for all configurations.\n",
    "    \n",
    "    Args:\n",
    "        configurations: List of configuration dicts\n",
    "        modes: List of modes ('global', 'single')\n",
    "        dataset: FinanceBench dataset\n",
    "        reranker_instances: Dict of loaded reranker models\n",
    "        vector_db_base_dir: Base directory for vector databases\n",
    "        collection_prefix: Collection name prefix\n",
    "        output_dir: Output directory\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(\"STARTING BATCH RE-RANKING\")\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    # Calculate total runs\n",
    "    total_runs = 0\n",
    "    for config in configurations:\n",
    "        chunk_sizes = config['chunk_sizes']\n",
    "        reranker_models = config['reranker_models']\n",
    "        total_runs += len(chunk_sizes) * len(reranker_models) * len(modes)\n",
    "    \n",
    "    print(f\"\\nTotal runs: {total_runs}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Track results\n",
    "    all_results = []\n",
    "    completed = 0\n",
    "    skipped = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # Iterate through all configurations\n",
    "    for config_idx, config in enumerate(configurations, 1):\n",
    "        provider = config['provider']\n",
    "        model = config['model']\n",
    "        chunk_sizes = config['chunk_sizes']\n",
    "        k_retrieve = config['k_retrieve']\n",
    "        k_rerank = config['k_rerank']\n",
    "        reranker_models = config['reranker_models']\n",
    "        \n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"Configuration {config_idx}/{len(configurations)}: {provider}/{model}\")\n",
    "        print(f\"{'#'*80}\")\n",
    "        \n",
    "        for chunk_size in chunk_sizes:\n",
    "            for reranker_model in reranker_models:\n",
    "                for mode in modes:\n",
    "                    # Get reranker instance\n",
    "                    reranker_instance = reranker_instances.get(reranker_model)\n",
    "                    \n",
    "                    if reranker_instance is None:\n",
    "                        print(f\"\\n⚠️  Reranker {reranker_model} not found. Skipping...\")\n",
    "                        failed += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Run evaluation\n",
    "                    try:\n",
    "                        result = evaluate_configuration(\n",
    "                            provider=provider,\n",
    "                            model=model,\n",
    "                            chunk_size=chunk_size,\n",
    "                            k_retrieve=k_retrieve,\n",
    "                            k_rerank=k_rerank,\n",
    "                            reranker_model=reranker_model,\n",
    "                            mode=mode,\n",
    "                            dataset=dataset,\n",
    "                            reranker_instance=reranker_instance,\n",
    "                            vector_db_base_dir=vector_db_base_dir,\n",
    "                            collection_prefix=collection_prefix,\n",
    "                            output_dir=output_dir\n",
    "                        )\n",
    "                        \n",
    "                        if result['status'] == 'completed':\n",
    "                            completed += 1\n",
    "                            all_results.append(result)\n",
    "                        elif result['status'] == 'skipped':\n",
    "                            skipped += 1\n",
    "                        else:\n",
    "                            failed += 1\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"\\n❌ Evaluation failed with exception: {e}\")\n",
    "                        import traceback\n",
    "                        traceback.print_exc()\n",
    "                        failed += 1\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(\"BATCH RE-RANKING COMPLETE\")\n",
    "    print(\"#\"*80)\n",
    "    print(f\"\\nTotal runs: {total_runs}\")\n",
    "    print(f\"  ✓ Completed: {completed}\")\n",
    "    print(f\"  ⊘ Skipped:   {skipped}\")\n",
    "    print(f\"  ✗ Failed:    {failed}\")\n",
    "    print(f\"\\nResults saved to: {output_dir}\")\n",
    "    \n",
    "    return {\n",
    "        'total_runs': total_runs,\n",
    "        'completed': completed,\n",
    "        'skipped': skipped,\n",
    "        'failed': failed,\n",
    "        'results': all_results\n",
    "    }\n",
    "\n",
    "print(\"✓ run_batch_evaluation() defined\")\n",
    "\n",
    "# %%\n",
    "# Display Evaluation Plan\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION PLAN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDataset: FinanceBench ({len(dataset)} queries)\")\n",
    "\n",
    "print(f\"\\nEvaluation Settings:\")\n",
    "print(f\"  Modes: {modes}\")\n",
    "\n",
    "print(f\"\\nConfigurations to evaluate:\")\n",
    "total_runs = 0\n",
    "for i, config in enumerate(configurations, 1):\n",
    "    provider = config['provider']\n",
    "    model = config['model']\n",
    "    chunk_sizes = config['chunk_sizes']\n",
    "    k_retrieve = config['k_retrieve']\n",
    "    k_rerank = config['k_rerank']\n",
    "    reranker_models = config['reranker_models']\n",
    "    \n",
    "    runs_for_config = len(chunk_sizes) * len(reranker_models) * len(modes)\n",
    "    total_runs += runs_for_config\n",
    "    \n",
    "    print(f\"\\n  {i}. {provider}/{model}\")\n",
    "    print(f\"     Chunk sizes: {chunk_sizes}\")\n",
    "    print(f\"     k_retrieve: {k_retrieve}, k_rerank: {k_rerank}\")\n",
    "    print(f\"     Reranker models: {len(reranker_models)}\")\n",
    "    for reranker in reranker_models:\n",
    "        print(f\"       - {reranker}\")\n",
    "    print(f\"     Total runs: {runs_for_config}\")\n",
    "    \n",
    "    # Show sample output filenames\n",
    "    print(f\"     Sample output files:\")\n",
    "    for chunk_size in chunk_sizes[:1]:  # Show only first chunk size\n",
    "        for reranker in reranker_models[:2]:  # Show only first 2 rerankers\n",
    "            for mode in modes:\n",
    "                filename = get_output_filename(provider, model, chunk_size, k_retrieve, k_rerank, mode, reranker)\n",
    "                exists = check_if_results_exist(provider, model, chunk_size, k_retrieve, k_rerank, mode, reranker, OUTPUT_DIR)\n",
    "                status = \"EXISTS\" if exists else \"TO CREATE\"\n",
    "                print(f\"       - {filename} [{status}]\")\n",
    "        if len(reranker_models) > 2:\n",
    "            print(f\"       ... ({len(reranker_models) - 2} more rerankers)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Total evaluation runs: {total_runs}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 8 COMPLETE: Batch Evaluation Ready\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTo run the evaluation:\")\n",
    "print(\"  1. Review the evaluation plan above\")\n",
    "print(\"  2. Run the next cell to start batch evaluation\")\n",
    "print(\"  3. Monitor progress (this may take time depending on configurations)\")\n",
    "print(\"\\nNote: You can interrupt and resume anytime - existing results are skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25492e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration (temporary - just for verification)\n",
    "test_configurations = [\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-3-large',\n",
    "        'chunk_sizes': [512],\n",
    "        'k_retrieve': 80,\n",
    "        'k_rerank': 20,\n",
    "        'reranker_models': [\n",
    "            'voyage-rerank-2.5'  # Just one reranker for testing\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35578be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 9. Execute Batch Evaluation\n",
    "# \n",
    "# **Run this cell to start the batch re-ranking process**\n",
    "\n",
    "# %%\n",
    "# Run batch evaluation\n",
    "batch_results = run_batch_evaluation(\n",
    "    configurations=configurations,\n",
    "    modes=modes,\n",
    "    dataset=dataset,\n",
    "    reranker_instances=reranker_instances,\n",
    "    vector_db_base_dir=VECTOR_DB_BASE_DIR,\n",
    "    collection_prefix=COLLECTION_PREFIX,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Display final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal runs attempted: {batch_results['total_runs']}\")\n",
    "print(f\"  ✓ Successfully completed: {batch_results['completed']}\")\n",
    "print(f\"  ⊘ Skipped (already exists): {batch_results['skipped']}\")\n",
    "print(f\"  ✗ Failed: {batch_results['failed']}\")\n",
    "\n",
    "if batch_results['completed'] > 0:\n",
    "    print(f\"\\n✅ Successfully created {batch_results['completed']} JSON files\")\n",
    "    print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "    print(\"\\nYou can now use these JSON files in your answer generation pipeline!\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No new files were created. Check if results already exist.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ret_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
