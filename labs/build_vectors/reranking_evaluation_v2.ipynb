{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "66a32206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n",
      "✓ API keys loaded successfully\n",
      "✓ Configuration parameters set\n",
      "  - Chunk text preview: 100 + 100 chars\n",
      "  - Text similarity threshold: 0.7\n",
      "  - Vector DB base directory: ../../vector_databases\n",
      "  - Collection prefix: financebench_docs_chunk_\n",
      "  - Output directory: ../../evaluation_results/reranking_results\n",
      "✓ Re-ranking configurations defined\n",
      "\n",
      "Total configurations: 1\n",
      "\n",
      "  Configuration 1:\n",
      "    Provider/Model: voyage/voyage-3-large\n",
      "    Chunk sizes: [1024]\n",
      "    k_retrieve: 80, k_rerank: 20\n",
      "    Reranker models: 1\n",
      "      - voyage-rerank-2.5\n",
      "\n",
      "  Evaluation modes: ['global', 'single']\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 1 COMPLETE: Imports and Configuration\n",
      "================================================================================\n",
      "\n",
      "Next steps:\n",
      "  2. Load dataset and evidence\n",
      "  3. Initialize reranker models\n",
      "  4. Implement retrieval and re-ranking functions\n",
      "  5. Implement evaluation metrics\n",
      "  6. Run batch evaluation\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Re-Ranking Evaluation Pipeline\n",
    "# This notebook implements re-ranking evaluation for RAG systems using multiple reranker models.\n",
    "# \n",
    "# **Workflow:**\n",
    "# 1. Retrieve top k_retrieve chunks from existing vector stores\n",
    "# 2. Apply re-ranking to get top k_rerank chunks\n",
    "# 3. Evaluate and compare: k_retrieve → k_rerank (initial) → k_rerank (reranked)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Imports and Configuration\n",
    "\n",
    "# %%\n",
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.schema import Document\n",
    "from langchain_voyageai import VoyageAIRerank\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import CohereEmbeddings\n",
    "\n",
    "# Sentence transformers for cross-encoders\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Hugging Face datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "\n",
    "# %%\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# API Keys\n",
    "VOYAGE_API_KEY = os.getenv(\"VOYAGE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "# Verify API keys\n",
    "assert VOYAGE_API_KEY is not None, \"VOYAGE_API_KEY not found in environment\"\n",
    "print(\"✓ API keys loaded successfully\")\n",
    "\n",
    "# %%\n",
    "# Configuration Parameters\n",
    "CHUNK_TEXT_PREFIX_CHARS = 100  # Characters to keep from start\n",
    "CHUNK_TEXT_SUFFIX_CHARS = 100  # Characters to keep from end\n",
    "TEXT_SIMILARITY_THRESHOLD = 0.7  # From baseline\n",
    "USE_PAGE_TOLERANCE = True  # From baseline\n",
    "\n",
    "# Paths\n",
    "VECTOR_DB_BASE_DIR = \"../../vector_databases\"  # Base directory for vector databases\n",
    "COLLECTION_PREFIX = \"financebench_docs_chunk_\"  # Collection name prefix\n",
    "OUTPUT_DIR = \"../../evaluation_results/reranking_results\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✓ Configuration parameters set\")\n",
    "print(f\"  - Chunk text preview: {CHUNK_TEXT_PREFIX_CHARS} + {CHUNK_TEXT_SUFFIX_CHARS} chars\")\n",
    "print(f\"  - Text similarity threshold: {TEXT_SIMILARITY_THRESHOLD}\")\n",
    "print(f\"  - Vector DB base directory: {VECTOR_DB_BASE_DIR}\")\n",
    "print(f\"  - Collection prefix: {COLLECTION_PREFIX}\")\n",
    "print(f\"  - Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Evaluation modes\n",
    "modes = ['global', 'single']\n",
    "\n",
    "print(\"✓ Re-ranking configurations defined\")\n",
    "print(f\"\\nTotal configurations: {len(configurations)}\")\n",
    "for i, config in enumerate(configurations, 1):\n",
    "    print(f\"\\n  Configuration {i}:\")\n",
    "    print(f\"    Provider/Model: {config['provider']}/{config['model']}\")\n",
    "    print(f\"    Chunk sizes: {config['chunk_sizes']}\")\n",
    "    print(f\"    k_retrieve: {config['k_retrieve']}, k_rerank: {config['k_rerank']}\")\n",
    "    print(f\"    Reranker models: {len(config['reranker_models'])}\")\n",
    "    for reranker in config['reranker_models']:\n",
    "        print(f\"      - {reranker}\")\n",
    "\n",
    "print(f\"\\n  Evaluation modes: {modes}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 1 COMPLETE: Imports and Configuration\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  2. Load dataset and evidence\")\n",
    "print(\"  3. Initialize reranker models\")\n",
    "print(\"  4. Implement retrieval and re-ranking functions\")\n",
    "print(\"  5. Implement evaluation metrics\")\n",
    "print(\"  6. Run batch evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "327d6e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FinanceBench dataset...\n",
      "✓ Loaded 150 questions from FinanceBench\n",
      "\n",
      "Extracting evidence items...\n",
      "✓ Extracted 189 evidence items\n",
      "  Unique documents: 84\n",
      "\n",
      "Loading Sentence-BERT model for text similarity...\n",
      "✓ Loaded all-MiniLM-L6-v2\n",
      "\n",
      "Computing embeddings for evidence texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c76a791f6a340c98bd33a4a7dc529fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Computed 189 evidence embeddings\n",
      "  Embedding shape: (189, 384)\n",
      "✓ Evidence embeddings added to evidence items\n",
      "\n",
      "Sample evidence item:\n",
      "  Doc: 3M_2018_10K\n",
      "  Page: 59\n",
      "  Text preview: Table of Contents \n",
      "3M Company and Subsidiaries\n",
      "Consolidated Statement of Cash Flow s\n",
      "Years ended December 31\n",
      " \n",
      "(Millions)\n",
      " \n",
      "2018\n",
      " \n",
      "2017\n",
      " \n",
      "2016\n",
      " \n",
      "Cash ...\n",
      "  Embedding shape: (384,)\n",
      "\n",
      "Sample query from dataset:\n",
      "  Question: What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.\n",
      "  Doc: 3M_2018_10K\n",
      "  Evidence pages: [59]\n",
      "  Answer: $1577.00\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 2 COMPLETE: Dataset, Evidence, and Embeddings Loaded\n",
      "================================================================================\n",
      "\n",
      "Dataset statistics:\n",
      "  Total queries: 150\n",
      "  Total evidence items: 189\n",
      "  Evidence embeddings computed: 189\n",
      "  SBERT model: all-MiniLM-L6-v2\n",
      "\n",
      "Next step: Initialize reranker models\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 2. Load Dataset and Evidence\n",
    "\n",
    "# %%\n",
    "# Load FinanceBench dataset\n",
    "print(\"Loading FinanceBench dataset...\")\n",
    "dataset = load_dataset(\"PatronusAI/financebench\", split=\"train\")\n",
    "print(f\"✓ Loaded {len(dataset)} questions from FinanceBench\")\n",
    "\n",
    "# %%\n",
    "# Extract all evidence items for evaluation\n",
    "print(\"\\nExtracting evidence items...\")\n",
    "all_evidence = []\n",
    "for item in dataset:\n",
    "    doc_name = item['doc_name']\n",
    "    evidence = item['evidence']\n",
    "    \n",
    "    for ev in evidence:\n",
    "        all_evidence.append({\n",
    "            'doc_name': doc_name,\n",
    "            'page_number': ev['evidence_page_num'],\n",
    "            'text': ev['evidence_text']\n",
    "        })\n",
    "\n",
    "print(f\"✓ Extracted {len(all_evidence)} evidence items\")\n",
    "print(f\"  Unique documents: {len(set(ev['doc_name'] for ev in all_evidence))}\")\n",
    "\n",
    "# %%\n",
    "# Load Sentence-BERT model for text similarity\n",
    "print(\"\\nLoading Sentence-BERT model for text similarity...\")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "SBERT_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "sbert_model = SentenceTransformer(SBERT_MODEL_NAME)\n",
    "print(f\"✓ Loaded {SBERT_MODEL_NAME}\")\n",
    "\n",
    "# %%\n",
    "# Compute embeddings for all evidence texts\n",
    "print(\"\\nComputing embeddings for evidence texts...\")\n",
    "evidence_texts = [ev['text'] for ev in all_evidence]\n",
    "evidence_embeddings = sbert_model.encode(\n",
    "    evidence_texts,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32\n",
    ")\n",
    "print(f\"✓ Computed {len(evidence_embeddings)} evidence embeddings\")\n",
    "print(f\"  Embedding shape: {evidence_embeddings.shape}\")\n",
    "\n",
    "# Add embeddings to evidence items\n",
    "for i, ev in enumerate(all_evidence):\n",
    "    ev['embedding'] = evidence_embeddings[i]\n",
    "\n",
    "print(\"✓ Evidence embeddings added to evidence items\")\n",
    "\n",
    "# %%\n",
    "# Display sample evidence\n",
    "print(\"\\nSample evidence item:\")\n",
    "sample_ev = all_evidence[0]\n",
    "print(f\"  Doc: {sample_ev['doc_name']}\")\n",
    "print(f\"  Page: {sample_ev['page_number']}\")\n",
    "print(f\"  Text preview: {sample_ev['text'][:150]}...\")\n",
    "print(f\"  Embedding shape: {sample_ev['embedding'].shape}\")\n",
    "\n",
    "# %%\n",
    "# Display sample query\n",
    "print(\"\\nSample query from dataset:\")\n",
    "sample_query = dataset[0]\n",
    "print(f\"  Question: {sample_query['question']}\")\n",
    "print(f\"  Doc: {sample_query['doc_name']}\")\n",
    "print(f\"  Evidence pages: {[ev['evidence_page_num'] for ev in sample_query['evidence']]}\")\n",
    "print(f\"  Answer: {sample_query['answer']}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 2 COMPLETE: Dataset, Evidence, and Embeddings Loaded\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"  Total queries: {len(dataset)}\")\n",
    "print(f\"  Total evidence items: {len(all_evidence)}\")\n",
    "print(f\"  Evidence embeddings computed: {len(evidence_embeddings)}\")\n",
    "print(f\"  SBERT model: {SBERT_MODEL_NAME}\")\n",
    "print(f\"\\nNext step: Initialize reranker models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7604c361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique reranker models to initialize: 1\n",
      "  - voyage-rerank-2.5\n",
      "\n",
      "================================================================================\n",
      "Initializing reranker models...\n",
      "================================================================================\n",
      "\n",
      "Loading: voyage-rerank-2.5\n",
      "  ✓ Voyage reranker marked as API-based (will initialize per-query)\n",
      "\n",
      "================================================================================\n",
      "✓ Initialized 1 reranker models\n",
      "================================================================================\n",
      "\n",
      "Loaded reranker models:\n",
      "  ✓ voyage-rerank-2.5 (API-based)\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 3 COMPLETE: Reranker Models Initialized\n",
      "================================================================================\n",
      "\n",
      "Ready rerankers:\n",
      "  - voyage-rerank-2.5\n",
      "\n",
      "Next step: Implement helper functions\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 3. Initialize Reranker Models\n",
    "\n",
    "# %%\n",
    "def get_all_unique_rerankers(configurations: List[Dict]) -> List[str]:\n",
    "    \"\"\"Extract all unique reranker models from configurations.\"\"\"\n",
    "    rerankers = set()\n",
    "    for config in configurations:\n",
    "        for reranker in config['reranker_models']:\n",
    "            rerankers.add(reranker)\n",
    "    return sorted(list(rerankers))\n",
    "\n",
    "# Get all unique rerankers\n",
    "unique_rerankers = get_all_unique_rerankers(configurations)\n",
    "print(f\"Unique reranker models to initialize: {len(unique_rerankers)}\")\n",
    "for reranker in unique_rerankers:\n",
    "    print(f\"  - {reranker}\")\n",
    "\n",
    "# %%\n",
    "# Initialize reranker models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Initializing reranker models...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "reranker_instances = {}\n",
    "\n",
    "for reranker_name in unique_rerankers:\n",
    "    print(f\"\\nLoading: {reranker_name}\")\n",
    "    \n",
    "    if reranker_name == 'voyage-rerank-2.5':\n",
    "        # Voyage reranker will be initialized per-query (API-based)\n",
    "        reranker_instances[reranker_name] = 'api'\n",
    "        print(f\"  ✓ Voyage reranker marked as API-based (will initialize per-query)\")\n",
    "        \n",
    "    elif reranker_name.startswith('cross-encoder/') or reranker_name.startswith('BAAI/'):\n",
    "        # Load Hugging Face cross-encoder models\n",
    "        try:\n",
    "            model = CrossEncoder(reranker_name)\n",
    "            reranker_instances[reranker_name] = model\n",
    "            print(f\"  ✓ Successfully loaded Hugging Face model\")\n",
    "            print(f\"    Max sequence length: {model.max_length}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Failed to load {reranker_name}: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown reranker type: {reranker_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"✓ Initialized {len(reranker_instances)} reranker models\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# %%\n",
    "# Display loaded models\n",
    "print(\"\\nLoaded reranker models:\")\n",
    "for name, instance in reranker_instances.items():\n",
    "    if instance == 'api':\n",
    "        print(f\"  ✓ {name} (API-based)\")\n",
    "    else:\n",
    "        print(f\"  ✓ {name} (Local model)\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 3 COMPLETE: Reranker Models Initialized\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nReady rerankers:\")\n",
    "for reranker_name in reranker_instances.keys():\n",
    "    print(f\"  - {reranker_name}\")\n",
    "print(f\"\\nNext step: Implement helper functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ff695c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncation test:\n",
      "  Original length: 75\n",
      "  Truncated: This is a very long ... storage efficiency.\n",
      "  Truncated length: 43\n",
      "\n",
      "Reranker name simplification:\n",
      "  cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "    -> cross-encoder-ms-marco-miniLM\n",
      "  BAAI/bge-reranker-large\n",
      "    -> bge-reranker-large\n",
      "  voyage-rerank-2.5\n",
      "    -> voyage-rerank-2.5\n",
      "\n",
      "Filename generation test:\n",
      "  voyage_voyage-3-large_chunk512_k100_global_rerank_k20-voyage-rerank-2.5.json\n",
      "\n",
      "✓ File management functions defined\n",
      "\n",
      "Collection name test:\n",
      "  voyage_voyage-3-large_chunk512\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 4 COMPLETE: Helper Functions Defined\n",
      "================================================================================\n",
      "\n",
      "Implemented functions:\n",
      "  ✓ truncate_chunk_text() - Truncate text for storage\n",
      "  ✓ simplify_reranker_name() - Simplify model names for filenames\n",
      "  ✓ get_output_filename() - Generate result filenames\n",
      "  ✓ check_if_results_exist() - Check for existing results\n",
      "  ✓ save_results() - Save evaluation results\n",
      "  ✓ get_collection_name() - Generate vector store collection names\n",
      "\n",
      "Next step: Implement retrieval functions\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 4. Helper Functions\n",
    "\n",
    "# %%\n",
    "def truncate_chunk_text(text: str, prefix_chars: int, suffix_chars: int) -> str:\n",
    "    \"\"\"\n",
    "    Truncate chunk text to keep only prefix and suffix characters.\n",
    "    \n",
    "    Args:\n",
    "        text: Full chunk text\n",
    "        prefix_chars: Number of characters to keep from start\n",
    "        suffix_chars: Number of characters to keep from end\n",
    "        \n",
    "    Returns:\n",
    "        Truncated text in format: \"prefix...suffix\"\n",
    "    \"\"\"\n",
    "    if len(text) <= (prefix_chars + suffix_chars):\n",
    "        return text\n",
    "    \n",
    "    prefix = text[:prefix_chars]\n",
    "    suffix = text[-suffix_chars:]\n",
    "    return f\"{prefix}...{suffix}\"\n",
    "\n",
    "# Test the function\n",
    "test_text = \"This is a very long text that needs to be truncated for storage efficiency.\"\n",
    "truncated = truncate_chunk_text(test_text, 20, 20)\n",
    "print(\"Truncation test:\")\n",
    "print(f\"  Original length: {len(test_text)}\")\n",
    "print(f\"  Truncated: {truncated}\")\n",
    "print(f\"  Truncated length: {len(truncated)}\")\n",
    "\n",
    "# %%\n",
    "def simplify_reranker_name(reranker_model: str) -> str:\n",
    "    \"\"\"\n",
    "    Simplify reranker model name for use in filenames.\n",
    "    \n",
    "    Examples:\n",
    "        'cross-encoder/ms-marco-MiniLM-L-12-v2' -> 'cross-encoder-ms-marco-miniLM'\n",
    "        'BAAI/bge-reranker-large' -> 'bge-reranker-large'\n",
    "        'voyage-rerank-2.5' -> 'voyage-rerank-2.5'\n",
    "    \"\"\"\n",
    "    # Replace slashes with hyphens\n",
    "    simplified = reranker_model.replace('/', '-')\n",
    "    \n",
    "    # Simplify long cross-encoder names\n",
    "    if 'ms-marco-MiniLM' in simplified:\n",
    "        simplified = 'cross-encoder-ms-marco-miniLM'\n",
    "    elif 'BAAI-bge-reranker' in simplified:\n",
    "        simplified = simplified.replace('BAAI-', '')\n",
    "    \n",
    "    return simplified\n",
    "\n",
    "# Test the function\n",
    "test_names = [\n",
    "    'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "    'BAAI/bge-reranker-large',\n",
    "    'voyage-rerank-2.5'\n",
    "]\n",
    "print(\"\\nReranker name simplification:\")\n",
    "for name in test_names:\n",
    "    print(f\"  {name}\")\n",
    "    print(f\"    -> {simplify_reranker_name(name)}\")\n",
    "\n",
    "# %%\n",
    "def get_output_filename(\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k_retrieve: int,\n",
    "    k_rerank: int,\n",
    "    mode: str,\n",
    "    reranker_model: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate output filename for reranking results.\n",
    "    \n",
    "    Format: {provider}_{model}_chunk{size}_k{k_retrieve}_{mode}_rerank_k{k_rerank}-{reranker}.json\n",
    "    \n",
    "    Example: voyage_voyage-3-large_chunk512_k100_global_rerank_k20-voyage-rerank-2.5.json\n",
    "    \"\"\"\n",
    "    reranker_simplified = simplify_reranker_name(reranker_model)\n",
    "    filename = f\"{provider}_{model}_chunk{chunk_size}_k{k_retrieve}_{mode}_rerank_k{k_rerank}-{reranker_simplified}.json\"\n",
    "    return filename\n",
    "\n",
    "# Test the function\n",
    "test_filename = get_output_filename(\n",
    "    provider='voyage',\n",
    "    model='voyage-3-large',\n",
    "    chunk_size=512,\n",
    "    k_retrieve=100,\n",
    "    k_rerank=20,\n",
    "    mode='global',\n",
    "    reranker_model='voyage-rerank-2.5'\n",
    ")\n",
    "print(f\"\\nFilename generation test:\")\n",
    "print(f\"  {test_filename}\")\n",
    "\n",
    "# %%\n",
    "def check_if_results_exist(\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k_retrieve: int,\n",
    "    k_rerank: int,\n",
    "    mode: str,\n",
    "    reranker_model: str,\n",
    "    output_dir: str\n",
    ") -> bool:\n",
    "    \"\"\"Check if results file already exists.\"\"\"\n",
    "    filename = get_output_filename(provider, model, chunk_size, k_retrieve, k_rerank, mode, reranker_model)\n",
    "    filepath = Path(output_dir) / filename\n",
    "    return filepath.exists()\n",
    "\n",
    "# %%\n",
    "def save_results(\n",
    "    results: Dict,\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k_retrieve: int,\n",
    "    k_rerank: int,\n",
    "    mode: str,\n",
    "    reranker_model: str,\n",
    "    output_dir: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Save evaluation results to JSON file.\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    filename = get_output_filename(provider, model, chunk_size, k_retrieve, k_rerank, mode, reranker_model)\n",
    "    filepath = Path(output_dir) / filename\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    return str(filepath)\n",
    "\n",
    "print(\"\\n✓ File management functions defined\")\n",
    "\n",
    "# %%\n",
    "def get_collection_name(provider: str, model: str, chunk_size: int) -> str:\n",
    "    \"\"\"\n",
    "    Generate ChromaDB collection name.\n",
    "    Format matches baseline: {provider}_{model}_chunk{size}\n",
    "    \"\"\"\n",
    "    return f\"{provider}_{model}_chunk{chunk_size}\"\n",
    "\n",
    "# Test the function\n",
    "test_collection = get_collection_name('voyage', 'voyage-3-large', 512)\n",
    "print(f\"\\nCollection name test:\")\n",
    "print(f\"  {test_collection}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 4 COMPLETE: Helper Functions Defined\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nImplemented functions:\")\n",
    "print(\"  ✓ truncate_chunk_text() - Truncate text for storage\")\n",
    "print(\"  ✓ simplify_reranker_name() - Simplify model names for filenames\")\n",
    "print(\"  ✓ get_output_filename() - Generate result filenames\")\n",
    "print(\"  ✓ check_if_results_exist() - Check for existing results\")\n",
    "print(\"  ✓ save_results() - Save evaluation results\")\n",
    "print(\"  ✓ get_collection_name() - Generate vector store collection names\")\n",
    "print(\"\\nNext step: Implement retrieval functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0966f56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ extract_doc_name_from_path() and extract_metadata_from_retrieved_doc() defined\n",
      "✓ load_vector_store() defined\n",
      "✓ retrieve_documents() defined\n",
      "\n",
      "================================================================================\n",
      "Testing retrieval functions...\n",
      "================================================================================\n",
      "\n",
      "Test parameters:\n",
      "  Provider: voyage\n",
      "  Model: voyage-3-large\n",
      "  Chunk size: 1024\n",
      "  k_retrieve: 80\n",
      "\n",
      "Loading vector store...\n",
      "  ✓ Loaded collection 'financebench_docs_chunk_1024' from ../../vector_databases/voyage_voyage-3-large\n",
      "    Documents: 15765\n",
      "\n",
      "Test query: What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the quest...\n",
      "Target doc: 3M_2018_10K\n",
      "\n",
      "Retrieving top 80 documents (global mode)...\n",
      "  ✓ Retrieved 80 documents\n",
      "  Top result: 3M_2018_10K (page 48, score: 0.5075)\n",
      "\n",
      "Retrieving top 80 documents (single mode)...\n",
      "  ✓ Retrieved 80 documents\n",
      "  Top result: 3M_2018_10K (page 48, score: 0.5075)\n",
      "\n",
      "✓ Retrieval test successful!\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 5 COMPLETE: Retrieval Functions Implemented\n",
      "================================================================================\n",
      "\n",
      "Implemented functions:\n",
      "  ✓ get_embedding_model() - Initialize embedding models\n",
      "  ✓ load_vector_store() - Load ChromaDB collections\n",
      "  ✓ retrieve_documents() - Retrieve top-k documents with filtering\n",
      "\n",
      "Next step: Implement re-ranking functions\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Retrieval Functions\n",
    "\n",
    "# %%\n",
    "def get_embedding_model(provider: str, model: str):\n",
    "    \"\"\"\n",
    "    Initialize the appropriate embedding model based on provider.\n",
    "    \n",
    "    Args:\n",
    "        provider: 'voyage', 'openai', or 'cohere'\n",
    "        model: Model name\n",
    "        \n",
    "    Returns:\n",
    "        Embedding model instance\n",
    "    \"\"\"\n",
    "    if provider == 'voyage':\n",
    "        return VoyageAIEmbeddings(\n",
    "            model=model,\n",
    "            voyage_api_key=VOYAGE_API_KEY\n",
    "        )\n",
    "    elif provider == 'openai':\n",
    "        return OpenAIEmbeddings(\n",
    "            model=model,\n",
    "            openai_api_key=OPENAI_API_KEY\n",
    "        )\n",
    "    elif provider == 'cohere':\n",
    "        return CohereEmbeddings(\n",
    "            model=model,\n",
    "            cohere_api_key=COHERE_API_KEY\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "# %%\n",
    "def extract_doc_name_from_path(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract document name from file path.\n",
    "    \n",
    "    Example:\n",
    "        \"../../documents/3M_2018_10K.pdf\" → \"3M_2018_10K\"\n",
    "    \n",
    "    Args:\n",
    "        file_path: Full path to document\n",
    "        \n",
    "    Returns:\n",
    "        Document name without extension\n",
    "    \"\"\"\n",
    "    return Path(file_path).stem\n",
    "\n",
    "\n",
    "def extract_metadata_from_retrieved_doc(doc) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract metadata from a retrieved LangChain document.\n",
    "    \n",
    "    ChromaDB metadata structure:\n",
    "        - file_path: Full path to PDF\n",
    "        - source: Page number (as integer or string)\n",
    "    \n",
    "    Args:\n",
    "        doc: LangChain Document object from vectorstore.similarity_search()\n",
    "        \n",
    "    Returns:\n",
    "        Dict with:\n",
    "            - doc_name: Document name (e.g., \"3M_2018_10K\")\n",
    "            - page_number: Page number (integer, 1-indexed)\n",
    "            - chunk_text: Full chunk text\n",
    "    \"\"\"\n",
    "    metadata = doc.metadata\n",
    "    \n",
    "    # Extract document name from file_path\n",
    "    file_path = metadata.get('file_path', '')\n",
    "    doc_name = extract_doc_name_from_path(file_path) if file_path else ''\n",
    "    \n",
    "    # Extract page number from 'source'\n",
    "    # FinanceBench uses 0-indexed pages, but we convert to 1-indexed\n",
    "    page_source = metadata.get('source', -1)\n",
    "    \n",
    "    # Handle both string and integer page numbers\n",
    "    if isinstance(page_source, str):\n",
    "        try:\n",
    "            page_number = int(page_source) + 1  # Convert to 1-indexed\n",
    "        except ValueError:\n",
    "            page_number = -1\n",
    "    elif isinstance(page_source, int):\n",
    "        page_number = page_source + 1  # Convert to 1-indexed\n",
    "    else:\n",
    "        page_number = -1\n",
    "    \n",
    "    return {\n",
    "        'doc_name': doc_name,\n",
    "        'page_number': page_number,\n",
    "        'chunk_text': doc.page_content\n",
    "    }\n",
    "\n",
    "print(\"✓ extract_doc_name_from_path() and extract_metadata_from_retrieved_doc() defined\")\n",
    "\n",
    "# %%\n",
    "def load_vector_store(provider: str, model: str, chunk_size: int, base_dir: str, collection_prefix: str = \"financebench_docs_chunk_\"):\n",
    "    \"\"\"\n",
    "    Load existing ChromaDB vector store.\n",
    "    \n",
    "    Matches the directory structure from baseline:\n",
    "    {base_dir}/{provider}_{model}/financebench_docs_chunk_{chunk_size}/\n",
    "    \n",
    "    Args:\n",
    "        provider: Embedding provider\n",
    "        model: Embedding model name\n",
    "        chunk_size: Chunk size used\n",
    "        base_dir: Base directory for vector databases\n",
    "        collection_prefix: Prefix for collection names\n",
    "        \n",
    "    Returns:\n",
    "        Chroma vector store instance\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If collection doesn't exist\n",
    "    \"\"\"\n",
    "    # Construct paths matching baseline structure\n",
    "    model_id = f\"{provider}_{model.replace('/', '_')}\"\n",
    "    db_path = os.path.join(base_dir, model_id)\n",
    "    collection_name = f\"{collection_prefix}{chunk_size}\"\n",
    "    \n",
    "    embedding_model = get_embedding_model(provider, model)\n",
    "    \n",
    "    try:\n",
    "        vectorstore = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=embedding_model,\n",
    "            persist_directory=db_path\n",
    "        )\n",
    "        \n",
    "        # Verify collection exists by checking count\n",
    "        count = vectorstore._collection.count()\n",
    "        if count == 0:\n",
    "            raise ValueError(f\"Collection '{collection_name}' is empty\")\n",
    "        \n",
    "        print(f\"  ✓ Loaded collection '{collection_name}' from {db_path}\")\n",
    "        print(f\"    Documents: {count}\")\n",
    "        return vectorstore\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load collection '{collection_name}' from {db_path}: {e}\")\n",
    "\n",
    "print(\"✓ load_vector_store() defined\")\n",
    "\n",
    "# %%\n",
    "def retrieve_documents(\n",
    "    vectorstore,\n",
    "    query: str,\n",
    "    k: int,\n",
    "    mode: str,\n",
    "    doc_name: str = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve top k documents from vector store.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: ChromaDB vector store\n",
    "        query: Search query\n",
    "        k: Number of documents to retrieve\n",
    "        mode: 'global' or 'single'\n",
    "        doc_name: Document name (required for 'single' mode)\n",
    "        \n",
    "    Returns:\n",
    "        List of retrieved documents with metadata\n",
    "        Format: [{'doc_name': str, 'page_number': int, 'content': str, 'rank': int, 'score': float}, ...]\n",
    "    \"\"\"\n",
    "    if mode == 'single':\n",
    "        if doc_name is None:\n",
    "            raise ValueError(\"doc_name required for single-document mode\")\n",
    "        \n",
    "        # Retrieve more documents and filter in Python\n",
    "        # Retrieve 3x to ensure we get enough from the target document\n",
    "        results = vectorstore.similarity_search_with_score(query, k=k * 3)\n",
    "        \n",
    "        # Filter to only documents matching the doc_name\n",
    "        filtered_results = []\n",
    "        for doc, score in results:\n",
    "            metadata = extract_metadata_from_retrieved_doc(doc)\n",
    "            if metadata['doc_name'] == doc_name:\n",
    "                filtered_results.append((doc, score))\n",
    "                if len(filtered_results) >= k:\n",
    "                    break\n",
    "        \n",
    "        results = filtered_results[:k]\n",
    "    else:  # global mode\n",
    "        results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    # Format results using metadata extraction\n",
    "    retrieved_docs = []\n",
    "    for rank, (doc, score) in enumerate(results, start=1):\n",
    "        metadata = extract_metadata_from_retrieved_doc(doc)\n",
    "        \n",
    "        retrieved_docs.append({\n",
    "            'doc_name': metadata['doc_name'],\n",
    "            'page_number': metadata['page_number'],\n",
    "            'content': metadata['chunk_text'],\n",
    "            'rank': rank,\n",
    "            'score': float(score)\n",
    "        })\n",
    "    \n",
    "    return retrieved_docs\n",
    "\n",
    "print(\"✓ retrieve_documents() defined\")\n",
    "\n",
    "# %%\n",
    "# Test retrieval with a sample configuration\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing retrieval functions...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use first configuration for testing\n",
    "test_config = configurations[0]\n",
    "test_provider = test_config['provider']\n",
    "test_model = test_config['model']\n",
    "test_chunk_size = test_config['chunk_sizes'][0]\n",
    "test_k_retrieve = test_config['k_retrieve']\n",
    "\n",
    "print(f\"\\nTest parameters:\")\n",
    "print(f\"  Provider: {test_provider}\")\n",
    "print(f\"  Model: {test_model}\")\n",
    "print(f\"  Chunk size: {test_chunk_size}\")\n",
    "print(f\"  k_retrieve: {test_k_retrieve}\")\n",
    "\n",
    "try:\n",
    "    # Load vector store\n",
    "    print(f\"\\nLoading vector store...\")\n",
    "    test_vectorstore = load_vector_store(\n",
    "        test_provider,\n",
    "        test_model,\n",
    "        test_chunk_size,\n",
    "        VECTOR_DB_BASE_DIR,\n",
    "        COLLECTION_PREFIX\n",
    "    )\n",
    "    \n",
    "    # Test retrieval with first query\n",
    "    test_query = dataset[0]\n",
    "    print(f\"\\nTest query: {test_query['question'][:100]}...\")\n",
    "    print(f\"Target doc: {test_query['doc_name']}\")\n",
    "    \n",
    "    # Test global mode\n",
    "    print(f\"\\nRetrieving top {test_k_retrieve} documents (global mode)...\")\n",
    "    retrieved_global = retrieve_documents(\n",
    "        test_vectorstore,\n",
    "        test_query['question'],\n",
    "        k=test_k_retrieve,\n",
    "        mode='global'\n",
    "    )\n",
    "    print(f\"  ✓ Retrieved {len(retrieved_global)} documents\")\n",
    "    print(f\"  Top result: {retrieved_global[0]['doc_name']} (page {retrieved_global[0]['page_number']}, score: {retrieved_global[0]['score']:.4f})\")\n",
    "    \n",
    "    # Test single mode\n",
    "    print(f\"\\nRetrieving top {test_k_retrieve} documents (single mode)...\")\n",
    "    retrieved_single = retrieve_documents(\n",
    "        test_vectorstore,\n",
    "        test_query['question'],\n",
    "        k=test_k_retrieve,\n",
    "        mode='single',\n",
    "        doc_name=test_query['doc_name']\n",
    "    )\n",
    "    print(f\"  ✓ Retrieved {len(retrieved_single)} documents\")\n",
    "    print(f\"  Top result: {retrieved_single[0]['doc_name']} (page {retrieved_single[0]['page_number']}, score: {retrieved_single[0]['score']:.4f})\")\n",
    "    \n",
    "    print(\"\\n✓ Retrieval test successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Retrieval test failed: {e}\")\n",
    "    print(\"\\nThis is expected if vector stores haven't been created yet.\")\n",
    "    print(\"Make sure you have run the baseline evaluation first to create vector stores.\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 5 COMPLETE: Retrieval Functions Implemented\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nImplemented functions:\")\n",
    "print(\"  ✓ get_embedding_model() - Initialize embedding models\")\n",
    "print(\"  ✓ load_vector_store() - Load ChromaDB collections\")\n",
    "print(\"  ✓ retrieve_documents() - Retrieve top-k documents with filtering\")\n",
    "print(\"\\nNext step: Implement re-ranking functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4b0bf80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ rerank_with_voyage() defined with rate limit handling\n",
      "✓ rerank_with_cross_encoder() defined\n",
      "✓ rerank_documents() defined\n",
      "\n",
      "================================================================================\n",
      "Testing re-ranking functions...\n",
      "================================================================================\n",
      "\n",
      "Sample query: What was the company's revenue in 2023?\n",
      "Sample documents: 3\n",
      "\n",
      "  Skipping test - cross-encoder/ms-marco-MiniLM-L-12-v2 not loaded\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 6 COMPLETE: Re-ranking Functions Implemented\n",
      "================================================================================\n",
      "\n",
      "Implemented functions:\n",
      "  ✓ rerank_with_voyage() - Voyage AI API re-ranking\n",
      "  ✓ rerank_with_cross_encoder() - Hugging Face cross-encoder re-ranking\n",
      "  ✓ rerank_documents() - Universal re-ranking router\n",
      "\n",
      "Next step: Implement evaluation metrics\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 6. Re-ranking Functions\n",
    "\n",
    "# %%\n",
    "import time\n",
    "\n",
    "def rerank_with_voyage(\n",
    "    query: str,\n",
    "    retrieved_docs: List[Dict],\n",
    "    reranker_model: str,\n",
    "    top_k: int,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay: int = 60\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Re-rank documents using Voyage AI reranker API with rate limit handling.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        retrieved_docs: List from retrieval (with 'content', 'rank', 'score')\n",
    "        reranker_model: Voyage model name (e.g., 'voyage-rerank-2.5')\n",
    "        top_k: Number of documents to return after re-ranking\n",
    "        max_retries: Maximum number of retries on rate limit\n",
    "        retry_delay: Seconds to wait between retries\n",
    "        \n",
    "    Returns:\n",
    "        Re-ranked list with {'doc_name', 'page_number', 'rank', 'initial_rank', 'initial_score', 'rerank_score', 'content'}\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document\n",
    "    from langchain_voyageai import VoyageAIRerank\n",
    "    \n",
    "    # Convert to LangChain documents\n",
    "    lc_docs = [\n",
    "        Document(\n",
    "            page_content=doc['content'],\n",
    "            metadata={\n",
    "                'doc_name': doc['doc_name'], \n",
    "                'page_number': doc['page_number'],\n",
    "                'initial_rank': doc['rank'],\n",
    "                'initial_score': doc['score']\n",
    "            }\n",
    "        )\n",
    "        for doc in retrieved_docs\n",
    "    ]\n",
    "    \n",
    "    # Initialize Voyage reranker\n",
    "    # Extract model name (e.g., \"rerank-2.5\" from \"voyage-rerank-2.5\")\n",
    "    model_name = reranker_model.replace('voyage-', '')\n",
    "    \n",
    "    reranker = VoyageAIRerank(\n",
    "        model=model_name,\n",
    "        voyage_api_key=VOYAGE_API_KEY,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    # Retry logic for rate limiting\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Rerank documents\n",
    "            reranked_docs = reranker.compress_documents(lc_docs, query)\n",
    "            break  # Success, exit retry loop\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Check if it's a rate limit error\n",
    "            if \"rate limit\" in error_msg.lower() or \"tpm\" in error_msg.lower():\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"\\n⚠️  Rate limit hit. Waiting {retry_delay} seconds before retry {attempt + 1}/{max_retries}...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    print(f\"\\n❌ Rate limit exceeded after {max_retries} attempts\")\n",
    "                    raise\n",
    "            else:\n",
    "                # Non-rate-limit error, raise immediately\n",
    "                raise\n",
    "    \n",
    "    # Convert back to our format\n",
    "    results = []\n",
    "    for rank, doc in enumerate(reranked_docs, start=1):\n",
    "        result = {\n",
    "            'doc_name': doc.metadata['doc_name'],\n",
    "            'page_number': doc.metadata['page_number'],\n",
    "            'content': doc.page_content,\n",
    "            'rank': rank,\n",
    "            'initial_rank': doc.metadata['initial_rank'],\n",
    "            'initial_score': doc.metadata['initial_score'],\n",
    "            'rerank_score': doc.metadata.get('relevance_score', 0.0)\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ rerank_with_voyage() defined with rate limit handling\")\n",
    "\n",
    "# %%\n",
    "def rerank_with_cross_encoder(\n",
    "    query: str,\n",
    "    retrieved_docs: List[Dict],\n",
    "    cross_encoder_model: CrossEncoder,\n",
    "    top_k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Re-rank documents using Hugging Face cross-encoder model.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        retrieved_docs: List from retrieval (with 'content', 'rank', 'score')\n",
    "        cross_encoder_model: Loaded CrossEncoder model instance\n",
    "        top_k: Number of documents to return after re-ranking\n",
    "        \n",
    "    Returns:\n",
    "        Re-ranked list with {'doc_name', 'page_number', 'rank', 'initial_rank', 'initial_score', 'rerank_score', 'content'}\n",
    "    \"\"\"\n",
    "    # Prepare query-document pairs\n",
    "    pairs = [[query, doc['content']] for doc in retrieved_docs]\n",
    "    \n",
    "    # Get relevance scores from cross-encoder\n",
    "    scores = cross_encoder_model.predict(pairs)\n",
    "    \n",
    "    # Combine scores with documents\n",
    "    docs_with_scores = []\n",
    "    for doc, score in zip(retrieved_docs, scores):\n",
    "        docs_with_scores.append({\n",
    "            'doc_name': doc['doc_name'],\n",
    "            'page_number': doc['page_number'],\n",
    "            'content': doc['content'],\n",
    "            'initial_rank': doc['rank'],\n",
    "            'initial_score': doc['score'],\n",
    "            'rerank_score': float(score)\n",
    "        })\n",
    "    \n",
    "    # Sort by rerank score (descending) and take top_k\n",
    "    docs_with_scores.sort(key=lambda x: x['rerank_score'], reverse=True)\n",
    "    top_docs = docs_with_scores[:top_k]\n",
    "    \n",
    "    # Assign new ranks\n",
    "    results = []\n",
    "    for rank, doc in enumerate(top_docs, start=1):\n",
    "        doc['rank'] = rank\n",
    "        results.append(doc)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ rerank_with_cross_encoder() defined\")\n",
    "\n",
    "# %%\n",
    "def rerank_documents(\n",
    "    query: str,\n",
    "    retrieved_docs: List[Dict],\n",
    "    reranker_model: str,\n",
    "    reranker_instance: Any,\n",
    "    top_k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Universal re-ranking function that routes to appropriate reranker.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        retrieved_docs: List from retrieval\n",
    "        reranker_model: Model name/identifier\n",
    "        reranker_instance: Loaded model instance or 'api' for Voyage\n",
    "        top_k: Number of documents to return after re-ranking\n",
    "        \n",
    "    Returns:\n",
    "        Re-ranked document list\n",
    "    \"\"\"\n",
    "    if reranker_model == 'voyage-rerank-2.5':\n",
    "        # Use Voyage API\n",
    "        return rerank_with_voyage(query, retrieved_docs, reranker_model, top_k)\n",
    "    \n",
    "    elif isinstance(reranker_instance, CrossEncoder):\n",
    "        # Use Hugging Face cross-encoder\n",
    "        return rerank_with_cross_encoder(query, retrieved_docs, reranker_instance, top_k)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown reranker type for model: {reranker_model}\")\n",
    "\n",
    "print(\"✓ rerank_documents() defined\")\n",
    "\n",
    "# %%\n",
    "# Test re-ranking with sample data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing re-ranking functions...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create sample retrieved documents for testing\n",
    "sample_retrieved_docs = [\n",
    "    {\n",
    "        'doc_name': 'test_doc.pdf',\n",
    "        'page_number': 1,\n",
    "        'content': 'This is a highly relevant document about financial reporting.',\n",
    "        'rank': 1,\n",
    "        'score': 0.95\n",
    "    },\n",
    "    {\n",
    "        'doc_name': 'test_doc.pdf',\n",
    "        'page_number': 2,\n",
    "        'content': 'This document discusses unrelated topics.',\n",
    "        'rank': 2,\n",
    "        'score': 0.85\n",
    "    },\n",
    "    {\n",
    "        'doc_name': 'test_doc.pdf',\n",
    "        'page_number': 3,\n",
    "        'content': 'Annual financial statements and revenue details.',\n",
    "        'rank': 3,\n",
    "        'score': 0.80\n",
    "    }\n",
    "]\n",
    "\n",
    "sample_query = \"What was the company's revenue in 2023?\"\n",
    "\n",
    "print(f\"\\nSample query: {sample_query}\")\n",
    "print(f\"Sample documents: {len(sample_retrieved_docs)}\")\n",
    "\n",
    "# Test with cross-encoder (if loaded)\n",
    "try:\n",
    "    test_reranker_name = 'cross-encoder/ms-marco-MiniLM-L-12-v2'\n",
    "    if test_reranker_name in reranker_instances:\n",
    "        print(f\"\\nTesting with {test_reranker_name}...\")\n",
    "        test_reranker = reranker_instances[test_reranker_name]\n",
    "        \n",
    "        reranked = rerank_with_cross_encoder(\n",
    "            sample_query,\n",
    "            sample_retrieved_docs,\n",
    "            test_reranker,\n",
    "            top_k=3\n",
    "        )\n",
    "        \n",
    "        print(f\"  ✓ Re-ranking successful!\")\n",
    "        print(f\"\\n  Initial order:\")\n",
    "        for doc in sample_retrieved_docs:\n",
    "            print(f\"    Rank {doc['rank']}: Page {doc['page_number']} (score: {doc['score']:.3f})\")\n",
    "        \n",
    "        print(f\"\\n  Re-ranked order:\")\n",
    "        for doc in reranked:\n",
    "            print(f\"    Rank {doc['rank']}: Page {doc['page_number']} (initial rank: {doc['initial_rank']}, rerank score: {doc['rerank_score']:.3f})\")\n",
    "    else:\n",
    "        print(f\"\\n  Skipping test - {test_reranker_name} not loaded\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n  ✗ Re-ranking test failed: {e}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 6 COMPLETE: Re-ranking Functions Implemented\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nImplemented functions:\")\n",
    "print(\"  ✓ rerank_with_voyage() - Voyage AI API re-ranking\")\n",
    "print(\"  ✓ rerank_with_cross_encoder() - Hugging Face cross-encoder re-ranking\")\n",
    "print(\"  ✓ rerank_documents() - Universal re-ranking router\")\n",
    "print(\"\\nNext step: Implement evaluation metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "256ab3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ compute_cosine_similarity() defined\n",
      "✓ calculate_text_similarities_for_chunk() defined\n",
      "✓ calculate_text_metrics_for_query() defined\n",
      "✓ evaluate_single_query() defined\n",
      "\n",
      "================================================================================\n",
      "Testing metrics calculation...\n",
      "================================================================================\n",
      "\n",
      "Test metrics:\n",
      "  MRR: 1.0000\n",
      "  Recall: 1.0000\n",
      "  Precision: 0.5000\n",
      "  F1: 0.6667\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 7 COMPLETE: Evaluation Metrics Implemented (Matching Baseline)\n",
      "================================================================================\n",
      "\n",
      "Implemented functions:\n",
      "  ✓ compute_cosine_similarity() - Cosine similarity computation\n",
      "  ✓ calculate_text_similarities_for_chunk() - Calculate similarities with all evidence\n",
      "  ✓ calculate_text_metrics_for_query() - Calculate MRR, Recall, Precision, F1\n",
      "  ✓ evaluate_single_query() - Evaluate with three metric sets\n",
      "\n",
      "Using Sentence-BERT model: all-MiniLM-L6-v2\n",
      "Similarity method: Cosine similarity (same as baseline)\n",
      "\n",
      "Next step: Implement main evaluation pipeline\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 7. Evaluation Metrics (Matching Baseline)\n",
    "\n",
    "# %%\n",
    "def compute_cosine_similarity(embedding1: np.ndarray, embedding2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embedding1: First embedding vector\n",
    "        embedding2: Second embedding vector\n",
    "        \n",
    "    Returns:\n",
    "        Cosine similarity score (between -1 and 1)\n",
    "    \"\"\"\n",
    "    # Normalize vectors\n",
    "    norm1 = np.linalg.norm(embedding1)\n",
    "    norm2 = np.linalg.norm(embedding2)\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity = np.dot(embedding1, embedding2) / (norm1 * norm2)\n",
    "    \n",
    "    return float(similarity)\n",
    "\n",
    "print(\"✓ compute_cosine_similarity() defined\")\n",
    "\n",
    "# %%\n",
    "def calculate_text_similarities_for_chunk(\n",
    "    chunk_text: str,\n",
    "    evidence_items: List[Dict],\n",
    "    sbert_model: SentenceTransformer\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between a chunk and all evidence items.\n",
    "    \n",
    "    Args:\n",
    "        chunk_text: Text content of retrieved chunk\n",
    "        evidence_items: List of evidence items (each has 'embedding', 'doc_name', 'page_number')\n",
    "        sbert_model: Sentence-BERT model for encoding chunk\n",
    "        \n",
    "    Returns:\n",
    "        List of similarity results with cosine similarity scores\n",
    "    \"\"\"\n",
    "    # Encode chunk text\n",
    "    chunk_embedding = sbert_model.encode(chunk_text, convert_to_numpy=True)\n",
    "    \n",
    "    # Calculate similarity with each evidence\n",
    "    similarities = []\n",
    "    \n",
    "    for evidence_idx, evidence in enumerate(evidence_items):\n",
    "        similarity_score = compute_cosine_similarity(\n",
    "            chunk_embedding,\n",
    "            evidence['embedding']\n",
    "        )\n",
    "        \n",
    "        similarities.append({\n",
    "            'evidence_index': evidence_idx,\n",
    "            'evidence_doc': evidence['doc_name'],\n",
    "            'evidence_page': evidence['page_number'],\n",
    "            'cosine_similarity': similarity_score\n",
    "        })\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "print(\"✓ calculate_text_similarities_for_chunk() defined\")\n",
    "\n",
    "# %%\n",
    "def calculate_text_metrics_for_query(\n",
    "    retrieved_docs: List[Dict],\n",
    "    evidence_items: List[Dict],\n",
    "    sbert_model: SentenceTransformer,\n",
    "    threshold: float = 0.7\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate TEXT-BASED metrics for a single query (matching baseline approach).\n",
    "    \n",
    "    For each retrieved chunk:\n",
    "    1. Encode chunk text\n",
    "    2. Calculate similarity with all evidence\n",
    "    3. Determine if chunk matches (max_similarity >= threshold)\n",
    "    \n",
    "    Metrics:\n",
    "    - Text MRR: Rank of first chunk where max(similarities) >= threshold\n",
    "    - Text Recall: # evidence matched / # total evidence\n",
    "    - Text Precision: # chunks matching / # total chunks\n",
    "    - Text F1: Harmonic mean of precision and recall\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs: List of retrieved docs with 'content'\n",
    "        evidence_items: List of evidence items with 'embedding'\n",
    "        sbert_model: Sentence-BERT model for encoding chunks\n",
    "        threshold: Similarity threshold for matching (default: 0.7)\n",
    "        \n",
    "    Returns:\n",
    "        Dict with 'text_mrr', 'text_recall', 'text_precision', 'text_f1'\n",
    "    \"\"\"\n",
    "    if len(evidence_items) == 0 or len(retrieved_docs) == 0:\n",
    "        return {\n",
    "            'text_mrr': 0.0,\n",
    "            'text_recall': 0.0,\n",
    "            'text_precision': 0.0,\n",
    "            'text_f1': 0.0\n",
    "        }\n",
    "    \n",
    "    # Track results\n",
    "    text_mrr = 0.0\n",
    "    evidence_found = set()  # Set of evidence indices matched\n",
    "    chunks_matching = 0\n",
    "    \n",
    "    # Process each retrieved chunk\n",
    "    for rank, retrieved_doc in enumerate(retrieved_docs, start=1):\n",
    "        chunk_text = retrieved_doc.get('content', '')\n",
    "        \n",
    "        if not chunk_text:\n",
    "            continue\n",
    "        \n",
    "        # Calculate similarities with all evidence\n",
    "        similarities = calculate_text_similarities_for_chunk(\n",
    "            chunk_text,\n",
    "            evidence_items,\n",
    "            sbert_model\n",
    "        )\n",
    "        \n",
    "        # Find maximum similarity\n",
    "        max_similarity = max([s['cosine_similarity'] for s in similarities])\n",
    "        \n",
    "        # Check if this chunk matches (above threshold)\n",
    "        chunk_matches_any_evidence = (max_similarity >= threshold)\n",
    "        \n",
    "        if chunk_matches_any_evidence:\n",
    "            chunks_matching += 1\n",
    "            \n",
    "            # Record which evidence items this chunk matched\n",
    "            for i, sim in enumerate(similarities):\n",
    "                if sim['cosine_similarity'] >= threshold:\n",
    "                    evidence_found.add(i)\n",
    "            \n",
    "            # Check for MRR (first match)\n",
    "            if text_mrr == 0.0:  # First match found\n",
    "                text_mrr = 1.0 / rank\n",
    "    \n",
    "    # Calculate recall and precision\n",
    "    text_recall = len(evidence_found) / len(evidence_items)\n",
    "    text_precision = chunks_matching / len(retrieved_docs)\n",
    "    \n",
    "    # Calculate F1\n",
    "    if text_precision + text_recall > 0:\n",
    "        text_f1 = 2 * (text_precision * text_recall) / (text_precision + text_recall)\n",
    "    else:\n",
    "        text_f1 = 0.0\n",
    "    \n",
    "    return {\n",
    "        'text_mrr': text_mrr,\n",
    "        'text_recall': text_recall,\n",
    "        'text_precision': text_precision,\n",
    "        'text_f1': text_f1\n",
    "    }\n",
    "\n",
    "print(\"✓ calculate_text_metrics_for_query() defined\")\n",
    "\n",
    "# %%\n",
    "def evaluate_single_query(\n",
    "    query_item: Dict,\n",
    "    retrieved_docs_k_retrieve: List[Dict],\n",
    "    retrieved_docs_k_rerank: List[Dict],\n",
    "    reranked_docs: List[Dict],\n",
    "    all_evidence: List[Dict],\n",
    "    sbert_model: SentenceTransformer,\n",
    "    threshold: float\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a single query with three sets of metrics.\n",
    "    \n",
    "    Args:\n",
    "        query_item: Query from dataset with evidence\n",
    "        retrieved_docs_k_retrieve: All k_retrieve documents\n",
    "        retrieved_docs_k_rerank: Top k_rerank documents (before reranking)\n",
    "        reranked_docs: Top k_rerank documents (after reranking)\n",
    "        all_evidence: All evidence items with embeddings\n",
    "        sbert_model: Sentence-BERT model\n",
    "        threshold: Text similarity threshold\n",
    "        \n",
    "    Returns:\n",
    "        Dict with all three metric sets\n",
    "    \"\"\"\n",
    "    # Get evidence for this query\n",
    "    doc_name = query_item['doc_name']\n",
    "    query_evidence = [\n",
    "        ev for ev in all_evidence \n",
    "        if ev['doc_name'] == doc_name and \n",
    "        ev['page_number'] in [e['evidence_page_num'] for e in query_item['evidence']]\n",
    "    ]\n",
    "    \n",
    "    if len(query_evidence) == 0:\n",
    "        # Fallback: use evidence from query_item\n",
    "        query_evidence = []\n",
    "        for ev in query_item['evidence']:\n",
    "            # Find matching evidence in all_evidence\n",
    "            matching = [\n",
    "                e for e in all_evidence \n",
    "                if e['doc_name'] == doc_name and \n",
    "                e['page_number'] == ev['evidence_page_num']\n",
    "            ]\n",
    "            if matching:\n",
    "                query_evidence.extend(matching)\n",
    "    \n",
    "    # Calculate metrics for k_retrieve\n",
    "    metrics_k_retrieve = calculate_text_metrics_for_query(\n",
    "        retrieved_docs_k_retrieve,\n",
    "        query_evidence,\n",
    "        sbert_model,\n",
    "        threshold\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics for k_rerank (initial)\n",
    "    metrics_k_rerank = calculate_text_metrics_for_query(\n",
    "        retrieved_docs_k_rerank,\n",
    "        query_evidence,\n",
    "        sbert_model,\n",
    "        threshold\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics for reranked\n",
    "    metrics_reranked = calculate_text_metrics_for_query(\n",
    "        reranked_docs,\n",
    "        query_evidence,\n",
    "        sbert_model,\n",
    "        threshold\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'initial_metrics_k_retrieve': metrics_k_retrieve,\n",
    "        'initial_metrics_k_rerank': metrics_k_rerank,\n",
    "        'reranked_metrics': metrics_reranked\n",
    "    }\n",
    "\n",
    "print(\"✓ evaluate_single_query() defined\")\n",
    "\n",
    "# %%\n",
    "# Test metrics calculation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Testing metrics calculation...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create test data\n",
    "test_retrieved = [\n",
    "    {\n",
    "        'doc_name': 'test_doc.pdf',\n",
    "        'page_number': 1,\n",
    "        'content': 'The company reported revenue of $50 million in 2023',\n",
    "        'rank': 1,\n",
    "        'score': 0.95\n",
    "    },\n",
    "    {\n",
    "        'doc_name': 'test_doc.pdf',\n",
    "        'page_number': 2,\n",
    "        'content': 'Unrelated information about products',\n",
    "        'rank': 2,\n",
    "        'score': 0.85\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create test evidence with embeddings\n",
    "test_evidence = [\n",
    "    {\n",
    "        'doc_name': 'test_doc.pdf',\n",
    "        'page_number': 1,\n",
    "        'text': 'The company reported revenue of $50 million in 2023',\n",
    "        'embedding': sbert_model.encode('The company reported revenue of $50 million in 2023', convert_to_numpy=True)\n",
    "    }\n",
    "]\n",
    "\n",
    "metrics = calculate_text_metrics_for_query(\n",
    "    test_retrieved,\n",
    "    test_evidence,\n",
    "    sbert_model,\n",
    "    threshold=0.7\n",
    ")\n",
    "\n",
    "print(\"\\nTest metrics:\")\n",
    "print(f\"  MRR: {metrics['text_mrr']:.4f}\")\n",
    "print(f\"  Recall: {metrics['text_recall']:.4f}\")\n",
    "print(f\"  Precision: {metrics['text_precision']:.4f}\")\n",
    "print(f\"  F1: {metrics['text_f1']:.4f}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 7 COMPLETE: Evaluation Metrics Implemented (Matching Baseline)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nImplemented functions:\")\n",
    "print(\"  ✓ compute_cosine_similarity() - Cosine similarity computation\")\n",
    "print(\"  ✓ calculate_text_similarities_for_chunk() - Calculate similarities with all evidence\")\n",
    "print(\"  ✓ calculate_text_metrics_for_query() - Calculate MRR, Recall, Precision, F1\")\n",
    "print(\"  ✓ evaluate_single_query() - Evaluate with three metric sets\")\n",
    "print(\"\\nUsing Sentence-BERT model: all-MiniLM-L6-v2\")\n",
    "print(\"Similarity method: Cosine similarity (same as baseline)\")\n",
    "print(\"\\nNext step: Implement main evaluation pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9c9899bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ evaluate_configuration() defined\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 8 COMPLETE: Main Evaluation Pipeline Implemented\n",
      "================================================================================\n",
      "\n",
      "Implemented functions:\n",
      "  ✓ evaluate_configuration() - Complete evaluation pipeline\n",
      "\n",
      "Next step: Batch evaluation execution\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 8. Main Evaluation Pipeline\n",
    "\n",
    "# %%\n",
    "def evaluate_configuration(\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k_retrieve: int,\n",
    "    k_rerank: int,\n",
    "    reranker_model: str,\n",
    "    mode: str,\n",
    "    dataset,\n",
    "    all_evidence: List[Dict],\n",
    "    sbert_model: SentenceTransformer,\n",
    "    reranker_instance,\n",
    "    vector_db_base_dir: str,\n",
    "    collection_prefix: str,\n",
    "    output_dir: str,\n",
    "    threshold: float,\n",
    "    use_page_tolerance: bool\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a single configuration (provider, model, chunk_size, k_retrieve, k_rerank, reranker, mode).\n",
    "    \n",
    "    Args:\n",
    "        provider: Embedding provider\n",
    "        model: Embedding model\n",
    "        chunk_size: Chunk size\n",
    "        k_retrieve: Number of documents to retrieve\n",
    "        k_rerank: Number of documents to keep after reranking\n",
    "        reranker_model: Reranker model name\n",
    "        mode: 'global' or 'single'\n",
    "        dataset: FinanceBench dataset\n",
    "        all_evidence: All evidence items with embeddings\n",
    "        sbert_model: Sentence-BERT model for text similarity\n",
    "        reranker_instance: Loaded reranker model instance\n",
    "        vector_db_base_dir: Base directory for vector databases\n",
    "        collection_prefix: Collection name prefix\n",
    "        output_dir: Output directory for results\n",
    "        threshold: Text similarity threshold\n",
    "        use_page_tolerance: Whether to use page tolerance (not used in text-based metrics)\n",
    "        \n",
    "    Returns:\n",
    "        Summary statistics dict\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating: {provider}/{model} | chunk={chunk_size} | k_retrieve={k_retrieve} | k_rerank={k_rerank} | {mode} | reranker={simplify_reranker_name(reranker_model)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Check if results already exist\n",
    "    if check_if_results_exist(provider, model, chunk_size, k_retrieve, k_rerank, mode, reranker_model, output_dir):\n",
    "        print(\"⚠️  Results already exist. Skipping...\")\n",
    "        return {'status': 'skipped'}\n",
    "    \n",
    "    # Load vector store\n",
    "    try:\n",
    "        print(\"\\n1. Loading vector store...\")\n",
    "        vectorstore = load_vector_store(provider, model, chunk_size, vector_db_base_dir, collection_prefix)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load vector store: {e}\")\n",
    "        return {'status': 'failed', 'error': str(e)}\n",
    "    \n",
    "    # Initialize results storage\n",
    "    query_results = []\n",
    "    \n",
    "    # Accumulators for averaging\n",
    "    sum_metrics_k_retrieve = {'text_mrr': 0, 'text_recall': 0, 'text_precision': 0, 'text_f1': 0}\n",
    "    sum_metrics_k_rerank = {'text_mrr': 0, 'text_recall': 0, 'text_precision': 0, 'text_f1': 0}\n",
    "    sum_metrics_reranked = {'text_mrr': 0, 'text_recall': 0, 'text_precision': 0, 'text_f1': 0}\n",
    "    \n",
    "    # Process each query\n",
    "    print(f\"\\n2. Processing {len(dataset)} queries...\")\n",
    "    for idx, query_item in enumerate(tqdm(dataset, desc=\"Queries\")):\n",
    "        question = query_item['question']\n",
    "        doc_name = query_item['doc_name']\n",
    "        \n",
    "        # Add small delay to avoid rate limits (only for API-based rerankers)\n",
    "        if reranker_model == 'voyage-rerank-2.5':\n",
    "            time.sleep(0.5)  # 500ms delay between queries\n",
    "        \n",
    "        # Step 1: Retrieve top k_retrieve documents\n",
    "        try:\n",
    "            retrieved_k_retrieve = retrieve_documents(\n",
    "                vectorstore,\n",
    "                question,\n",
    "                k=k_retrieve,\n",
    "                mode=mode,\n",
    "                doc_name=doc_name if mode == 'single' else None\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️  Query {idx} retrieval failed: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Step 2: Get initial top k_rerank (before reranking)\n",
    "        retrieved_k_rerank = retrieved_k_retrieve[:k_rerank]\n",
    "        \n",
    "        # Step 3: Rerank ALL k_retrieve documents (not just top k_rerank)\n",
    "        try:\n",
    "            reranked_all = rerank_documents(\n",
    "                question,\n",
    "                retrieved_k_retrieve,  # Pass all 100 documents\n",
    "                reranker_model,\n",
    "                reranker_instance,\n",
    "                k_retrieve  # Rerank all documents\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️  Query {idx} reranking failed: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Step 4: Get top k_rerank from reranked results\n",
    "        reranked_top_k = reranked_all[:k_rerank]\n",
    "        \n",
    "        # Step 4: Get top k_rerank from reranked results\n",
    "        reranked_top_k = reranked_all[:k_rerank]\n",
    "        \n",
    "        # Step 5: Evaluate with three metric sets\n",
    "        metrics = evaluate_single_query(\n",
    "            query_item,\n",
    "            retrieved_k_retrieve,\n",
    "            retrieved_k_rerank,\n",
    "            reranked_top_k,  # Top k_rerank from reranked ALL documents\n",
    "            all_evidence,\n",
    "            sbert_model,\n",
    "            threshold\n",
    "        )\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        for key in sum_metrics_k_retrieve.keys():\n",
    "            sum_metrics_k_retrieve[key] += metrics['initial_metrics_k_retrieve'][key]\n",
    "            sum_metrics_k_rerank[key] += metrics['initial_metrics_k_rerank'][key]\n",
    "            sum_metrics_reranked[key] += metrics['reranked_metrics'][key]\n",
    "        \n",
    "        # Prepare documents for storage (with truncated text)\n",
    "        def prepare_doc_for_storage(doc):\n",
    "            return {\n",
    "                'doc_name': doc['doc_name'],\n",
    "                'page_number': doc['page_number'],\n",
    "                'chunk_text': truncate_chunk_text(doc['content'], CHUNK_TEXT_PREFIX_CHARS, CHUNK_TEXT_SUFFIX_CHARS),\n",
    "                'rank': doc['rank'],\n",
    "                'initial_rank': doc.get('initial_rank', doc['rank']),\n",
    "                'initial_score': round(doc.get('initial_score', doc.get('score', 0.0)), 4),\n",
    "                'rerank_score': round(doc.get('rerank_score'), 4) if doc.get('rerank_score') is not None else None\n",
    "            }\n",
    "        \n",
    "        # Store query result (only initial top k_rerank and reranked top k_rerank)\n",
    "        query_result = {\n",
    "            'question_id': idx,\n",
    "            'question': question,\n",
    "            'doc_name': doc_name,\n",
    "            'evidence_pages': [ev['evidence_page_num'] for ev in query_item['evidence']],\n",
    "            'retrieved_docs_k_rerank': [prepare_doc_for_storage(doc) for doc in retrieved_k_rerank],\n",
    "            'reranked_docs': [prepare_doc_for_storage(doc) for doc in reranked_top_k],\n",
    "            'initial_metrics_k_retrieve': {k: round(v, 4) for k, v in metrics['initial_metrics_k_retrieve'].items()},\n",
    "            'initial_metrics_k_rerank': {k: round(v, 4) for k, v in metrics['initial_metrics_k_rerank'].items()},\n",
    "            'reranked_metrics': {k: round(v, 4) for k, v in metrics['reranked_metrics'].items()}\n",
    "        }\n",
    "        query_results.append(query_result)\n",
    "    \n",
    "    # Calculate averages\n",
    "    num_queries = len(query_results)\n",
    "    if num_queries == 0:\n",
    "        print(\"\\n❌ No queries were successfully processed\")\n",
    "        return {'status': 'failed', 'error': 'No queries processed'}\n",
    "    \n",
    "    avg_metrics_k_retrieve = {k: round(v / num_queries, 4) for k, v in sum_metrics_k_retrieve.items()}\n",
    "    avg_metrics_k_rerank = {k: round(v / num_queries, 4) for k, v in sum_metrics_k_rerank.items()}\n",
    "    avg_metrics_reranked = {k: round(v / num_queries, 4) for k, v in sum_metrics_reranked.items()}\n",
    "    \n",
    "    # Calculate improvements\n",
    "    improvement = {\n",
    "        'mrr_delta': round(avg_metrics_reranked['text_mrr'] - avg_metrics_k_rerank['text_mrr'], 4),\n",
    "        'recall_delta': round(avg_metrics_reranked['text_recall'] - avg_metrics_k_rerank['text_recall'], 4),\n",
    "        'precision_delta': round(avg_metrics_reranked['text_precision'] - avg_metrics_k_rerank['text_precision'], 4),\n",
    "        'f1_delta': round(avg_metrics_reranked['text_f1'] - avg_metrics_k_rerank['text_f1'], 4)\n",
    "    }\n",
    "    \n",
    "    # Prepare final results\n",
    "    results = {\n",
    "        'query_results': query_results,\n",
    "        'summary': {\n",
    "            'configuration': {\n",
    "                'provider': provider,\n",
    "                'model': model,\n",
    "                'chunk_size': chunk_size,\n",
    "                'k_retrieve': k_retrieve,\n",
    "                'k_rerank': k_rerank,\n",
    "                'reranker_model': reranker_model,\n",
    "                'mode': mode,\n",
    "                'text_similarity_threshold': threshold,\n",
    "                'use_page_tolerance': use_page_tolerance\n",
    "            },\n",
    "            'total_queries': num_queries,\n",
    "            'average_initial_metrics_k_retrieve': avg_metrics_k_retrieve,\n",
    "            'average_initial_metrics_k_rerank': avg_metrics_k_rerank,\n",
    "            'average_reranked_metrics': avg_metrics_reranked,\n",
    "            'improvement_from_k_rerank_to_reranked': improvement\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save results\n",
    "    print(f\"\\n3. Saving results...\")\n",
    "    save_path = save_results(\n",
    "        results,\n",
    "        provider,\n",
    "        model,\n",
    "        chunk_size,\n",
    "        k_retrieve,\n",
    "        k_rerank,\n",
    "        mode,\n",
    "        reranker_model,\n",
    "        output_dir\n",
    "    )\n",
    "    print(f\"✓ Results saved to: {save_path}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nMetrics at k_retrieve={k_retrieve}:\")\n",
    "    print(f\"  MRR:       {avg_metrics_k_retrieve['text_mrr']:.4f}\")\n",
    "    print(f\"  Recall:    {avg_metrics_k_retrieve['text_recall']:.4f}\")\n",
    "    print(f\"  Precision: {avg_metrics_k_retrieve['text_precision']:.4f}\")\n",
    "    print(f\"  F1:        {avg_metrics_k_retrieve['text_f1']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMetrics at k_rerank={k_rerank} (before reranking):\")\n",
    "    print(f\"  MRR:       {avg_metrics_k_rerank['text_mrr']:.4f}\")\n",
    "    print(f\"  Recall:    {avg_metrics_k_rerank['text_recall']:.4f}\")\n",
    "    print(f\"  Precision: {avg_metrics_k_rerank['text_precision']:.4f}\")\n",
    "    print(f\"  F1:        {avg_metrics_k_rerank['text_f1']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nMetrics at k_rerank={k_rerank} (after reranking):\")\n",
    "    print(f\"  MRR:       {avg_metrics_reranked['text_mrr']:.4f}\")\n",
    "    print(f\"  Recall:    {avg_metrics_reranked['text_recall']:.4f}\")\n",
    "    print(f\"  Precision: {avg_metrics_reranked['text_precision']:.4f}\")\n",
    "    print(f\"  F1:        {avg_metrics_reranked['text_f1']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nImprovement (reranked vs initial k_rerank):\")\n",
    "    print(f\"  MRR:       {improvement['mrr_delta']:+.4f}\")\n",
    "    print(f\"  Recall:    {improvement['recall_delta']:+.4f}\")\n",
    "    print(f\"  Precision: {improvement['precision_delta']:+.4f}\")\n",
    "    print(f\"  F1:        {improvement['f1_delta']:+.4f}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return {\n",
    "        'status': 'completed',\n",
    "        'summary': results['summary']\n",
    "    }\n",
    "\n",
    "print(\"✓ evaluate_configuration() defined\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 8 COMPLETE: Main Evaluation Pipeline Implemented\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nImplemented functions:\")\n",
    "print(\"  ✓ evaluate_configuration() - Complete evaluation pipeline\")\n",
    "print(\"\\nNext step: Batch evaluation execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "db30bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Re-ranking Configurations\n",
    "configurations = [\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-3-large',\n",
    "        'chunk_sizes': [1024],\n",
    "        'k_retrieve': 80,       # Retrieve 100 documents from vector store\n",
    "        'k_rerank': 20,          # Keep top 20 after re-ranking\n",
    "        'reranker_models': [\n",
    "            # 'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "            #'BAAI/bge-reranker-large',\n",
    "            'voyage-rerank-2.5'\n",
    "        ]\n",
    "    }\n",
    "    # {\n",
    "    #     'provider': 'ollama',\n",
    "    #     'model': 'ollama-bge-m3',\n",
    "    #     'chunk_sizes': [1024],\n",
    "    #     'k_retrieve': 100,       # Retrieve 100 documents from vector store\n",
    "    #     'k_rerank': 20,          # Keep top 20 after re-ranking\n",
    "    #     'reranker_models': [\n",
    "    #         # 'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "    #         # 'BAAI/bge-reranker-large',\n",
    "    #         'voyage-rerank-2.5'\n",
    "    #     ]\n",
    "    # }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "87d7ca2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ run_batch_evaluation() defined\n",
      "\n",
      "================================================================================\n",
      "EVALUATION PLAN\n",
      "================================================================================\n",
      "\n",
      "Dataset: FinanceBench (150 queries)\n",
      "\n",
      "Evaluation Settings:\n",
      "  Modes: ['global', 'single']\n",
      "  Text similarity threshold: 0.7\n",
      "  Page tolerance: ENABLED\n",
      "\n",
      "Configurations to evaluate:\n",
      "\n",
      "  1. voyage/voyage-3-large\n",
      "     Chunk sizes: [1024]\n",
      "     k_retrieve: 80, k_rerank: 20\n",
      "     Reranker models: 1\n",
      "       - voyage-rerank-2.5\n",
      "     Evaluation runs: 2\n",
      "     Output files (sample):\n",
      "       - voyage_voyage-3-large_chunk1024_k80_global_rerank_k20-voyage-rerank-2.5.json [TO CREATE]\n",
      "       - voyage_voyage-3-large_chunk1024_k80_single_rerank_k20-voyage-rerank-2.5.json [TO CREATE]\n",
      "\n",
      "================================================================================\n",
      "Total evaluation runs: 2\n",
      "Output directory: ../../evaluation_results/reranking_results\n",
      "================================================================================\n",
      "\n",
      "################################################################################\n",
      "STARTING BATCH EVALUATION\n",
      "################################################################################\n",
      "\n",
      "Total evaluation runs: 2\n",
      "Output directory: ../../evaluation_results/reranking_results\n",
      "\n",
      "################################################################################\n",
      "Configuration 1/1: voyage/voyage-3-large\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "Evaluating: voyage/voyage-3-large | chunk=1024 | k_retrieve=80 | k_rerank=20 | global | reranker=voyage-rerank-2.5\n",
      "================================================================================\n",
      "\n",
      "1. Loading vector store...\n",
      "  ✓ Loaded collection 'financebench_docs_chunk_1024' from ../../vector_databases/voyage_voyage-3-large\n",
      "    Documents: 15765\n",
      "\n",
      "2. Processing 150 queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ef6aaa1d184e9bae38ecb9990352d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Queries:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Saving results...\n",
      "✓ Results saved to: ../../evaluation_results/reranking_results/voyage_voyage-3-large_chunk1024_k80_global_rerank_k20-voyage-rerank-2.5.json\n",
      "\n",
      "================================================================================\n",
      "RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Metrics at k_retrieve=80:\n",
      "  MRR:       0.5422\n",
      "  Recall:    0.8731\n",
      "  Precision: 0.0946\n",
      "  F1:        0.1633\n",
      "\n",
      "Metrics at k_rerank=20 (before reranking):\n",
      "  MRR:       0.5409\n",
      "  Recall:    0.8120\n",
      "  Precision: 0.1763\n",
      "  F1:        0.2723\n",
      "\n",
      "Metrics at k_rerank=20 (after reranking):\n",
      "  MRR:       0.6440\n",
      "  Recall:    0.8309\n",
      "  Precision: 0.2117\n",
      "  F1:        0.3188\n",
      "\n",
      "Improvement (reranked vs initial k_rerank):\n",
      "  MRR:       +0.1031\n",
      "  Recall:    +0.0189\n",
      "  Precision: +0.0354\n",
      "  F1:        +0.0465\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Evaluating: voyage/voyage-3-large | chunk=1024 | k_retrieve=80 | k_rerank=20 | single | reranker=voyage-rerank-2.5\n",
      "================================================================================\n",
      "\n",
      "1. Loading vector store...\n",
      "  ✓ Loaded collection 'financebench_docs_chunk_1024' from ../../vector_databases/voyage_voyage-3-large\n",
      "    Documents: 15765\n",
      "\n",
      "2. Processing 150 queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d7d985fbf94acbbc977f73b454c899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Queries:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Saving results...\n",
      "✓ Results saved to: ../../evaluation_results/reranking_results/voyage_voyage-3-large_chunk1024_k80_single_rerank_k20-voyage-rerank-2.5.json\n",
      "\n",
      "================================================================================\n",
      "RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Metrics at k_retrieve=80:\n",
      "  MRR:       0.5689\n",
      "  Recall:    0.8376\n",
      "  Precision: 0.0906\n",
      "  F1:        0.1547\n",
      "\n",
      "Metrics at k_rerank=20 (before reranking):\n",
      "  MRR:       0.5682\n",
      "  Recall:    0.8176\n",
      "  Precision: 0.1495\n",
      "  F1:        0.2383\n",
      "\n",
      "Metrics at k_rerank=20 (after reranking):\n",
      "  MRR:       0.6914\n",
      "  Recall:    0.8309\n",
      "  Precision: 0.1802\n",
      "  F1:        0.2788\n",
      "\n",
      "Improvement (reranked vs initial k_rerank):\n",
      "  MRR:       +0.1232\n",
      "  Recall:    +0.0133\n",
      "  Precision: +0.0307\n",
      "  F1:        +0.0405\n",
      "================================================================================\n",
      "\n",
      "################################################################################\n",
      "BATCH EVALUATION COMPLETE\n",
      "################################################################################\n",
      "\n",
      "Total runs: 2\n",
      "  ✓ Completed: 2\n",
      "  ⊘ Skipped:   0\n",
      "  ✗ Failed:    0\n",
      "\n",
      "Results saved to: ../../evaluation_results/reranking_results\n",
      "\n",
      "================================================================================\n",
      "✓ STEP 9 COMPLETE: Batch Evaluation Ready\n",
      "================================================================================\n",
      "\n",
      "To run the evaluation:\n",
      "  1. Review the evaluation plan above\n",
      "  2. Uncomment the batch evaluation code in section 9.2\n",
      "  3. Run the cell to start evaluation\n",
      "  4. Monitor progress (may take hours depending on configurations)\n",
      "\n",
      "Next step: Results analysis and visualization\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 9. Batch Evaluation Execution\n",
    "\n",
    "# %%\n",
    "def run_batch_evaluation(\n",
    "    configurations: List[Dict],\n",
    "    modes: List[str],\n",
    "    dataset,\n",
    "    all_evidence: List[Dict],\n",
    "    sbert_model: SentenceTransformer,\n",
    "    reranker_instances: Dict,\n",
    "    vector_db_base_dir: str,\n",
    "    collection_prefix: str,\n",
    "    output_dir: str,\n",
    "    threshold: float,\n",
    "    use_page_tolerance: bool\n",
    "):\n",
    "    \"\"\"\n",
    "    Run evaluation for all configurations.\n",
    "    \n",
    "    Args:\n",
    "        configurations: List of configuration dicts\n",
    "        modes: List of modes ('global', 'single')\n",
    "        dataset: FinanceBench dataset\n",
    "        all_evidence: All evidence items with embeddings\n",
    "        sbert_model: Sentence-BERT model for text similarity\n",
    "        reranker_instances: Dict of loaded reranker models\n",
    "        vector_db_base_dir: Base directory for vector databases\n",
    "        collection_prefix: Collection name prefix\n",
    "        output_dir: Output directory\n",
    "        threshold: Text similarity threshold\n",
    "        use_page_tolerance: Whether to use page tolerance\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(\"STARTING BATCH EVALUATION\")\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    # Calculate total runs\n",
    "    total_runs = 0\n",
    "    for config in configurations:\n",
    "        chunk_sizes = config['chunk_sizes']\n",
    "        reranker_models = config['reranker_models']\n",
    "        total_runs += len(chunk_sizes) * len(reranker_models) * len(modes)\n",
    "    \n",
    "    print(f\"\\nTotal evaluation runs: {total_runs}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Track results\n",
    "    all_results = []\n",
    "    completed = 0\n",
    "    skipped = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # Iterate through all configurations\n",
    "    for config_idx, config in enumerate(configurations, 1):\n",
    "        provider = config['provider']\n",
    "        model = config['model']\n",
    "        chunk_sizes = config['chunk_sizes']\n",
    "        k_retrieve = config['k_retrieve']\n",
    "        k_rerank = config['k_rerank']\n",
    "        reranker_models = config['reranker_models']\n",
    "        \n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"Configuration {config_idx}/{len(configurations)}: {provider}/{model}\")\n",
    "        print(f\"{'#'*80}\")\n",
    "        \n",
    "        for chunk_size in chunk_sizes:\n",
    "            for reranker_model in reranker_models:\n",
    "                for mode in modes:\n",
    "                    # Get reranker instance\n",
    "                    reranker_instance = reranker_instances.get(reranker_model)\n",
    "                    \n",
    "                    if reranker_instance is None:\n",
    "                        print(f\"\\n⚠️  Reranker {reranker_model} not found. Skipping...\")\n",
    "                        failed += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Run evaluation\n",
    "                    try:\n",
    "                        result = evaluate_configuration(\n",
    "                            provider=provider,\n",
    "                            model=model,\n",
    "                            chunk_size=chunk_size,\n",
    "                            k_retrieve=k_retrieve,\n",
    "                            k_rerank=k_rerank,\n",
    "                            reranker_model=reranker_model,\n",
    "                            mode=mode,\n",
    "                            dataset=dataset,\n",
    "                            all_evidence=all_evidence,\n",
    "                            sbert_model=sbert_model,\n",
    "                            reranker_instance=reranker_instance,\n",
    "                            vector_db_base_dir=vector_db_base_dir,\n",
    "                            collection_prefix=collection_prefix,\n",
    "                            output_dir=output_dir,\n",
    "                            threshold=threshold,\n",
    "                            use_page_tolerance=use_page_tolerance\n",
    "                        )\n",
    "                        \n",
    "                        if result['status'] == 'completed':\n",
    "                            completed += 1\n",
    "                            all_results.append(result)\n",
    "                        elif result['status'] == 'skipped':\n",
    "                            skipped += 1\n",
    "                        else:\n",
    "                            failed += 1\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"\\n❌ Evaluation failed with exception: {e}\")\n",
    "                        import traceback\n",
    "                        traceback.print_exc()\n",
    "                        failed += 1\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(\"BATCH EVALUATION COMPLETE\")\n",
    "    print(\"#\"*80)\n",
    "    print(f\"\\nTotal runs: {total_runs}\")\n",
    "    print(f\"  ✓ Completed: {completed}\")\n",
    "    print(f\"  ⊘ Skipped:   {skipped}\")\n",
    "    print(f\"  ✗ Failed:    {failed}\")\n",
    "    print(f\"\\nResults saved to: {output_dir}\")\n",
    "    \n",
    "    return {\n",
    "        'total_runs': total_runs,\n",
    "        'completed': completed,\n",
    "        'skipped': skipped,\n",
    "        'failed': failed,\n",
    "        'results': all_results\n",
    "    }\n",
    "\n",
    "print(\"✓ run_batch_evaluation() defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9.1 Display Evaluation Plan\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION PLAN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDataset: FinanceBench ({len(dataset)} queries)\")\n",
    "\n",
    "print(f\"\\nEvaluation Settings:\")\n",
    "print(f\"  Modes: {modes}\")\n",
    "print(f\"  Text similarity threshold: {TEXT_SIMILARITY_THRESHOLD}\")\n",
    "print(f\"  Page tolerance: {'ENABLED' if USE_PAGE_TOLERANCE else 'DISABLED'}\")\n",
    "\n",
    "print(f\"\\nConfigurations to evaluate:\")\n",
    "total_runs = 0\n",
    "for i, config in enumerate(configurations, 1):\n",
    "    provider = config['provider']\n",
    "    model = config['model']\n",
    "    chunk_sizes = config['chunk_sizes']\n",
    "    k_retrieve = config['k_retrieve']\n",
    "    k_rerank = config['k_rerank']\n",
    "    reranker_models = config['reranker_models']\n",
    "    \n",
    "    runs_for_config = len(chunk_sizes) * len(reranker_models) * len(modes)\n",
    "    total_runs += runs_for_config\n",
    "    \n",
    "    print(f\"\\n  {i}. {provider}/{model}\")\n",
    "    print(f\"     Chunk sizes: {chunk_sizes}\")\n",
    "    print(f\"     k_retrieve: {k_retrieve}, k_rerank: {k_rerank}\")\n",
    "    print(f\"     Reranker models: {len(reranker_models)}\")\n",
    "    for reranker in reranker_models:\n",
    "        print(f\"       - {reranker}\")\n",
    "    print(f\"     Evaluation runs: {runs_for_config}\")\n",
    "    \n",
    "    # Show output filenames that will be generated\n",
    "    print(f\"     Output files (sample):\")\n",
    "    for chunk_size in chunk_sizes[:1]:  # Show only first chunk size\n",
    "        for reranker in reranker_models[:2]:  # Show only first 2 rerankers\n",
    "            for mode in modes:\n",
    "                filename = get_output_filename(provider, model, chunk_size, k_retrieve, k_rerank, mode, reranker)\n",
    "                exists = check_if_results_exist(provider, model, chunk_size, k_retrieve, k_rerank, mode, reranker, OUTPUT_DIR)\n",
    "                status = \"EXISTS\" if exists else \"TO CREATE\"\n",
    "                print(f\"       - {filename} [{status}]\")\n",
    "        if len(reranker_models) > 2:\n",
    "            print(f\"       ... ({len(reranker_models) - 2} more rerankers)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Total evaluation runs: {total_runs}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9.2 Execute Batch Evaluation\n",
    "# \n",
    "# **IMPORTANT**: This cell will run the full evaluation.\n",
    "# - Depending on configurations, this may take significant time (hours)\n",
    "# - Progress will be shown for each configuration\n",
    "# - Results are saved incrementally (existing results are skipped)\n",
    "# - You can interrupt and resume anytime\n",
    "\n",
    "# %%\n",
    "# Uncomment the lines below to start the evaluation\n",
    "\n",
    "batch_results = run_batch_evaluation(\n",
    "    configurations=configurations,\n",
    "    modes=modes,\n",
    "    dataset=dataset,\n",
    "    all_evidence=all_evidence,\n",
    "    sbert_model=sbert_model,\n",
    "    reranker_instances=reranker_instances,\n",
    "    vector_db_base_dir=VECTOR_DB_BASE_DIR,\n",
    "    collection_prefix=COLLECTION_PREFIX,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    threshold=TEXT_SIMILARITY_THRESHOLD,\n",
    "    use_page_tolerance=USE_PAGE_TOLERANCE\n",
    ")\n",
    "\n",
    "# print(\"\\n⚠️  Batch evaluation is commented out.\")\n",
    "# print(\"Uncomment the code above to start the evaluation.\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ STEP 9 COMPLETE: Batch Evaluation Ready\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTo run the evaluation:\")\n",
    "print(\"  1. Review the evaluation plan above\")\n",
    "print(\"  2. Uncomment the batch evaluation code in section 9.2\")\n",
    "print(\"  3. Run the cell to start evaluation\")\n",
    "print(\"  4. Monitor progress (may take hours depending on configurations)\")\n",
    "print(\"\\nNext step: Results analysis and visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
