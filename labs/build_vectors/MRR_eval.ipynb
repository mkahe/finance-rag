{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce3a7f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "✓ OpenAI API key loaded\n",
      "✓ Ollama URL: http://localhost:11434\n",
      "✓ Configuration set\n",
      "  Vector DB Directory: ../../vector_databases\n",
      "  Output Directory: ../../evaluation_results/mrr_embeddings\n",
      "Loading FinanceBench dataset...\n",
      "✓ Loaded 150 queries\n",
      "\n",
      "Sample query:\n",
      "  ID: financebench_id_03029\n",
      "  Company: 3M\n",
      "  Question: What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the quest...\n",
      "  Doc: 3M_2018_10K\n",
      "  Evidence items: 1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FinanceBench Evaluation: MRR Analysis\n",
    "# Comparing Embedding Models and Chunk Sizes\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # FinanceBench RAG Evaluation\n",
    "# \n",
    "# This notebook evaluates different embedding models and chunk sizes using\n",
    "# Mean Reciprocal Rank (MRR) on the FinanceBench dataset.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.1 Imports\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Progress\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Data\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Vector stores\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.2 Configuration\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    print(\"✓ OpenAI API key loaded\")\n",
    "else:\n",
    "    print(\"⚠ OpenAI API key not found (only needed if using OpenAI embeddings)\")\n",
    "\n",
    "print(f\"✓ Ollama URL: {OLLAMA_BASE_URL}\")\n",
    "\n",
    "# %%\n",
    "# Paths\n",
    "VECTOR_DB_BASE_DIR = \"../../vector_databases\"\n",
    "OUTPUT_DIR = \"../../evaluation_results/mrr_embeddings\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = \"PatronusAI/financebench\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "\n",
    "# Collection settings\n",
    "COLLECTION_PREFIX = \"financebench_docs_chunk_\"\n",
    "\n",
    "print(\"✓ Configuration set\")\n",
    "print(f\"  Vector DB Directory: {VECTOR_DB_BASE_DIR}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.3 Load Dataset\n",
    "\n",
    "# %%\n",
    "print(\"Loading FinanceBench dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=DATASET_SPLIT)\n",
    "print(f\"✓ Loaded {len(dataset)} queries\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample query:\")\n",
    "sample = dataset[0]\n",
    "print(f\"  ID: {sample['financebench_id']}\")\n",
    "print(f\"  Company: {sample['company']}\")\n",
    "print(f\"  Question: {sample['question'][:100]}...\")\n",
    "print(f\"  Doc: {sample['doc_name']}\")\n",
    "print(f\"  Evidence items: {len(sample['evidence'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3167cbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.1 Metadata Extraction\n",
    "\n",
    "# %%\n",
    "def extract_doc_name_from_path(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract document name from file path.\n",
    "    \n",
    "    Example: \"../../financebench/documents/GENERALMILLS_2019_10K.pdf\" \n",
    "             -> \"GENERALMILLS_2019_10K\"\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(file_path)\n",
    "    doc_name = filename.replace('.pdf', '')\n",
    "    return doc_name\n",
    "\n",
    "\n",
    "def extract_metadata_from_retrieved_doc(doc) -> Dict:\n",
    "    \"\"\"Extract relevant metadata from retrieved document.\"\"\"\n",
    "    file_path = doc.metadata.get('file_path', '')\n",
    "    doc_name = extract_doc_name_from_path(file_path)\n",
    "    page_num = doc.metadata.get('source', -1)\n",
    "    \n",
    "    # Ensure page_num is an integer\n",
    "    if isinstance(page_num, str):\n",
    "        page_num = int(page_num)\n",
    "    \n",
    "    return {\n",
    "        'doc_name': doc_name,\n",
    "        'page_number': page_num\n",
    "    }\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.2 Vector Store Loading\n",
    "\n",
    "# %%\n",
    "def get_embedding_function(provider: str, model: str):\n",
    "    \"\"\"Get embedding function.\"\"\"\n",
    "    if provider == \"ollama\":\n",
    "        return OllamaEmbeddings(model=model)\n",
    "    elif provider == \"openai\":\n",
    "        return OpenAIEmbeddings(model=model, openai_api_key=OPENAI_API_KEY)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "\n",
    "def load_vectorstore(\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    base_dir: str = VECTOR_DB_BASE_DIR,\n",
    "    collection_prefix: str = COLLECTION_PREFIX\n",
    ") -> Chroma:\n",
    "    \"\"\"Load a vector store.\"\"\"\n",
    "    model_id = f\"{provider}_{model.replace('/', '_')}\"\n",
    "    db_path = os.path.join(base_dir, model_id)\n",
    "    collection_name = f\"{collection_prefix}{chunk_size}\"\n",
    "    \n",
    "    emb_fn = get_embedding_function(provider, model)\n",
    "    \n",
    "    vectorstore = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=emb_fn,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.3 Evidence Matching\n",
    "\n",
    "# %%\n",
    "def check_match(retrieved_doc: Dict, evidence_list: List[Dict]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if retrieved document matches any evidence.\n",
    "    \n",
    "    Note: FinanceBench evidence page numbers are 0-indexed (page 0 = first page)\n",
    "          ChromaDB source metadata is 1-indexed (source 1 = first page)\n",
    "          So we add 1 to evidence page number for comparison.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_doc: {doc_name, page_number} where page_number is from ChromaDB source (1-indexed)\n",
    "        evidence_list: List of evidence dicts from FinanceBench (0-indexed page numbers)\n",
    "        \n",
    "    Returns:\n",
    "        True if match found\n",
    "    \"\"\"\n",
    "    retrieved_doc_name = retrieved_doc['doc_name']\n",
    "    retrieved_page = retrieved_doc['page_number']\n",
    "    \n",
    "    for evidence in evidence_list:\n",
    "        evidence_doc_name = evidence['doc_name']\n",
    "        evidence_page = evidence['evidence_page_num'] + 1  # Convert 0-indexed to 1-indexed\n",
    "        \n",
    "        if retrieved_doc_name == evidence_doc_name and retrieved_page == evidence_page:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.4 MRR Calculation (Modular)\n",
    "\n",
    "# %%\n",
    "def calculate_mrr_for_query(retrieved_docs: List[Dict], evidence_list: List[Dict]) -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    Calculate MRR for a single query.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs: List of {doc_name, page_number}\n",
    "        evidence_list: Ground truth evidence\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (mrr_score, rank)\n",
    "        - mrr_score: 1/rank if found, 0 if not found\n",
    "        - rank: Position of first match (1-indexed), -1 if not found\n",
    "    \"\"\"\n",
    "    for rank, retrieved_doc in enumerate(retrieved_docs, start=1):\n",
    "        if check_match(retrieved_doc, evidence_list):\n",
    "            mrr_score = 1.0 / rank\n",
    "            return mrr_score, rank\n",
    "    \n",
    "    # No match found\n",
    "    return 0.0, -1\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.5 Retrieval Functions\n",
    "\n",
    "# %%\n",
    "def retrieve_global(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve documents globally (search all documents).\n",
    "    \n",
    "    Returns:\n",
    "        List of {doc_name, page_number, rank}\n",
    "    \"\"\"\n",
    "    results = vectorstore.similarity_search(query, k=k)\n",
    "    retrieved = []\n",
    "    for rank, doc in enumerate(results, start=1):\n",
    "        metadata = extract_metadata_from_retrieved_doc(doc)\n",
    "        metadata['rank'] = rank\n",
    "        retrieved.append(metadata)\n",
    "    return retrieved\n",
    "\n",
    "\n",
    "def retrieve_single_doc(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    target_doc_name: str,\n",
    "    k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve documents filtered to single document.\n",
    "    \n",
    "    Since ChromaDB doesn't support $contains, we retrieve more documents\n",
    "    and filter them post-retrieval.\n",
    "    \n",
    "    Returns:\n",
    "        List of {doc_name, page_number, rank}\n",
    "    \"\"\"\n",
    "    # Retrieve more documents than needed (k * 10) to ensure we get enough from target doc\n",
    "    # Then filter to only the target document\n",
    "    \n",
    "    fetch_k = min(k * 10, 100)  # Fetch up to 10x k, max 100\n",
    "    \n",
    "    results = vectorstore.similarity_search(query, k=fetch_k)\n",
    "    \n",
    "    # Filter to only documents from target doc\n",
    "    filtered = []\n",
    "    for doc in results:\n",
    "        metadata = extract_metadata_from_retrieved_doc(doc)\n",
    "        if metadata['doc_name'] == target_doc_name:\n",
    "            filtered.append(metadata)\n",
    "            if len(filtered) >= k:\n",
    "                break\n",
    "    \n",
    "    # Add rank after filtering\n",
    "    for rank, doc_meta in enumerate(filtered[:k], start=1):\n",
    "        doc_meta['rank'] = rank\n",
    "    \n",
    "    # Return top k from target document\n",
    "    return filtered[:k]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.6 File Management\n",
    "\n",
    "# %%\n",
    "def get_output_filename(provider: str, model: str, chunk_size: int, k: int, mode: str) -> str:\n",
    "    \"\"\"Generate output filename.\"\"\"\n",
    "    model_clean = model.replace('/', '_')\n",
    "    filename = f\"{provider}_{model_clean}_chunk{chunk_size}_k{k}_{mode}.json\"\n",
    "    return filename\n",
    "\n",
    "\n",
    "def check_if_results_exist(provider: str, model: str, chunk_size: int, k: int, mode: str, output_dir: str) -> bool:\n",
    "    \"\"\"Check if results JSON already exists.\"\"\"\n",
    "    filename = get_output_filename(provider, model, chunk_size, k, mode)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    return os.path.exists(filepath)\n",
    "\n",
    "\n",
    "def save_results(results: List[Dict], provider: str, model: str, chunk_size: int, k: int, mode: str, output_dir: str):\n",
    "    \"\"\"Save results to JSON file.\"\"\"\n",
    "    filename = get_output_filename(provider, model, chunk_size, k, mode)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Saved: {filename}\")\n",
    "\n",
    "# %%\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e962333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Evaluation Loop\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.1 Single Evaluation Run\n",
    "\n",
    "# %%\n",
    "def evaluate_single_configuration(\n",
    "    dataset,\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k: int,\n",
    "    mode: str,\n",
    "    output_dir: str = OUTPUT_DIR\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a single configuration (provider, model, chunk_size, k, mode).\n",
    "    \n",
    "    Args:\n",
    "        dataset: FinanceBench dataset\n",
    "        provider: \"ollama\" or \"openai\"\n",
    "        model: Model name\n",
    "        chunk_size: Chunk size\n",
    "        k: Number of documents to retrieve\n",
    "        mode: \"global\" or \"singledoc\"\n",
    "        output_dir: Output directory\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATING: {provider}/{model}, chunk={chunk_size}, k={k}, mode={mode}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Check if already exists\n",
    "    if check_if_results_exist(provider, model, chunk_size, k, mode, output_dir):\n",
    "        print(\"✓ Results already exist - SKIPPING\")\n",
    "        return {'status': 'skipped'}\n",
    "    \n",
    "    # Load vectorstore\n",
    "    print(\"Loading vectorstore...\")\n",
    "    try:\n",
    "        vectorstore = load_vectorstore(provider, model, chunk_size)\n",
    "        doc_count = vectorstore._collection.count()\n",
    "        print(f\"✓ Loaded ({doc_count:,} documents)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load vectorstore: {e}\")\n",
    "        return {'status': 'failed', 'error': str(e)}\n",
    "    \n",
    "    # Process all queries\n",
    "    results = []\n",
    "    mrr_scores = []\n",
    "    \n",
    "    print(f\"\\nProcessing {len(dataset)} queries...\")\n",
    "    \n",
    "    for record in tqdm(dataset, desc=\"Queries\"):\n",
    "        query_id = record['financebench_id']\n",
    "        query = record['question']\n",
    "        evidence = record['evidence']\n",
    "        doc_name = record['doc_name']\n",
    "        \n",
    "        # Retrieve documents\n",
    "        try:\n",
    "            if mode == \"global\":\n",
    "                retrieved_docs = retrieve_global(vectorstore, query, k)\n",
    "            elif mode == \"singledoc\":\n",
    "                retrieved_docs = retrieve_single_doc(vectorstore, query, doc_name, k)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown mode: {mode}\")\n",
    "            \n",
    "            # Calculate MRR\n",
    "            mrr_score, rank = calculate_mrr_for_query(retrieved_docs, evidence)\n",
    "            mrr_scores.append(mrr_score)\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                'query_id': query_id,\n",
    "                'query': query,\n",
    "                'expected_doc': doc_name,\n",
    "                'expected_evidence': [\n",
    "                    {\n",
    "                        'doc_name': ev['doc_name'],\n",
    "                        'page_number': ev['evidence_page_num']\n",
    "                    }\n",
    "                    for ev in evidence\n",
    "                ],\n",
    "                'retrieved_docs': retrieved_docs,\n",
    "                'mrr_score': mrr_score,\n",
    "                'rank': rank\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Error processing query {query_id}: {e}\")\n",
    "            results.append({\n",
    "                'query_id': query_id,\n",
    "                'query': query,\n",
    "                'error': str(e),\n",
    "                'mrr_score': 0.0,\n",
    "                'rank': -1\n",
    "            })\n",
    "            mrr_scores.append(0.0)\n",
    "    \n",
    "    # Calculate average MRR\n",
    "    avg_mrr = sum(mrr_scores) / len(mrr_scores) if mrr_scores else 0.0\n",
    "    \n",
    "    # Add summary\n",
    "    results.append({\n",
    "        'summary': {\n",
    "            'provider': provider,\n",
    "            'model': model,\n",
    "            'chunk_size': chunk_size,\n",
    "            'k': k,\n",
    "            'mode': mode,\n",
    "            'total_queries': len(dataset),\n",
    "            'average_mrr': avg_mrr\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Save results\n",
    "    save_results(results, provider, model, chunk_size, k, mode, output_dir)\n",
    "    \n",
    "    print(f\"\\n✓ Average MRR: {avg_mrr:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'status': 'completed',\n",
    "        'average_mrr': avg_mrr,\n",
    "        'total_queries': len(dataset)\n",
    "    }\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.2 Batch Evaluation\n",
    "\n",
    "# %%\n",
    "def evaluate_multiple_configurations(\n",
    "    dataset,\n",
    "    configurations: List[Dict],\n",
    "    k_values: List[int],\n",
    "    modes: List[str],\n",
    "    output_dir: str = OUTPUT_DIR\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate multiple configurations.\n",
    "    \n",
    "    Args:\n",
    "        dataset: FinanceBench dataset\n",
    "        configurations: List of {provider, model, chunk_sizes}\n",
    "        k_values: List of k values to test\n",
    "        modes: List of modes [\"global\", \"singledoc\"]\n",
    "        output_dir: Output directory\n",
    "        \n",
    "    Returns:\n",
    "        Summary of all evaluations\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BATCH EVALUATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Configurations: {len(configurations)}\")\n",
    "    print(f\"K values: {k_values}\")\n",
    "    print(f\"Modes: {modes}\")\n",
    "    \n",
    "    # Calculate total runs\n",
    "    total_runs = 0\n",
    "    for config in configurations:\n",
    "        total_runs += len(config['chunk_sizes']) * len(k_values) * len(modes)\n",
    "    \n",
    "    print(f\"Total evaluation runs: {total_runs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Track results\n",
    "    all_results = []\n",
    "    completed = 0\n",
    "    skipped = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # Iterate through all combinations\n",
    "    for config in configurations:\n",
    "        provider = config['provider']\n",
    "        model = config['model']\n",
    "        chunk_sizes = config['chunk_sizes']\n",
    "        \n",
    "        for chunk_size in chunk_sizes:\n",
    "            for k in k_values:\n",
    "                for mode in modes:\n",
    "                    result = evaluate_single_configuration(\n",
    "                        dataset=dataset,\n",
    "                        provider=provider,\n",
    "                        model=model,\n",
    "                        chunk_size=chunk_size,\n",
    "                        k=k,\n",
    "                        mode=mode,\n",
    "                        output_dir=output_dir\n",
    "                    )\n",
    "                    \n",
    "                    all_results.append({\n",
    "                        'provider': provider,\n",
    "                        'model': model,\n",
    "                        'chunk_size': chunk_size,\n",
    "                        'k': k,\n",
    "                        'mode': mode,\n",
    "                        'result': result\n",
    "                    })\n",
    "                    \n",
    "                    if result['status'] == 'completed':\n",
    "                        completed += 1\n",
    "                    elif result['status'] == 'skipped':\n",
    "                        skipped += 1\n",
    "                    else:\n",
    "                        failed += 1\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BATCH EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total runs: {total_runs}\")\n",
    "    print(f\"Completed: {completed}\")\n",
    "    print(f\"Skipped: {skipped}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return {\n",
    "        'total_runs': total_runs,\n",
    "        'completed': completed,\n",
    "        'skipped': skipped,\n",
    "        'failed': failed,\n",
    "        'results': all_results\n",
    "    }\n",
    "\n",
    "# %%\n",
    "print(\"✓ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a114f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Test Retrieval Function\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Test Retrieval with Sample Text\n",
    "# \n",
    "# Use this to verify embeddings are working correctly by testing with \n",
    "# known text from your documents.\n",
    "\n",
    "# %%\n",
    "def test_retrieval(\n",
    "    query_text: str,\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k: int = 10,\n",
    "    mode: str = \"global\",\n",
    "    target_doc_name: str = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Test retrieval with a sample query.\n",
    "    \n",
    "    Args:\n",
    "        query_text: Text to search for (copy from actual document)\n",
    "        provider: \"ollama\" or \"openai\"\n",
    "        model: Model name\n",
    "        chunk_size: Chunk size to test\n",
    "        k: Number of results\n",
    "        mode: \"global\" or \"singledoc\"\n",
    "        target_doc_name: Required if mode is \"singledoc\"\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"TEST RETRIEVAL\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Provider: {provider}\")\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Chunk size: {chunk_size}\")\n",
    "    print(f\"Mode: {mode}\")\n",
    "    print(f\"K: {k}\")\n",
    "    print(f\"\\nQuery (first 200 chars):\")\n",
    "    print(f\"{query_text[:200]}...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load vectorstore\n",
    "    try:\n",
    "        vectorstore = load_vectorstore(provider, model, chunk_size)\n",
    "        print(f\"✓ Vectorstore loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Retrieve\n",
    "    try:\n",
    "        if mode == \"global\":\n",
    "            results = retrieve_global(vectorstore, query_text, k)\n",
    "        elif mode == \"singledoc\":\n",
    "            if not target_doc_name:\n",
    "                print(\"✗ target_doc_name required for singledoc mode\")\n",
    "                return\n",
    "            results = retrieve_single_doc(vectorstore, query_text, target_doc_name, k)\n",
    "        else:\n",
    "            print(f\"✗ Unknown mode: {mode}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n✓ Retrieved {len(results)} documents\")\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for doc_meta in results:\n",
    "            print(f\"\\nRank {doc_meta['rank']}: {doc_meta['doc_name']}, Page {doc_meta['page_number']}\")\n",
    "        \n",
    "        # Check if top result seems correct\n",
    "        if results:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"TOP RESULT ANALYSIS\")\n",
    "            print(\"=\"*60)\n",
    "            top = results[0]\n",
    "            print(f\"Document: {top['doc_name']}\")\n",
    "            print(f\"Page: {top['page_number']}\")\n",
    "            print(f\"Rank: {top['rank']}\")\n",
    "            \n",
    "            # Try to get the actual content\n",
    "            try:\n",
    "                full_results = vectorstore.similarity_search(query_text, k=1)\n",
    "                if full_results:\n",
    "                    content = full_results[0].page_content\n",
    "                    print(f\"\\nTop result content (first 300 chars):\")\n",
    "                    print(f\"{content[:300]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not fetch content: {e}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Retrieval failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c3002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST RETRIEVAL\n",
      "============================================================\n",
      "Provider: ollama\n",
      "Model: nomic-embed-text\n",
      "Chunk size: 256\n",
      "Mode: singledoc\n",
      "K: 10\n",
      "\n",
      "Query (first 200 chars):\n",
      "\n",
      "What drove operating margin change as of FY2022 for 3M? If operating margin is not a useful metric for a company like this, then please state that and explain why.\n",
      "...\n",
      "============================================================\n",
      "✓ Vectorstore loaded\n",
      "\n",
      "✓ Retrieved 10 documents\n",
      "\n",
      "============================================================\n",
      "RESULTS\n",
      "============================================================\n",
      "\n",
      "Rank 1: 3M_2022_10K, Page 38\n",
      "\n",
      "Rank 2: 3M_2022_10K, Page 19\n",
      "\n",
      "Rank 3: 3M_2022_10K, Page 121\n",
      "\n",
      "Rank 4: 3M_2022_10K, Page 26\n",
      "\n",
      "Rank 5: 3M_2022_10K, Page 33\n",
      "\n",
      "Rank 6: 3M_2022_10K, Page 28\n",
      "\n",
      "Rank 7: 3M_2022_10K, Page 38\n",
      "\n",
      "Rank 8: 3M_2022_10K, Page 20\n",
      "\n",
      "Rank 9: 3M_2022_10K, Page 19\n",
      "\n",
      "Rank 10: 3M_2022_10K, Page 10\n",
      "\n",
      "============================================================\n",
      "TOP RESULT ANALYSIS\n",
      "============================================================\n",
      "Document: 3M_2022_10K\n",
      "Page: 38\n",
      "Rank: 1\n",
      "\n",
      "Top result content (first 300 chars):\n",
      "Management believes operating margin provides investors with useful\n",
      "information related to the profitability of our business after considering all of the selling, general and administrative expenses and other operating charges\n",
      "incurred. Management uses this measure in making financial, operating and...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'doc_name': '3M_2022_10K', 'page_number': 38, 'rank': 1},\n",
       " {'doc_name': '3M_2022_10K', 'page_number': 19, 'rank': 2},\n",
       " {'doc_name': '3M_2022_10K', 'page_number': 121, 'rank': 3},\n",
       " {'doc_name': '3M_2022_10K', 'page_number': 26, 'rank': 4},\n",
       " {'doc_name': '3M_2022_10K', 'page_number': 33, 'rank': 5},\n",
       " {'doc_name': '3M_2022_10K', 'page_number': 28, 'rank': 6},\n",
       " {'doc_name': '3M_2022_10K', 'page_number': 38, 'rank': 7},\n",
       " {'doc_name': '3M_2022_10K', 'page_number': 20, 'rank': 8},\n",
       " {'doc_name': '3M_2022_10K', 'page_number': 19, 'rank': 9},\n",
       " {'doc_name': '3M_2022_10K', 'page_number': 10, 'rank': 10}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Example Usage\n",
    "\n",
    "# %%\n",
    "# Example #1: GENERALMILLS 2019 10K - Page 50/140\n",
    "sample_query1 = \"\"\"\n",
    "The management of General Mills, Inc. is responsible for the fairness and accuracy of the consolidated financial statements. The statements have been\n",
    "prepared in accordance with accounting principles that are generally accepted in the United States, using management’s best estimates and judgments where\n",
    "appropriate. The financial information throughout this Annual Report on Form 10-K is consistent with our consolidated financial statements.\n",
    "Management has established a system of internal controls that provides reasonable assurance that assets are adequately safeguarded and transactions are\n",
    "recorded accurately in all material respects, in accordance with management’s authorization. We maintain a strong audit program that independently\n",
    "evaluates the adequacy and effectiveness of internal controls. Our internal controls provide for appropriate separation of duties and responsibilities, and\n",
    "there are documented policies regarding use of our assets and proper financial reporting. These formally stated and regularly communicated policies\n",
    "demand highly ethical conduct from all employees.\n",
    "The Audit Committee of the Board of Directors meets regularly with management, internal auditors, and our independent registered public accounting firm\n",
    "to review internal control, auditing, and financial reporting matters. The independent registered public accounting firm, internal auditors, and employees\n",
    "have full and free access to the Audit Committee at any time.\n",
    "The Audit Committee reviewed and approved the Company’s annual financial statements. The Audit Committee recommended, and the Board of Directors\n",
    "approved, that the consolidated financial statements be included in the Annual Report. The Audit Committee also appointed KPMG LLP to serve as the\n",
    "Company’s independent registered public accounting firm for fiscal 2020\n",
    "\"\"\"\n",
    "\n",
    "test_retrieval(\n",
    "    query_text=sample_query1,\n",
    "    provider=\"ollama\",\n",
    "    model=\"nomic-embed-text\",\n",
    "    chunk_size=512,\n",
    "    k=10,\n",
    "    mode= \"singledoc\", # \"global\", \"singledoc\"\n",
    "    target_doc_name= \"GENERALMILLS_2019_10K\" # no need if mode is \"global\"\n",
    ")\n",
    "\n",
    "sample_query2 = \"\"\"\n",
    "SG&A, measured as a percent of sales, increased in 2022 when compared to the same period last year. SG&A was impacted by increased special item costs for significant\\nlitigation primarily related to steps toward resolving Combat Arms Earplugs litigation (discussed in Note 16) resulting in a 2022 second quarter pre-tax charge of approximately\\n$1.2 billion, certain impairment costs related to exiting PFAS manufacturing (see Note 15), costs related to exiting Russia (see Note 15), divestiture-related\n",
    "\"\"\"\n",
    "\n",
    "test_retrieval(\n",
    "    query_text=sample_query2,\n",
    "    provider=\"ollama\",\n",
    "    model=\"nomic-embed-text\",\n",
    "    chunk_size=256,\n",
    "    k=10,\n",
    "    mode= \"singledoc\", # \"global\", \"singledoc\"\n",
    "    target_doc_name= \"3M_2022_10K\" # no need if mode is \"global\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Example: Test single-doc mode\n",
    "# test_retrieval(\n",
    "#     query_text=\"Your sample text here\",\n",
    "#     provider=\"ollama\",\n",
    "#     model=\"nomic-embed-text\",\n",
    "#     chunk_size=512,\n",
    "#     k=10,\n",
    "#     mode=\"singledoc\",\n",
    "#     target_doc_name=\"3M_2018_10K\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f096ca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION PLAN\n",
      "============================================================\n",
      "\n",
      "ollama/bge-m3\n",
      "  Chunk sizes: [256]\n",
      "  Evaluation runs: 2\n",
      "  Output files:\n",
      "    - ollama_bge-m3_chunk256_k40_global.json [EXISTS]\n",
      "    - ollama_bge-m3_chunk256_k40_singledoc.json [EXISTS]\n",
      "\n",
      "ollama/nomic-embed-text\n",
      "  Chunk sizes: [256, 512, 1024]\n",
      "  Evaluation runs: 6\n",
      "  Output files:\n",
      "    - ollama_nomic-embed-text_chunk256_k40_global.json [EXISTS]\n",
      "    - ollama_nomic-embed-text_chunk256_k40_singledoc.json [EXISTS]\n",
      "    - ollama_nomic-embed-text_chunk512_k40_global.json [EXISTS]\n",
      "    - ollama_nomic-embed-text_chunk512_k40_singledoc.json [EXISTS]\n",
      "    - ollama_nomic-embed-text_chunk1024_k40_global.json [EXISTS]\n",
      "    - ollama_nomic-embed-text_chunk1024_k40_singledoc.json [EXISTS]\n",
      "\n",
      "openai/text-embedding-3-small\n",
      "  Chunk sizes: [512]\n",
      "  Evaluation runs: 2\n",
      "  Output files:\n",
      "    - openai_text-embedding-3-small_chunk512_k40_global.json [EXISTS]\n",
      "    - openai_text-embedding-3-small_chunk512_k40_singledoc.json [EXISTS]\n",
      "\n",
      "============================================================\n",
      "Total evaluation runs: 10\n",
      "Output directory: ../../evaluation_results/mrr_embeddings\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "BATCH EVALUATION\n",
      "============================================================\n",
      "Configurations: 3\n",
      "K values: [40]\n",
      "Modes: ['global', 'singledoc']\n",
      "Total evaluation runs: 10\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EVALUATING: ollama/bge-m3, chunk=256, k=40, mode=global\n",
      "============================================================\n",
      "✓ Results already exist - SKIPPING\n",
      "\n",
      "============================================================\n",
      "EVALUATING: ollama/bge-m3, chunk=256, k=40, mode=singledoc\n",
      "============================================================\n",
      "✓ Results already exist - SKIPPING\n",
      "\n",
      "============================================================\n",
      "EVALUATING: ollama/nomic-embed-text, chunk=256, k=40, mode=global\n",
      "============================================================\n",
      "✓ Results already exist - SKIPPING\n",
      "\n",
      "============================================================\n",
      "EVALUATING: ollama/nomic-embed-text, chunk=256, k=40, mode=singledoc\n",
      "============================================================\n",
      "✓ Results already exist - SKIPPING\n",
      "\n",
      "============================================================\n",
      "EVALUATING: ollama/nomic-embed-text, chunk=512, k=40, mode=global\n",
      "============================================================\n",
      "✓ Results already exist - SKIPPING\n",
      "\n",
      "============================================================\n",
      "EVALUATING: ollama/nomic-embed-text, chunk=512, k=40, mode=singledoc\n",
      "============================================================\n",
      "✓ Results already exist - SKIPPING\n",
      "\n",
      "============================================================\n",
      "EVALUATING: ollama/nomic-embed-text, chunk=1024, k=40, mode=global\n",
      "============================================================\n",
      "✓ Results already exist - SKIPPING\n",
      "\n",
      "============================================================\n",
      "EVALUATING: ollama/nomic-embed-text, chunk=1024, k=40, mode=singledoc\n",
      "============================================================\n",
      "✓ Results already exist - SKIPPING\n",
      "\n",
      "============================================================\n",
      "EVALUATING: openai/text-embedding-3-small, chunk=512, k=40, mode=global\n",
      "============================================================\n",
      "✓ Results already exist - SKIPPING\n",
      "\n",
      "============================================================\n",
      "EVALUATING: openai/text-embedding-3-small, chunk=512, k=40, mode=singledoc\n",
      "============================================================\n",
      "✓ Results already exist - SKIPPING\n",
      "\n",
      "============================================================\n",
      "BATCH EVALUATION SUMMARY\n",
      "============================================================\n",
      "Total runs: 10\n",
      "Completed: 0\n",
      "Skipped: 10\n",
      "Failed: 0\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "DETAILED RESULTS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "GENERATED FILES\n",
      "============================================================\n",
      "\n",
      "Total JSON files: 10\n",
      "\n",
      "  ollama_bge-m3_chunk256_k40_global.json (682.7 KB)\n",
      "  ollama_bge-m3_chunk256_k40_singledoc.json (427.1 KB)\n",
      "  ollama_nomic-embed-text_chunk1024_k40_global.json (681.0 KB)\n",
      "  ollama_nomic-embed-text_chunk1024_k40_singledoc.json (295.3 KB)\n",
      "  ollama_nomic-embed-text_chunk256_k40_global.json (681.2 KB)\n",
      "  ollama_nomic-embed-text_chunk256_k40_singledoc.json (355.3 KB)\n",
      "  ollama_nomic-embed-text_chunk512_k40_global.json (681.8 KB)\n",
      "  ollama_nomic-embed-text_chunk512_k40_singledoc.json (345.5 KB)\n",
      "  openai_text-embedding-3-small_chunk512_k40_global.json (685.1 KB)\n",
      "  openai_text-embedding-3-small_chunk512_k40_singledoc.json (479.5 KB)\n",
      "\n",
      "============================================================\n",
      "EVALUATION COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 4: Configuration and Execution\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.1 Define Configurations to Test\n",
    "\n",
    "# %%\n",
    "# Define which embedding models and chunk sizes to evaluate\n",
    "configurations = [\n",
    "    {\n",
    "        'provider': 'ollama',\n",
    "        'model': 'bge-m3',\n",
    "        'chunk_sizes': [256]\n",
    "    },\n",
    "    {\n",
    "        'provider': 'ollama',\n",
    "        'model': 'nomic-embed-text',\n",
    "        'chunk_sizes': [256, 512, 1024]\n",
    "    },\n",
    "    # Add more configurations as needed:\n",
    "    {\n",
    "        'provider': 'openai',\n",
    "        'model': 'text-embedding-3-small',\n",
    "        'chunk_sizes': [512]\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.2 Define Evaluation Parameters\n",
    "\n",
    "# %%\n",
    "# K values to test (number of documents to retrieve)\n",
    "k_values = [40]\n",
    "\n",
    "# Modes to test\n",
    "modes = ['global', 'singledoc']\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.3 Display Evaluation Plan\n",
    "\n",
    "# %%\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATION PLAN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_runs = 0\n",
    "for config in configurations:\n",
    "    provider = config['provider']\n",
    "    model = config['model']\n",
    "    chunk_sizes = config['chunk_sizes']\n",
    "    \n",
    "    print(f\"\\n{provider}/{model}\")\n",
    "    print(f\"  Chunk sizes: {chunk_sizes}\")\n",
    "    \n",
    "    runs_for_config = len(chunk_sizes) * len(k_values) * len(modes)\n",
    "    total_runs += runs_for_config\n",
    "    \n",
    "    print(f\"  Evaluation runs: {runs_for_config}\")\n",
    "    \n",
    "    # Show output filenames that will be generated\n",
    "    print(f\"  Output files:\")\n",
    "    for chunk_size in chunk_sizes:\n",
    "        for k in k_values:\n",
    "            for mode in modes:\n",
    "                filename = get_output_filename(provider, model, chunk_size, k, mode)\n",
    "                exists = check_if_results_exist(provider, model, chunk_size, k, mode, OUTPUT_DIR)\n",
    "                status = \"EXISTS\" if exists else \"TO CREATE\"\n",
    "                print(f\"    - {filename} [{status}]\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total evaluation runs: {total_runs}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.4 Execute Evaluation\n",
    "\n",
    "# %%\n",
    "# Run batch evaluation\n",
    "summary = evaluate_multiple_configurations(\n",
    "    dataset=dataset,\n",
    "    configurations=configurations,\n",
    "    k_values=k_values,\n",
    "    modes=modes,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.5 View Results Summary\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for result in summary['results']:\n",
    "    if result['result']['status'] == 'completed':\n",
    "        print(f\"\\n{result['provider']}/{result['model']}, \"\n",
    "              f\"chunk={result['chunk_size']}, \"\n",
    "              f\"k={result['k']}, \"\n",
    "              f\"mode={result['mode']}\")\n",
    "        print(f\"  Average MRR: {result['result']['average_mrr']:.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.6 List Generated Files\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATED FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "json_files = sorted(output_path.glob(\"*.json\"))\n",
    "\n",
    "print(f\"\\nTotal JSON files: {len(json_files)}\\n\")\n",
    "\n",
    "for filepath in json_files:\n",
    "    file_size = filepath.stat().st_size / 1024  # KB\n",
    "    print(f\"  {filepath.name} ({file_size:.1f} KB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbaff51d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Step 5: Plotting Results\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 5: Plotting Results\n",
    "# ============================================================================\n",
    "\n",
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.1 Load Results from JSON Files\n",
    "\n",
    "# %%\n",
    "def load_all_results(output_dir: str = OUTPUT_DIR) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all JSON results into a DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: provider, model, chunk_size, k, mode, average_mrr\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    json_files = list(output_path.glob(\"*.json\"))\n",
    "    \n",
    "    print(f\"Loading {len(json_files)} result files...\")\n",
    "    \n",
    "    for filepath in json_files:\n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Get summary (last item in array)\n",
    "            summary = data[-1].get('summary', {})\n",
    "            \n",
    "            if summary:\n",
    "                results.append({\n",
    "                    'provider': summary['provider'],\n",
    "                    'model': summary['model'],\n",
    "                    'chunk_size': summary['chunk_size'],\n",
    "                    'k': summary['k'],\n",
    "                    'mode': summary['mode'],\n",
    "                    'average_mrr': summary['average_mrr'],\n",
    "                    'total_queries': summary['total_queries']\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filepath.name}: {e}\")\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    print(f\"✓ Loaded {len(df)} results\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# %%\n",
    "# Load results\n",
    "results_df = load_all_results()\n",
    "print(\"\\nResults summary:\")\n",
    "print(results_df)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.2 Plot: MRR by Chunk Size\n",
    "\n",
    "# %%\n",
    "def plot_mrr_by_chunk_size(df: pd.DataFrame, mode: str = \"global\"):\n",
    "    \"\"\"Plot MRR comparison across chunk sizes for a specific mode.\"\"\"\n",
    "    \n",
    "    # Filter by mode\n",
    "    df_filtered = df[df['mode'] == mode].copy()\n",
    "    \n",
    "    if df_filtered.empty:\n",
    "        print(f\"No data for mode: {mode}\")\n",
    "        return\n",
    "    \n",
    "    # Create model labels\n",
    "    df_filtered['model_label'] = df_filtered['provider'] + '_' + df_filtered['model']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot bars\n",
    "    models = df_filtered['model_label'].unique()\n",
    "    x = df_filtered['chunk_size'].unique()\n",
    "    width = 0.8 / len(models)\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        model_data = df_filtered[df_filtered['model_label'] == model]\n",
    "        offset = (i - len(models)/2 + 0.5) * width\n",
    "        \n",
    "        ax.bar(\n",
    "            model_data['chunk_size'] + offset,\n",
    "            model_data['average_mrr'],\n",
    "            width=width,\n",
    "            label=model,\n",
    "            alpha=0.8\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Chunk Size', fontsize=12)\n",
    "    ax.set_ylabel('Average MRR', fontsize=12)\n",
    "    ax.set_title(f'MRR Comparison by Chunk Size ({mode.capitalize()} Mode)', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "# Plot global mode\n",
    "plot_mrr_by_chunk_size(results_df, mode=\"global\")\n",
    "\n",
    "# %%\n",
    "# Plot single-doc mode\n",
    "plot_mrr_by_chunk_size(results_df, mode=\"singledoc\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.3 Plot: Global vs Single-Doc Comparison\n",
    "\n",
    "# %%\n",
    "def plot_global_vs_singledoc(df: pd.DataFrame):\n",
    "    \"\"\"Compare global and single-doc modes.\"\"\"\n",
    "    \n",
    "    # Pivot data\n",
    "    pivot_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        model_label = f\"{row['provider']}_{row['model']}\"\n",
    "        chunk_label = f\"chunk_{row['chunk_size']}\"\n",
    "        \n",
    "        pivot_data.append({\n",
    "            'model': model_label,\n",
    "            'chunk_size': row['chunk_size'],\n",
    "            'config': f\"{model_label}\\n{chunk_label}\",\n",
    "            'mode': row['mode'],\n",
    "            'mrr': row['average_mrr']\n",
    "        })\n",
    "    \n",
    "    pivot_df = pd.DataFrame(pivot_data)\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    configs = pivot_df['config'].unique()\n",
    "    x = range(len(configs))\n",
    "    width = 0.35\n",
    "    \n",
    "    global_data = []\n",
    "    singledoc_data = []\n",
    "    \n",
    "    for config in configs:\n",
    "        config_data = pivot_df[pivot_df['config'] == config]\n",
    "        global_mrr = config_data[config_data['mode'] == 'global']['mrr'].values\n",
    "        singledoc_mrr = config_data[config_data['mode'] == 'singledoc']['mrr'].values\n",
    "        \n",
    "        global_data.append(global_mrr[0] if len(global_mrr) > 0 else 0)\n",
    "        singledoc_data.append(singledoc_mrr[0] if len(singledoc_mrr) > 0 else 0)\n",
    "    \n",
    "    ax.bar([i - width/2 for i in x], global_data, width, label='Global', alpha=0.8, color='steelblue')\n",
    "    ax.bar([i + width/2 for i in x], singledoc_data, width, label='Single-Doc', alpha=0.8, color='coral')\n",
    "    \n",
    "    ax.set_xlabel('Configuration', fontsize=12)\n",
    "    ax.set_ylabel('Average MRR', fontsize=12)\n",
    "    ax.set_title('Global vs Single-Doc Mode Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(configs, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "plot_global_vs_singledoc(results_df)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.4 Plot: Heatmap of MRR Scores\n",
    "\n",
    "# %%\n",
    "def plot_mrr_heatmap(df: pd.DataFrame, mode: str = \"global\"):\n",
    "    \"\"\"Create heatmap of MRR scores.\"\"\"\n",
    "    \n",
    "    df_filtered = df[df['mode'] == mode].copy()\n",
    "    \n",
    "    if df_filtered.empty:\n",
    "        print(f\"No data for mode: {mode}\")\n",
    "        return\n",
    "    \n",
    "    # Create pivot table\n",
    "    df_filtered['model_label'] = df_filtered['provider'] + '_' + df_filtered['model']\n",
    "    pivot = df_filtered.pivot(index='model_label', columns='chunk_size', values='average_mrr')\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        pivot,\n",
    "        annot=True,\n",
    "        fmt='.4f',\n",
    "        cmap='YlOrRd',\n",
    "        cbar_kws={'label': 'Average MRR'},\n",
    "        ax=ax,\n",
    "        vmin=0,\n",
    "        vmax=pivot.max().max()\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'MRR Heatmap ({mode.capitalize()} Mode)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Chunk Size', fontsize=12)\n",
    "    ax.set_ylabel('Model', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "# Heatmap for global mode\n",
    "plot_mrr_heatmap(results_df, mode=\"global\")\n",
    "\n",
    "# %%\n",
    "# Heatmap for single-doc mode\n",
    "plot_mrr_heatmap(results_df, mode=\"singledoc\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.5 Summary Table\n",
    "\n",
    "# %%\n",
    "def display_summary_table(df: pd.DataFrame):\n",
    "    \"\"\"Display formatted summary table.\"\"\"\n",
    "    \n",
    "    # Sort by MRR\n",
    "    df_sorted = df.sort_values('average_mrr', ascending=False)\n",
    "    \n",
    "    # Format for display\n",
    "    display_df = df_sorted.copy()\n",
    "    display_df['average_mrr'] = display_df['average_mrr'].apply(lambda x: f\"{x:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY TABLE (Sorted by MRR)\")\n",
    "    print(\"=\"*80)\n",
    "    print(display_df.to_string(index=False))\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# %%\n",
    "display_summary_table(results_df)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.6 Save Plots\n",
    "\n",
    "# %%\n",
    "def save_all_plots(df: pd.DataFrame, output_dir: str = OUTPUT_DIR):\n",
    "    \"\"\"Save all plots to files.\"\"\"\n",
    "    \n",
    "    plots_dir = Path(output_dir) / \"plots\"\n",
    "    plots_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"Saving plots to {plots_dir}...\")\n",
    "    \n",
    "    # Plot 1: Global mode by chunk size\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_mrr_by_chunk_size(df, mode=\"global\")\n",
    "    plt.savefig(plots_dir / \"mrr_by_chunk_global.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 2: Single-doc mode by chunk size\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_mrr_by_chunk_size(df, mode=\"singledoc\")\n",
    "    plt.savefig(plots_dir / \"mrr_by_chunk_singledoc.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 3: Global vs Single-doc\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plot_global_vs_singledoc(df)\n",
    "    plt.savefig(plots_dir / \"global_vs_singledoc.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 4: Heatmap global\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_mrr_heatmap(df, mode=\"global\")\n",
    "    plt.savefig(plots_dir / \"heatmap_global.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 5: Heatmap single-doc\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_mrr_heatmap(df, mode=\"singledoc\")\n",
    "    plt.savefig(plots_dir / \"heatmap_singledoc.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"✓ Saved 5 plots to {plots_dir}\")\n",
    "\n",
    "# %%\n",
    "# Uncomment to save plots\n",
    "# save_all_plots(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
