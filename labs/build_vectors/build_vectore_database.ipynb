{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee342d2a",
   "metadata": {},
   "source": [
    "# General Tips\n",
    "## Using virtual environments\n",
    "**Step 1:** CD to desired directory and Create a Virtual Environment `python3 -m venv myenv`. (Run `py -3.13 -m venv myenv` for a specific version of python)\n",
    "\n",
    "Check your python installed versions with `py -0` on Windows (`python3 --version` on Linux)\n",
    "\n",
    "**Step 2:** Activate the Environment `source myenv/bin/activate` (on Linux) and `myenv\\Scripts\\activate` (on Windows).\n",
    "\n",
    "**Step 3:** Install Any Needed Packages. e.g: `pip install requests pandas`. Or better to use `requirements.txt` file (`pip install -r requirements.txt`)\n",
    "\n",
    "**Step 4:** List All Installed Packages using `pip list`\n",
    "\n",
    "## Connecting the Jupyter Notebook to the vistual env\n",
    "1. Make sure that myenv is activate (`myenv\\Scripts\\activate`)\n",
    "2. Run this inside the virtual environment: `pip install ipykernel`\n",
    "3. Still inside the environment: `python -m ipykernel install --user --name=myenv --display-name \"Whatever Python Kernel Name\"`\n",
    "   \n",
    "   --name=myenv: internal identifier for the kernel\n",
    "   \n",
    "   --display-name: name that shows up in VS Code kernel picker\n",
    "4. Open VS Code and select the kernel\n",
    "\n",
    "   At the top-right, click \"Select Kernel\".\n",
    "   Look for “Whatever Python Kernel Name” — pick that.\n",
    "5. If you don’t see it right away, try: Reloading VS Code, Or running Reload Window from Command Palette (Ctrl+Shift+P)\n",
    "\n",
    "## Useful Commands\n",
    "1. Use `py -0` to check which python installation we have on Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc52eb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n",
      "✓ OpenAI API key loaded\n",
      "✓ Ollama URL: http://localhost:11434\n",
      "✓ VoyageAI API key loaded\n",
      "✓ Configuration set\n",
      "  PDF Directory: ../../financebench/documents\n",
      "  Vector DB Directory: ../../vector_databases\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 1: Setup and Imports\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # FinanceBench RAG Pipeline - Clean Modular Approach\n",
    "# \n",
    "# This notebook processes financial documents and creates vector embeddings\n",
    "# for retrieval-augmented generation (RAG).\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.1 Install Requirements\n",
    "# \n",
    "# Make sure you have installed:\n",
    "# ```bash\n",
    "# pip install -r requirements.txt\n",
    "# ```\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.2 Imports\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Progress\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Document processing\n",
    "from llama_index.core.schema import Document, BaseNode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "# Vector stores\n",
    "from langchain.docstore.document import Document as LCDocument\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.3 Load Environment Variables\n",
    "\n",
    "# %%\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "VOYAGE_API_KEY = os.getenv(\"VOYAGE_API_KEY\")\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    print(\"✓ OpenAI API key loaded\")\n",
    "else:\n",
    "    print(\"⚠ OpenAI API key not found (only needed if using OpenAI embeddings)\")\n",
    "\n",
    "print(f\"✓ Ollama URL: {OLLAMA_BASE_URL}\")\n",
    "\n",
    "if VOYAGE_API_KEY:\n",
    "    print(\"✓ VoyageAI API key loaded\")\n",
    "else:\n",
    "    print(\"⚠ VoyageAI API key not found (only needed if using VoyageAI embeddings)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.4 Configuration Variables\n",
    "\n",
    "# %%\n",
    "# Paths\n",
    "PDF_DIR = \"../../financebench/documents\"\n",
    "VECTOR_DB_DIR = \"../../vector_databases\"\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = \"PatronusAI/financebench\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "\n",
    "# Processing\n",
    "COLLECTION_PREFIX = \"financebench_docs_chunk_\"\n",
    "CHUNK_OVERLAP_PERCENTAGE = 15\n",
    "\n",
    "print(\"✓ Configuration set\")\n",
    "print(f\"  PDF Directory: {PDF_DIR}\")\n",
    "print(f\"  Vector DB Directory: {VECTOR_DB_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "666927ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: PatronusAI/financebench\n",
      "✓ Loaded 150 records\n",
      "\n",
      "Sample record keys:\n",
      "  - financebench_id\n",
      "  - company\n",
      "  - doc_name\n",
      "  - question_type\n",
      "  - question_reasoning\n",
      "  - domain_question_num\n",
      "  - question\n",
      "  - answer\n",
      "  - justification\n",
      "  - dataset_subset_label\n",
      "  - evidence\n",
      "  - gics_sector\n",
      "  - doc_type\n",
      "  - doc_period\n",
      "  - doc_link\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f6e2a66a354f3eb1660efeaaba632a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning for PDFs:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found 84 unique PDFs required\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038f7942596e4edc95961d58c1b85574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Verifying PDFs:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Available: 84 PDFs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e1c73b7fa74eb8b9a425195d686f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading PDFs:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Loaded 12013 pages from 84 PDFs\n",
      "\n",
      "============================================================\n",
      "DOCUMENT STATISTICS\n",
      "============================================================\n",
      "Total Pages:           12,013\n",
      "Total Characters:      40,649,449\n",
      "Estimated Tokens:      10,162,362\n",
      "\n",
      "Per-Page Statistics:\n",
      "  Average:             3,384 chars\n",
      "  Min:                 0 chars\n",
      "  Max:                 10,738 chars\n",
      "============================================================\n",
      "\n",
      "✓ Step 2 complete!\n",
      "  Dataset records: 150\n",
      "  PDFs loaded: 84\n",
      "  Document pages: 12013\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Load Dataset and Documents\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.1 Load FinanceBench Dataset\n",
    "\n",
    "# %%\n",
    "def load_financebench_dataset(dataset_name: str, split: str):\n",
    "    \"\"\"Load the FinanceBench dataset from HuggingFace.\"\"\"\n",
    "    print(f\"Loading dataset: {dataset_name}\")\n",
    "    ds = load_dataset(dataset_name, split=split)\n",
    "    print(f\"✓ Loaded {len(ds)} records\")\n",
    "    return ds\n",
    "\n",
    "# %%\n",
    "# Load dataset\n",
    "dataset = load_financebench_dataset(DATASET_NAME, DATASET_SPLIT)\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample record keys:\")\n",
    "for key in dataset[0].keys():\n",
    "    print(f\"  - {key}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.2 Extract Required PDFs\n",
    "\n",
    "# %%\n",
    "def get_required_pdfs(dataset) -> set:\n",
    "    \"\"\"Extract unique PDF filenames needed.\"\"\"\n",
    "    unique_pdfs = set()\n",
    "    for record in tqdm(dataset, desc=\"Scanning for PDFs\"):\n",
    "        pdf_filename = record[\"doc_name\"] + \".pdf\"\n",
    "        unique_pdfs.add(pdf_filename)\n",
    "    print(f\"✓ Found {len(unique_pdfs)} unique PDFs required\")\n",
    "    return unique_pdfs\n",
    "\n",
    "# %%\n",
    "required_pdfs = get_required_pdfs(dataset)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.3 Verify PDF Availability\n",
    "\n",
    "# %%\n",
    "def verify_pdfs(pdf_dir: str, required_pdfs: set) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Check which PDFs are available.\"\"\"\n",
    "    available = []\n",
    "    missing = []\n",
    "    \n",
    "    for pdf in tqdm(required_pdfs, desc=\"Verifying PDFs\"):\n",
    "        path = os.path.join(pdf_dir, pdf)\n",
    "        if os.path.isfile(path):\n",
    "            available.append(pdf)\n",
    "        else:\n",
    "            missing.append(pdf)\n",
    "    \n",
    "    print(f\"\\n✓ Available: {len(available)} PDFs\")\n",
    "    if missing:\n",
    "        print(f\"✗ Missing: {len(missing)} PDFs\")\n",
    "        for f in missing[:5]:\n",
    "            print(f\"  - {f}\")\n",
    "        if len(missing) > 5:\n",
    "            print(f\"  ... and {len(missing)-5} more\")\n",
    "    \n",
    "    return available, missing\n",
    "\n",
    "# %%\n",
    "available_pdfs, missing_pdfs = verify_pdfs(PDF_DIR, required_pdfs)\n",
    "\n",
    "# %%\n",
    "# Check if we can proceed\n",
    "if missing_pdfs:\n",
    "    print(\"\\n⚠ Some PDFs are missing\")\n",
    "    proceed = input(\"Continue with available PDFs only? (y/n): \").lower().strip()\n",
    "    if proceed != 'y':\n",
    "        raise SystemExit(\"Stopped by user\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.4 Load PDF Documents\n",
    "\n",
    "# %%\n",
    "def load_pdf_documents(pdf_dir: str, pdf_files: List[str]) -> List[Document]:\n",
    "    \"\"\"Load PDFs using PyMuPDF.\"\"\"\n",
    "    reader = PyMuPDFReader()\n",
    "    documents = []\n",
    "    failed = []\n",
    "    \n",
    "    for pdf in tqdm(pdf_files, desc=\"Loading PDFs\"):\n",
    "        path = os.path.join(pdf_dir, pdf)\n",
    "        try:\n",
    "            docs = reader.load(path)\n",
    "            documents.extend(docs)\n",
    "        except Exception as e:\n",
    "            failed.append((pdf, str(e)))\n",
    "            print(f\"\\n✗ Failed: {pdf}: {e}\")\n",
    "    \n",
    "    print(f\"\\n✓ Loaded {len(documents)} pages from {len(pdf_files)-len(failed)} PDFs\")\n",
    "    if failed:\n",
    "        print(f\"✗ Failed to load {len(failed)} PDFs\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# %%\n",
    "documents = load_pdf_documents(PDF_DIR, available_pdfs)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.5 Analyze Documents\n",
    "\n",
    "# %%\n",
    "def analyze_documents(documents: List[Document]) -> Dict:\n",
    "    \"\"\"Analyze loaded documents.\"\"\"\n",
    "    total_pages = len(documents)\n",
    "    total_chars = sum(len(doc.text) for doc in documents)\n",
    "    estimated_tokens = total_chars // 4\n",
    "    \n",
    "    char_counts = [len(doc.text) for doc in documents]\n",
    "    avg_chars = total_chars / total_pages if total_pages > 0 else 0\n",
    "    \n",
    "    stats = {\n",
    "        'total_pages': total_pages,\n",
    "        'total_characters': total_chars,\n",
    "        'estimated_tokens': estimated_tokens,\n",
    "        'avg_chars_per_page': avg_chars,\n",
    "        'min_chars': min(char_counts) if char_counts else 0,\n",
    "        'max_chars': max(char_counts) if char_counts else 0\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DOCUMENT STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Pages:           {stats['total_pages']:,}\")\n",
    "    print(f\"Total Characters:      {stats['total_characters']:,}\")\n",
    "    print(f\"Estimated Tokens:      {stats['estimated_tokens']:,}\")\n",
    "    print(f\"\\nPer-Page Statistics:\")\n",
    "    print(f\"  Average:             {stats['avg_chars_per_page']:,.0f} chars\")\n",
    "    print(f\"  Min:                 {stats['min_chars']:,} chars\")\n",
    "    print(f\"  Max:                 {stats['max_chars']:,} chars\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# %%\n",
    "doc_stats = analyze_documents(documents)\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 2 complete!\")\n",
    "print(f\"  Dataset records: {len(dataset)}\")\n",
    "print(f\"  PDFs loaded: {len(available_pdfs)}\")\n",
    "print(f\"  Document pages: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a0f336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 5 chunk size(s)...\n",
      "\n",
      "============================================================\n",
      "PROCESSING CHUNK SIZE: 256\n",
      "============================================================\n",
      "Overlap: 38 chars (15%)\n",
      "Generating nodes (size=256, overlap=38)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbcbc56b3d34aed96361bef80b3a01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/12013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 57,903 nodes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222e728f3b6c4efd90dc16e2c558f44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting to LangChain:   0%|          | 0/57903 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted 57,903 documents\n",
      "\n",
      "============================================================\n",
      "CHUNK STATISTICS (Size: 256)\n",
      "============================================================\n",
      "Total Chunks:          57,903\n",
      "Total Characters:      43,436,009\n",
      "Estimated Tokens:      10,859,002\n",
      "\n",
      "Per-Chunk Statistics:\n",
      "  Average:             750 chars\n",
      "  Min:                 0 chars\n",
      "  Max:                 2,121 chars\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PROCESSING CHUNK SIZE: 512\n",
      "============================================================\n",
      "Overlap: 76 chars (15%)\n",
      "Generating nodes (size=512, overlap=76)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364e7d07f8ee4e848b45d83eb8b95fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/12013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 28,657 nodes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c2be94f9a44a29bfe1dcce2c37d618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting to LangChain:   0%|          | 0/28657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted 28,657 documents\n",
      "\n",
      "============================================================\n",
      "CHUNK STATISTICS (Size: 512)\n",
      "============================================================\n",
      "Total Chunks:          28,657\n",
      "Total Characters:      43,783,209\n",
      "Estimated Tokens:      10,945,802\n",
      "\n",
      "Per-Chunk Statistics:\n",
      "  Average:             1,528 chars\n",
      "  Min:                 0 chars\n",
      "  Max:                 4,103 chars\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PROCESSING CHUNK SIZE: 1024\n",
      "============================================================\n",
      "Overlap: 153 chars (15%)\n",
      "Generating nodes (size=1024, overlap=153)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d06fc9d1aac4209b5cf6305793f6eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/12013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 15,787 nodes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d076031efb46d2959999e4238bdc5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting to LangChain:   0%|          | 0/15787 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted 15,787 documents\n",
      "\n",
      "============================================================\n",
      "CHUNK STATISTICS (Size: 1024)\n",
      "============================================================\n",
      "Total Chunks:          15,787\n",
      "Total Characters:      42,395,002\n",
      "Estimated Tokens:      10,598,750\n",
      "\n",
      "Per-Chunk Statistics:\n",
      "  Average:             2,685 chars\n",
      "  Min:                 0 chars\n",
      "  Max:                 7,205 chars\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PROCESSING CHUNK SIZE: 2048\n",
      "============================================================\n",
      "Overlap: 307 chars (15%)\n",
      "Generating nodes (size=2048, overlap=307)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1055b06589ca4c88ada859b758a3a8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/12013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 12,099 nodes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9bcb14b65649b8bca0bd616e7e25c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting to LangChain:   0%|          | 0/12099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted 12,099 documents\n",
      "\n",
      "============================================================\n",
      "CHUNK STATISTICS (Size: 2048)\n",
      "============================================================\n",
      "Total Chunks:          12,099\n",
      "Total Characters:      40,711,884\n",
      "Estimated Tokens:      10,177,971\n",
      "\n",
      "Per-Chunk Statistics:\n",
      "  Average:             3,365 chars\n",
      "  Min:                 0 chars\n",
      "  Max:                 10,737 chars\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PROCESSING CHUNK SIZE: 4096\n",
      "============================================================\n",
      "Overlap: 614 chars (15%)\n",
      "Generating nodes (size=4096, overlap=614)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6e1a38aabb41d2a41dc27bc68d9fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/12013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 11,970 nodes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56123ba8fcda4edc96f8a9ec7ea23158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting to LangChain:   0%|          | 0/11970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted 11,970 documents\n",
      "\n",
      "============================================================\n",
      "CHUNK STATISTICS (Size: 4096)\n",
      "============================================================\n",
      "Total Chunks:          11,970\n",
      "Total Characters:      40,623,247\n",
      "Estimated Tokens:      10,155,811\n",
      "\n",
      "Per-Chunk Statistics:\n",
      "  Average:             3,394 chars\n",
      "  Min:                 0 chars\n",
      "  Max:                 10,737 chars\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PROCESSING SUMMARY\n",
      "============================================================\n",
      "Chunk 256: 57,903 chunks, ~10,859,002 tokens\n",
      "Chunk 512: 28,657 chunks, ~10,945,802 tokens\n",
      "Chunk 1024: 15,787 chunks, ~10,598,750 tokens\n",
      "Chunk 2048: 12,099 chunks, ~10,177,971 tokens\n",
      "Chunk 4096: 11,970 chunks, ~10,155,811 tokens\n",
      "============================================================\n",
      "\n",
      "✓ Step 3 complete!\n",
      "Processed chunk sizes: [256, 512, 1024, 2048, 4096]\n",
      "Total chunks across all sizes: 126,416\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Process Documents into Chunks\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.1 Generate Nodes (Chunks)\n",
    "\n",
    "# %%\n",
    "def generate_nodes(\n",
    "    documents: List[Document],\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int\n",
    ") -> List[BaseNode]:\n",
    "    \"\"\"Generate nodes from documents using SentenceSplitter.\"\"\"\n",
    "    parser = SentenceSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    print(f\"Generating nodes (size={chunk_size}, overlap={chunk_overlap})...\")\n",
    "    nodes = parser.get_nodes_from_documents(documents, show_progress=True)\n",
    "    print(f\"✓ Created {len(nodes):,} nodes\")\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.2 Convert to LangChain Documents\n",
    "\n",
    "# %%\n",
    "def nodes_to_langchain_docs(\n",
    "    nodes: List[BaseNode],\n",
    "    chunk_size: int\n",
    ") -> List[LCDocument]:\n",
    "    \"\"\"Convert LlamaIndex nodes to LangChain documents.\"\"\"\n",
    "    lc_docs = []\n",
    "    \n",
    "    for node in tqdm(nodes, desc=\"Converting to LangChain\"):\n",
    "        metadata = {\"chunk_size\": chunk_size}\n",
    "        \n",
    "        # Add original metadata\n",
    "        if hasattr(node, 'metadata'):\n",
    "            metadata.update(node.metadata)\n",
    "        \n",
    "        doc = LCDocument(\n",
    "            page_content=node.get_content(),\n",
    "            metadata=metadata\n",
    "        )\n",
    "        lc_docs.append(doc)\n",
    "    \n",
    "    print(f\"✓ Converted {len(lc_docs):,} documents\")\n",
    "    return lc_docs\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.3 Analyze Chunks\n",
    "\n",
    "# %%\n",
    "def analyze_chunks(lc_docs: List[LCDocument], chunk_size: int) -> Dict:\n",
    "    \"\"\"Analyze generated chunks.\"\"\"\n",
    "    chunk_lengths = [len(doc.page_content) for doc in lc_docs]\n",
    "    total_chunks = len(lc_docs)\n",
    "    total_chars = sum(chunk_lengths)\n",
    "    \n",
    "    stats = {\n",
    "        'total_chunks': total_chunks,\n",
    "        'total_characters': total_chars,\n",
    "        'estimated_tokens': total_chars // 4,\n",
    "        'avg_length': total_chars / total_chunks if total_chunks > 0 else 0,\n",
    "        'min_length': min(chunk_lengths) if chunk_lengths else 0,\n",
    "        'max_length': max(chunk_lengths) if chunk_lengths else 0\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"CHUNK STATISTICS (Size: {chunk_size})\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Chunks:          {stats['total_chunks']:,}\")\n",
    "    print(f\"Total Characters:      {stats['total_characters']:,}\")\n",
    "    print(f\"Estimated Tokens:      {stats['estimated_tokens']:,}\")\n",
    "    print(f\"\\nPer-Chunk Statistics:\")\n",
    "    print(f\"  Average:             {stats['avg_length']:,.0f} chars\")\n",
    "    print(f\"  Min:                 {stats['min_length']:,} chars\")\n",
    "    print(f\"  Max:                 {stats['max_length']:,} chars\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.4 Process Single Chunk Size\n",
    "\n",
    "# %%\n",
    "def process_chunk_size(\n",
    "    documents: List[Document],\n",
    "    chunk_size: int,\n",
    "    overlap_percentage: int = 15\n",
    ") -> Dict:\n",
    "    \"\"\"Process documents for a single chunk size.\"\"\"\n",
    "    # Calculate overlap\n",
    "    chunk_overlap = int(chunk_size * (overlap_percentage / 100))\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING CHUNK SIZE: {chunk_size}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Overlap: {chunk_overlap} chars ({overlap_percentage}%)\")\n",
    "    \n",
    "    # Generate nodes\n",
    "    nodes = generate_nodes(documents, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Convert to LangChain docs\n",
    "    lc_docs = nodes_to_langchain_docs(nodes, chunk_size)\n",
    "    \n",
    "    # Analyze\n",
    "    stats = analyze_chunks(lc_docs, chunk_size)\n",
    "    \n",
    "    return {\n",
    "        'chunk_size': chunk_size,\n",
    "        'chunk_overlap': chunk_overlap,\n",
    "        'nodes': nodes,\n",
    "        'lc_docs': lc_docs,\n",
    "        'stats': stats\n",
    "    }\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.5 Process Multiple Chunk Sizes\n",
    "\n",
    "# %%\n",
    "def process_multiple_chunk_sizes(\n",
    "    documents: List[Document],\n",
    "    chunk_sizes: List[int],\n",
    "    overlap_percentage: int = 15\n",
    ") -> Dict[int, Dict]:\n",
    "    \"\"\"Process documents for multiple chunk sizes.\"\"\"\n",
    "    print(f\"\\nProcessing {len(chunk_sizes)} chunk size(s)...\")\n",
    "    \n",
    "    processed_data = {}\n",
    "    \n",
    "    for chunk_size in chunk_sizes:\n",
    "        data = process_chunk_size(documents, chunk_size, overlap_percentage)\n",
    "        processed_data[chunk_size] = data\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    for cs, data in processed_data.items():\n",
    "        stats = data['stats']\n",
    "        print(f\"Chunk {cs}: {stats['total_chunks']:,} chunks, \"\n",
    "              f\"~{stats['estimated_tokens']:,} tokens\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.6 Execute Processing\n",
    "\n",
    "# %%\n",
    "# Define which chunk sizes you want to process\n",
    "CHUNK_SIZES = [256, 512, 1024, 2048, 4096]  # Add more as needed: [256, 512, 1024, 2048]\n",
    "\n",
    "# %%\n",
    "# Process all chunk sizes\n",
    "processed_data = process_multiple_chunk_sizes(\n",
    "    documents=documents,\n",
    "    chunk_sizes=CHUNK_SIZES,\n",
    "    overlap_percentage=CHUNK_OVERLAP_PERCENTAGE\n",
    ")\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 3 complete!\")\n",
    "print(f\"Processed chunk sizes: {list(processed_data.keys())}\")\n",
    "print(f\"Total chunks across all sizes: {sum(d['stats']['total_chunks'] for d in processed_data.values()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff374659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SCANNING DATABASES\n",
      "============================================================\n",
      "Location: ../../vector_databases\n",
      "\n",
      "Database: voyage_voyage-3-large\n",
      "  Provider: voyage\n",
      "  Model: voyage-3-large\n",
      "\n",
      "============================================================\n",
      "DATABASE SUMMARY\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VoyageAIEmbeddings' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 138\u001b[39m\n\u001b[32m    134\u001b[39m all_databases = inspect_all_databases(VECTOR_DB_DIR)\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# Display summary\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m \u001b[43mdisplay_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_databases\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Step 4 complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mdisplay_summary\u001b[39m\u001b[34m(databases)\u001b[39m\n\u001b[32m    112\u001b[39m total_colls = \u001b[32m0\u001b[39m\n\u001b[32m    113\u001b[39m total_docs = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m db_name, info \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdatabases\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m():\n\u001b[32m    116\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    117\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Provider: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo[\u001b[33m'\u001b[39m\u001b[33mprovider\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/pydantic/main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'VoyageAIEmbeddings' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 4: Inspect Existing Databases\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.1 Scan All Databases\n",
    "\n",
    "# %%\n",
    "def inspect_all_databases(base_dir: str = \"../../vector_databases\") -> Dict:\n",
    "    \"\"\"Scan and inspect all embedding databases.\"\"\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"No databases found at: {base_dir}\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SCANNING DATABASES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Location: {base_dir}\\n\")\n",
    "    \n",
    "    all_dbs = {}\n",
    "    \n",
    "    for item in os.listdir(base_dir):\n",
    "        item_path = os.path.join(base_dir, item)\n",
    "        if not os.path.isdir(item_path):\n",
    "            continue\n",
    "        \n",
    "        # Parse provider_model format\n",
    "        if '_' not in item:\n",
    "            continue\n",
    "        \n",
    "        parts = item.split('_', 1)\n",
    "        provider = parts[0]\n",
    "        model = parts[1]\n",
    "        \n",
    "        print(f\"Database: {item}\")\n",
    "        print(f\"  Provider: {provider}\")\n",
    "        print(f\"  Model: {model}\")\n",
    "        \n",
    "        # Check for ChromaDB\n",
    "        if not os.path.exists(os.path.join(item_path, \"chroma.sqlite3\")):\n",
    "            print(f\"  Status: Not a valid ChromaDB\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Inspect collections\n",
    "        collections = {}\n",
    "        try:\n",
    "            # Import appropriate embedding\n",
    "            if provider == \"ollama\":\n",
    "                from langchain_ollama import OllamaEmbeddings\n",
    "                emb = OllamaEmbeddings(model=model)\n",
    "            elif provider == \"openai\":\n",
    "                from langchain_openai import OpenAIEmbeddings\n",
    "                emb = OpenAIEmbeddings(model=model)\n",
    "            elif provider == \"voyage\":\n",
    "                from langchain_voyageai import VoyageAIEmbeddings\n",
    "                voyage_api_key = os.getenv(\"VOYAGE_API_KEY\")\n",
    "                return VoyageAIEmbeddings(\n",
    "                    model=model,\n",
    "                    voyage_api_key=voyage_api_key\n",
    "                )\n",
    "            else:\n",
    "                print(f\"  Status: Unknown provider\\n\")\n",
    "                continue\n",
    "            \n",
    "            # Check common chunk sizes\n",
    "            for cs in [128, 256, 512, 1024, 2048]:\n",
    "                coll_name = f\"{COLLECTION_PREFIX}{cs}\"\n",
    "                try:\n",
    "                    vs = Chroma(\n",
    "                        collection_name=coll_name,\n",
    "                        embedding_function=emb,\n",
    "                        persist_directory=item_path\n",
    "                    )\n",
    "                    count = vs._collection.count()\n",
    "                    if count > 0:\n",
    "                        collections[cs] = count\n",
    "                        print(f\"    • Chunk {cs}: {count:,} documents\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            if collections:\n",
    "                all_dbs[item] = {\n",
    "                    'provider': provider,\n",
    "                    'model': model,\n",
    "                    'path': item_path,\n",
    "                    'collections': collections,\n",
    "                    'total_docs': sum(collections.values())\n",
    "                }\n",
    "                print(f\"  Total: {sum(collections.values()):,} documents\\n\")\n",
    "            else:\n",
    "                print(f\"  Status: No collections found\\n\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\\n\")\n",
    "    \n",
    "    return all_dbs\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.2 Display Summary\n",
    "\n",
    "# %%\n",
    "def display_summary(databases: Dict):\n",
    "    \"\"\"Display summary of all databases.\"\"\"\n",
    "    if not databases:\n",
    "        print(\"\\n❌ No databases found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DATABASE SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    total_colls = 0\n",
    "    total_docs = 0\n",
    "    \n",
    "    for db_name, info in databases.items():\n",
    "        print(f\"\\n{db_name}\")\n",
    "        print(f\"  Provider: {info['provider']}\")\n",
    "        print(f\"  Model: {info['model']}\")\n",
    "        print(f\"  Collections: {len(info['collections'])}\")\n",
    "        print(f\"  Documents: {info['total_docs']:,}\")\n",
    "        \n",
    "        total_colls += len(info['collections'])\n",
    "        total_docs += info['total_docs']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Total: {len(databases)} database(s), {total_colls} collection(s), {total_docs:,} documents\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.3 Execute Inspection\n",
    "\n",
    "# %%\n",
    "# Scan all databases\n",
    "all_databases = inspect_all_databases(VECTOR_DB_DIR)\n",
    "\n",
    "# %%\n",
    "# Display summary\n",
    "display_summary(all_databases)\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 4 complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26a3b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 5: Add Embeddings Flexibly\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.1 Helper Functions\n",
    "\n",
    "# %%\n",
    "def get_embedding_function(provider: str, model: str):\n",
    "    \"\"\"Get embedding function for a provider/model.\"\"\"\n",
    "    if provider == \"ollama\":\n",
    "        from langchain_ollama import OllamaEmbeddings\n",
    "        return OllamaEmbeddings(model=model, base_url=OLLAMA_BASE_URL)\n",
    "    elif provider == \"openai\":\n",
    "        from langchain_openai import OpenAIEmbeddings\n",
    "        return OpenAIEmbeddings(model=model, openai_api_key=OPENAI_API_KEY)\n",
    "    elif provider == \"voyage\":\n",
    "        from langchain_voyageai import VoyageAIEmbeddings\n",
    "        return VoyageAIEmbeddings(model=model, voyage_api_key=VOYAGE_API_KEY)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "\n",
    "def get_db_path(base_dir: str, provider: str, model: str) -> str:\n",
    "    \"\"\"Get database path for embedding.\"\"\"\n",
    "    model_id = f\"{provider}_{model.replace('/', '_')}\"\n",
    "    return os.path.join(base_dir, model_id)\n",
    "\n",
    "\n",
    "def check_collection_exists(db_path: str, collection_name: str, embedding_fn) -> Tuple[bool, int]:\n",
    "    \"\"\"Check if collection exists and get count.\"\"\"\n",
    "    if not os.path.exists(db_path):\n",
    "        return False, 0\n",
    "    \n",
    "    try:\n",
    "        vs = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=embedding_fn,\n",
    "            persist_directory=db_path\n",
    "        )\n",
    "        count = vs._collection.count()\n",
    "        return count > 0, count\n",
    "    except Exception:\n",
    "        return False, 0\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.2 Add Single Chunk Size\n",
    "\n",
    "def add_chunk_size_to_embedding(\n",
    "    processed_data: Dict[int, Dict],\n",
    "    chunk_size: int,\n",
    "    embedding_provider: str,\n",
    "    embedding_model: str,\n",
    "    base_db_dir: str = \"../../vector_databases\",\n",
    "    collection_prefix: str = \"financebench_docs_chunk_\",\n",
    "    batch_size: int = 100,\n",
    "    max_tokens_per_batch: int = None,\n",
    "    skip_if_exists: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Add a single chunk size to an embedding database.\n",
    "    \n",
    "    Args:\n",
    "        processed_data: Output from Step 3\n",
    "        chunk_size: Which chunk size (must exist in processed_data)\n",
    "        embedding_provider: \"ollama\" or \"openai\"\n",
    "        embedding_model: Model name\n",
    "        base_db_dir: Base database directory\n",
    "        collection_prefix: Collection name prefix\n",
    "        batch_size: Maximum documents per batch (fallback if max_tokens_per_batch not set)\n",
    "        max_tokens_per_batch: Maximum tokens per batch (overrides batch_size for smart batching)\n",
    "        skip_if_exists: Skip if collection already exists\n",
    "        \n",
    "    Returns:\n",
    "        Statistics dictionary\n",
    "    \"\"\"\n",
    "    # Validate\n",
    "    if chunk_size not in processed_data:\n",
    "        raise ValueError(f\"Chunk size {chunk_size} not in processed_data. \"\n",
    "                        f\"Available: {list(processed_data.keys())}\")\n",
    "    \n",
    "    # Setup\n",
    "    db_path = get_db_path(base_db_dir, embedding_provider, embedding_model)\n",
    "    collection_name = f\"{collection_prefix}{chunk_size}\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ADDING CHUNK SIZE {chunk_size}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Provider: {embedding_provider}\")\n",
    "    print(f\"Model: {embedding_model}\")\n",
    "    print(f\"Database: {db_path}\")\n",
    "    print(f\"Collection: {collection_name}\")\n",
    "    \n",
    "    # Set default token limits for OpenAI\n",
    "    if max_tokens_per_batch is None and embedding_provider == \"openai\":\n",
    "        # Use 250k to leave safety margin below 300k limit\n",
    "        max_tokens_per_batch = 250000\n",
    "        print(f\"Using OpenAI token limit: {max_tokens_per_batch:,} tokens per batch\")\n",
    "    \n",
    "    # Get embedding function\n",
    "    emb_fn = get_embedding_function(embedding_provider, embedding_model)\n",
    "    \n",
    "    # Check if exists\n",
    "    exists, count = check_collection_exists(db_path, collection_name, emb_fn)\n",
    "    if exists and skip_if_exists:\n",
    "        print(f\"\\n✓ Already exists with {count:,} documents - SKIPPING\")\n",
    "        return {\n",
    "            'status': 'skipped',\n",
    "            'chunk_size': chunk_size,\n",
    "            'collection_name': collection_name,\n",
    "            'document_count': count\n",
    "        }\n",
    "    \n",
    "    # Get documents\n",
    "    lc_docs = processed_data[chunk_size]['lc_docs']\n",
    "    print(f\"Documents: {len(lc_docs):,}\")\n",
    "    \n",
    "    # Create database directory\n",
    "    os.makedirs(db_path, exist_ok=True)\n",
    "    \n",
    "    # Initialize vectorstore\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=emb_fn,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    \n",
    "    # Create batches (smart token-aware batching for OpenAI)\n",
    "    if max_tokens_per_batch:\n",
    "        print(f\"\\nUsing token-aware batching (max {max_tokens_per_batch:,} tokens/batch)\")\n",
    "        batches = create_token_aware_batches(lc_docs, max_tokens_per_batch)\n",
    "    else:\n",
    "        print(f\"\\nUsing document-count batching ({batch_size} docs/batch)\")\n",
    "        total = len(lc_docs)\n",
    "        num_batches = (total + batch_size - 1) // batch_size\n",
    "        batches = [lc_docs[i*batch_size:(i+1)*batch_size] for i in range(num_batches)]\n",
    "    \n",
    "    print(f\"Created {len(batches)} batch(es)\")\n",
    "    \n",
    "    # Add in batches\n",
    "    added = 0\n",
    "    failed_batches = []\n",
    "    \n",
    "    with tqdm(total=len(lc_docs), desc=\"Progress\") as pbar:\n",
    "        for i, batch in enumerate(batches):\n",
    "            try:\n",
    "                vectorstore.add_documents(batch)\n",
    "                added += len(batch)\n",
    "                pbar.update(len(batch))\n",
    "            except Exception as e:\n",
    "                print(f\"\\nBatch {i+1} failed: {e}\")\n",
    "                failed_batches.append((i+1, len(batch), str(e)))\n",
    "                pbar.update(len(batch))\n",
    "    \n",
    "    # Persist\n",
    "    vectorstore.persist()\n",
    "    final_count = vectorstore._collection.count()\n",
    "    \n",
    "    # Report\n",
    "    print(f\"\\n✓ Complete: {added:,}/{len(lc_docs):,} added, {final_count:,} final\")\n",
    "    \n",
    "    if failed_batches:\n",
    "        print(f\"\\n⚠ {len(failed_batches)} batch(es) failed:\")\n",
    "        for batch_num, batch_size, error in failed_batches[:3]:\n",
    "            print(f\"  Batch {batch_num} ({batch_size} docs): {error[:100]}\")\n",
    "        if len(failed_batches) > 3:\n",
    "            print(f\"  ... and {len(failed_batches)-3} more\")\n",
    "    \n",
    "    return {\n",
    "        'status': 'completed' if not failed_batches else 'completed_with_errors',\n",
    "        'chunk_size': chunk_size,\n",
    "        'collection_name': collection_name,\n",
    "        'added': added,\n",
    "        'failed': len(failed_batches),\n",
    "        'final_count': final_count\n",
    "    }\n",
    "\n",
    "\n",
    "def create_token_aware_batches(\n",
    "    documents: List[LCDocument],\n",
    "    max_tokens: int,\n",
    "    chars_per_token: float = 4.0\n",
    ") -> List[List[LCDocument]]:\n",
    "    \"\"\"\n",
    "    Create batches that respect token limits.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of documents to batch\n",
    "        max_tokens: Maximum tokens per batch\n",
    "        chars_per_token: Estimated characters per token (default 4.0)\n",
    "        \n",
    "    Returns:\n",
    "        List of document batches\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Estimate tokens for this document\n",
    "        doc_tokens = len(doc.page_content) / chars_per_token\n",
    "        \n",
    "        # If adding this doc would exceed limit, start new batch\n",
    "        if current_batch and (current_tokens + doc_tokens > max_tokens):\n",
    "            batches.append(current_batch)\n",
    "            current_batch = [doc]\n",
    "            current_tokens = doc_tokens\n",
    "        else:\n",
    "            current_batch.append(doc)\n",
    "            current_tokens += doc_tokens\n",
    "    \n",
    "    # Add final batch\n",
    "    if current_batch:\n",
    "        batches.append(current_batch)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "def delete_collection(\n",
    "    embedding_provider: str,\n",
    "    embedding_model: str,\n",
    "    chunk_size: int,\n",
    "    base_db_dir: str = \"../../vector_databases\",\n",
    "    collection_prefix: str = \"financebench_docs_chunk_\"\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Delete a specific collection from a database.\n",
    "    \n",
    "    Args:\n",
    "        embedding_provider: \"ollama\" or \"openai\"\n",
    "        embedding_model: Model name\n",
    "        chunk_size: Chunk size of collection to delete\n",
    "        base_db_dir: Base database directory\n",
    "        collection_prefix: Collection name prefix\n",
    "        \n",
    "    Returns:\n",
    "        True if deleted successfully, False otherwise\n",
    "    \"\"\"\n",
    "    import chromadb\n",
    "    \n",
    "    # Build paths\n",
    "    model_id = f\"{embedding_provider}_{embedding_model.replace('/', '_')}\"\n",
    "    db_path = os.path.join(base_db_dir, model_id)\n",
    "    collection_name = f\"{collection_prefix}{chunk_size}\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DELETING COLLECTION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Provider: {embedding_provider}\")\n",
    "    print(f\"Model: {embedding_model}\")\n",
    "    print(f\"Database: {db_path}\")\n",
    "    print(f\"Collection: {collection_name}\")\n",
    "    \n",
    "    if not os.path.exists(db_path):\n",
    "        print(f\"\\n✗ Database does not exist\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        client = chromadb.PersistentClient(path=db_path)\n",
    "        \n",
    "        # Check if collection exists\n",
    "        existing_collections = [col.name for col in client.list_collections()]\n",
    "        \n",
    "        if collection_name not in existing_collections:\n",
    "            print(f\"\\n✗ Collection does not exist\")\n",
    "            return False\n",
    "        \n",
    "        # Get count before deletion\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "        count = collection.count()\n",
    "        \n",
    "        # Delete\n",
    "        client.delete_collection(name=collection_name)\n",
    "        print(f\"\\n✓ Deleted collection with {count:,} documents\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cde9c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DELETING COLLECTION\n",
      "============================================================\n",
      "Provider: voyage\n",
      "Model: voyage-3-large\n",
      "Database: ../../vector_databases/voyage_voyage-3-large\n",
      "Collection: financebench_docs_chunk_512\n",
      "\n",
      "✓ Deleted collection with 2,041 documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete collection example\n",
    "\n",
    "# delete_collection(\n",
    "#     embedding_provider=\"voyage\",\n",
    "#     embedding_model=\"voyage-3-large\",\n",
    "#     chunk_size=512\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b4e833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 12:45:52,153 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ADDING CHUNK SIZE 2048\n",
      "============================================================\n",
      "Provider: voyage\n",
      "Model: voyage-finance-2\n",
      "Database: ../../vector_databases/voyage_voyage-finance-2\n",
      "Collection: financebench_docs_chunk_2048\n",
      "Documents: 12,099\n",
      "\n",
      "Using document-count batching (1 docs/batch)\n",
      "Created 12099 batch(es)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c464f0c0d8f4e768caa02a2672e2f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/12099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 12:47:05,880 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 132 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 12:48:57,610 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 189 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 12:55:34,029 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 453 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 12:55:56,195 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 470 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 12:55:56,878 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 472 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 12:55:57,381 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 474 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 12:55:58,153 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 476 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 12:55:58,963 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 478 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 12:55:59,477 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 480 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 12:55:59,994 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 482 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 13:17:30,405 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 1333 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 13:30:19,891 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 2112 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 13:40:28,752 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 2663 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 13:40:30,404 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 2667 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 14:42:14,731 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 6371 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:37:30,295 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 9379 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:55:02,046 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 10322 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:59:17,354 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 10543 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 16:00:32,823 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 10610 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 16:13:06,904 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 11378 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 16:19:11,881 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 11736 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 16:23:28,018 - INFO - error_message=\"The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\" message='Voyage API error received'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 11969 failed: The request body is not valid JSON, or some arguments were not specified properly. In particular, Error for argument 'input': Value error, Input cannot contain empty strings or empty lists\n",
      "\n",
      "✓ Complete: 12,077/12,099 added, 12,077 final\n",
      "\n",
      "⚠ 22 batch(es) failed:\n",
      "  Batch 132 (1 docs): The request body is not valid JSON, or some arguments were not specified properly. In particular, Er\n",
      "  Batch 189 (1 docs): The request body is not valid JSON, or some arguments were not specified properly. In particular, Er\n",
      "  Batch 453 (1 docs): The request body is not valid JSON, or some arguments were not specified properly. In particular, Er\n",
      "  ... and 19 more\n",
      "\n",
      "✓ Step 5 complete!\n",
      "Status: completed_with_errors\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 5.3 Example Usage\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Example 1: Add chunk 512 to Ollama\n",
    "\n",
    "# %%\n",
    "# Uncomment to run:\n",
    "# stats = add_chunk_size_to_embedding(\n",
    "#     processed_data=processed_data,\n",
    "#     chunk_size=512,\n",
    "#     embedding_provider=\"ollama\",\n",
    "#     embedding_model=\"nomic-embed-text\",\n",
    "#     skip_if_exists=True\n",
    "# )\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Example 2: Add chunk 1024 to same Ollama database\n",
    "\n",
    "# %%\n",
    "# Uncomment to run:\n",
    "# stats = add_chunk_size_to_embedding(\n",
    "#     processed_data=processed_data,\n",
    "#     chunk_size=1024,\n",
    "#     embedding_provider=\"ollama\",\n",
    "#     embedding_model=\"nomic-embed-text\",\n",
    "#     skip_if_exists=True\n",
    "# )\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Example 3: Add chunk 512 to OpenAI (different database)\n",
    "\n",
    "# %%\n",
    "# Uncomment to run:\n",
    "# stats = add_chunk_size_to_embedding(\n",
    "#     processed_data=processed_data,\n",
    "#     chunk_size=512,\n",
    "#     embedding_provider=\"openai\",\n",
    "#     embedding_model=\"text-embedding-3-small\",\n",
    "#     skip_if_exists=True\n",
    "# )\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.4 Your Turn: Add Your Embeddings\n",
    "\n",
    "# %%\n",
    "# Define what you want to add\n",
    "# Modify these parameters:\n",
    "\n",
    "# EMBED_PROVIDER = \"ollama\"  # or \"openai\" pr \"voyage\"\n",
    "EMBED_PROVIDER = \"voyage\"\n",
    "EMBED_MODEL = \"voyage-3-large\"  # or \"text-embedding-3-small\"\n",
    "CHUNK_TO_ADD = 4096  # Must exist in processed_data\n",
    "\n",
    "\n",
    "stats = add_chunk_size_to_embedding(\n",
    "    processed_data=processed_data,\n",
    "    chunk_size=CHUNK_TO_ADD,\n",
    "    embedding_provider=EMBED_PROVIDER,\n",
    "    embedding_model=EMBED_MODEL,\n",
    "    skip_if_exists=True,\n",
    "    #max_tokens_per_batch=200000,  # Only for OpenAI\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 5 complete!\")\n",
    "print(f\"Status: {stats['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01602be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB Directory: ../../vector_databases\n",
      "\n",
      "============================================================\n",
      "SCANNING DATABASES\n",
      "============================================================\n",
      "Location: ../../vector_databases\n",
      "\n",
      "Database: voyage_voyage-3-large\n",
      "  Provider: voyage\n",
      "  Model: voyage-3-large\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Scan all databases\n",
    "print(f\"Vector DB Directory: {VECTOR_DB_DIR}\")\n",
    "all_databases = inspect_all_databases(VECTOR_DB_DIR)\n",
    "\n",
    "# %%\n",
    "# Display summary\n",
    "display_summary(all_databases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78311713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 6: Query and Test\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.1 Load Vector Store\n",
    "\n",
    "# %%\n",
    "def load_vector_store(\n",
    "    embedding_provider: str,\n",
    "    embedding_model: str,\n",
    "    chunk_size: int,\n",
    "    base_db_dir: str = \"../../vector_databases\",\n",
    "    collection_prefix: str = \"financebench_docs_chunk_\"\n",
    ") -> Chroma:\n",
    "    \"\"\"\n",
    "    Load a vector store for querying.\n",
    "    \n",
    "    Args:\n",
    "        embedding_provider: \"ollama\" or \"openai\"\n",
    "        embedding_model: Model name\n",
    "        chunk_size: Which chunk size collection to load\n",
    "        base_db_dir: Base database directory\n",
    "        collection_prefix: Collection name prefix\n",
    "        \n",
    "    Returns:\n",
    "        Chroma vectorstore instance\n",
    "    \"\"\"\n",
    "    # Get paths\n",
    "    db_path = get_db_path(base_db_dir, embedding_provider, embedding_model)\n",
    "    collection_name = f\"{collection_prefix}{chunk_size}\"\n",
    "    \n",
    "    print(f\"Loading vector store:\")\n",
    "    print(f\"  Provider: {embedding_provider}\")\n",
    "    print(f\"  Model: {embedding_model}\")\n",
    "    print(f\"  Database: {db_path}\")\n",
    "    print(f\"  Collection: {collection_name}\")\n",
    "    \n",
    "    # Get embedding function\n",
    "    emb_fn = get_embedding_function(embedding_provider, embedding_model)\n",
    "    \n",
    "    # Load\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=emb_fn,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    \n",
    "    count = vectorstore._collection.count()\n",
    "    print(f\"  Documents: {count:,}\")\n",
    "    print(\"✓ Loaded\")\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.2 Simple Search\n",
    "\n",
    "# %%\n",
    "def search(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    k: int = 5\n",
    ") -> List:\n",
    "    \"\"\"Perform similarity search.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SEARCH\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Top-{k} results:\\n\")\n",
    "    \n",
    "    docs = vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"[{i}] {doc.page_content[:200]}...\")\n",
    "        if 'file_name' in doc.metadata:\n",
    "            print(f\"    Source: {doc.metadata['file_name']}\")\n",
    "        print()\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.3 Search with Scores\n",
    "\n",
    "# %%\n",
    "def search_with_scores(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    k: int = 5\n",
    ") -> List[Tuple]:\n",
    "    \"\"\"Perform similarity search with relevance scores.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SEARCH WITH SCORES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Top-{k} results:\\n\")\n",
    "    \n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"[{i}] Score: {score:.4f}\")\n",
    "        print(f\"{doc.page_content[:200]}...\")\n",
    "        if 'file_name' in doc.metadata:\n",
    "            print(f\"Source: {doc.metadata['file_name']}\")\n",
    "        if 'page_label' in doc.metadata:\n",
    "            print(f\"Page: {doc.metadata['page_label']}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.4 Compare Chunk Sizes\n",
    "\n",
    "# %%\n",
    "def compare_chunk_sizes(\n",
    "    embedding_provider: str,\n",
    "    embedding_model: str,\n",
    "    query: str,\n",
    "    chunk_sizes: List[int],\n",
    "    k: int = 3,\n",
    "    base_db_dir: str = \"../../vector_databases\"\n",
    "):\n",
    "    \"\"\"Compare retrieval across different chunk sizes.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPARING CHUNK SIZES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    for chunk_size in chunk_sizes:\n",
    "        print(f\"--- Chunk Size: {chunk_size} ---\")\n",
    "        try:\n",
    "            vs = load_vector_store(\n",
    "                embedding_provider, \n",
    "                embedding_model, \n",
    "                chunk_size,\n",
    "                base_db_dir\n",
    "            )\n",
    "            \n",
    "            results = vs.similarity_search_with_score(query, k=k)\n",
    "            if results:\n",
    "                doc, score = results[0]\n",
    "                print(f\"Top result (Score: {score:.4f}):\")\n",
    "                print(f\"{doc.page_content[:250]}...\")\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\\n\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.5 Test with FinanceBench Questions\n",
    "\n",
    "# %%\n",
    "def test_with_dataset_questions(\n",
    "    vectorstore: Chroma,\n",
    "    dataset,\n",
    "    num_questions: int = 3,\n",
    "    k: int = 3\n",
    "):\n",
    "    \"\"\"Test with actual FinanceBench questions.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TESTING WITH FINANCEBENCH QUESTIONS\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    import random\n",
    "    indices = random.sample(range(len(dataset)), num_questions)\n",
    "    \n",
    "    for idx in indices:\n",
    "        record = dataset[idx]\n",
    "        question = record['question']\n",
    "        answer = record['answer']\n",
    "        company = record['company']\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Company: {company}\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected Answer: {answer}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Retrieve\n",
    "        docs = vectorstore.similarity_search(question, k=k)\n",
    "        \n",
    "        print(f\"Retrieved {len(docs)} documents:\\n\")\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            print(f\"[{i}] {doc.page_content[:150]}...\")\n",
    "            if 'file_name' in doc.metadata:\n",
    "                print(f\"    Source: {doc.metadata['file_name']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8869dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 10:57:38,824 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 10:57:38,873 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector store:\n",
      "  Provider: ollama\n",
      "  Model: nomic-embed-text\n",
      "  Database: ../../vector_databases/ollama_nomic-embed-text\n",
      "  Collection: financebench_docs_chunk_512\n",
      "  Documents: 28,657\n",
      "✓ Loaded\n",
      "\n",
      "============================================================\n",
      "SEARCH\n",
      "============================================================\n",
      "Query: What was the capital expenditure in 2018?\n",
      "Top-3 results:\n",
      "\n",
      "[1] Capital Spending\n",
      " \n",
      "Capital spending was $1.6 billion in 2021, an increase of $260 million when compared to 2020. We expect our 2022 capital expenditures to be consistent with 2021.\n",
      "Cash Flows\n",
      " \n",
      "Summar...\n",
      "\n",
      "[2] Total U.S. capital expenditures decreased $478 million for fiscal 2018 , when compared to the previous fiscal year. Capital expenditures related to new stores and\n",
      "clubs, including expansions and reloc...\n",
      "\n",
      "[3] Year Ended December 31,\n",
      " \n",
      " \n",
      " \n",
      "2020\n",
      "  \n",
      "2019\n",
      "  \n",
      "2018\n",
      " \n",
      "Capital expenditures:\n",
      " \n",
      "(In thousands)\n",
      " \n",
      "Las Vegas Strip Resorts\n",
      " \n",
      "$\n",
      "87,511   \n",
      "$\n",
      "285,863   \n",
      "$\n",
      "501,044 \n",
      "Regional Operations\n",
      " \n",
      " \n",
      "41,456   \n",
      " \n",
      "187,489 ...\n",
      "\n",
      "\n",
      "============================================================\n",
      "SEARCH WITH SCORES\n",
      "============================================================\n",
      "Query: What is the total revenue for fiscal year 2022?\n",
      "Top-5 results:\n",
      "\n",
      "[1] Score: 0.4673\n",
      "Our segment revenue and results for fiscal 2022, 2021 and 2020 were as follows:\n",
      "(dollars in millions)\n",
      "Digital \n",
      "Media\n",
      "Digital \n",
      "Experience\n",
      "Publishing and \n",
      "Advertising\n",
      "Total\n",
      "Fiscal 2022\n",
      "Revenue\n",
      "$ \n",
      "12,842...\n",
      "\n",
      "[2] Score: 0.4702\n",
      "Revenue by geographic area for fiscal 2022, 2021 and 2020 were as follows:\n",
      "(in millions)\n",
      "2022\n",
      "2021\n",
      "2020\n",
      "Americas:\n",
      " \n",
      " \n",
      " \n",
      "United States\n",
      "$ \n",
      "9,217 $ \n",
      "8,104 $ \n",
      "6,745 \n",
      "Other\n",
      " \n",
      "1,034  \n",
      "892  \n",
      "709 \n",
      "Total Ameri...\n",
      "\n",
      "[3] Score: 0.5037\n",
      "2023\n",
      " \n",
      "2022\n",
      " \n",
      "2021\n",
      "Revenue by product category\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Domestic:\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Computing and Mobile Phones\n",
      "$\n",
      " 18,191 \n",
      "  \n",
      "$\n",
      " 20,693 \n",
      "  \n",
      "$\n",
      " 19,799 \n",
      " \n",
      "Consumer Electronics\n",
      " \n",
      " 13,040...\n",
      "\n",
      "[4] Score: 0.5073\n",
      "Operating revenues for the Regulated Businesses were $3,505 million for 2022,\n",
      "$3,384 million for 2021 and $3,255 million for 2020, accounting for 92%, 86% and 86%, respectively, of the Company’s total...\n",
      "\n",
      "[5] Score: 0.5109\n",
      "85 | 2022 Annual Report\n",
      "Consolidated Revenue and Operating Margin\n",
      "Year Ended December 31, 2022 Compared to Year Ended December 31, 2021\n",
      "Revenue\n",
      "(in millions)\n",
      "Consolidated Revenue — Revenue increased $...\n",
      "\n",
      "\n",
      "============================================================\n",
      "COMPARING CHUNK SIZES\n",
      "============================================================\n",
      "Query: What were the operating expenses in 2021?\n",
      "\n",
      "--- Chunk Size: 512 ---\n",
      "Loading vector store:\n",
      "  Provider: ollama\n",
      "  Model: nomic-embed-text\n",
      "  Database: ../../vector_databases/ollama_nomic-embed-text\n",
      "  Collection: financebench_docs_chunk_512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 10:57:38,952 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 10:57:39,042 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Documents: 28,657\n",
      "✓ Loaded\n",
      "Top result (Score: 0.4891):\n",
      "Table of Contents\n",
      "Operating Expenses\n",
      "Information about operating expenses is as follows (in millions):\n",
      " \n",
      " \n",
      "Year Ended December 31,\n",
      "  \n",
      "2015\n",
      "2016\n",
      "2017\n",
      "Operating expenses:\n",
      "Cost of sales\n",
      "$\n",
      "71,651\n",
      "$\n",
      "88,265\n",
      "$\n",
      "111,934\n",
      "Fulfillment\n",
      "13,410\n",
      "17,619\n",
      "25,249\n",
      "Market...\n",
      "\n",
      "--- Chunk Size: 1024 ---\n",
      "Loading vector store:\n",
      "  Provider: ollama\n",
      "  Model: nomic-embed-text\n",
      "  Database: ../../vector_databases/ollama_nomic-embed-text\n",
      "  Collection: financebench_docs_chunk_1024\n",
      "  Documents: 0\n",
      "✓ Loaded\n",
      "\n",
      "\n",
      "============================================================\n",
      "TESTING WITH FINANCEBENCH QUESTIONS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Company: Corning\n",
      "Question: Does Corning have positive working capital based on FY2022 data? If working capital is not a useful or relevant metric for this company, then please state that and explain why.\n",
      "Expected Answer: Yes. Corning had a positive working capital amount of $831 million by FY 2022 close. This answer considers only operating current assets and current liabilities that were clearly shown in the balance sheet.\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 10:57:39,128 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 10:57:39,168 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 10:57:39,209 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 documents:\n",
      "\n",
      "[1] Effective January 1, 2019, Corning began using constant-currency reporting for our Environmental Technologies \n",
      "and Life Sciences segments. The Company...\n",
      "[2] Despite the pandemic and resulting global disruptions, Corning adapted rapidly and remained resilient. We acted quickly to preserve our financial stre...\n",
      "[3] Our probability of success increases as we invest in our world-class capabilities.  Corning is concentrating approximately 80% of its research, develo...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Company: Boeing\n",
      "Question: Who are the primary customers of Boeing as of FY2022?\n",
      "Expected Answer: Boeing's primary customers as of FY2022 are a limited number of commercial airlines and the US government. The US government accounted for 40% of Boeing's total revenues in FY2022.\n",
      "============================================================\n",
      "\n",
      "Retrieved 3 documents:\n",
      "\n",
      "[1] We address employee concerns and take appropriate\n",
      "actions that uphold our Boeing values.\n",
      "Competition\n",
      "The commercial jet aircraft market and the airlin...\n",
      "[2] Table of Contents\n",
      "Backlog\n",
      "BGS total backlog of $19,338 million at December 31, 2022 decreased by 6% from $20,496 million at December 31, 2021, primari...\n",
      "[3] Susan Doniz\n",
      "53\n",
      "Chief Information Officer and Senior Vice President, Information Technology & Data Analytics\n",
      "since May 2020. Prior to joining Boeing, M...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Company: General Mills\n",
      "Question: By drawing conclusions from the information stated only in the statement of financial position, what is General Mills's FY2020 working capital ratio? Define working capital ratio as total current assets divided by total current liabilities. Round your answer to two decimal places.\n",
      "Expected Answer: 0.68\n",
      "============================================================\n",
      "\n",
      "Retrieved 3 documents:\n",
      "\n",
      "[1] Balance Sheet:\n",
      "3M’s strong balance sheet and liquidity provide the Company with significant flexibility to fund its numerous opportunities going forwa...\n",
      "[2] Balance Sheet:\n",
      "3M’s strong balance sheet and liquidity provide the Company with significant flexibility to fund its numerous opportunities going forwa...\n",
      "[3] The Company\n",
      "will continue to invest in its operations to drive growth, including continual review of acquisition opportunities.\n",
      " \n",
      "The Company uses wor...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "✓ Step 6 complete!\n",
      "You can now query your RAG system!\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 6.6 Execute Tests\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Load Vector Store\n",
    "\n",
    "# %%\n",
    "# Configure which embedding to test\n",
    "TEST_PROVIDER = \"ollama\"  # or \"openai\"\n",
    "TEST_MODEL = \"nomic-embed-text\"  # or \"text-embedding-3-small\"\n",
    "TEST_CHUNK_SIZE = 512\n",
    "\n",
    "# %%\n",
    "# Load vector store\n",
    "vectorstore = load_vector_store(\n",
    "    embedding_provider=TEST_PROVIDER,\n",
    "    embedding_model=TEST_MODEL,\n",
    "    chunk_size=TEST_CHUNK_SIZE\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Test 1: Simple Search\n",
    "\n",
    "# %%\n",
    "query = \"What was the capital expenditure in 2018?\"\n",
    "docs = search(vectorstore, query, k=3)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Test 2: Search with Scores\n",
    "\n",
    "# %%\n",
    "query = \"What is the total revenue for fiscal year 2022?\"\n",
    "results = search_with_scores(vectorstore, query, k=5)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Test 3: Compare Chunk Sizes\n",
    "\n",
    "# %%\n",
    "# Only works if you have multiple chunk sizes for the same embedding\n",
    "query = \"What were the operating expenses in 2021?\"\n",
    "compare_chunk_sizes(\n",
    "    embedding_provider=TEST_PROVIDER,\n",
    "    embedding_model=TEST_MODEL,\n",
    "    query=query,\n",
    "    chunk_sizes=[512, 1024],  # Adjust based on what you have\n",
    "    k=3\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Test 4: Test with Real Questions\n",
    "\n",
    "# %%\n",
    "test_with_dataset_questions(\n",
    "    vectorstore=vectorstore,\n",
    "    dataset=dataset,\n",
    "    num_questions=3,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.7 Quick Query Function\n",
    "\n",
    "# %%\n",
    "def quick_query(query: str, provider: str = \"openai\", model: str = \"nomic-embed-text\", chunk_size: int = 512, k: int = 5):\n",
    "    \"\"\"Quick helper for ad-hoc queries.\"\"\"\n",
    "    vs = load_vector_store(provider, model, chunk_size)\n",
    "    return search_with_scores(vs, query, k)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Your Custom Queries\n",
    "\n",
    "# %%\n",
    "# Try your own queries here\n",
    "# results = quick_query(\n",
    "#     query=\"Your question here\",\n",
    "#     provider=\"ollama\",\n",
    "#     model=\"nomic-embed-text\",\n",
    "#     chunk_size=512,\n",
    "#     k=5\n",
    "# )\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 6 complete!\")\n",
    "print(\"You can now query your RAG system!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1aa9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INSPECTING METADATA\n",
      "============================================================\n",
      "Provider: ollama\n",
      "Model: nomic-embed-text\n",
      "Database: ../../vector_databases/ollama_nomic-embed-text\n",
      "Collection: financebench_docs_chunk_512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lj/175ptt0d6knb0gg0lg2h4n2h0000gp/T/ipykernel_54799/2740928373.py:62: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n",
      "2025-10-06 19:05:07,954 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 28,657\n",
      "\n",
      "Fetching 5 sample documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 19:05:09,050 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "METADATA ANALYSIS\n",
      "============================================================\n",
      "\n",
      "All metadata keys found: ['chunk_size', 'file_path', 'source', 'total_pages']\n",
      "\n",
      "============================================================\n",
      "SAMPLE DOCUMENTS\n",
      "============================================================\n",
      "\n",
      "[Sample 1]\n",
      "Content preview: Table of Contents\n",
      "The table below presents the estimated maximum potential VAR arising from a one-day loss in fair value for our interest rate, foreig...\n",
      "\n",
      "Metadata:\n",
      "  chunk_size: 512\n",
      "  file_path: ../../financebench/documents/GENERALMILLS_2019_10K.pdf\n",
      "  source: 49\n",
      "  total_pages: 140\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Sample 2]\n",
      "Content preview: The VaR model \n",
      "results across all portfolios are aggregated at the Firm \n",
      "level.\n",
      "As VaR is based on historical data, it is an imperfect \n",
      "measure of mar...\n",
      "\n",
      "Metadata:\n",
      "  chunk_size: 512\n",
      "  file_path: ../../financebench/documents/JPMORGAN_2022_10K.pdf\n",
      "  source: 135\n",
      "  total_pages: 382\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Sample 3]\n",
      "Content preview: ___\n",
      " \n",
      "Indicate by check mark whether the Registrant is a shell company (as defined in Rule 12b-2 of the Act):     Yes              No     X  \n",
      " \n",
      "The ag...\n",
      "\n",
      "Metadata:\n",
      "  chunk_size: 512\n",
      "  file_path: ../../financebench/documents/MGMRESORTS_2018_10K.pdf\n",
      "  source: 1\n",
      "  total_pages: 188\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Sample 4]\n",
      "Content preview: Table of Contents\n",
      "Impairment testing for goodwill is done at a reporting unit level, with all goodwill assigned to a reporting unit. Reporting units a...\n",
      "\n",
      "Metadata:\n",
      "  chunk_size: 512\n",
      "  file_path: ../../financebench/documents/3M_2022_10K.pdf\n",
      "  source: 36\n",
      "  total_pages: 252\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Sample 5]\n",
      "Content preview: Table of Contents\n",
      " \n",
      "PART II\n",
      "Item 7A\n",
      " \n",
      "The following table sets forth the one-day VaR for substantially all of our positions as of June 30, 2016 and 20...\n",
      "\n",
      "Metadata:\n",
      "  chunk_size: 512\n",
      "  file_path: ../../financebench/documents/MICROSOFT_2016_10K.pdf\n",
      "  source: 51\n",
      "  total_pages: 121\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "REQUIRED FIELDS CHECK\n",
      "============================================================\n",
      "✗ file_name: Document name (e.g., 3M_2018_10K)\n",
      "✗ page_label: Page number or label\n",
      "✗ page_number: Page number (alternative field)\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "✗ Collection is missing required metadata\n",
      "  Missing: file_name (document name)\n",
      "  Missing: page_label or page_number\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Test: Check Metadata in ChromaDB Collections\n",
    "# ============================================================================\n",
    "\n",
    "# %%\n",
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Function to Inspect Metadata\n",
    "\n",
    "# %%\n",
    "def inspect_collection_metadata(\n",
    "    embedding_provider: str,\n",
    "    embedding_model: str,\n",
    "    chunk_size: int,\n",
    "    base_db_dir: str = \"../../vector_databases\",\n",
    "    collection_prefix: str = \"financebench_docs_chunk_\",\n",
    "    num_samples: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Inspect metadata from a specific collection.\n",
    "    \n",
    "    Args:\n",
    "        embedding_provider: \"ollama\" or \"openai\"\n",
    "        embedding_model: Model name\n",
    "        chunk_size: Chunk size of collection\n",
    "        base_db_dir: Base directory\n",
    "        collection_prefix: Collection prefix\n",
    "        num_samples: Number of sample documents to inspect\n",
    "    \"\"\"\n",
    "    # Build paths\n",
    "    model_id = f\"{embedding_provider}_{embedding_model.replace('/', '_')}\"\n",
    "    db_path = os.path.join(base_db_dir, model_id)\n",
    "    collection_name = f\"{collection_prefix}{chunk_size}\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"INSPECTING METADATA\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Provider: {embedding_provider}\")\n",
    "    print(f\"Model: {embedding_model}\")\n",
    "    print(f\"Database: {db_path}\")\n",
    "    print(f\"Collection: {collection_name}\")\n",
    "    \n",
    "    if not os.path.exists(db_path):\n",
    "        print(f\"\\n✗ Database does not exist\")\n",
    "        return\n",
    "    \n",
    "    # Get embedding function\n",
    "    if embedding_provider == \"ollama\":\n",
    "        emb_fn = OllamaEmbeddings(model=embedding_model)\n",
    "    elif embedding_provider == \"openai\":\n",
    "        emb_fn = OpenAIEmbeddings(model=embedding_model)\n",
    "    elif embedding_provider == \"voyage\":\n",
    "        emb_fn = VoyageAIEmbeddings(model=embedding_model, voyage_api_key=VOYAGE_API_KEY)   \n",
    "    else:\n",
    "        print(f\"\\n✗ Unknown provider\")\n",
    "        return\n",
    "    \n",
    "    # Load vectorstore\n",
    "    try:\n",
    "        vectorstore = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=emb_fn,\n",
    "            persist_directory=db_path\n",
    "        )\n",
    "        \n",
    "        total_docs = vectorstore._collection.count()\n",
    "        print(f\"Total documents: {total_docs:,}\")\n",
    "        \n",
    "        # Get sample documents\n",
    "        print(f\"\\nFetching {num_samples} sample documents...\")\n",
    "        results = vectorstore.similarity_search(\"sample query\", k=num_samples)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"METADATA ANALYSIS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Collect all unique metadata keys\n",
    "        all_keys = set()\n",
    "        for doc in results:\n",
    "            all_keys.update(doc.metadata.keys())\n",
    "        \n",
    "        print(f\"\\nAll metadata keys found: {sorted(all_keys)}\")\n",
    "        \n",
    "        # Show detailed samples\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"SAMPLE DOCUMENTS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for i, doc in enumerate(results, 1):\n",
    "            print(f\"\\n[Sample {i}]\")\n",
    "            print(f\"Content preview: {doc.page_content[:150]}...\")\n",
    "            print(f\"\\nMetadata:\")\n",
    "            for key, value in sorted(doc.metadata.items()):\n",
    "                # Truncate long values\n",
    "                if isinstance(value, str) and len(value) > 100:\n",
    "                    value = value[:100] + \"...\"\n",
    "                print(f\"  {key}: {value}\")\n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        # Check for required fields\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"REQUIRED FIELDS CHECK\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        required_fields = {\n",
    "            'file_name': 'Document name (e.g., 3M_2018_10K)',\n",
    "            'page_label': 'Page number or label',\n",
    "            'page_number': 'Page number (alternative field)'\n",
    "        }\n",
    "        \n",
    "        for field, description in required_fields.items():\n",
    "            has_field = field in all_keys\n",
    "            symbol = \"✓\" if has_field else \"✗\"\n",
    "            print(f\"{symbol} {field}: {description}\")\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        has_doc_name = 'file_name' in all_keys\n",
    "        has_page_info = 'page_label' in all_keys or 'page_number' in all_keys\n",
    "        \n",
    "        if has_doc_name and has_page_info:\n",
    "            print(\"✓ Collection has required metadata for evaluation\")\n",
    "            print(\"  - Document name: file_name\")\n",
    "            page_field = 'page_label' if 'page_label' in all_keys else 'page_number'\n",
    "            print(f\"  - Page info: {page_field}\")\n",
    "        else:\n",
    "            print(\"✗ Collection is missing required metadata\")\n",
    "            if not has_doc_name:\n",
    "                print(\"  Missing: file_name (document name)\")\n",
    "            if not has_page_info:\n",
    "                print(\"  Missing: page_label or page_number\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error: {e}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Test Your Collections\n",
    "\n",
    "# %%\n",
    "# Test collection 1: Ollama nomic-embed-text, chunk 512\n",
    "inspect_collection_metadata(\n",
    "    embedding_provider=\"ollama\",\n",
    "    embedding_model=\"nomic-embed-text\",\n",
    "    chunk_size=512,\n",
    "    num_samples=5\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Test collection 2: Add more as needed\n",
    "# inspect_collection_metadata(\n",
    "#     embedding_provider=\"ollama\",\n",
    "#     embedding_model=\"nomic-embed-text\",\n",
    "#     chunk_size=1024,\n",
    "#     num_samples=5\n",
    "# )\n",
    "\n",
    "# %%\n",
    "# Test collection 3: OpenAI example\n",
    "# inspect_collection_metadata(\n",
    "#     embedding_provider=\"openai\",\n",
    "#     embedding_model=\"text-embedding-3-small\",\n",
    "#     chunk_size=512,\n",
    "#     num_samples=5\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
