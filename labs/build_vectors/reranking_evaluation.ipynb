{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3fab246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n",
      "✓ OpenAI API key loaded\n",
      "✓ VoyageAI API key loaded\n",
      "✓ Ollama URL: http://localhost:11434\n",
      "✓ Configuration set\n",
      "  Vector DB Directory: ../../vector_databases\n",
      "  Output Directory: ../../evaluation_results/reranking\n",
      "Loading FinanceBench dataset...\n",
      "✓ Loaded 150 queries\n",
      "\n",
      "Sample query:\n",
      "  ID: financebench_id_03029\n",
      "  Company: 3M\n",
      "  Question: What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the quest...\n",
      "  Doc: 3M_2018_10K\n",
      "  Evidence items: 1\n",
      "\n",
      "✓ Step 1 complete!\n",
      "  Environment variables loaded\n",
      "  Paths configured\n",
      "  Dataset loaded: 150 queries\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Re-Ranking Evaluation Notebook - Step 1: Setup and Imports\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # Re-Ranking Evaluation for RAG Systems\n",
    "# \n",
    "# This notebook evaluates the impact of re-ranking on retrieval performance.\n",
    "# It compares baseline retrieval against two-stage retrieval (retrieve + re-rank).\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1.1: Imports\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Progress\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Data\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Vector stores\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "\n",
    "# Re-ranking\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_voyageai import VoyageAIRerank\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1.2: Load Environment Variables\n",
    "\n",
    "# %%\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "VOYAGE_API_KEY = os.getenv(\"VOYAGE_API_KEY\")\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    print(\"✓ OpenAI API key loaded\")\n",
    "else:\n",
    "    print(\"⚠ OpenAI API key not found (only needed if using OpenAI embeddings)\")\n",
    "\n",
    "if VOYAGE_API_KEY:\n",
    "    print(\"✓ VoyageAI API key loaded\")\n",
    "else:\n",
    "    print(\"⚠ VoyageAI API key not found (only needed if using VoyageAI embeddings/reranking)\")\n",
    "\n",
    "print(f\"✓ Ollama URL: {OLLAMA_BASE_URL}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1.3: Configuration Variables\n",
    "\n",
    "# %%\n",
    "# Paths\n",
    "VECTOR_DB_BASE_DIR = \"../../vector_databases\"\n",
    "OUTPUT_DIR = \"../../evaluation_results/reranking\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = \"PatronusAI/financebench\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "\n",
    "# Collection settings\n",
    "COLLECTION_PREFIX = \"financebench_docs_chunk_\"\n",
    "\n",
    "print(\"✓ Configuration set\")\n",
    "print(f\"  Vector DB Directory: {VECTOR_DB_BASE_DIR}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1.4: Load FinanceBench Dataset\n",
    "\n",
    "# %%\n",
    "print(\"Loading FinanceBench dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=DATASET_SPLIT)\n",
    "print(f\"✓ Loaded {len(dataset)} queries\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample query:\")\n",
    "sample = dataset[0]\n",
    "print(f\"  ID: {sample['financebench_id']}\")\n",
    "print(f\"  Company: {sample['company']}\")\n",
    "print(f\"  Question: {sample['question'][:100]}...\")\n",
    "print(f\"  Doc: {sample['doc_name']}\")\n",
    "print(f\"  Evidence items: {len(sample['evidence'])}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 1 complete!\")\n",
    "print(\"  Environment variables loaded\")\n",
    "print(\"  Paths configured\")\n",
    "print(f\"  Dataset loaded: {len(dataset)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d819cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Metadata extraction functions defined\n",
      "✓ Vector store loading functions defined\n",
      "✓ Evidence matching functions defined\n",
      "✓ File management functions defined\n",
      "\n",
      "✓ Step 2 complete!\n",
      "  Metadata extraction: extract_doc_name_from_path, extract_metadata_from_retrieved_doc\n",
      "  Vector store: get_embedding_function, load_vectorstore\n",
      "  Evidence matching: check_match, calculate_mrr_for_query\n",
      "  File management: get_output_filename, check_if_results_exist, save_results\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 2.1: Metadata Extraction\n",
    "\n",
    "# %%\n",
    "def extract_doc_name_from_path(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract document name from file path.\n",
    "    \n",
    "    Example: \"../../financebench/documents/3M_2018_10K.pdf\" -> \"3M_2018_10K\"\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(file_path)\n",
    "    doc_name = filename.replace('.pdf', '')\n",
    "    return doc_name\n",
    "\n",
    "\n",
    "def extract_metadata_from_retrieved_doc(doc) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract relevant metadata from retrieved document.\n",
    "    \n",
    "    Returns dict with: doc_name, page_number, content\n",
    "    \"\"\"\n",
    "    file_path = doc.metadata.get('file_path', '')\n",
    "    doc_name = extract_doc_name_from_path(file_path)\n",
    "    page_num = doc.metadata.get('source', -1)\n",
    "    \n",
    "    # Ensure page_num is an integer\n",
    "    if isinstance(page_num, str):\n",
    "        page_num = int(page_num)\n",
    "    \n",
    "    return {\n",
    "        'doc_name': doc_name,\n",
    "        'page_number': page_num,\n",
    "        'content': doc.page_content\n",
    "    }\n",
    "\n",
    "print(\"✓ Metadata extraction functions defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 2.2: Vector Store Loading\n",
    "\n",
    "# %%\n",
    "def get_embedding_function(provider: str, model: str):\n",
    "    \"\"\"Get embedding function for a provider/model.\"\"\"\n",
    "    if provider == \"ollama\":\n",
    "        return OllamaEmbeddings(model=model, base_url=OLLAMA_BASE_URL)\n",
    "    elif provider == \"openai\":\n",
    "        return OpenAIEmbeddings(model=model, openai_api_key=OPENAI_API_KEY)\n",
    "    elif provider == \"voyage\":\n",
    "        return VoyageAIEmbeddings(model=model, voyage_api_key=VOYAGE_API_KEY)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "\n",
    "def load_vectorstore(\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    base_dir: str = VECTOR_DB_BASE_DIR,\n",
    "    collection_prefix: str = COLLECTION_PREFIX\n",
    ") -> Chroma:\n",
    "    \"\"\"\n",
    "    Load a vector store.\n",
    "    \n",
    "    Args:\n",
    "        provider: \"ollama\", \"openai\", or \"voyage\"\n",
    "        model: Model name\n",
    "        chunk_size: Chunk size (must match existing collection)\n",
    "        base_dir: Base directory for vector databases\n",
    "        collection_prefix: Collection name prefix\n",
    "        \n",
    "    Returns:\n",
    "        Chroma vectorstore instance\n",
    "    \"\"\"\n",
    "    model_id = f\"{provider}_{model.replace('/', '_')}\"\n",
    "    db_path = os.path.join(base_dir, model_id)\n",
    "    collection_name = f\"{collection_prefix}{chunk_size}\"\n",
    "    \n",
    "    emb_fn = get_embedding_function(provider, model)\n",
    "    \n",
    "    vectorstore = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=emb_fn,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "print(\"✓ Vector store loading functions defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 2.3: Evidence Matching\n",
    "\n",
    "# %%\n",
    "def check_match(\n",
    "    retrieved_doc: Dict, \n",
    "    evidence_list: List[Dict],\n",
    "    chunk_size: int = 512,\n",
    "    use_page_tolerance: bool = True\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Check if retrieved document matches any evidence.\n",
    "    \n",
    "    Uses chunk-size-aware page tolerance:\n",
    "    - chunk_size <= 512: tolerance = 0 (exact match)\n",
    "    - chunk_size 513-1024: tolerance = 1\n",
    "    - chunk_size 1025-2048: tolerance = 2\n",
    "    - chunk_size > 2048: tolerance = 2\n",
    "    \n",
    "    Args:\n",
    "        retrieved_doc: {doc_name, page_number}\n",
    "        evidence_list: Ground truth evidence\n",
    "        chunk_size: Chunk size (for page tolerance calculation)\n",
    "        use_page_tolerance: If True, use chunk-size-aware tolerance\n",
    "        \n",
    "    Returns:\n",
    "        True if match found\n",
    "    \"\"\"\n",
    "    retrieved_doc_name = retrieved_doc['doc_name']\n",
    "    retrieved_page = retrieved_doc['page_number']\n",
    "    \n",
    "    # Calculate page tolerance based on chunk size\n",
    "    if use_page_tolerance:\n",
    "        if chunk_size <= 512:\n",
    "            page_tolerance = 0\n",
    "        elif chunk_size <= 1024:\n",
    "            page_tolerance = 1\n",
    "        elif chunk_size <= 2048:\n",
    "            page_tolerance = 2\n",
    "        else:\n",
    "            page_tolerance = 2\n",
    "    else:\n",
    "        page_tolerance = 0  # Exact match only\n",
    "    \n",
    "    for evidence in evidence_list:\n",
    "        evidence_doc_name = evidence['doc_name']\n",
    "        evidence_page = evidence['evidence_page_num'] + 1  # Convert 0-indexed to 1-indexed\n",
    "        \n",
    "        # Check document name match\n",
    "        if retrieved_doc_name != evidence_doc_name:\n",
    "            continue\n",
    "        \n",
    "        # Check page match with tolerance\n",
    "        # Only match if retrieved page is BEFORE or AT evidence page\n",
    "        if retrieved_page <= evidence_page <= retrieved_page + page_tolerance:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def calculate_mrr_for_query(\n",
    "    retrieved_docs: List[Dict], \n",
    "    evidence_list: List[Dict],\n",
    "    chunk_size: int = 512,\n",
    "    use_page_tolerance: bool = True\n",
    ") -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    Calculate MRR for a single query.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs: List of {doc_name, page_number}\n",
    "        evidence_list: Ground truth evidence\n",
    "        chunk_size: Chunk size (for page tolerance)\n",
    "        use_page_tolerance: If True, use chunk-size-aware tolerance\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (mrr_score, rank)\n",
    "        - mrr_score: 1/rank if found, 0 if not found\n",
    "        - rank: Position of first match (1-indexed), -1 if not found\n",
    "    \"\"\"\n",
    "    for rank, retrieved_doc in enumerate(retrieved_docs, start=1):\n",
    "        if check_match(retrieved_doc, evidence_list, chunk_size, use_page_tolerance):\n",
    "            mrr_score = 1.0 / rank\n",
    "            return mrr_score, rank\n",
    "    \n",
    "    # No match found\n",
    "    return 0.0, -1\n",
    "\n",
    "print(\"✓ Evidence matching functions defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 2.4: File Management\n",
    "\n",
    "# %%\n",
    "def get_reranker_short_name(reranker_model: str) -> str:\n",
    "    \"\"\"\n",
    "    Get short name for reranker model for filename.\n",
    "    \n",
    "    Examples:\n",
    "        'cross-encoder/ms-marco-MiniLM-L-12-v2' -> 'ms-marco-L-12'\n",
    "        'voyage-rerank-2' -> 'voyage-rerank-2'\n",
    "    \"\"\"\n",
    "    name_mapping = {\n",
    "        'cross-encoder/ms-marco-MiniLM-L-6-v2': 'ms-marco-L-6',\n",
    "        'cross-encoder/ms-marco-MiniLM-L-12-v2': 'ms-marco-L-12',\n",
    "        'BAAI/bge-reranker-base': 'bge-base',\n",
    "        'BAAI/bge-reranker-large': 'bge-large',\n",
    "        'voyage-rerank-2': 'voyage-rerank-2',\n",
    "        'voyage-rerank-2-lite': 'voyage-rerank-2-lite',\n",
    "    }\n",
    "    return name_mapping.get(reranker_model, reranker_model.replace('/', '_'))\n",
    "\n",
    "\n",
    "def get_output_filename(\n",
    "    provider: str, \n",
    "    model: str, \n",
    "    chunk_size: int, \n",
    "    k: int, \n",
    "    mode: str,\n",
    "    reranker_model: Optional[str] = None,\n",
    "    k_rerank: Optional[int] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate output filename.\n",
    "    \n",
    "    Examples:\n",
    "        Baseline: \"voyage_voyage-finance-2_chunk512_k40_global.json\"\n",
    "        Rerank:   \"voyage_voyage-finance-2_chunk512_k40_global_rerank_k10-ms-marco-L-12.json\"\n",
    "    \n",
    "    Args:\n",
    "        provider: Embedding provider\n",
    "        model: Embedding model name\n",
    "        chunk_size: Chunk size\n",
    "        k: k_retrieve for baseline, or k_retrieve for rerank\n",
    "        mode: \"global\" or \"singledoc\"\n",
    "        reranker_model: Re-ranker model name (None for baseline)\n",
    "        k_rerank: Number kept after re-ranking (only for rerank files)\n",
    "    \"\"\"\n",
    "    model_clean = model.replace('/', '_')\n",
    "    \n",
    "    if reranker_model:\n",
    "        # Re-ranking file: show both k_retrieve and k_rerank\n",
    "        reranker_short = get_reranker_short_name(reranker_model)\n",
    "        filename = f\"{provider}_{model_clean}_chunk{chunk_size}_k{k}_{mode}_rerank_k{k_rerank}-{reranker_short}.json\"\n",
    "    else:\n",
    "        # Baseline file: only show k_retrieve\n",
    "        filename = f\"{provider}_{model_clean}_chunk{chunk_size}_k{k}_{mode}.json\"\n",
    "    \n",
    "    return filename\n",
    "\n",
    "\n",
    "def check_if_results_exist(\n",
    "    provider: str, \n",
    "    model: str, \n",
    "    chunk_size: int, \n",
    "    k: int, \n",
    "    mode: str,\n",
    "    output_dir: str,\n",
    "    reranker_model: Optional[str] = None,\n",
    "    k_rerank: Optional[int] = None\n",
    ") -> bool:\n",
    "    \"\"\"Check if results JSON already exists.\"\"\"\n",
    "    filename = get_output_filename(provider, model, chunk_size, k, mode, reranker_model, k_rerank)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    return os.path.exists(filepath)\n",
    "\n",
    "\n",
    "def save_results(\n",
    "    results: List[Dict], \n",
    "    provider: str, \n",
    "    model: str, \n",
    "    chunk_size: int, \n",
    "    k: int, \n",
    "    mode: str,\n",
    "    output_dir: str,\n",
    "    reranker_model: Optional[str] = None,\n",
    "    k_rerank: Optional[int] = None\n",
    "):\n",
    "    \"\"\"Save results to JSON file.\"\"\"\n",
    "    filename = get_output_filename(provider, model, chunk_size, k, mode, reranker_model, k_rerank)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Saved: {filename}\")\n",
    "\n",
    "print(\"✓ File management functions defined\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 2 complete!\")\n",
    "print(\"  Metadata extraction: extract_doc_name_from_path, extract_metadata_from_retrieved_doc\")\n",
    "print(\"  Vector store: get_embedding_function, load_vectorstore\")\n",
    "print(\"  Evidence matching: check_match, calculate_mrr_for_query\")\n",
    "print(\"  File management: get_output_filename, check_if_results_exist, save_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8acb100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Global retrieval function defined\n",
      "✓ Single-document retrieval function defined\n",
      "✓ Unified retrieval interface defined\n",
      "\\nTesting retrieval functions...\n",
      "Loading vectorstore: voyage/voyage-3-large, chunk=512\n",
      "\\nTest query: What was the capital expenditure in 2018?\n",
      "\\n--- Global Retrieval (k=5) ---\n",
      "1. BOEING_2018_10K, page 39, score: 0.7928\n",
      "2. CVSHEALTH_2018_10K, page 280, score: 0.8283\n",
      "3. MGMRESORTS_2018_10K, page 46, score: 0.8411\n",
      "4. ACTIVISIONBLIZZARD_2019_10K, page 52, score: 0.8509\n",
      "5. CORNING_2020_10K, page 126, score: 0.8647\n",
      "\\n--- Single-Doc Retrieval (k=5, doc='3M_2018_10K') ---\n",
      "1. 3M_2018_10K, page 39, score: 0.9703\n",
      "2. 3M_2018_10K, page 47, score: 0.9908\n",
      "3. 3M_2018_10K, page 109, score: 1.0054\n",
      "4. 3M_2018_10K, page 81, score: 1.0193\n",
      "\\n✓ Retrieval functions working correctly!\n",
      "\n",
      "✓ Step 3 complete!\n",
      "  Global retrieval: retrieve_global_with_scores\n",
      "  Single-doc retrieval: retrieve_single_doc_with_scores\n",
      "  Unified interface: retrieve_with_scores\n",
      "\n",
      "These functions return documents with similarity scores that will be\n",
      "preserved as 'initial_score' when we apply re-ranking in Step 4.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Baseline Retrieval Functions\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3.1: Global Retrieval with Scores\n",
    "\n",
    "# %%\n",
    "def retrieve_global_with_scores(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve documents globally (search all documents) with similarity scores.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: ChromaDB vectorstore instance\n",
    "        query: Search query\n",
    "        k: Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        List of {doc_name, page_number, content, rank, score}\n",
    "    \"\"\"\n",
    "    # Use similarity_search_with_score to get scores\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    retrieved = []\n",
    "    for rank, (doc, score) in enumerate(results, start=1):\n",
    "        metadata = extract_metadata_from_retrieved_doc(doc)\n",
    "        metadata['rank'] = rank\n",
    "        metadata['score'] = float(score)\n",
    "        retrieved.append(metadata)\n",
    "    \n",
    "    return retrieved\n",
    "\n",
    "print(\"✓ Global retrieval function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3.2: Single-Document Retrieval with Scores\n",
    "\n",
    "# %%\n",
    "def retrieve_single_doc_with_scores(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    target_doc_name: str,\n",
    "    k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve documents filtered to single document with scores.\n",
    "    \n",
    "    Since ChromaDB doesn't support filtering by document name directly,\n",
    "    we retrieve more documents (k * 10) and filter them post-retrieval.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: ChromaDB vectorstore instance\n",
    "        query: Search query\n",
    "        target_doc_name: Document name to filter to (e.g., \"3M_2018_10K\")\n",
    "        k: Number of documents to return after filtering\n",
    "        \n",
    "    Returns:\n",
    "        List of {doc_name, page_number, content, rank, score}\n",
    "    \"\"\"\n",
    "    # Retrieve more documents than needed to ensure we get enough from target doc\n",
    "    fetch_k = min(k * 10, 100)  # Fetch up to 10x k, max 100\n",
    "    \n",
    "    results = vectorstore.similarity_search_with_score(query, k=fetch_k)\n",
    "    \n",
    "    # Filter to only documents from target doc\n",
    "    filtered = []\n",
    "    for doc, score in results:\n",
    "        metadata = extract_metadata_from_retrieved_doc(doc)\n",
    "        if metadata['doc_name'] == target_doc_name:\n",
    "            metadata['score'] = float(score)\n",
    "            filtered.append(metadata)\n",
    "            if len(filtered) >= k:\n",
    "                break\n",
    "    \n",
    "    # Add rank after filtering\n",
    "    for rank, doc_meta in enumerate(filtered[:k], start=1):\n",
    "        doc_meta['rank'] = rank\n",
    "    \n",
    "    # Return top k from target document\n",
    "    return filtered[:k]\n",
    "\n",
    "print(\"✓ Single-document retrieval function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3.3: Unified Retrieval Interface\n",
    "\n",
    "# %%\n",
    "def retrieve_with_scores(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    k: int,\n",
    "    mode: str,\n",
    "    target_doc_name: Optional[str] = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Unified interface for retrieval with scores.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: ChromaDB vectorstore instance\n",
    "        query: Search query\n",
    "        k: Number of documents to retrieve\n",
    "        mode: \"global\" or \"singledoc\"\n",
    "        target_doc_name: Required if mode is \"singledoc\"\n",
    "        \n",
    "    Returns:\n",
    "        List of {doc_name, page_number, content, rank, score}\n",
    "    \"\"\"\n",
    "    if mode == \"global\":\n",
    "        return retrieve_global_with_scores(vectorstore, query, k)\n",
    "    elif mode == \"singledoc\":\n",
    "        if not target_doc_name:\n",
    "            raise ValueError(\"target_doc_name required for singledoc mode\")\n",
    "        return retrieve_single_doc_with_scores(vectorstore, query, target_doc_name, k)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}. Must be 'global' or 'singledoc'\")\n",
    "\n",
    "print(\"✓ Unified retrieval interface defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3.4: Test Retrieval (Optional)\n",
    "\n",
    "# %%\n",
    "# Uncomment to test retrieval functions\n",
    "\n",
    "# Test with a sample configuration\n",
    "test_provider = \"voyage\"\n",
    "test_model = \"voyage-3-large\"\n",
    "test_chunk_size = 512\n",
    "\n",
    "print(\"\\\\nTesting retrieval functions...\")\n",
    "print(f\"Loading vectorstore: {test_provider}/{test_model}, chunk={test_chunk_size}\")\n",
    "\n",
    "try:\n",
    "    test_vs = load_vectorstore(test_provider, test_model, test_chunk_size)\n",
    "    test_query = \"What was the capital expenditure in 2018?\"\n",
    "    \n",
    "    print(f\"\\\\nTest query: {test_query}\")\n",
    "    \n",
    "    # Test global retrieval\n",
    "    print(\"\\\\n--- Global Retrieval (k=5) ---\")\n",
    "    results_global = retrieve_global_with_scores(test_vs, test_query, k=5)\n",
    "    for i, doc in enumerate(results_global, 1):\n",
    "        print(f\"{i}. {doc['doc_name']}, page {doc['page_number']}, score: {doc['score']:.4f}\")\n",
    "    \n",
    "    # Test single-doc retrieval\n",
    "    print(\"\\\\n--- Single-Doc Retrieval (k=5, doc='3M_2018_10K') ---\")\n",
    "    results_single = retrieve_single_doc_with_scores(test_vs, test_query, \"3M_2018_10K\", k=5)\n",
    "    for i, doc in enumerate(results_single, 1):\n",
    "        print(f\"{i}. {doc['doc_name']}, page {doc['page_number']}, score: {doc['score']:.4f}\")\n",
    "    \n",
    "    print(\"\\\\n✓ Retrieval functions working correctly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Test failed: {e}\")\n",
    "\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 3 complete!\")\n",
    "print(\"  Global retrieval: retrieve_global_with_scores\")\n",
    "print(\"  Single-doc retrieval: retrieve_single_doc_with_scores\")\n",
    "print(\"  Unified interface: retrieve_with_scores\")\n",
    "print(\"\\nThese functions return documents with similarity scores that will be\")\n",
    "print(\"preserved as 'initial_score' when we apply re-ranking in Step 4.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e3d4d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cross-encoder re-ranking function defined\n",
      "✓ Voyage AI re-ranking function defined\n",
      "✓ Unified re-ranking interface defined\n",
      "\\nTesting re-ranking functions...\n",
      "Embedding: voyage/voyage-finance-2, chunk=512\n",
      "Re-ranker: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "\\nTest query: What was the capital expenditure in 2018?\n",
      "\\n--- Step 1: Baseline Retrieval (k=20) ---\n",
      "Top 5 from baseline:\n",
      "1. CORNING_2020_10K, page 126, score: 1.0388\n",
      "2. 3M_2018_10K, page 39, score: 1.0564\n",
      "3. 3M_2018_10K, page 126, score: 1.0647\n",
      "4. WALMART_2020_10K, page 81, score: 1.0980\n",
      "5. WALMART_2018_10K, page 86, score: 1.1052\n",
      "\\n--- Step 2: Re-Ranking (top_k=10) ---\n",
      "Top 5 after re-ranking:\n",
      "1. MGMRESORTS_2018_10K, page 46\n",
      "   Initial rank: 19, Initial score: 1.1713\n",
      "   Rerank score: 7.0702\n",
      "   Rank change: +18\n",
      "2. BESTBUY_2017_10K, page 45\n",
      "   Initial rank: 16, Initial score: 1.1660\n",
      "   Rerank score: 6.5696\n",
      "   Rank change: +14\n",
      "3. BESTBUY_2019_10K, page 42\n",
      "   Initial rank: 11, Initial score: 1.1458\n",
      "   Rerank score: 6.4908\n",
      "   Rank change: +8\n",
      "4. 3M_2018_10K, page 39\n",
      "   Initial rank: 2, Initial score: 1.0564\n",
      "   Rerank score: 5.5597\n",
      "   Rank change: -2\n",
      "5. CVSHEALTH_2018_10K, page 280\n",
      "   Initial rank: 6, Initial score: 1.1070\n",
      "   Rerank score: 5.4722\n",
      "   Rank change: +1\n",
      "\\n✓ Re-ranking functions working correctly!\n",
      "\n",
      "✓ Step 4 complete!\n",
      "  Cross-encoder re-ranking: rerank_with_cross_encoder\n",
      "  Voyage AI re-ranking: rerank_with_voyage\n",
      "  Unified interface: rerank_documents\n",
      "\n",
      "Supported re-ranker models:\n",
      "  • Cross-encoders: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "                    cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "                    BAAI/bge-reranker-base\n",
      "                    BAAI/bge-reranker-large\n",
      "  • Voyage AI:      voyage-rerank-2\n",
      "                    voyage-rerank-2-lite\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 4: Re-Ranking Functions\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 4.1: Cross-Encoder Re-Ranking (Hugging Face Models)\n",
    "\n",
    "# %%\n",
    "def rerank_with_cross_encoder(\n",
    "    query: str,\n",
    "    retrieved_docs: List[Dict],\n",
    "    reranker_model: str,\n",
    "    top_k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Re-rank documents using a cross-encoder model from Hugging Face.\n",
    "    \n",
    "    Cross-encoders process query and document together, providing more\n",
    "    accurate relevance scores than bi-encoders (embeddings).\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        retrieved_docs: List from baseline retrieval (with 'content', 'rank', 'score')\n",
    "        reranker_model: Hugging Face model name (e.g., 'cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "        top_k: Number of documents to return after re-ranking\n",
    "        \n",
    "    Returns:\n",
    "        Re-ranked list with {doc_name, page_number, rank, initial_rank, initial_score, rerank_score}\n",
    "    \"\"\"\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    \n",
    "    # Initialize cross-encoder\n",
    "    cross_encoder = CrossEncoder(reranker_model)\n",
    "    \n",
    "    # Prepare query-document pairs for cross-encoder\n",
    "    pairs = [[query, doc.get('content', '')] for doc in retrieved_docs]\n",
    "    \n",
    "    # Get cross-encoder relevance scores\n",
    "    rerank_scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Preserve initial ranking information and add rerank scores\n",
    "    for doc, rerank_score in zip(retrieved_docs, rerank_scores):\n",
    "        doc['initial_rank'] = doc['rank']\n",
    "        doc['initial_score'] = doc['score']\n",
    "        doc['rerank_score'] = float(rerank_score)\n",
    "        # Remove temporary fields\n",
    "        del doc['rank']\n",
    "        del doc['score']\n",
    "        # Remove content to save space in JSON (optional)\n",
    "        if 'content' in doc:\n",
    "            del doc['content']\n",
    "    \n",
    "    # Sort by rerank score (descending - higher is more relevant)\n",
    "    reranked = sorted(retrieved_docs, key=lambda x: x['rerank_score'], reverse=True)\n",
    "    \n",
    "    # Assign new ranks\n",
    "    for rank, doc in enumerate(reranked[:top_k], start=1):\n",
    "        doc['rank'] = rank\n",
    "    \n",
    "    return reranked[:top_k]\n",
    "\n",
    "print(\"✓ Cross-encoder re-ranking function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 4.2: Voyage AI Re-Ranking (API-based)\n",
    "\n",
    "# %%\n",
    "def rerank_with_voyage(\n",
    "    query: str,\n",
    "    retrieved_docs: List[Dict],\n",
    "    reranker_model: str,\n",
    "    top_k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Re-rank documents using Voyage AI reranker API.\n",
    "    \n",
    "    Voyage provides specialized rerankers optimized for different domains.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        retrieved_docs: List from baseline retrieval (with 'content', 'rank', 'score')\n",
    "        reranker_model: Voyage model name (e.g., 'voyage-rerank-2', 'voyage-rerank-2-lite')\n",
    "        top_k: Number of documents to return after re-ranking\n",
    "        \n",
    "    Returns:\n",
    "        Re-ranked list with {doc_name, page_number, rank, initial_rank, initial_score, rerank_score}\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document\n",
    "    from langchain_voyageai import VoyageAIRerank\n",
    "    \n",
    "    # Convert to LangChain documents (required by VoyageAIRerank)\n",
    "    lc_docs = [\n",
    "        Document(\n",
    "            page_content=doc['content'],\n",
    "            metadata={\n",
    "                'doc_name': doc['doc_name'], \n",
    "                'page_number': doc['page_number'],\n",
    "                'initial_rank': doc['rank'],\n",
    "                'initial_score': doc['score']\n",
    "            }\n",
    "        )\n",
    "        for doc in retrieved_docs\n",
    "    ]\n",
    "    \n",
    "    # Initialize Voyage reranker\n",
    "    # Extract model name (e.g., \"rerank-2\" from \"voyage-rerank-2\")\n",
    "    model_name = reranker_model.replace('voyage-', '')\n",
    "    \n",
    "    reranker = VoyageAIRerank(\n",
    "        model=model_name,\n",
    "        voyage_api_key=VOYAGE_API_KEY,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    # Rerank documents\n",
    "    reranked_docs = reranker.compress_documents(lc_docs, query)\n",
    "    \n",
    "    # Convert back to our format\n",
    "    results = []\n",
    "    for rank, doc in enumerate(reranked_docs, start=1):\n",
    "        result = {\n",
    "            'doc_name': doc.metadata['doc_name'],\n",
    "            'page_number': doc.metadata['page_number'],\n",
    "            'rank': rank,\n",
    "            'initial_rank': doc.metadata['initial_rank'],\n",
    "            'initial_score': doc.metadata['initial_score'],\n",
    "            'rerank_score': doc.metadata.get('relevance_score', 0.0)\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Voyage AI re-ranking function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 4.3: Unified Re-Ranking Interface\n",
    "\n",
    "# %%\n",
    "def rerank_documents(\n",
    "    query: str,\n",
    "    retrieved_docs: List[Dict],\n",
    "    reranker_model: str,\n",
    "    top_k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Unified interface for re-ranking documents.\n",
    "    \n",
    "    Automatically selects the appropriate re-ranker based on model name:\n",
    "    - Voyage models (starting with 'voyage-'): Use Voyage API\n",
    "    - Others: Use Hugging Face cross-encoders\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        retrieved_docs: List from baseline retrieval\n",
    "        reranker_model: Model name (e.g., 'voyage-rerank-2' or 'cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "        top_k: Number of documents to return after re-ranking\n",
    "        \n",
    "    Returns:\n",
    "        Re-ranked list with scores and ranking information\n",
    "    \"\"\"\n",
    "    if reranker_model.startswith('voyage-'):\n",
    "        return rerank_with_voyage(query, retrieved_docs, reranker_model, top_k)\n",
    "    else:\n",
    "        return rerank_with_cross_encoder(query, retrieved_docs, reranker_model, top_k)\n",
    "\n",
    "print(\"✓ Unified re-ranking interface defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 4.4: Test Re-Ranking (Optional)\n",
    "\n",
    "# %%\n",
    "# Uncomment to test re-ranking functions\n",
    "\n",
    "# Test with a sample configuration\n",
    "test_provider = \"voyage\"\n",
    "test_model = \"voyage-finance-2\"\n",
    "test_chunk_size = 512\n",
    "test_reranker = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"  # or \"voyage-rerank-2\"\n",
    "\n",
    "print(\"\\\\nTesting re-ranking functions...\")\n",
    "print(f\"Embedding: {test_provider}/{test_model}, chunk={test_chunk_size}\")\n",
    "print(f\"Re-ranker: {test_reranker}\")\n",
    "\n",
    "try:\n",
    "    # Load vectorstore and retrieve\n",
    "    test_vs = load_vectorstore(test_provider, test_model, test_chunk_size)\n",
    "    test_query = \"What was the capital expenditure in 2018?\"\n",
    "    \n",
    "    print(f\"\\\\nTest query: {test_query}\")\n",
    "    print(\"\\\\n--- Step 1: Baseline Retrieval (k=20) ---\")\n",
    "    \n",
    "    baseline_results = retrieve_global_with_scores(test_vs, test_query, k=20)\n",
    "    \n",
    "    print(\"Top 5 from baseline:\")\n",
    "    for i, doc in enumerate(baseline_results[:5], 1):\n",
    "        print(f\"{i}. {doc['doc_name']}, page {doc['page_number']}, score: {doc['score']:.4f}\")\n",
    "    \n",
    "    print(\"\\\\n--- Step 2: Re-Ranking (top_k=10) ---\")\n",
    "    \n",
    "    reranked_results = rerank_documents(\n",
    "        query=test_query,\n",
    "        retrieved_docs=baseline_results,\n",
    "        reranker_model=test_reranker,\n",
    "        top_k=10\n",
    "    )\n",
    "    \n",
    "    print(\"Top 5 after re-ranking:\")\n",
    "    for i, doc in enumerate(reranked_results[:5], 1):\n",
    "        print(f\"{i}. {doc['doc_name']}, page {doc['page_number']}\")\n",
    "        print(f\"   Initial rank: {doc['initial_rank']}, Initial score: {doc['initial_score']:.4f}\")\n",
    "        print(f\"   Rerank score: {doc['rerank_score']:.4f}\")\n",
    "        print(f\"   Rank change: {doc['initial_rank'] - doc['rank']:+d}\")\n",
    "    \n",
    "    print(\"\\\\n✓ Re-ranking functions working correctly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 4 complete!\")\n",
    "print(\"  Cross-encoder re-ranking: rerank_with_cross_encoder\")\n",
    "print(\"  Voyage AI re-ranking: rerank_with_voyage\")\n",
    "print(\"  Unified interface: rerank_documents\")\n",
    "print(\"\\nSupported re-ranker models:\")\n",
    "print(\"  • Cross-encoders: cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "print(\"                    cross-encoder/ms-marco-MiniLM-L-12-v2\")\n",
    "print(\"                    BAAI/bge-reranker-base\")\n",
    "print(\"                    BAAI/bge-reranker-large\")\n",
    "print(\"  • Voyage AI:      voyage-rerank-2\")\n",
    "print(\"                    voyage-rerank-2-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4261c700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Baseline evaluation function defined\n",
      "✓ Re-ranking evaluation function defined\n",
      "✓ Batch evaluation helper defined\n",
      "\n",
      "✓ Step 5 complete!\n",
      "  Baseline evaluation: evaluate_baseline\n",
      "  Re-ranking evaluation: evaluate_with_reranking\n",
      "  Batch helper: evaluate_single_configuration\n",
      "\n",
      "These functions will:\n",
      "  • Skip existing results automatically\n",
      "  • Calculate MRR for both baseline and re-ranked results\n",
      "  • Track improvements/degradations per query\n",
      "  • Save comprehensive JSON with all statistics\n",
      "\n",
      "Configuration structure:\n",
      "  {\n",
      "    'provider': 'voyage',\n",
      "    'model': 'voyage-finance-2',\n",
      "    'chunk_sizes': [512, 1024],\n",
      "    'k_retrieve': 40,  # Retrieve 40 for baseline\n",
      "    'k_rerank': 10,    # Keep top 10 after re-ranking\n",
      "    'reranker_models': ['cross-encoder/ms-marco-MiniLM-L-12-v2']\n",
      "  }\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 5: Evaluation Functions\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 5.1: Baseline Evaluation (No Re-Ranking)\n",
    "\n",
    "# %%\n",
    "def evaluate_baseline(\n",
    "    dataset,\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k_retrieve: int,\n",
    "    mode: str,\n",
    "    use_page_tolerance: bool = True,\n",
    "    output_dir: str = OUTPUT_DIR\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate baseline retrieval without re-ranking.\n",
    "    \n",
    "    This creates the baseline results that we'll compare against re-ranking.\n",
    "    \n",
    "    Args:\n",
    "        dataset: FinanceBench dataset\n",
    "        provider: \"ollama\", \"openai\", or \"voyage\"\n",
    "        model: Embedding model name\n",
    "        chunk_size: Chunk size\n",
    "        k_retrieve: Number of documents to retrieve\n",
    "        mode: \"global\" or \"singledoc\"\n",
    "        use_page_tolerance: If True, use chunk-size-aware page tolerance\n",
    "        output_dir: Output directory for results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation status and statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"BASELINE EVALUATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Provider: {provider}\")\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Chunk: {chunk_size}, k_retrieve={k_retrieve}, mode={mode}\")\n",
    "    print(f\"Page Tolerance: {'ENABLED' if use_page_tolerance else 'DISABLED'}\")\n",
    "    \n",
    "    # Check if already exists\n",
    "    if check_if_results_exist(provider, model, chunk_size, k_retrieve, mode, output_dir):\n",
    "        print(\"✓ Baseline results already exist - SKIPPING\")\n",
    "        return {'status': 'skipped'}\n",
    "    \n",
    "    # Load vectorstore\n",
    "    print(\"Loading vectorstore...\")\n",
    "    try:\n",
    "        vectorstore = load_vectorstore(provider, model, chunk_size)\n",
    "        doc_count = vectorstore._collection.count()\n",
    "        print(f\"✓ Loaded ({doc_count:,} documents)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load vectorstore: {e}\")\n",
    "        return {'status': 'failed', 'error': str(e)}\n",
    "    \n",
    "    # Process all queries\n",
    "    results = []\n",
    "    mrr_scores = []\n",
    "    \n",
    "    print(f\"\\nProcessing {len(dataset)} queries...\")\n",
    "    \n",
    "    for record in tqdm(dataset, desc=\"Queries\"):\n",
    "        query_id = record['financebench_id']\n",
    "        query = record['question']\n",
    "        evidence = record['evidence']\n",
    "        doc_name = record['doc_name']\n",
    "        \n",
    "        try:\n",
    "            # Retrieve documents\n",
    "            retrieved_docs = retrieve_with_scores(\n",
    "                vectorstore=vectorstore,\n",
    "                query=query,\n",
    "                k=k_retrieve,\n",
    "                mode=mode,\n",
    "                target_doc_name=doc_name if mode == \"singledoc\" else None\n",
    "            )\n",
    "            \n",
    "            # Remove content before calculating MRR (save memory)\n",
    "            retrieved_for_mrr = [\n",
    "                {k: v for k, v in doc.items() if k != 'content'}\n",
    "                for doc in retrieved_docs\n",
    "            ]\n",
    "            \n",
    "            # Calculate MRR\n",
    "            mrr_score, rank = calculate_mrr_for_query(\n",
    "                retrieved_for_mrr, evidence, chunk_size, use_page_tolerance\n",
    "            )\n",
    "            mrr_scores.append(mrr_score)\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                'query_id': query_id,\n",
    "                'query': query,\n",
    "                'expected_doc': doc_name,\n",
    "                'expected_evidence': [\n",
    "                    {\n",
    "                        'doc_name': ev['doc_name'],\n",
    "                        'page_number': ev['evidence_page_num'] + 1  # Convert to 1-indexed\n",
    "                    }\n",
    "                    for ev in evidence\n",
    "                ],\n",
    "                'retrieved_docs': retrieved_for_mrr,\n",
    "                'mrr_score': mrr_score,\n",
    "                'rank': rank\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Error processing query {query_id}: {e}\")\n",
    "            results.append({\n",
    "                'query_id': query_id,\n",
    "                'query': query,\n",
    "                'error': str(e),\n",
    "                'mrr_score': 0.0,\n",
    "                'rank': -1\n",
    "            })\n",
    "            mrr_scores.append(0.0)\n",
    "    \n",
    "    # Calculate average MRR\n",
    "    avg_mrr = sum(mrr_scores) / len(mrr_scores) if mrr_scores else 0.0\n",
    "    \n",
    "    # Add summary\n",
    "    summary = {\n",
    "        'provider': provider,\n",
    "        'model': model,\n",
    "        'chunk_size': chunk_size,\n",
    "        'k_retrieve': k_retrieve,\n",
    "        'mode': mode,\n",
    "        'use_page_tolerance': use_page_tolerance,\n",
    "        'total_queries': len(dataset),\n",
    "        'average_mrr': avg_mrr\n",
    "    }\n",
    "    results.append({'summary': summary})\n",
    "    \n",
    "    # Save results (pass k_retrieve as k parameter)\n",
    "    save_results(results, provider, model, chunk_size, k_retrieve, mode, output_dir)\n",
    "    \n",
    "    print(f\"\\n✓ Average MRR: {avg_mrr:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'status': 'completed',\n",
    "        'average_mrr': avg_mrr,\n",
    "        'total_queries': len(dataset)\n",
    "    }\n",
    "\n",
    "print(\"✓ Baseline evaluation function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 5.2: Re-Ranking Evaluation\n",
    "\n",
    "# %%\n",
    "def evaluate_with_reranking(\n",
    "    dataset,\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k_retrieve: int,\n",
    "    k_rerank: int,\n",
    "    mode: str,\n",
    "    reranker_model: str,\n",
    "    use_page_tolerance: bool = True,\n",
    "    output_dir: str = OUTPUT_DIR\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate retrieval with re-ranking.\n",
    "    \n",
    "    Two-stage process:\n",
    "    1. Retrieve k_retrieve documents (e.g., 40)\n",
    "    2. Re-rank all k_retrieve documents and keep top k_rerank (e.g., 10)\n",
    "    \n",
    "    Args:\n",
    "        dataset: FinanceBench dataset\n",
    "        provider: \"ollama\", \"openai\", or \"voyage\"\n",
    "        model: Embedding model name\n",
    "        chunk_size: Chunk size\n",
    "        k_retrieve: Number of documents to retrieve in first stage\n",
    "        k_rerank: Number of documents to keep after re-ranking\n",
    "        mode: \"global\" or \"singledoc\"\n",
    "        reranker_model: Re-ranker model name\n",
    "        use_page_tolerance: If True, use chunk-size-aware page tolerance\n",
    "        output_dir: Output directory for results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation status and statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RE-RANKING EVALUATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Embedding: {provider}/{model}\")\n",
    "    print(f\"Chunk: {chunk_size}, mode={mode}\")\n",
    "    print(f\"Retrieval: k_retrieve={k_retrieve} → Re-rank: k_rerank={k_rerank}\")\n",
    "    print(f\"Re-ranker: {reranker_model}\")\n",
    "    print(f\"Page Tolerance: {'ENABLED' if use_page_tolerance else 'DISABLED'}\")\n",
    "    \n",
    "    # Check if already exists\n",
    "    if check_if_results_exist(provider, model, chunk_size, k_retrieve, mode, output_dir, reranker_model, k_rerank):\n",
    "        print(\"✓ Re-ranking results already exist - SKIPPING\")\n",
    "        return {'status': 'skipped'}\n",
    "    \n",
    "    # Load vectorstore\n",
    "    print(\"Loading vectorstore...\")\n",
    "    try:\n",
    "        vectorstore = load_vectorstore(provider, model, chunk_size)\n",
    "        doc_count = vectorstore._collection.count()\n",
    "        print(f\"✓ Loaded ({doc_count:,} documents)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load vectorstore: {e}\")\n",
    "        return {'status': 'failed', 'error': str(e)}\n",
    "    \n",
    "    # Process all queries\n",
    "    results = []\n",
    "    mrr_scores_rerank = []\n",
    "    mrr_scores_baseline = []\n",
    "    rank_improvements = []\n",
    "    queries_improved = 0\n",
    "    queries_degraded = 0\n",
    "    queries_unchanged = 0\n",
    "    \n",
    "    print(f\"\\nProcessing {len(dataset)} queries...\")\n",
    "    \n",
    "    for record in tqdm(dataset, desc=\"Queries\"):\n",
    "        query_id = record['financebench_id']\n",
    "        query = record['question']\n",
    "        evidence = record['evidence']\n",
    "        doc_name = record['doc_name']\n",
    "        \n",
    "        try:\n",
    "            # Stage 1: Retrieve k_retrieve documents\n",
    "            retrieved_docs = retrieve_with_scores(\n",
    "                vectorstore=vectorstore,\n",
    "                query=query,\n",
    "                k=k_retrieve,\n",
    "                mode=mode,\n",
    "                target_doc_name=doc_name if mode == \"singledoc\" else None\n",
    "            )\n",
    "            \n",
    "            # Calculate baseline MRR (on first k_rerank documents from initial retrieval)\n",
    "            baseline_docs_for_mrr = [\n",
    "                {k: v for k, v in doc.items() if k != 'content'}\n",
    "                for doc in retrieved_docs[:k_rerank]\n",
    "            ]\n",
    "            mrr_baseline, rank_baseline = calculate_mrr_for_query(\n",
    "                baseline_docs_for_mrr, evidence, chunk_size, use_page_tolerance\n",
    "            )\n",
    "            mrr_scores_baseline.append(mrr_baseline)\n",
    "            \n",
    "            # Stage 2: Re-rank all k_retrieve documents\n",
    "            reranked_docs = rerank_documents(\n",
    "                query=query,\n",
    "                retrieved_docs=retrieved_docs,\n",
    "                reranker_model=reranker_model,\n",
    "                top_k=k_rerank\n",
    "            )\n",
    "            \n",
    "            # Calculate MRR after re-ranking\n",
    "            mrr_rerank, rank_rerank = calculate_mrr_for_query(\n",
    "                reranked_docs, evidence, chunk_size, use_page_tolerance\n",
    "            )\n",
    "            mrr_scores_rerank.append(mrr_rerank)\n",
    "            \n",
    "            # Track improvements\n",
    "            rank_improvement = rank_baseline - rank_rerank if rank_baseline > 0 and rank_rerank > 0 else 0\n",
    "            rank_improvements.append(rank_improvement)\n",
    "            \n",
    "            if rank_improvement > 0:\n",
    "                queries_improved += 1\n",
    "            elif rank_improvement < 0:\n",
    "                queries_degraded += 1\n",
    "            else:\n",
    "                queries_unchanged += 1\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                'query_id': query_id,\n",
    "                'query': query,\n",
    "                'expected_doc': doc_name,\n",
    "                'expected_evidence': [\n",
    "                    {\n",
    "                        'doc_name': ev['doc_name'],\n",
    "                        'page_number': ev['evidence_page_num'] + 1\n",
    "                    }\n",
    "                    for ev in evidence\n",
    "                ],\n",
    "                'retrieval_config': {\n",
    "                    'k_retrieve': k_retrieve,\n",
    "                    'k_rerank': k_rerank,\n",
    "                    'reranker_model': reranker_model\n",
    "                },\n",
    "                'retrieved_docs': reranked_docs,\n",
    "                'mrr_score': mrr_rerank,\n",
    "                'rank': rank_rerank,\n",
    "                'mrr_baseline': mrr_baseline,\n",
    "                'rank_baseline': rank_baseline,\n",
    "                'rank_improvement': rank_improvement\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Error processing query {query_id}: {e}\")\n",
    "            results.append({\n",
    "                'query_id': query_id,\n",
    "                'query': query,\n",
    "                'error': str(e),\n",
    "                'mrr_score': 0.0,\n",
    "                'rank': -1,\n",
    "                'mrr_baseline': 0.0,\n",
    "                'rank_baseline': -1\n",
    "            })\n",
    "            mrr_scores_rerank.append(0.0)\n",
    "            mrr_scores_baseline.append(0.0)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_mrr_rerank = sum(mrr_scores_rerank) / len(mrr_scores_rerank) if mrr_scores_rerank else 0.0\n",
    "    avg_mrr_baseline = sum(mrr_scores_baseline) / len(mrr_scores_baseline) if mrr_scores_baseline else 0.0\n",
    "    mrr_improvement = avg_mrr_rerank - avg_mrr_baseline\n",
    "    mrr_improvement_pct = (mrr_improvement / avg_mrr_baseline * 100) if avg_mrr_baseline > 0 else 0.0\n",
    "    \n",
    "    avg_rank_improvement = sum(rank_improvements) / len(rank_improvements) if rank_improvements else 0.0\n",
    "    \n",
    "    # Add summary\n",
    "    summary = {\n",
    "        'provider': provider,\n",
    "        'model': model,\n",
    "        'chunk_size': chunk_size,\n",
    "        'k_retrieve': k_retrieve,\n",
    "        'k_rerank': k_rerank,\n",
    "        'mode': mode,\n",
    "        'use_page_tolerance': use_page_tolerance,\n",
    "        'retrieval_config': {\n",
    "            'k_retrieve': k_retrieve,\n",
    "            'k_rerank': k_rerank,\n",
    "            'reranker_model': reranker_model\n",
    "        },\n",
    "        'total_queries': len(dataset),\n",
    "        'average_mrr': avg_mrr_rerank,\n",
    "        'average_mrr_baseline': avg_mrr_baseline,\n",
    "        'mrr_improvement': mrr_improvement,\n",
    "        'mrr_improvement_percentage': mrr_improvement_pct,\n",
    "        'queries_improved': queries_improved,\n",
    "        'queries_degraded': queries_degraded,\n",
    "        'queries_unchanged': queries_unchanged,\n",
    "        'average_rank_improvement': avg_rank_improvement\n",
    "    }\n",
    "    results.append({'summary': summary})\n",
    "    \n",
    "    # Save results (pass k_retrieve as k, and k_rerank separately)\n",
    "    save_results(results, provider, model, chunk_size, k_retrieve, mode, output_dir, reranker_model, k_rerank)\n",
    "    \n",
    "    print(f\"\\n✓ Results:\")\n",
    "    print(f\"  Baseline MRR: {avg_mrr_baseline:.4f} (top {k_rerank} from {k_retrieve} retrieved)\")\n",
    "    print(f\"  Re-rank MRR:  {avg_mrr_rerank:.4f} (top {k_rerank} after re-ranking {k_retrieve})\")\n",
    "    print(f\"  Improvement:  {mrr_improvement:+.4f} ({mrr_improvement_pct:+.2f}%)\")\n",
    "    print(f\"  Queries improved: {queries_improved}, degraded: {queries_degraded}, unchanged: {queries_unchanged}\")\n",
    "    \n",
    "    return {\n",
    "        'status': 'completed',\n",
    "        'average_mrr': avg_mrr_rerank,\n",
    "        'average_mrr_baseline': avg_mrr_baseline,\n",
    "        'improvement': mrr_improvement,\n",
    "        'improvement_percentage': mrr_improvement_pct,\n",
    "        'total_queries': len(dataset)\n",
    "    }\n",
    "\n",
    "print(\"✓ Re-ranking evaluation function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 5.3: Batch Evaluation Helper\n",
    "\n",
    "# %%\n",
    "def evaluate_single_configuration(\n",
    "    dataset,\n",
    "    config: Dict,\n",
    "    modes: List[str],\n",
    "    use_page_tolerance: bool = True,\n",
    "    output_dir: str = OUTPUT_DIR\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a single embedding configuration across all settings.\n",
    "    \n",
    "    For each configuration, this runs:\n",
    "    1. Baseline evaluation (if not exists)\n",
    "    2. Re-ranking evaluation for each re-ranker (if not exists)\n",
    "    \n",
    "    Args:\n",
    "        dataset: FinanceBench dataset\n",
    "        config: Configuration dict with:\n",
    "            - provider: Embedding provider\n",
    "            - model: Embedding model name\n",
    "            - chunk_sizes: List of chunk sizes to test\n",
    "            - k_retrieve: Number of documents to retrieve\n",
    "            - k_rerank: Number of documents to keep after re-ranking\n",
    "            - reranker_models: List of re-ranker models to test\n",
    "        modes: List of modes (e.g., ['global', 'singledoc'])\n",
    "        use_page_tolerance: If True, use chunk-size-aware tolerance\n",
    "        output_dir: Output directory\n",
    "        \n",
    "    Returns:\n",
    "        Summary of evaluations\n",
    "    \"\"\"\n",
    "    provider = config['provider']\n",
    "    model = config['model']\n",
    "    chunk_sizes = config['chunk_sizes']\n",
    "    k_retrieve = config['k_retrieve']\n",
    "    k_rerank = config['k_rerank']\n",
    "    reranker_models = config.get('reranker_models', [])\n",
    "    \n",
    "    results_summary = []\n",
    "    \n",
    "    for chunk_size in chunk_sizes:\n",
    "        for mode in modes:\n",
    "            # Baseline evaluation\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"CONFIG: {provider}/{model}, chunk={chunk_size}, k_retrieve={k_retrieve}, mode={mode}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            baseline_result = evaluate_baseline(\n",
    "                dataset=dataset,\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                chunk_size=chunk_size,\n",
    "                k_retrieve=k_retrieve,\n",
    "                mode=mode,\n",
    "                use_page_tolerance=use_page_tolerance,\n",
    "                output_dir=output_dir\n",
    "            )\n",
    "            \n",
    "            results_summary.append({\n",
    "                'type': 'baseline',\n",
    "                'provider': provider,\n",
    "                'model': model,\n",
    "                'chunk_size': chunk_size,\n",
    "                'k_retrieve': k_retrieve,\n",
    "                'mode': mode,\n",
    "                'result': baseline_result\n",
    "            })\n",
    "            \n",
    "            # Re-ranking evaluations\n",
    "            for reranker_model in reranker_models:\n",
    "                rerank_result = evaluate_with_reranking(\n",
    "                    dataset=dataset,\n",
    "                    provider=provider,\n",
    "                    model=model,\n",
    "                    chunk_size=chunk_size,\n",
    "                    k_retrieve=k_retrieve,\n",
    "                    k_rerank=k_rerank,\n",
    "                    mode=mode,\n",
    "                    reranker_model=reranker_model,\n",
    "                    use_page_tolerance=use_page_tolerance,\n",
    "                    output_dir=output_dir\n",
    "                )\n",
    "                \n",
    "                results_summary.append({\n",
    "                    'type': 'rerank',\n",
    "                    'provider': provider,\n",
    "                    'model': model,\n",
    "                    'chunk_size': chunk_size,\n",
    "                    'k_retrieve': k_retrieve,\n",
    "                    'k_rerank': k_rerank,\n",
    "                    'mode': mode,\n",
    "                    'reranker': reranker_model,\n",
    "                    'result': rerank_result\n",
    "                })\n",
    "    \n",
    "    return results_summary\n",
    "\n",
    "print(\"✓ Batch evaluation helper defined\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 5 complete!\")\n",
    "print(\"  Baseline evaluation: evaluate_baseline\")\n",
    "print(\"  Re-ranking evaluation: evaluate_with_reranking\")\n",
    "print(\"  Batch helper: evaluate_single_configuration\")\n",
    "print(\"\\nThese functions will:\")\n",
    "print(\"  • Skip existing results automatically\")\n",
    "print(\"  • Calculate MRR for both baseline and re-ranked results\")\n",
    "print(\"  • Track improvements/degradations per query\")\n",
    "print(\"  • Save comprehensive JSON with all statistics\")\n",
    "print(\"\\nConfiguration structure:\")\n",
    "print(\"  {\")\n",
    "print(\"    'provider': 'voyage',\")\n",
    "print(\"    'model': 'voyage-finance-2',\")\n",
    "print(\"    'chunk_sizes': [512, 1024],\")\n",
    "print(\"    'k_retrieve': 40,  # Retrieve 40 for baseline\")\n",
    "print(\"    'k_rerank': 10,    # Keep top 10 after re-ranking\")\n",
    "print(\"    'reranker_models': ['cross-encoder/ms-marco-MiniLM-L-12-v2']\")\n",
    "print(\"  }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d784e904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configurations defined\n",
      "  Total configurations: 1\n",
      "✓ Evaluation parameters set\n",
      "  Modes: ['global', 'singledoc']\n",
      "  Page Tolerance: ENABLED\n",
      "\n",
      "============================================================\n",
      "EVALUATION PLAN\n",
      "============================================================\n",
      "\n",
      "voyage/voyage-finance-2\n",
      "  Chunk sizes: [1024]\n",
      "  Retrieval strategy: k_retrieve=20, k_rerank=[5]\n",
      "  Re-rankers: 1\n",
      "    • cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "  Total runs: 4 (baseline + 1 re-rankers)\n",
      "\n",
      "============================================================\n",
      "TOTAL EVALUATION RUNS: 4\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "OUTPUT FILES\n",
      "============================================================\n",
      "Output directory: ../../evaluation_results/reranking\n",
      "\n",
      "\n",
      "voyage/voyage-finance-2:\n",
      "  ○ TO CREATE voyage_voyage-finance-2_chunk1024_k5_global.json\n",
      "  ○ TO CREATE voyage_voyage-finance-2_chunk1024_k5_global_rerank-ms-marco-L-12.json\n",
      "  ○ TO CREATE voyage_voyage-finance-2_chunk1024_k5_singledoc.json\n",
      "  ○ TO CREATE voyage_voyage-finance-2_chunk1024_k5_singledoc_rerank-ms-marco-L-12.json\n",
      "\n",
      "============================================================\n",
      "Total files: 4\n",
      "  Existing: 0\n",
      "  To create: 4\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "STARTING BATCH EVALUATION\n",
      "============================================================\n",
      "This will process 1 configuration(s)\n",
      "Existing results will be skipped automatically\n",
      "\n",
      "\n",
      "============================================================\n",
      "PROCESSING CONFIGURATION 1/1\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "CONFIG: voyage/voyage-finance-2, chunk=1024, k=5, mode=global\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "BASELINE EVALUATION\n",
      "============================================================\n",
      "Provider: voyage\n",
      "Model: voyage-finance-2\n",
      "Chunk: 1024, k=5, mode=global\n",
      "Page Tolerance: ENABLED\n",
      "Loading vectorstore...\n",
      "✓ Loaded (15,765 documents)\n",
      "\n",
      "Processing 150 queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9ec12f38fa4e199f5739a80cff2977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Queries:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: voyage_voyage-finance-2_chunk1024_k5_global.json\n",
      "\n",
      "✓ Average MRR: 0.3884\n",
      "\n",
      "============================================================\n",
      "RE-RANKING EVALUATION\n",
      "============================================================\n",
      "Embedding: voyage/voyage-finance-2\n",
      "Chunk: 1024, mode=global\n",
      "Retrieval: k=20 → Re-rank: k=5\n",
      "Re-ranker: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "Page Tolerance: ENABLED\n",
      "Loading vectorstore...\n",
      "✓ Loaded (15,765 documents)\n",
      "\n",
      "Processing 150 queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff87c9eed4734eb997893e144c474f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Queries:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: voyage_voyage-finance-2_chunk1024_k5_global_rerank-ms-marco-L-12.json\n",
      "\n",
      "✓ Results:\n",
      "  Baseline MRR: 0.3884\n",
      "  Re-rank MRR:  0.2628\n",
      "  Improvement:  -0.1257 (-32.35%)\n",
      "  Queries improved: 13, degraded: 26, unchanged: 111\n",
      "\n",
      "============================================================\n",
      "CONFIG: voyage/voyage-finance-2, chunk=1024, k=5, mode=singledoc\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "BASELINE EVALUATION\n",
      "============================================================\n",
      "Provider: voyage\n",
      "Model: voyage-finance-2\n",
      "Chunk: 1024, k=5, mode=singledoc\n",
      "Page Tolerance: ENABLED\n",
      "Loading vectorstore...\n",
      "✓ Loaded (15,765 documents)\n",
      "\n",
      "Processing 150 queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f5cb3bff1241298f5f824a1d380c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Queries:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: voyage_voyage-finance-2_chunk1024_k5_singledoc.json\n",
      "\n",
      "✓ Average MRR: 0.4729\n",
      "\n",
      "============================================================\n",
      "RE-RANKING EVALUATION\n",
      "============================================================\n",
      "Embedding: voyage/voyage-finance-2\n",
      "Chunk: 1024, mode=singledoc\n",
      "Retrieval: k=20 → Re-rank: k=5\n",
      "Re-ranker: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
      "Page Tolerance: ENABLED\n",
      "Loading vectorstore...\n",
      "✓ Loaded (15,765 documents)\n",
      "\n",
      "Processing 150 queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588dd5534d144bad89b92ea282f4ab1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Queries:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved: voyage_voyage-finance-2_chunk1024_k5_singledoc_rerank-ms-marco-L-12.json\n",
      "\n",
      "✓ Results:\n",
      "  Baseline MRR: 0.4729\n",
      "  Re-rank MRR:  0.3797\n",
      "  Improvement:  -0.0932 (-19.71%)\n",
      "  Queries improved: 16, degraded: 25, unchanged: 109\n",
      "\n",
      "============================================================\n",
      "BATCH EVALUATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "voyage/voyage-finance-2, chunk=1024, mode=global\n",
      "------------------------------------------------------------\n",
      "  Baseline:  MRR = 0.3884\n",
      "  ms-marco-L-12             MRR = 0.2628  (-0.1257, +0.00%)\n",
      "\n",
      "voyage/voyage-finance-2, chunk=1024, mode=singledoc\n",
      "------------------------------------------------------------\n",
      "  Baseline:  MRR = 0.4729\n",
      "  ms-marco-L-12             MRR = 0.3797  (-0.0932, +0.00%)\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "GENERATED FILES\n",
      "============================================================\n",
      "Directory: ../../evaluation_results/reranking\n",
      "\n",
      "Total files: 4\n",
      "\n",
      "Baseline files (2):\n",
      "  voyage_voyage-finance-2_chunk1024_k5_global.json (173.7 KB)\n",
      "  voyage_voyage-finance-2_chunk1024_k5_singledoc.json (171.7 KB)\n",
      "\n",
      "Re-ranking files (2):\n",
      "  voyage_voyage-finance-2_chunk1024_k5_global_rerank-ms-marco-L-12.json (264.7 KB)\n",
      "  voyage_voyage-finance-2_chunk1024_k5_singledoc_rerank-ms-marco-L-12.json (263.8 KB)\n",
      "\n",
      "Total size: 0.85 MB\n",
      "============================================================\n",
      "✓ Quick comparison function defined\n",
      "\n",
      "============================================================\n",
      "✓ STEP 6 COMPLETE - EVALUATION READY!\n",
      "============================================================\n",
      "\n",
      "Next steps:\n",
      "  1. Review the evaluation plan above\n",
      "  2. Run the batch evaluation cells (6.5)\n",
      "  3. View results summary (6.6)\n",
      "  4. Check generated files (6.7)\n",
      "  5. Use quick_comparison() for specific configs (6.9)\n",
      "\n",
      "All results are saved to: ../../evaluation_results/reranking\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 6: Configuration & Execution\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.1: Define Configurations to Test\n",
    "\n",
    "# %%\n",
    "# Define which embedding models, chunk sizes, and re-rankers to evaluate\n",
    "configurations = [\n",
    "    # {\n",
    "    #     'provider': 'voyage',\n",
    "    #     'model': 'voyage-finance-2',\n",
    "    #     'chunk_sizes': [512, 1024],\n",
    "    #     'k_retrieve': 40,       # Baseline: retrieve 40 documents\n",
    "    #     'k_rerank': 10,         # Re-ranking: keep top 10 after re-ranking\n",
    "    #     'reranker_models': [\n",
    "    #         'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "    #         'BAAI/bge-reranker-large',\n",
    "    #         'voyage-rerank-2'\n",
    "    #     ]\n",
    "    # },\n",
    "    # {\n",
    "    #     'provider': 'openai',\n",
    "    #     'model': 'text-embedding-3-large',\n",
    "    #     'chunk_sizes': [512, 1024],\n",
    "    #     'k_retrieve': 40,\n",
    "    #     'k_rerank': 10,\n",
    "    #     'reranker_models': [\n",
    "    #         'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "    #         'BAAI/bge-reranker-large'\n",
    "    #     ]\n",
    "    # },\n",
    "    {\n",
    "        'provider': 'ollama',\n",
    "        'model': 'nomic-embed-text',\n",
    "        'chunk_sizes': [512],\n",
    "        'k_retrieve': 5,\n",
    "        'k_rerank': 3,\n",
    "        'reranker_models': [\n",
    "            'cross-encoder/ms-marco-MiniLM-L-12-v2'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"✓ Configurations defined\")\n",
    "print(f\"  Total configurations: {len(configurations)}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.2: Define Evaluation Parameters\n",
    "\n",
    "# %%\n",
    "# Modes to test\n",
    "modes = ['global', 'singledoc']\n",
    "\n",
    "# Page tolerance setting\n",
    "USE_PAGE_TOLERANCE = True\n",
    "\n",
    "print(\"✓ Evaluation parameters set\")\n",
    "print(f\"  Modes: {modes}\")\n",
    "print(f\"  Page Tolerance: {'ENABLED' if USE_PAGE_TOLERANCE else 'DISABLED'}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.3: Display Evaluation Plan\n",
    "\n",
    "# %%\n",
    "def display_evaluation_plan(configurations, modes):\n",
    "    \"\"\"Display what will be evaluated.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EVALUATION PLAN\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    total_runs = 0\n",
    "    \n",
    "    for config in configurations:\n",
    "        provider = config['provider']\n",
    "        model = config['model']\n",
    "        chunk_sizes = config['chunk_sizes']\n",
    "        k_retrieve = config['k_retrieve']\n",
    "        k_rerank = config['k_rerank']\n",
    "        reranker_models = config.get('reranker_models', [])\n",
    "        \n",
    "        print(f\"\\n{provider}/{model}\")\n",
    "        print(f\"  Chunk sizes: {chunk_sizes}\")\n",
    "        print(f\"  Baseline: retrieve k={k_retrieve}\")\n",
    "        print(f\"  Re-ranking: retrieve k={k_retrieve}, keep top k={k_rerank}\")\n",
    "        print(f\"  Re-rankers: {len(reranker_models)}\")\n",
    "        for rm in reranker_models:\n",
    "            print(f\"    • {rm}\")\n",
    "        \n",
    "        # Calculate runs for this config\n",
    "        # For each chunk + mode: 1 baseline + N re-rankers\n",
    "        runs_per_config = len(chunk_sizes) * len(modes) * (1 + len(reranker_models))\n",
    "        total_runs += runs_per_config\n",
    "        print(f\"  Total runs: {runs_per_config} (baseline + {len(reranker_models)} re-rankers)\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TOTAL EVALUATION RUNS: {total_runs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return total_runs\n",
    "\n",
    "# %%\n",
    "total_runs = display_evaluation_plan(configurations, modes)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.4: Show Output Files That Will Be Created\n",
    "\n",
    "# %%\n",
    "def display_output_files(configurations, modes, output_dir):\n",
    "    \"\"\"Display all output files that will be created.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"OUTPUT FILES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Output directory: {output_dir}\\n\")\n",
    "    \n",
    "    all_files = []\n",
    "    existing_count = 0\n",
    "    \n",
    "    for config in configurations:\n",
    "        provider = config['provider']\n",
    "        model = config['model']\n",
    "        chunk_sizes = config['chunk_sizes']\n",
    "        k_retrieve = config['k_retrieve']\n",
    "        k_rerank = config['k_rerank']\n",
    "        reranker_models = config.get('reranker_models', [])\n",
    "        \n",
    "        print(f\"\\n{provider}/{model}:\")\n",
    "        \n",
    "        for chunk_size in chunk_sizes:\n",
    "            for mode in modes:\n",
    "                # Baseline file\n",
    "                baseline_file = get_output_filename(provider, model, chunk_size, k_retrieve, mode)\n",
    "                exists = check_if_results_exist(provider, model, chunk_size, k_retrieve, mode, output_dir)\n",
    "                status = \"✓ EXISTS\" if exists else \"○ TO CREATE\"\n",
    "                if exists:\n",
    "                    existing_count += 1\n",
    "                print(f\"  {status} {baseline_file}\")\n",
    "                all_files.append((baseline_file, exists))\n",
    "                \n",
    "                # Re-ranking files\n",
    "                for reranker in reranker_models:\n",
    "                    rerank_file = get_output_filename(provider, model, chunk_size, k_retrieve, mode, reranker, k_rerank)\n",
    "                    exists = check_if_results_exist(provider, model, chunk_size, k_retrieve, mode, output_dir, reranker, k_rerank)\n",
    "                    status = \"✓ EXISTS\" if exists else \"○ TO CREATE\"\n",
    "                    if exists:\n",
    "                        existing_count += 1\n",
    "                    print(f\"  {status} {rerank_file}\")\n",
    "                    all_files.append((rerank_file, exists))\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Total files: {len(all_files)}\")\n",
    "    print(f\"  Existing: {existing_count}\")\n",
    "    print(f\"  To create: {len(all_files) - existing_count}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "# %%\n",
    "output_files = display_output_files(configurations, modes, OUTPUT_DIR)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.5: Execute Batch Evaluation\n",
    "\n",
    "# %%\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STARTING BATCH EVALUATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"This will process {len(configurations)} configuration(s)\")\n",
    "print(f\"Existing results will be skipped automatically\\n\")\n",
    "\n",
    "# Confirm before starting (optional - comment out to run without confirmation)\n",
    "# proceed = input(\"Proceed with evaluation? (y/n): \").lower().strip()\n",
    "# if proceed != 'y':\n",
    "#     print(\"Evaluation cancelled by user\")\n",
    "# else:\n",
    "\n",
    "# Run evaluation for each configuration\n",
    "all_results = []\n",
    "\n",
    "for i, config in enumerate(configurations, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING CONFIGURATION {i}/{len(configurations)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    config_results = evaluate_single_configuration(\n",
    "        dataset=dataset,\n",
    "        config=config,\n",
    "        modes=modes,\n",
    "        use_page_tolerance=USE_PAGE_TOLERANCE,\n",
    "        output_dir=OUTPUT_DIR\n",
    "    )\n",
    "    \n",
    "    all_results.extend(config_results)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BATCH EVALUATION COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.6: Display Results Summary\n",
    "\n",
    "# %%\n",
    "def display_results_summary(all_results):\n",
    "    \"\"\"Display summary of all evaluation results.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EVALUATION RESULTS SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Group by configuration\n",
    "    by_config = defaultdict(list)\n",
    "    \n",
    "    for result in all_results:\n",
    "        if result['result']['status'] == 'completed':\n",
    "            key = (result['provider'], result['model'], result['chunk_size'], result['mode'], result.get('k_retrieve', 0))\n",
    "            by_config[key].append(result)\n",
    "    \n",
    "    # Display results\n",
    "    for key, results in sorted(by_config.items()):\n",
    "        provider, model, chunk_size, mode, k_retrieve = key\n",
    "        \n",
    "        print(f\"\\n{provider}/{model}, chunk={chunk_size}, mode={mode}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Find baseline\n",
    "        baseline = next((r for r in results if r['type'] == 'baseline'), None)\n",
    "        if baseline and 'average_mrr' in baseline['result']:\n",
    "            baseline_mrr = baseline['result']['average_mrr']\n",
    "            print(f\"  Baseline (k={k_retrieve}):  MRR = {baseline_mrr:.4f}\")\n",
    "        else:\n",
    "            baseline_mrr = None\n",
    "            print(f\"  Baseline:  (skipped)\")\n",
    "        \n",
    "        # Show re-ranking results\n",
    "        rerank_results = [r for r in results if r['type'] == 'rerank']\n",
    "        for r in sorted(rerank_results, key=lambda x: x['result'].get('average_mrr', 0), reverse=True):\n",
    "            if 'average_mrr' in r['result']:\n",
    "                mrr = r['result']['average_mrr']\n",
    "                k_rerank = r.get('k_rerank', 0)\n",
    "                improvement = r['result'].get('improvement', 0)\n",
    "                improvement_pct = r['result'].get('improvement_percentage', 0)\n",
    "                reranker_short = get_reranker_short_name(r['reranker'])\n",
    "                print(f\"  {reranker_short:25s} (k={k_rerank})  MRR = {mrr:.4f}  ({improvement:+.4f}, {improvement_pct:+.2f}%)\")\n",
    "            else:\n",
    "                reranker_short = get_reranker_short_name(r['reranker'])\n",
    "                print(f\"  {reranker_short:25s} (skipped)\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "\n",
    "# %%\n",
    "display_results_summary(all_results)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.7: List All Generated Files\n",
    "\n",
    "# %%\n",
    "def list_generated_files(output_dir):\n",
    "    \"\"\"List all JSON files in output directory.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"GENERATED FILES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Directory: {output_dir}\\n\")\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    json_files = sorted(output_path.glob(\"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"No JSON files found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Total files: {len(json_files)}\\n\")\n",
    "    \n",
    "    # Group by type\n",
    "    baseline_files = []\n",
    "    rerank_files = []\n",
    "    \n",
    "    for filepath in json_files:\n",
    "        file_size = filepath.stat().st_size / 1024  # KB\n",
    "        if 'rerank-' in filepath.name:\n",
    "            rerank_files.append((filepath.name, file_size))\n",
    "        else:\n",
    "            baseline_files.append((filepath.name, file_size))\n",
    "    \n",
    "    if baseline_files:\n",
    "        print(f\"Baseline files ({len(baseline_files)}):\")\n",
    "        for name, size in baseline_files:\n",
    "            print(f\"  {name} ({size:.1f} KB)\")\n",
    "    \n",
    "    if rerank_files:\n",
    "        print(f\"\\nRe-ranking files ({len(rerank_files)}):\")\n",
    "        for name, size in rerank_files:\n",
    "            print(f\"  {name} ({size:.1f} KB)\")\n",
    "    \n",
    "    total_size = sum(f.stat().st_size for f in json_files) / (1024 * 1024)  # MB\n",
    "    print(f\"\\nTotal size: {total_size:.2f} MB\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# %%\n",
    "list_generated_files(OUTPUT_DIR)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.8: Quick Comparison Function\n",
    "\n",
    "# %%\n",
    "def quick_comparison(provider, model, chunk_size, mode, k_retrieve, k_rerank=None, output_dir=OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Quick comparison of baseline vs all re-rankers for a specific configuration.\n",
    "    \n",
    "    Args:\n",
    "        provider: Embedding provider\n",
    "        model: Embedding model name\n",
    "        chunk_size: Chunk size\n",
    "        mode: \"global\" or \"singledoc\"\n",
    "        k_retrieve: Number retrieved for baseline\n",
    "        k_rerank: Number kept after re-ranking (optional, will find all if None)\n",
    "        output_dir: Output directory\n",
    "    \n",
    "    Usage:\n",
    "        quick_comparison('voyage', 'voyage-finance-2', 512, 'global', 40, 10)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"QUICK COMPARISON\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{provider}/{model}, chunk={chunk_size}, mode={mode}\\n\")\n",
    "    \n",
    "    # Load baseline\n",
    "    baseline_file = get_output_filename(provider, model, chunk_size, k_retrieve, mode)\n",
    "    baseline_path = os.path.join(output_dir, baseline_file)\n",
    "    \n",
    "    if not os.path.exists(baseline_path):\n",
    "        print(f\"✗ Baseline file not found: {baseline_file}\")\n",
    "        return\n",
    "    \n",
    "    with open(baseline_path, 'r') as f:\n",
    "        baseline_data = json.load(f)\n",
    "    \n",
    "    baseline_summary = next((item['summary'] for item in baseline_data if 'summary' in item), None)\n",
    "    if not baseline_summary:\n",
    "        print(\"✗ No summary found in baseline file\")\n",
    "        return\n",
    "    \n",
    "    baseline_mrr = baseline_summary['average_mrr']\n",
    "    baseline_k = baseline_summary.get('k_retrieve', k_retrieve)\n",
    "    print(f\"Baseline (k={baseline_k}): MRR = {baseline_mrr:.4f}\\n\")\n",
    "    \n",
    "    # Find all rerank files for this config\n",
    "    model_clean = model.replace('/', '_')\n",
    "    pattern = f\"{provider}_{model_clean}_chunk{chunk_size}_k{k_retrieve}_{mode}_rerank_k*\"\n",
    "    \n",
    "    rerank_files = []\n",
    "    for f in Path(output_dir).glob(pattern):\n",
    "        rerank_files.append(f)\n",
    "    \n",
    "    if not rerank_files:\n",
    "        print(\"No re-ranking files found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Re-ranking results:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    for rerank_path in rerank_files:\n",
    "        with open(rerank_path, 'r') as f:\n",
    "            rerank_data = json.load(f)\n",
    "        \n",
    "        rerank_summary = next((item['summary'] for item in rerank_data if 'summary' in item), None)\n",
    "        if rerank_summary:\n",
    "            reranker = rerank_summary['retrieval_config']['reranker_model']\n",
    "            reranker_short = get_reranker_short_name(reranker)\n",
    "            k_r = rerank_summary.get('k_rerank', 0)\n",
    "            mrr = rerank_summary['average_mrr']\n",
    "            improvement = rerank_summary['mrr_improvement']\n",
    "            improvement_pct = rerank_summary['mrr_improvement_percentage']\n",
    "            \n",
    "            results.append((reranker_short, k_r, mrr, improvement, improvement_pct))\n",
    "    \n",
    "    # Sort by MRR descending\n",
    "    for reranker_short, k_r, mrr, improvement, improvement_pct in sorted(results, key=lambda x: x[2], reverse=True):\n",
    "        print(f\"{reranker_short:25s} (k={k_r})  MRR = {mrr:.4f}  ({improvement:+.4f}, {improvement_pct:+.2f}%)\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "print(\"✓ Quick comparison function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.9: Example Usage of Quick Comparison\n",
    "\n",
    "# %%\n",
    "# Uncomment to use quick comparison after evaluation is complete\n",
    "\"\"\"\n",
    "# Example: Compare all re-rankers for a specific configuration\n",
    "quick_comparison(\n",
    "    provider='voyage',\n",
    "    model='voyage-finance-2',\n",
    "    chunk_size=512,\n",
    "    mode='global',\n",
    "    k_retrieve=40,\n",
    "    k_rerank=10\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ STEP 6 COMPLETE - EVALUATION READY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nConfiguration format:\")\n",
    "print(\"  {\")\n",
    "print(\"    'provider': 'voyage',\")\n",
    "print(\"    'model': 'voyage-finance-2',\")\n",
    "print(\"    'chunk_sizes': [512, 1024],\")\n",
    "print(\"    'k_retrieve': 40,  # Baseline retrieves 40\")\n",
    "print(\"    'k_rerank': 10,    # Re-ranking keeps top 10\")\n",
    "print(\"    'reranker_models': ['cross-encoder/ms-marco-MiniLM-L-12-v2']\")\n",
    "print(\"  }\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Review the evaluation plan (6.3)\")\n",
    "print(\"  2. Check output files (6.4)\")\n",
    "print(\"  3. Run batch evaluation (6.5)\")\n",
    "print(\"  4. View results summary (6.6)\")\n",
    "print(\"  5. Check generated files (6.7)\")\n",
    "print(\"  6. Use quick_comparison() for specific configs (6.8)\")\n",
    "print(\"\\nAll results are saved to:\", OUTPUT_DIR)\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
