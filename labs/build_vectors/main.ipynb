{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee342d2a",
   "metadata": {},
   "source": [
    "# General Tips\n",
    "## Using virtual environments\n",
    "**Step 1:** CD to desired directory and Create a Virtual Environment `python3 -m venv myenv`. (Run `py -3.13 -m venv myenv` for a specific version of python)\n",
    "\n",
    "Check your python installed versions with `py -0` on Windows (`python3 --version` on Linux)\n",
    "\n",
    "**Step 2:** Activate the Environment `source myenv/bin/activate` (on Linux) and `myenv\\Scripts\\activate` (on Windows).\n",
    "\n",
    "**Step 3:** Install Any Needed Packages. e.g: `pip install requests pandas`. Or better to use `requirements.txt` file (`pip install -r requirements.txt`)\n",
    "\n",
    "**Step 4:** List All Installed Packages using `pip list`\n",
    "\n",
    "## Connecting the Jupyter Notebook to the vistual env\n",
    "1. Make sure that myenv is activate (`myenv\\Scripts\\activate`)\n",
    "2. Run this inside the virtual environment: `pip install ipykernel`\n",
    "3. Still inside the environment: `python -m ipykernel install --user --name=myenv --display-name \"Whatever Python Kernel Name\"`\n",
    "   \n",
    "   --name=myenv: internal identifier for the kernel\n",
    "   \n",
    "   --display-name: name that shows up in VS Code kernel picker\n",
    "4. Open VS Code and select the kernel\n",
    "\n",
    "   At the top-right, click \"Select Kernel\".\n",
    "   Look for “Whatever Python Kernel Name” — pick that.\n",
    "5. If you don’t see it right away, try: Reloading VS Code, Or running Reload Window from Command Palette (Ctrl+Shift+P)\n",
    "\n",
    "## Useful Commands\n",
    "1. Use `py -0` to check which python installation we have on Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc52eb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenAI API key loaded\n",
      "✓ Ollama base URL: http://localhost:11434\n",
      "✓ Configuration validated successfully\n",
      "\n",
      "Configuration:\n",
      "  - Embedding Provider: ollama\n",
      "  - Model: nomic-embed-text\n",
      "  - Chunk Sizes: [512]\n",
      "  - Chunk Overlap: 15%\n",
      "  - PDF Directory: ../../financebench/documents\n",
      "  - Vector DB Directory: ../../vector_databases\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FinanceBench RAG Pipeline - Clean Implementation\n",
    "# Step 1: Configuration and Imports\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # FinanceBench RAG Pipeline\n",
    "# \n",
    "# This notebook provides a clean, modular approach to building a RAG system with:\n",
    "# - Multiple embedding providers (Ollama, OpenAI)\n",
    "# - Configurable chunk sizes and overlaps\n",
    "# - Progress tracking\n",
    "# - Proper error handling\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.1 Imports\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Environment and progress\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Document processing\n",
    "from llama_index.core.schema import Document, BaseNode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "# Embeddings and vector stores\n",
    "from langchain.docstore.document import Document as LCDocument\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.2 Load Environment Variables\n",
    "\n",
    "# %%\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify critical environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"⚠️  Warning: OPENAI_API_KEY not found in .env file\")\n",
    "else:\n",
    "    print(\"✓ OpenAI API key loaded\")\n",
    "\n",
    "print(f\"✓ Ollama base URL: {OLLAMA_BASE_URL}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.3 Configuration Dataclass\n",
    "\n",
    "# %%\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration for RAG pipeline.\"\"\"\n",
    "    \n",
    "    # Dataset\n",
    "    dataset_name: str = \"PatronusAI/financebench\"\n",
    "    dataset_split: str = \"train\"\n",
    "    \n",
    "    # Paths\n",
    "    pdf_dir: str = \"../../financebench/documents\"\n",
    "    vector_db_dir: str = \"../../vector_databases\"\n",
    "    \n",
    "    # Chunking\n",
    "    chunk_sizes: List[int] = None\n",
    "    chunk_overlap_percentage: int = 15  # Percentage overlap between chunks\n",
    "    \n",
    "    # Embeddings\n",
    "    embedding_provider: str = \"ollama\"  # \"ollama\" or \"openai\"\n",
    "    ollama_model: str = \"nomic-embed-text\"\n",
    "    openai_model: str = \"text-embedding-3-small\"\n",
    "    \n",
    "    # Vector store\n",
    "    collection_name_prefix: str = \"financebench_docs_chunk_\"\n",
    "    \n",
    "    # Processing\n",
    "    batch_size: int = 500\n",
    "    clear_existing_db: bool = False\n",
    "    keep_node_metadata: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Set defaults for mutable attributes.\"\"\"\n",
    "        if self.chunk_sizes is None:\n",
    "            self.chunk_sizes = [128, 256, 512, 1024]\n",
    "    \n",
    "    def get_model_identifier(self) -> str:\n",
    "        \"\"\"Get a filesystem-safe identifier for the current model.\"\"\"\n",
    "        if self.embedding_provider == \"ollama\":\n",
    "            return f\"ollama_{self.ollama_model.replace('/', '_')}\"\n",
    "        elif self.embedding_provider == \"openai\":\n",
    "            return f\"openai_{self.openai_model.replace('/', '_')}\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown embedding provider: {self.embedding_provider}\")\n",
    "    \n",
    "    def get_vector_db_dir(self) -> str:\n",
    "        \"\"\"Get the vector database directory for the current embedding model.\"\"\"\n",
    "        model_id = self.get_model_identifier()\n",
    "        return os.path.join(self.vector_db_dir, model_id)\n",
    "    \n",
    "    def get_embedding_function(self):\n",
    "        \"\"\"Get the appropriate embedding function based on provider.\"\"\"\n",
    "        if self.embedding_provider == \"ollama\":\n",
    "            return OllamaEmbeddings(\n",
    "                model=self.ollama_model,\n",
    "                base_url=OLLAMA_BASE_URL\n",
    "            )\n",
    "        elif self.embedding_provider == \"openai\":\n",
    "            return OpenAIEmbeddings(\n",
    "                model=self.openai_model,\n",
    "                openai_api_key=OPENAI_API_KEY\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown embedding provider: {self.embedding_provider}\")\n",
    "    \n",
    "    def validate(self) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"Validate configuration.\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        # Check paths\n",
    "        if not os.path.exists(self.pdf_dir):\n",
    "            errors.append(f\"PDF directory not found: {self.pdf_dir}\")\n",
    "        \n",
    "        # Check chunk sizes\n",
    "        if not self.chunk_sizes or not all(isinstance(s, int) and s > 0 for s in self.chunk_sizes):\n",
    "            errors.append(\"Chunk sizes must be a list of positive integers\")\n",
    "        \n",
    "        # Check overlap\n",
    "        if not (1 <= self.chunk_overlap_percentage <= 99):\n",
    "            errors.append(\"Chunk overlap percentage must be between 1 and 99\")\n",
    "        \n",
    "        # Check embedding provider\n",
    "        if self.embedding_provider not in [\"ollama\", \"openai\"]:\n",
    "            errors.append(\"Embedding provider must be 'ollama' or 'openai'\")\n",
    "        \n",
    "        if self.embedding_provider == \"openai\" and not OPENAI_API_KEY:\n",
    "            errors.append(\"OpenAI provider selected but OPENAI_API_KEY not set\")\n",
    "        \n",
    "        return len(errors) == 0, errors\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.4 Create and Validate Configuration\n",
    "\n",
    "# %%\n",
    "# Create configuration\n",
    "config = RAGConfig(\n",
    "    chunk_sizes=[512],  # Start with one size for testing\n",
    "    embedding_provider=\"ollama\",  # Change to \"openai\" if needed\n",
    "    clear_existing_db=False\n",
    ")\n",
    "\n",
    "# Validate configuration\n",
    "is_valid, errors = config.validate()\n",
    "\n",
    "if is_valid:\n",
    "    print(\"✓ Configuration validated successfully\")\n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  - Embedding Provider: {config.embedding_provider}\")\n",
    "    print(f\"  - Model: {config.ollama_model if config.embedding_provider == 'ollama' else config.openai_model}\")\n",
    "    print(f\"  - Chunk Sizes: {config.chunk_sizes}\")\n",
    "    print(f\"  - Chunk Overlap: {config.chunk_overlap_percentage}%\")\n",
    "    print(f\"  - PDF Directory: {config.pdf_dir}\")\n",
    "    print(f\"  - Vector DB Directory: {config.vector_db_dir}\")\n",
    "else:\n",
    "    print(\"❌ Configuration validation failed:\")\n",
    "    for error in errors:\n",
    "        print(f\"  - {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d848be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: PatronusAI/financebench\n",
      "✓ Loaded 150 records from FinanceBench\n",
      "\n",
      "Sample record keys:\n",
      "  - financebench_id\n",
      "  - company\n",
      "  - doc_name\n",
      "  - question_type\n",
      "  - question_reasoning\n",
      "  - domain_question_num\n",
      "  - question\n",
      "  - answer\n",
      "  - justification\n",
      "  - dataset_subset_label\n",
      "  - evidence\n",
      "  - gics_sector\n",
      "  - doc_type\n",
      "  - doc_period\n",
      "  - doc_link\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a70b035f7a84321bb72ef0f50feda02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning dataset for PDFs:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Found 84 unique PDF files required\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cef6dbf36f4c83adcd2de895745a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Verifying PDFs:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Available: 84 PDFs\n",
      "✓ All required PDFs are available\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117b4a572265430380c1b4c7452f74b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading PDFs:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully loaded 12013 document pages\n",
      "Analyzing documents...\n",
      "\n",
      "============================================================\n",
      "DOCUMENT STATISTICS\n",
      "============================================================\n",
      "Total Pages Loaded:        12,013\n",
      "Unique PDF Documents:      0\n",
      "\n",
      "Content Size:\n",
      "  Total Characters:        40,649,449\n",
      "  Estimated Tokens:        10,162,362\n",
      "\n",
      "Per-Page Statistics:\n",
      "  Average Characters:      3,384\n",
      "  Min Characters:          0\n",
      "  Max Characters:          10,738\n",
      "\n",
      "Estimated Chunks (with 15% overlap):\n",
      "  Chunk Size  512:        ~105,245 chunks\n",
      "============================================================\n",
      "\n",
      "✓ Dataset loading complete!\n",
      "  - Dataset records: 150\n",
      "  - PDF files loaded: 84\n",
      "  - Document pages: 12013\n",
      "  - Estimated tokens: 10,162,362\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Dataset Loading Utilities\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.1 Load FinanceBench Dataset\n",
    "\n",
    "# %%\n",
    "def load_financebench_dataset(config: RAGConfig):\n",
    "    \"\"\"\n",
    "    Load the FinanceBench dataset from HuggingFace.\n",
    "    \n",
    "    Args:\n",
    "        config: RAGConfig instance\n",
    "        \n",
    "    Returns:\n",
    "        Dataset object\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset: {config.dataset_name}\")\n",
    "    \n",
    "    ds = load_dataset(config.dataset_name, split=config.dataset_split)\n",
    "    \n",
    "    print(f\"✓ Loaded {len(ds)} records from FinanceBench\")\n",
    "    \n",
    "    # Display sample record structure\n",
    "    if len(ds) > 0:\n",
    "        print(\"\\nSample record keys:\")\n",
    "        for key in ds[0].keys():\n",
    "            print(f\"  - {key}\")\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.2 Extract Unique PDF Requirements\n",
    "\n",
    "# %%\n",
    "def get_required_pdfs(dataset) -> set:\n",
    "    \"\"\"\n",
    "    Extract unique PDF filenames required by the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace dataset\n",
    "        \n",
    "    Returns:\n",
    "        Set of PDF filenames\n",
    "    \"\"\"\n",
    "    unique_pdfs = set()\n",
    "    \n",
    "    for record in tqdm(dataset, desc=\"Scanning dataset for PDFs\"):\n",
    "        pdf_filename = record[\"doc_name\"] + \".pdf\"\n",
    "        unique_pdfs.add(pdf_filename)\n",
    "    \n",
    "    print(f\"\\n✓ Found {len(unique_pdfs)} unique PDF files required\")\n",
    "    \n",
    "    return unique_pdfs\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.3 Verify PDF Availability\n",
    "\n",
    "# %%\n",
    "def verify_pdfs(pdf_dir: str, required_pdfs: set) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Verify which PDFs are available and which are missing.\n",
    "    \n",
    "    Args:\n",
    "        pdf_dir: Directory containing PDFs\n",
    "        required_pdfs: Set of required PDF filenames\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (available_pdfs, missing_pdfs)\n",
    "    \"\"\"\n",
    "    available_pdfs = []\n",
    "    missing_pdfs = []\n",
    "    \n",
    "    for pdf_filename in tqdm(required_pdfs, desc=\"Verifying PDFs\"):\n",
    "        pdf_path = os.path.join(pdf_dir, pdf_filename)\n",
    "        if os.path.isfile(pdf_path):\n",
    "            available_pdfs.append(pdf_filename)\n",
    "        else:\n",
    "            missing_pdfs.append(pdf_filename)\n",
    "    \n",
    "    print(f\"\\n✓ Available: {len(available_pdfs)} PDFs\")\n",
    "    \n",
    "    if missing_pdfs:\n",
    "        print(f\"✗ Missing: {len(missing_pdfs)} PDFs\")\n",
    "        print(\"\\nMissing files:\")\n",
    "        for filename in missing_pdfs[:10]:  # Show first 10\n",
    "            print(f\"  - {filename}\")\n",
    "        if len(missing_pdfs) > 10:\n",
    "            print(f\"  ... and {len(missing_pdfs) - 10} more\")\n",
    "    else:\n",
    "        print(\"✓ All required PDFs are available\")\n",
    "    \n",
    "    return available_pdfs, missing_pdfs\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.4 Load PDF Documents\n",
    "\n",
    "# %%\n",
    "def load_pdf_documents(pdf_dir: str, pdf_filenames: List[str]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load PDF documents using PyMuPDF via LlamaIndex.\n",
    "    \n",
    "    Args:\n",
    "        pdf_dir: Directory containing PDFs\n",
    "        pdf_filenames: List of PDF filenames to load\n",
    "        \n",
    "    Returns:\n",
    "        List of LlamaIndex Document objects\n",
    "    \"\"\"\n",
    "    pdf_reader = PyMuPDFReader()\n",
    "    documents = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for pdf_file in tqdm(pdf_filenames, desc=\"Loading PDFs\"):\n",
    "        file_path = os.path.join(pdf_dir, pdf_file)\n",
    "        try:\n",
    "            doc = pdf_reader.load(file_path)\n",
    "            documents.extend(doc)\n",
    "        except Exception as e:\n",
    "            failed_files.append((pdf_file, str(e)))\n",
    "            print(f\"\\n✗ Failed to load {pdf_file}: {e}\")\n",
    "    \n",
    "    print(f\"\\n✓ Successfully loaded {len(documents)} document pages\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"✗ Failed to load {len(failed_files)} files\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.5 Analyze Document Statistics\n",
    "\n",
    "# %%\n",
    "def analyze_documents(documents: List[Document]) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze loaded documents and provide detailed statistics.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of LlamaIndex Document objects\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing document statistics\n",
    "    \"\"\"\n",
    "    print(\"Analyzing documents...\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_pages = len(documents)\n",
    "    total_chars = sum(len(doc.text) for doc in documents)\n",
    "    \n",
    "    # Approximate token count (rough estimate: 1 token ≈ 4 characters)\n",
    "    estimated_tokens = total_chars // 4\n",
    "    \n",
    "    # Character statistics per page\n",
    "    char_counts = [len(doc.text) for doc in documents]\n",
    "    avg_chars_per_page = total_chars / total_pages if total_pages > 0 else 0\n",
    "    min_chars = min(char_counts) if char_counts else 0\n",
    "    max_chars = max(char_counts) if char_counts else 0\n",
    "    \n",
    "    # Estimate chunks for different sizes\n",
    "    chunk_estimates = {}\n",
    "    for chunk_size in config.chunk_sizes:\n",
    "        overlap = int(chunk_size * (config.chunk_overlap_percentage / 100))\n",
    "        effective_chunk_size = chunk_size - overlap\n",
    "        estimated_chunks = (total_chars // effective_chunk_size) + len(documents)\n",
    "        chunk_estimates[chunk_size] = estimated_chunks\n",
    "    \n",
    "    # Get unique document sources\n",
    "    unique_sources = set()\n",
    "    for doc in documents:\n",
    "        if hasattr(doc, 'metadata') and 'file_name' in doc.metadata:\n",
    "            unique_sources.add(doc.metadata['file_name'])\n",
    "    \n",
    "    stats = {\n",
    "        'total_pages': total_pages,\n",
    "        'unique_documents': len(unique_sources),\n",
    "        'total_characters': total_chars,\n",
    "        'estimated_tokens': estimated_tokens,\n",
    "        'avg_chars_per_page': avg_chars_per_page,\n",
    "        'min_chars_per_page': min_chars,\n",
    "        'max_chars_per_page': max_chars,\n",
    "        'chunk_estimates': chunk_estimates\n",
    "    }\n",
    "    \n",
    "    # Display statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DOCUMENT STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Pages Loaded:        {stats['total_pages']:,}\")\n",
    "    print(f\"Unique PDF Documents:      {stats['unique_documents']:,}\")\n",
    "    print(f\"\\nContent Size:\")\n",
    "    print(f\"  Total Characters:        {stats['total_characters']:,}\")\n",
    "    print(f\"  Estimated Tokens:        {stats['estimated_tokens']:,}\")\n",
    "    print(f\"\\nPer-Page Statistics:\")\n",
    "    print(f\"  Average Characters:      {stats['avg_chars_per_page']:,.0f}\")\n",
    "    print(f\"  Min Characters:          {stats['min_chars_per_page']:,}\")\n",
    "    print(f\"  Max Characters:          {stats['max_chars_per_page']:,}\")\n",
    "    print(f\"\\nEstimated Chunks (with {config.chunk_overlap_percentage}% overlap):\")\n",
    "    for chunk_size, estimated_chunks in stats['chunk_estimates'].items():\n",
    "        print(f\"  Chunk Size {chunk_size:4d}:        ~{estimated_chunks:,} chunks\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.6 Execute Dataset Loading Pipeline\n",
    "\n",
    "# %%\n",
    "# Load dataset\n",
    "dataset = load_financebench_dataset(config)\n",
    "\n",
    "# %%\n",
    "# Get required PDFs\n",
    "required_pdfs = get_required_pdfs(dataset)\n",
    "\n",
    "# %%\n",
    "# Verify PDFs\n",
    "available_pdfs, missing_pdfs = verify_pdfs(config.pdf_dir, required_pdfs)\n",
    "\n",
    "# %%\n",
    "# Check if we can proceed\n",
    "if missing_pdfs:\n",
    "    print(\"\\n⚠️  Warning: Some PDFs are missing. Proceeding with available PDFs only.\")\n",
    "    proceed = input(\"Continue? (y/n): \").lower().strip() == 'y'\n",
    "    if not proceed:\n",
    "        raise SystemExit(\"Aborted by user\")\n",
    "\n",
    "# %%\n",
    "# Load documents\n",
    "documents = load_pdf_documents(config.pdf_dir, available_pdfs)\n",
    "\n",
    "# %%\n",
    "# Analyze documents\n",
    "doc_stats = analyze_documents(documents)\n",
    "\n",
    "# %%\n",
    "# Display summary\n",
    "print(f\"\\n✓ Dataset loading complete!\")\n",
    "print(f\"  - Dataset records: {len(dataset)}\")\n",
    "print(f\"  - PDF files loaded: {len(available_pdfs)}\")\n",
    "print(f\"  - Document pages: {len(documents)}\")\n",
    "print(f\"  - Estimated tokens: {doc_stats['estimated_tokens']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5a13186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 1 chunk size(s)...\n",
      "============================================================\n",
      "\n",
      ">>> Processing chunk size: 512\n",
      "Overlap: 76 chars (15%)\n",
      "Generating nodes (chunk_size=512, overlap=76)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518fcce937bd4459b7bfd1af95059d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/12013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 28,657 nodes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ba8c0107d84dfea6d1a46ac6076fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting to LangChain docs:   0%|          | 0/28657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted 28,657 nodes to LangChain documents\n",
      "\n",
      "============================================================\n",
      "CHUNK STATISTICS (Chunk Size: 512)\n",
      "============================================================\n",
      "Total Chunks:              28,657\n",
      "\n",
      "Content Size:\n",
      "  Total Characters:        43,783,209\n",
      "  Estimated Tokens:        10,945,802\n",
      "\n",
      "Chunk Statistics:\n",
      "  Average Length:          1,528 chars (382 tokens)\n",
      "  Min Length:              0 chars\n",
      "  Max Length:              4,103 chars\n",
      "\n",
      "Length Distribution:\n",
      "  < 256               :    465 chunks (  1.6%)\n",
      "  256-512             :  1,212 chunks (  4.2%)\n",
      "  512+                : 26,980 chunks ( 94.1%)\n",
      "============================================================\n",
      "✓ Chunk size 512 processing complete\n",
      "\n",
      "============================================================\n",
      "✓ All chunk sizes processed successfully\n",
      "\n",
      "============================================================\n",
      "PROCESSING SUMMARY\n",
      "============================================================\n",
      "\n",
      "Chunk Size 512:\n",
      "  Chunks:          28,657\n",
      "  Est. Tokens:     10,945,802\n",
      "  Avg per Chunk:   382 tokens\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Document Processing Functions\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.1 Generate Nodes from Documents\n",
    "\n",
    "# %%\n",
    "def generate_nodes(\n",
    "    documents: List[Document],\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int\n",
    ") -> List[BaseNode]:\n",
    "    \"\"\"\n",
    "    Generate nodes from documents using LlamaIndex SentenceSplitter.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of LlamaIndex Document objects\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of nodes\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If chunk_size or chunk_overlap is invalid\n",
    "    \"\"\"\n",
    "    # Validation\n",
    "    if chunk_size <= 0:\n",
    "        raise ValueError(\"Chunk size must be positive\")\n",
    "    if chunk_overlap < 0:\n",
    "        raise ValueError(\"Chunk overlap cannot be negative\")\n",
    "    if chunk_overlap >= chunk_size:\n",
    "        raise ValueError(\"Chunk overlap must be less than chunk size\")\n",
    "    \n",
    "    # Initialize parser\n",
    "    parser = SentenceSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    # Generate nodes with progress bar\n",
    "    print(f\"Generating nodes (chunk_size={chunk_size}, overlap={chunk_overlap})...\")\n",
    "    nodes = parser.get_nodes_from_documents(documents, show_progress=True)\n",
    "    \n",
    "    print(f\"✓ Created {len(nodes):,} nodes\")\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.2 Convert Nodes to LangChain Documents\n",
    "\n",
    "# %%\n",
    "def nodes_to_langchain_docs(\n",
    "    nodes: List[BaseNode],\n",
    "    chunk_size: int,\n",
    "    keep_node_metadata: bool = True\n",
    ") -> List[LCDocument]:\n",
    "    \"\"\"\n",
    "    Convert LlamaIndex nodes to LangChain documents.\n",
    "    \n",
    "    Args:\n",
    "        nodes: List of LlamaIndex nodes\n",
    "        chunk_size: Chunk size (for metadata)\n",
    "        keep_node_metadata: If True, include original node metadata\n",
    "        \n",
    "    Returns:\n",
    "        List of LangChain Document objects\n",
    "    \"\"\"\n",
    "    lc_docs = []\n",
    "    \n",
    "    for node in tqdm(nodes, desc=\"Converting to LangChain docs\"):\n",
    "        # Base metadata\n",
    "        metadata = {\"chunk_size\": chunk_size}\n",
    "        \n",
    "        # Add original metadata if requested\n",
    "        if keep_node_metadata and hasattr(node, 'metadata'):\n",
    "            metadata.update(node.metadata)\n",
    "        \n",
    "        # Create LangChain document\n",
    "        doc = LCDocument(\n",
    "            page_content=node.get_content(),\n",
    "            metadata=metadata\n",
    "        )\n",
    "        lc_docs.append(doc)\n",
    "    \n",
    "    print(f\"✓ Converted {len(lc_docs):,} nodes to LangChain documents\")\n",
    "    \n",
    "    return lc_docs\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.3 Analyze Chunks\n",
    "\n",
    "# %%\n",
    "def analyze_chunks(lc_docs: List[LCDocument], chunk_size: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze generated chunks and provide statistics.\n",
    "    \n",
    "    Args:\n",
    "        lc_docs: List of LangChain documents\n",
    "        chunk_size: Configured chunk size\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing chunk statistics\n",
    "    \"\"\"\n",
    "    # Calculate statistics\n",
    "    chunk_lengths = [len(doc.page_content) for doc in lc_docs]\n",
    "    total_chunks = len(lc_docs)\n",
    "    total_chars = sum(chunk_lengths)\n",
    "    avg_length = total_chars / total_chunks if total_chunks > 0 else 0\n",
    "    min_length = min(chunk_lengths) if chunk_lengths else 0\n",
    "    max_length = max(chunk_lengths) if chunk_lengths else 0\n",
    "    \n",
    "    # Estimate tokens\n",
    "    estimated_tokens = total_chars // 4\n",
    "    avg_tokens_per_chunk = estimated_tokens / total_chunks if total_chunks > 0 else 0\n",
    "    \n",
    "    # Calculate distribution\n",
    "    length_ranges = {\n",
    "        f\"< {chunk_size//2}\": sum(1 for l in chunk_lengths if l < chunk_size//2),\n",
    "        f\"{chunk_size//2}-{chunk_size}\": sum(1 for l in chunk_lengths if chunk_size//2 <= l < chunk_size),\n",
    "        f\"{chunk_size}+\": sum(1 for l in chunk_lengths if l >= chunk_size)\n",
    "    }\n",
    "    \n",
    "    stats = {\n",
    "        'total_chunks': total_chunks,\n",
    "        'total_characters': total_chars,\n",
    "        'estimated_tokens': estimated_tokens,\n",
    "        'avg_length': avg_length,\n",
    "        'avg_tokens_per_chunk': avg_tokens_per_chunk,\n",
    "        'min_length': min_length,\n",
    "        'max_length': max_length,\n",
    "        'length_distribution': length_ranges\n",
    "    }\n",
    "    \n",
    "    # Display statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"CHUNK STATISTICS (Chunk Size: {chunk_size})\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Chunks:              {stats['total_chunks']:,}\")\n",
    "    print(f\"\\nContent Size:\")\n",
    "    print(f\"  Total Characters:        {stats['total_characters']:,}\")\n",
    "    print(f\"  Estimated Tokens:        {stats['estimated_tokens']:,}\")\n",
    "    print(f\"\\nChunk Statistics:\")\n",
    "    print(f\"  Average Length:          {stats['avg_length']:,.0f} chars ({stats['avg_tokens_per_chunk']:,.0f} tokens)\")\n",
    "    print(f\"  Min Length:              {stats['min_length']:,} chars\")\n",
    "    print(f\"  Max Length:              {stats['max_length']:,} chars\")\n",
    "    print(f\"\\nLength Distribution:\")\n",
    "    for range_label, count in stats['length_distribution'].items():\n",
    "        percentage = (count / total_chunks * 100) if total_chunks > 0 else 0\n",
    "        print(f\"  {range_label:20s}: {count:6,} chunks ({percentage:5.1f}%)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.4 Process All Chunk Sizes\n",
    "\n",
    "# %%\n",
    "def process_all_chunk_sizes(\n",
    "    documents: List[Document],\n",
    "    config: RAGConfig\n",
    ") -> Dict[int, Dict]:\n",
    "    \"\"\"\n",
    "    Process documents for all configured chunk sizes.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of LlamaIndex documents\n",
    "        config: RAGConfig instance\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping chunk_size to processed data\n",
    "        {chunk_size: {'nodes': nodes, 'lc_docs': lc_docs, 'stats': stats}}\n",
    "    \"\"\"\n",
    "    processed_data = {}\n",
    "    \n",
    "    print(f\"\\nProcessing {len(config.chunk_sizes)} chunk size(s)...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for chunk_size in config.chunk_sizes:\n",
    "        print(f\"\\n>>> Processing chunk size: {chunk_size}\")\n",
    "        \n",
    "        # Calculate overlap\n",
    "        chunk_overlap = int(chunk_size * (config.chunk_overlap_percentage / 100))\n",
    "        print(f\"Overlap: {chunk_overlap} chars ({config.chunk_overlap_percentage}%)\")\n",
    "        \n",
    "        # Generate nodes\n",
    "        nodes = generate_nodes(\n",
    "            documents=documents,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        \n",
    "        # Convert to LangChain documents\n",
    "        lc_docs = nodes_to_langchain_docs(\n",
    "            nodes=nodes,\n",
    "            chunk_size=chunk_size,\n",
    "            keep_node_metadata=config.keep_node_metadata\n",
    "        )\n",
    "        \n",
    "        # Analyze chunks\n",
    "        stats = analyze_chunks(lc_docs, chunk_size)\n",
    "        \n",
    "        # Store processed data\n",
    "        processed_data[chunk_size] = {\n",
    "            'nodes': nodes,\n",
    "            'lc_docs': lc_docs,\n",
    "            'stats': stats,\n",
    "            'chunk_overlap': chunk_overlap\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ Chunk size {chunk_size} processing complete\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ All chunk sizes processed successfully\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.5 Execute Document Processing\n",
    "\n",
    "# %%\n",
    "# Process all configured chunk sizes\n",
    "processed_data = process_all_chunk_sizes(documents, config)\n",
    "\n",
    "# %%\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for chunk_size, data in processed_data.items():\n",
    "    stats = data['stats']\n",
    "    print(f\"\\nChunk Size {chunk_size}:\")\n",
    "    print(f\"  Chunks:          {stats['total_chunks']:,}\")\n",
    "    print(f\"  Est. Tokens:     {stats['estimated_tokens']:,}\")\n",
    "    print(f\"  Avg per Chunk:   {stats['avg_tokens_per_chunk']:,.0f} tokens\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b389a53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting: ../../vector_databases\n",
      "============================================================\n",
      "\n",
      "No existing vector databases found\n",
      "\n",
      "Your current configuration will create:\n",
      "  Database: ollama_nomic-embed-text\n",
      "  Location: ../../vector_databases/ollama_nomic-embed-text\n",
      "  Collections: ['chunk_512']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 4: Database Inspection\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.1 Inspect Existing Vector Databases\n",
    "\n",
    "# %%\n",
    "def inspect_vector_databases(base_dir: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Inspect all existing vector databases and their collections.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base directory containing vector databases\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with database information\n",
    "    \"\"\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"No databases found at: {base_dir}\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Inspecting: {base_dir}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    databases = {}\n",
    "    \n",
    "    # Scan for model-specific directories\n",
    "    for item in os.listdir(base_dir):\n",
    "        item_path = os.path.join(base_dir, item)\n",
    "        if not os.path.isdir(item_path):\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nDatabase: {item}\")\n",
    "        collections = {}\n",
    "        \n",
    "        # Check if valid ChromaDB exists\n",
    "        if not os.path.exists(os.path.join(item_path, \"chroma.sqlite3\")):\n",
    "            print(f\"  No valid ChromaDB found\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Valid ChromaDB detected\")\n",
    "        \n",
    "        # Try to find collections by scanning common chunk sizes\n",
    "        for chunk_size in [128, 256, 512, 1024, 2048]:\n",
    "            collection_name = f\"financebench_docs_chunk_{chunk_size}\"\n",
    "            \n",
    "            try:\n",
    "                # Use a dummy embedding for inspection\n",
    "                from langchain_ollama import OllamaEmbeddings\n",
    "                dummy_embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "                \n",
    "                vectorstore = Chroma(\n",
    "                    collection_name=collection_name,\n",
    "                    embedding_function=dummy_embedding,\n",
    "                    persist_directory=item_path\n",
    "                )\n",
    "                \n",
    "                count = vectorstore._collection.count()\n",
    "                if count > 0:\n",
    "                    collections[collection_name] = {\n",
    "                        'chunk_size': chunk_size,\n",
    "                        'document_count': count\n",
    "                    }\n",
    "                    print(f\"    • {collection_name}: {count:,} documents\")\n",
    "            except Exception:\n",
    "                pass  # Collection doesn't exist\n",
    "        \n",
    "        databases[item] = {\n",
    "            'path': item_path,\n",
    "            'collections': collections,\n",
    "            'total_documents': sum(c['document_count'] for c in collections.values())\n",
    "        }\n",
    "    \n",
    "    return databases\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.2 Display Summary\n",
    "\n",
    "# %%\n",
    "def display_database_summary(databases: Dict):\n",
    "    \"\"\"Display a summary of all databases and collections.\"\"\"\n",
    "    if not databases:\n",
    "        print(\"\\nNo existing vector databases found\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"VECTOR DATABASE SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    total_collections = 0\n",
    "    total_documents = 0\n",
    "    \n",
    "    for db_name, db_info in databases.items():\n",
    "        print(f\"\\n{db_name}\")\n",
    "        print(f\"  Path: {db_info['path']}\")\n",
    "        print(f\"  Collections: {len(db_info['collections'])}\")\n",
    "        print(f\"  Documents: {db_info['total_documents']:,}\")\n",
    "        \n",
    "        if db_info['collections']:\n",
    "            for coll_name, coll_info in db_info['collections'].items():\n",
    "                print(f\"    • Chunk {coll_info['chunk_size']}: {coll_info['document_count']:,} docs\")\n",
    "        \n",
    "        total_collections += len(db_info['collections'])\n",
    "        total_documents += db_info['total_documents']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Total: {len(databases)} database(s), {total_collections} collection(s), {total_documents:,} documents\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.3 Execute Inspection\n",
    "\n",
    "# %%\n",
    "# Inspect existing databases\n",
    "existing_databases = inspect_vector_databases(config.vector_db_dir)\n",
    "\n",
    "# %%\n",
    "# Display summary\n",
    "display_database_summary(existing_databases)\n",
    "\n",
    "# %%\n",
    "# Store for later use\n",
    "print(f\"\\nYour current configuration will create:\")\n",
    "print(f\"  Database: {config.get_model_identifier()}\")\n",
    "print(f\"  Location: {config.get_vector_db_dir()}\")\n",
    "print(f\"  Collections: {[f'chunk_{cs}' for cs in config.chunk_sizes]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a18486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding: ollama\n",
      "Model: nomic-embed-text\n",
      "✓ Initialized\n",
      "Database: ../../vector_databases/ollama_nomic-embed-text\n",
      "\n",
      "Checking existing collections...\n",
      "  → chunk_512: CREATE\n",
      "\n",
      "Processing 1 chunk size(s)...\n",
      "\n",
      "============================================================\n",
      "Chunk Size 512\n",
      "============================================================\n",
      "Collection: financebench_docs_chunk_512\n",
      "Documents: 28,657\n",
      "Adding in 58 batch(es)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be97724585f49f3a668696e99d19cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/28657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 17:52:59,531 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 1 failed: Query error: Database error: error returned from database: (code: 1032) attempt to write a readonly database\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 249\u001b[39m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m    244\u001b[39m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m# ## 5.5 Execute Population\u001b[39;00m\n\u001b[32m    246\u001b[39m \n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# Populate with smart skip logic\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m population_stats = \u001b[43msmart_populate_vector_stores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_recreate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set True to recreate all\u001b[39;49;00m\n\u001b[32m    253\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[32m    257\u001b[39m display_population_summary(population_stats, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 205\u001b[39m, in \u001b[36msmart_populate_vector_stores\u001b[39m\u001b[34m(processed_data, config, force_recreate)\u001b[39m\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    203\u001b[39m lc_docs = processed_data[chunk_size][\u001b[33m'\u001b[39m\u001b[33mlc_docs\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m stats = \u001b[43mpopulate_vector_store_for_chunk_size\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlc_docs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlc_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_function\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m all_stats[chunk_size] = stats\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(to_process) > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mpopulate_vector_store_for_chunk_size\u001b[39m\u001b[34m(lc_docs, chunk_size, config, embedding_function)\u001b[39m\n\u001b[32m     97\u001b[39m batch = lc_docs[batch_start:batch_end]\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[43mvectorstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m     added_count += \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[32m    102\u001b[39m     pbar.update(\u001b[38;5;28mlen\u001b[39m(batch))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/langchain_core/vectorstores/base.py:279\u001b[39m, in \u001b[36mVectorStore.add_documents\u001b[39m\u001b[34m(self, documents, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m     texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    278\u001b[39m     metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m msg = (\n\u001b[32m    281\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    283\u001b[39m )\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/langchain_community/vectorstores/chroma.py:277\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    279\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    280\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    281\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/langchain_ollama/embeddings.py:302\u001b[39m, in \u001b[36mOllamaEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    297\u001b[39m     msg = (\n\u001b[32m    298\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOllama client is not initialized. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    299\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure Ollama is running and the model is loaded.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_default_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkeep_alive\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/ollama/_client.py:377\u001b[39m, in \u001b[36mClient.embed\u001b[39m\u001b[34m(self, model, input, truncate, options, keep_alive, dimensions)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed\u001b[39m(\n\u001b[32m    369\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    370\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    375\u001b[39m   dimensions: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    376\u001b[39m ) -> EmbedResponse:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mEmbedResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/embed\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEmbedRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/ollama/_client.py:189\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    187\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/ollama/_client.py:129\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    128\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m     r.raise_for_status()\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/finance-rag/finance-rag/labs/build_vectors/myenv/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 5: Vector Store Population\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.1 Helper Functions\n",
    "\n",
    "# %%\n",
    "def clear_vector_db(db_path: str, max_attempts: int = 5, delay: float = 1.0) -> bool:\n",
    "    \"\"\"Clear existing vector database directory with retry logic.\"\"\"\n",
    "    if not os.path.exists(db_path):\n",
    "        return True\n",
    "    \n",
    "    print(f\"Clearing: {db_path}\")\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            shutil.rmtree(db_path, ignore_errors=True)\n",
    "            print(f\"✓ Cleared successfully\")\n",
    "            return True\n",
    "        except PermissionError as e:\n",
    "            print(f\"Attempt {attempt + 1}/{max_attempts} failed\")\n",
    "            if attempt < max_attempts - 1:\n",
    "                time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def check_existing_collections(\n",
    "    persist_directory: str,\n",
    "    collection_name: str,\n",
    "    embedding_function\n",
    ") -> Tuple[bool, int]:\n",
    "    \"\"\"Check if collection exists and get document count.\"\"\"\n",
    "    if not os.path.exists(persist_directory):\n",
    "        return False, 0\n",
    "    \n",
    "    try:\n",
    "        vectorstore = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=embedding_function,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        count = vectorstore._collection.count()\n",
    "        return count > 0, count\n",
    "    except Exception:\n",
    "        return False, 0\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.2 Populate Single Chunk Size\n",
    "\n",
    "# %%\n",
    "def populate_vector_store_for_chunk_size(\n",
    "    lc_docs: List[LCDocument],\n",
    "    chunk_size: int,\n",
    "    config: RAGConfig,\n",
    "    embedding_function\n",
    ") -> Dict:\n",
    "    \"\"\"Populate vector store for a specific chunk size.\"\"\"\n",
    "    collection_name = f\"{config.collection_name_prefix}{chunk_size}\"\n",
    "    persist_directory = config.get_vector_db_dir()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Chunk Size {chunk_size}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Collection: {collection_name}\")\n",
    "    print(f\"Documents: {len(lc_docs):,}\")\n",
    "    \n",
    "    # Create directory\n",
    "    os.makedirs(persist_directory, exist_ok=True)\n",
    "    \n",
    "    # Initialize vector store\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=embedding_function,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    \n",
    "    # Add documents in batches\n",
    "    total_docs = len(lc_docs)\n",
    "    batch_size = config.batch_size\n",
    "    num_batches = (total_docs + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"Adding in {num_batches} batch(es)...\")\n",
    "    \n",
    "    added_count = 0\n",
    "    failed_batches = []\n",
    "    \n",
    "    with tqdm(total=total_docs, desc=\"Progress\") as pbar:\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = min(batch_start + batch_size, total_docs)\n",
    "            batch = lc_docs[batch_start:batch_end]\n",
    "            \n",
    "            try:\n",
    "                vectorstore.add_documents(batch)\n",
    "                added_count += len(batch)\n",
    "                pbar.update(len(batch))\n",
    "            except Exception as e:\n",
    "                failed_batches.append((batch_idx, str(e)))\n",
    "                print(f\"\\nBatch {batch_idx + 1} failed: {e}\")\n",
    "                pbar.update(len(batch))\n",
    "    \n",
    "    # Persist\n",
    "    vectorstore.persist()\n",
    "    final_count = vectorstore._collection.count()\n",
    "    \n",
    "    print(f\"\\n✓ Added: {added_count:,} / {total_docs:,}\")\n",
    "    print(f\"✓ Final count: {final_count:,}\")\n",
    "    \n",
    "    return {\n",
    "        'chunk_size': chunk_size,\n",
    "        'collection_name': collection_name,\n",
    "        'status': 'completed' if not failed_batches else 'partial',\n",
    "        'total_documents': total_docs,\n",
    "        'added_documents': added_count,\n",
    "        'final_count': final_count,\n",
    "        'failed_batches': len(failed_batches)\n",
    "    }\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.3 Smart Populate (Skip Existing)\n",
    "\n",
    "# %%\n",
    "def smart_populate_vector_stores(\n",
    "    processed_data: Dict[int, Dict],\n",
    "    config: RAGConfig,\n",
    "    force_recreate: bool = False\n",
    ") -> Dict[int, Dict]:\n",
    "    \"\"\"\n",
    "    Populate vector stores, skipping existing collections.\n",
    "    \n",
    "    Args:\n",
    "        processed_data: Dictionary from Step 3\n",
    "        config: RAGConfig instance\n",
    "        force_recreate: If True, recreate existing collections\n",
    "        \n",
    "    Returns:\n",
    "        Population statistics\n",
    "    \"\"\"\n",
    "    # Initialize embedding\n",
    "    print(f\"Embedding: {config.embedding_provider}\")\n",
    "    if config.embedding_provider == \"ollama\":\n",
    "        print(f\"Model: {config.ollama_model}\")\n",
    "    else:\n",
    "        print(f\"Model: {config.openai_model}\")\n",
    "    \n",
    "    embedding_function = config.get_embedding_function()\n",
    "    print(\"✓ Initialized\")\n",
    "    \n",
    "    db_dir = config.get_vector_db_dir()\n",
    "    print(f\"Database: {db_dir}\")\n",
    "    \n",
    "    # Clear if force recreate\n",
    "    if force_recreate or config.clear_existing_db:\n",
    "        print(\"\\n⚠️  Force recreate enabled\")\n",
    "        if not clear_vector_db(db_dir):\n",
    "            raise RuntimeError(\"Failed to clear database\")\n",
    "    \n",
    "    # Check existing collections\n",
    "    print(f\"\\nChecking existing collections...\")\n",
    "    all_stats = {}\n",
    "    to_process = []\n",
    "    skipped = []\n",
    "    \n",
    "    for chunk_size in config.chunk_sizes:\n",
    "        collection_name = f\"{config.collection_name_prefix}{chunk_size}\"\n",
    "        exists, count = check_existing_collections(\n",
    "            db_dir, collection_name, embedding_function\n",
    "        )\n",
    "        \n",
    "        if exists and not force_recreate:\n",
    "            print(f\"  ✓ chunk_{chunk_size}: EXISTS ({count:,} docs) - SKIP\")\n",
    "            skipped.append(chunk_size)\n",
    "            all_stats[chunk_size] = {\n",
    "                'chunk_size': chunk_size,\n",
    "                'collection_name': collection_name,\n",
    "                'status': 'skipped',\n",
    "                'document_count': count\n",
    "            }\n",
    "        else:\n",
    "            status = \"RECREATE\" if exists else \"CREATE\"\n",
    "            print(f\"  → chunk_{chunk_size}: {status}\")\n",
    "            to_process.append(chunk_size)\n",
    "    \n",
    "    if not to_process:\n",
    "        print(\"\\n✓ All collections exist. Nothing to do.\")\n",
    "        return all_stats\n",
    "    \n",
    "    print(f\"\\nProcessing {len(to_process)} chunk size(s)...\")\n",
    "    \n",
    "    # Process each chunk size\n",
    "    for chunk_size in to_process:\n",
    "        if chunk_size not in processed_data:\n",
    "            print(f\"\\n✗ No data for chunk size {chunk_size}\")\n",
    "            continue\n",
    "        \n",
    "        lc_docs = processed_data[chunk_size]['lc_docs']\n",
    "        \n",
    "        stats = populate_vector_store_for_chunk_size(\n",
    "            lc_docs=lc_docs,\n",
    "            chunk_size=chunk_size,\n",
    "            config=config,\n",
    "            embedding_function=embedding_function\n",
    "        )\n",
    "        \n",
    "        all_stats[chunk_size] = stats\n",
    "        \n",
    "        if len(to_process) > 1:\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return all_stats\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.4 Display Results\n",
    "\n",
    "# %%\n",
    "def display_population_summary(stats: Dict[int, Dict], config: RAGConfig):\n",
    "    \"\"\"Display summary of population results.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"POPULATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Database: {config.get_model_identifier()}\")\n",
    "    print(f\"Location: {config.get_vector_db_dir()}\")\n",
    "    \n",
    "    total_docs = 0\n",
    "    for chunk_size, stat in stats.items():\n",
    "        status = \"✓\" if stat['status'] in ['completed', 'skipped'] else \"⚠\"\n",
    "        doc_count = stat.get('final_count', stat.get('document_count', 0))\n",
    "        print(f\"\\n{status} Chunk {chunk_size}: {stat['status'].upper()}\")\n",
    "        print(f\"    Documents: {doc_count:,}\")\n",
    "        total_docs += doc_count\n",
    "    \n",
    "    print(f\"\\nTotal: {total_docs:,} documents\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.5 Execute Population\n",
    "\n",
    "# %%\n",
    "# Populate with smart skip logic\n",
    "population_stats = smart_populate_vector_stores(\n",
    "    processed_data=processed_data,\n",
    "    config=config,\n",
    "    force_recreate=False  # Set True to recreate all\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Display results\n",
    "display_population_summary(population_stats, config)\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Population complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24310bb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RAGConfig.__init__() got an unexpected keyword argument 'vector_db_base_dir'. Did you mean 'vector_db_dir'?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m new_config\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# ## 6.2 Example 1: Create OpenAI Embeddings\u001b[39;00m\n\u001b[32m     69\u001b[39m \n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Create OpenAI configuration\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m new_config = \u001b[43mcreate_embedding_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopenai\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-embedding-3-small\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Or None for all chunk sizes, or [512, 1024] for specific ones\u001b[39;49;00m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclear_existing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     78\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m     81\u001b[39m is_valid, errors = new_config.validate()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mcreate_embedding_config\u001b[39m\u001b[34m(base_config, embedding_provider, model_name, chunk_sizes, clear_existing)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_embedding_config\u001b[39m(\n\u001b[32m     13\u001b[39m     base_config: RAGConfig,\n\u001b[32m     14\u001b[39m     embedding_provider: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     clear_existing: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     18\u001b[39m ) -> RAGConfig:\n\u001b[32m     19\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03m    Create configuration for a different embedding model.\u001b[39;00m\n\u001b[32m     21\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m \u001b[33;03m        New RAGConfig instance\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     new_config = \u001b[43mRAGConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Copy dataset settings\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_split\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpdf_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpdf_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvector_db_base_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvector_db_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Chunking settings\u001b[39;49;00m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_overlap_percentage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchunk_overlap_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# NEW embedding settings\u001b[39;49;00m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43membedding_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43mollama_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43membedding_provider\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mollama\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mollama_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopenai_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43membedding_provider\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopenai\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopenai_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Other settings\u001b[39;49;00m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_name_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollection_name_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclear_existing_db\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclear_existing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_node_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeep_node_metadata\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     56\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNEW EMBEDDING CONFIGURATION\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: RAGConfig.__init__() got an unexpected keyword argument 'vector_db_base_dir'. Did you mean 'vector_db_dir'?"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 6: Create Additional Embeddings (Optional)\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.1 Create New Configuration for Different Embedding\n",
    "# \n",
    "# Use this step when you want to create embeddings with a different model\n",
    "# WITHOUT reloading documents from Step 2-3.\n",
    "\n",
    "# %%\n",
    "def create_embedding_config(\n",
    "    base_config: RAGConfig,\n",
    "    embedding_provider: str,\n",
    "    model_name: str,\n",
    "    chunk_sizes: List[int] = None,\n",
    "    clear_existing: bool = False\n",
    ") -> RAGConfig:\n",
    "    \"\"\"\n",
    "    Create configuration for a different embedding model.\n",
    "    \n",
    "    Args:\n",
    "        base_config: Your existing RAGConfig\n",
    "        embedding_provider: \"ollama\" or \"openai\"\n",
    "        model_name: Model name\n",
    "        chunk_sizes: Which chunk sizes to process (None = all)\n",
    "        clear_existing: Whether to clear existing database\n",
    "        \n",
    "    Returns:\n",
    "        New RAGConfig instance\n",
    "    \"\"\"\n",
    "    new_config = RAGConfig(\n",
    "        # Copy dataset settings\n",
    "        dataset_name=base_config.dataset_name,\n",
    "        dataset_split=base_config.dataset_split,\n",
    "        pdf_dir=base_config.pdf_dir,\n",
    "        vector_db_dir=base_config.vector_db_dir,\n",
    "        \n",
    "        # Chunking settings\n",
    "        chunk_sizes=chunk_sizes if chunk_sizes else base_config.chunk_sizes,\n",
    "        chunk_overlap_percentage=base_config.chunk_overlap_percentage,\n",
    "        \n",
    "        # NEW embedding settings\n",
    "        embedding_provider=embedding_provider,\n",
    "        ollama_model=model_name if embedding_provider == \"ollama\" else base_config.ollama_model,\n",
    "        openai_model=model_name if embedding_provider == \"openai\" else base_config.openai_model,\n",
    "        \n",
    "        # Other settings\n",
    "        collection_name_prefix=base_config.collection_name_prefix,\n",
    "        batch_size=base_config.batch_size,\n",
    "        clear_existing_db=clear_existing,\n",
    "        keep_node_metadata=base_config.keep_node_metadata\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"NEW EMBEDDING CONFIGURATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Provider: {new_config.embedding_provider}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Database: {new_config.get_vector_db_dir()}\")\n",
    "    print(f\"Chunk sizes: {new_config.chunk_sizes}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return new_config\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.2 Example 1: Create OpenAI Embeddings\n",
    "\n",
    "# %%\n",
    "# Create OpenAI configuration\n",
    "new_config = create_embedding_config(\n",
    "    base_config=config,\n",
    "    embedding_provider=\"openai\",\n",
    "    model_name=\"text-embedding-3-small\",\n",
    "    chunk_sizes=[512],  # Or None for all chunk sizes, or [512, 1024] for specific ones\n",
    "    clear_existing=False\n",
    ")\n",
    "\n",
    "# Validate\n",
    "is_valid, errors = new_config.validate()\n",
    "if not is_valid:\n",
    "    print(\"\\n❌ Configuration errors:\")\n",
    "    for error in errors:\n",
    "        print(f\"  - {error}\")\n",
    "else:\n",
    "    print(\"\\n✓ Configuration valid\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.3 Example 2: Create Different Ollama Model\n",
    "\n",
    "# %%\n",
    "# Uncomment to use a different Ollama model\n",
    "# new_config = create_embedding_config(\n",
    "#     base_config=config,\n",
    "#     embedding_provider=\"ollama\",\n",
    "#     model_name=\"mxbai-embed-large\",  # Or any other Ollama model\n",
    "#     chunk_sizes=[512, 1024],\n",
    "#     clear_existing=False\n",
    "# )\n",
    "\n",
    "# # Validate\n",
    "# is_valid, errors = new_config.validate()\n",
    "# if not is_valid:\n",
    "#     print(\"\\n❌ Configuration errors:\")\n",
    "#     for error in errors:\n",
    "#         print(f\"  - {error}\")\n",
    "# else:\n",
    "#     print(\"\\n✓ Configuration valid\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.4 Verify Chunk Sizes Match\n",
    "\n",
    "# %%\n",
    "# Check what chunk sizes are available in processed_data\n",
    "print(\"Available chunk sizes in processed_data:\", list(processed_data.keys()))\n",
    "print(\"Requested chunk sizes in new_config:\", new_config.chunk_sizes)\n",
    "\n",
    "# Make sure they match!\n",
    "missing = set(new_config.chunk_sizes) - set(processed_data.keys())\n",
    "if missing:\n",
    "    print(f\"\\n⚠️  Warning: These chunk sizes are not in processed_data: {missing}\")\n",
    "    print(\"You need to either:\")\n",
    "    print(\"  1. Adjust new_config.chunk_sizes to match available sizes\")\n",
    "    print(\"  2. Or go back to Step 3 and process those chunk sizes\")\n",
    "else:\n",
    "    print(\"\\n✓ All requested chunk sizes are available\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.5 Execute Population for New Embedding\n",
    "\n",
    "# %%\n",
    "# Populate with the new embedding model\n",
    "new_population_stats = smart_populate_vector_stores(\n",
    "    processed_data=processed_data,\n",
    "    config=new_config,\n",
    "    force_recreate=False\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Display results\n",
    "display_population_summary(new_population_stats, new_config)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.6 Final Overview of All Databases\n",
    "\n",
    "# %%\n",
    "# Re-inspect to see all databases\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL DATABASES OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_databases = inspect_vector_databases(config.vector_db_dir)\n",
    "display_database_summary(all_databases)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.7 Quick Reference: Common Embedding Models\n",
    "# \n",
    "# **Ollama Models:**\n",
    "# - `nomic-embed-text` - Fast, good quality\n",
    "# - `mxbai-embed-large` - Larger, better quality\n",
    "# - `all-minilm` - Small, fast\n",
    "# \n",
    "# **OpenAI Models:**\n",
    "# - `text-embedding-3-small` - Cost-effective, 1536 dimensions\n",
    "# - `text-embedding-3-large` - Higher quality, 3072 dimensions\n",
    "# - `text-embedding-ada-002` - Legacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18c4df34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector store:\n",
      "  Database: ../../vector_databases/ollama_nomic-embed-text\n",
      "  Collection: financebench_docs_chunk_512\n",
      "  Documents: 2,000\n",
      "✓ Loaded successfully\n",
      "\n",
      "============================================================\n",
      "SIMILARITY SEARCH\n",
      "============================================================\n",
      "Query: What was the capital expenditure in 2018?\n",
      "Top-3 results:\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 18:02:44,236 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-05 18:02:44,272 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-05 18:02:44,327 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-05 18:02:44,355 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-05 18:02:44,395 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-05 18:02:44,416 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Result 1]\n",
      "Content: Other Division Information \n",
      "Total assets and capital spending of each division are as follows:\n",
      " \n",
      "Total Assets\n",
      "Capital Spending\n",
      " \n",
      "2022\n",
      "2021\n",
      "2022\n",
      "2021\n",
      "2020\n",
      "FLNA\n",
      "$\n",
      "11,042 \n",
      "$\n",
      "9,763 \n",
      "$\n",
      "1,464 \n",
      "$\n",
      "1,411 \n",
      "$\n",
      "1,...\n",
      "Metadata: {'total_pages': 503, 'chunk_size': 512, 'file_path': '../../financebench/documents/PEPSICO_2022_10K.pdf', 'source': '73'}\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Result 2]\n",
      "Content: 2018, respectively\n",
      "—\n",
      " \n",
      "—\n",
      "Additional paid-in capital\n",
      "11,174\n",
      " \n",
      "10,963\n",
      "Less: Treasury stock, at cost, 428,676,471 shares at December 31, 2019 and December 31, 2018\n",
      "(5,563)  \n",
      "(5,563)\n",
      "Retained earnings\n",
      "7,8...\n",
      "Metadata: {'chunk_size': 512, 'source': '69', 'total_pages': 198, 'file_path': '../../financebench/documents/ACTIVISIONBLIZZARD_2019_10K.pdf'}\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Result 3]\n",
      "Content: The deferred revenue is tracked on a per-customer contract-unit basis. As customers take delivery of the committed volumes under the terms \n",
      "of the contract, a per unit amount of deferred revenue is re...\n",
      "Metadata: {'file_path': '../../financebench/documents/CORNING_2020_10K.pdf', 'source': '42', 'chunk_size': 512, 'total_pages': 138}\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "SIMILARITY SEARCH WITH SCORES\n",
      "============================================================\n",
      "Query: What is the total revenue for fiscal year 2022?\n",
      "Top-5 results:\n",
      "============================================================\n",
      "\n",
      "[Result 1] Score: 0.5109\n",
      "Content: 85 | 2022 Annual Report\n",
      "Consolidated Revenue and Operating Margin\n",
      "Year Ended December 31, 2022 Compared to Year Ended December 31, 2021\n",
      "Revenue\n",
      "(in millions)\n",
      "Consolidated Revenue — Revenue increased $...\n",
      "Chunk Size: 512\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Result 2] Score: 0.5359\n",
      "Content: Total Revenue\n",
      "Long-Lived Assets\n",
      "Year Ended December 31,\n",
      "2022\n",
      "2021\n",
      "2020\n",
      "2022\n",
      "2021\n",
      "United States \n",
      "$\n",
      "4,093 \n",
      "$\n",
      "3,531 \n",
      "$\n",
      "3,243 \n",
      "$\n",
      "13,833 \n",
      "$\n",
      "11,034 \n",
      "Non-U.S.:\n",
      "Chile\n",
      "2,064 \n",
      "2,297 \n",
      "2,092 \n",
      "2,730 \n",
      "2,241 \n",
      "Domini...\n",
      "Chunk Size: 512\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Result 3] Score: 0.5558\n",
      "Content: 84 | 2022 Annual Report\n",
      "Review of Consolidated Results of Operations\n",
      "Years Ended December 31,\n",
      "2022\n",
      "2021\n",
      "2020\n",
      "% Change 2022 vs.\n",
      "2021\n",
      "% Change 2021 vs.\n",
      "2020\n",
      "(in millions, except per share amounts)\n",
      "Reven...\n",
      "Chunk Size: 512\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Result 4] Score: 0.5817\n",
      "Content: 129    \n",
      "Consolidated Statements of Operations\n",
      "Years ended December 31, 2022, 2021, and 2020\n",
      "2022\n",
      "2021\n",
      "2020\n",
      "(in millions, except per share amounts)\n",
      "Revenue:\n",
      "Regulated\n",
      "$\n",
      "3,538 \n",
      "$\n",
      "2,868 \n",
      "$\n",
      "2,661 \n",
      "Non-Reg...\n",
      "Chunk Size: 512\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Result 5] Score: 0.5868\n",
      "Content: 91 | 2022 Annual Report\n",
      "The 2022 effective tax rate was impacted by the current year nondeductible goodwill impairments at AES Andes and AES El Salvador, as\n",
      "well as the current year asset impairment o...\n",
      "Chunk Size: 512\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "COMPARING CHUNK SIZES\n",
      "============================================================\n",
      "Query: What were the operating expenses in 2021?\n",
      "============================================================\n",
      "\n",
      "--- Chunk Size: 512 ---\n",
      "Loading vector store:\n",
      "  Database: ../../vector_databases/ollama_nomic-embed-text\n",
      "  Collection: financebench_docs_chunk_512\n",
      "  Documents: 2,000\n",
      "✓ Loaded successfully\n",
      "\n",
      "Top Result (Score: 0.6138):\n",
      "Index\n",
      " \n",
      " \n",
      " \n",
      "RESULTS OF OPERATIONS\n",
      " \n",
      "Selected highlights from our operations follow (in millions):\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Year ended December 31,\n",
      " \n",
      "% change\n",
      " \n",
      "2020\n",
      " \n",
      "2019\n",
      " \n",
      "2018\n",
      " \n",
      "20 vs. 19\n",
      " \n",
      "19 vs. 18\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "   \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Net sales\n",
      "$\n",
      " 11,303  \n",
      "$\n",
      " 11,503  \n",
      "$\n",
      " 11,290 ...\n",
      "\n",
      "============================================================\n",
      "TESTING WITH FINANCEBENCH QUESTIONS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Company: Amcor\n",
      "Question: What was the key agenda of the AMCOR's 8k filing dated 1st July 2022?\n",
      "Expected Answer: Amcor Finance (USA), Inc. and Amcor Flexibles North America, Inc., entered into supplemental indentures relating to Guaranteed Senior Notes due 2026 and 2028. This involved the substitution of the Substitute Issuer (Amcor Flexibles North America) for the Former Issuer (Amcor Finance) and the assumption of covenants under the indentures. (In essence a novation agreement)\n",
      "============================================================\n",
      "\n",
      "Retrieved 3 documents:\n",
      "\n",
      "[Doc 1]\n",
      "104 | 2022 Annual Report\n",
      "As a result of the bankruptcy filing, AES Puerto Rico and AES Ilumina’s non-recourse debt of $143 million and $27 million, respectively, continue\n",
      "to be in technical default an...\n",
      "\n",
      "[Doc 2]\n",
      "83 | 2022 Annual Report\n",
      "ITEM 7. MANAGEMENT'S DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\n",
      "Executive Summary\n",
      "In 2022, AES delivered on its strategic and financial objectives...\n",
      "\n",
      "[Doc 3]\n",
      "124 | 2022 Annual Report\n",
      "ITEM 8. FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\n",
      "Part A — Report of Independent Registered Public Accounting Firm\n",
      "Our auditors are Ernst & Young LLP, located in Tysons, Vir...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Company: Walmart\n",
      "Question: What is FY2018 days payable outstanding (DPO) for Walmart? DPO is defined as: 365 * (average accounts payable between FY2017 and FY2018) / (FY2018 COGS + change in inventory between FY2017 and FY2018). Round your answer to two decimal places. Please base your judgments on the information provided primarily in the statement of financial position and the P&L statement.\n",
      "Expected Answer: 42.69\n",
      "============================================================\n",
      "\n",
      "Retrieved 3 documents:\n",
      "\n",
      "[Doc 1]\n",
      "Table of Contents\n",
      "is to replace damaged and out-of-date products. As a result, we record reserves, based on estimates, for anticipated damaged and\n",
      "out-of-date products.\n",
      "Our products are sold for cash ...\n",
      "\n",
      "[Doc 2]\n",
      "68 | 2022 Annual Report\n",
      "conditions, such as population changes, job and income growth, housing starts, new business formation and the overall level of economic\n",
      "activity. A lack of growth, or a decline...\n",
      "\n",
      "[Doc 3]\n",
      "Table of Contents\n",
      "Our critical accounting policies and estimates are:\n",
      "•\n",
      "revenue recognition;\n",
      "•\n",
      "goodwill and other intangible assets;\n",
      "•\n",
      "income tax expense and accruals; and\n",
      "•\n",
      "pension and retiree medica...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Company: Ulta Beauty\n",
      "Question: What drove the increase in Ulta Beauty's merchandise inventories balance at end of FY2023?\n",
      "Expected Answer: Increase in Merchandise inventories balance was driven by the opening of 47 new stores. The answer here assumes FY2023 refers to the 12 months ended on January 28, 2023 (although the company refers to this period as its fiscal 2022.\n",
      "============================================================\n",
      "\n",
      "Retrieved 3 documents:\n",
      "\n",
      "[Doc 1]\n",
      "Index\n",
      " \n",
      " \n",
      " \n",
      "Changes in the carrying amount of goodwill for the twelve months ended December 31, 2020 and 2019, were as follows (in millions):\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "...\n",
      "\n",
      "[Doc 2]\n",
      "*    Percent change not meaningful.\n",
      " \n",
      "For the year ended December 31, 2020, segment net sales decreased by $204 million, or 2%, when compared to the same period in 2019. The \n",
      "primary sales drivers by ...\n",
      "\n",
      "[Doc 3]\n",
      "FLNA\n",
      "Net revenue grew 19%, primarily driven by effective net pricing and a 2-percentage-point contribution from the 53  reporting\n",
      "week.\n",
      "Unit volume decreased 1%, primarily reflecting a double-digit de...\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 7: Testing and Querying Your Vector Stores\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7.1 Load Vector Store for Querying\n",
    "\n",
    "# %%\n",
    "def load_vector_store(\n",
    "    config: RAGConfig,\n",
    "    chunk_size: int\n",
    ") -> Chroma:\n",
    "    \"\"\"\n",
    "    Load an existing vector store for querying.\n",
    "    \n",
    "    Args:\n",
    "        config: RAGConfig instance\n",
    "        chunk_size: Which chunk size to load\n",
    "        \n",
    "    Returns:\n",
    "        Chroma vectorstore instance\n",
    "    \"\"\"\n",
    "    collection_name = f\"{config.collection_name_prefix}{chunk_size}\"\n",
    "    persist_directory = config.get_vector_db_dir()\n",
    "    \n",
    "    print(f\"Loading vector store:\")\n",
    "    print(f\"  Database: {persist_directory}\")\n",
    "    print(f\"  Collection: {collection_name}\")\n",
    "    \n",
    "    # Get embedding function\n",
    "    embedding_function = config.get_embedding_function()\n",
    "    \n",
    "    # Load vector store\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=embedding_function,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    \n",
    "    doc_count = vectorstore._collection.count()\n",
    "    print(f\"  Documents: {doc_count:,}\")\n",
    "    print(\"✓ Loaded successfully\")\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7.2 Simple Similarity Search\n",
    "\n",
    "# %%\n",
    "def test_similarity_search(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    k: int = 5\n",
    ") -> List:\n",
    "    \"\"\"\n",
    "    Perform similarity search and display results.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: Loaded Chroma vectorstore\n",
    "        query: Search query\n",
    "        k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of documents\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SIMILARITY SEARCH\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Top-{k} results:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Perform search\n",
    "    docs = vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "    # Display results\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"\\n[Result {i}]\")\n",
    "        print(f\"Content: {doc.page_content[:200]}...\")\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7.3 Similarity Search with Scores\n",
    "\n",
    "# %%\n",
    "def test_similarity_search_with_scores(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    k: int = 5\n",
    ") -> List:\n",
    "    \"\"\"\n",
    "    Perform similarity search with relevance scores.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: Loaded Chroma vectorstore\n",
    "        query: Search query\n",
    "        k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of (document, score) tuples\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SIMILARITY SEARCH WITH SCORES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Top-{k} results:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Perform search with scores\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    # Display results\n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"\\n[Result {i}] Score: {score:.4f}\")\n",
    "        print(f\"Content: {doc.page_content[:200]}...\")\n",
    "        \n",
    "        # Show relevant metadata\n",
    "        if 'file_name' in doc.metadata:\n",
    "            print(f\"File: {doc.metadata['file_name']}\")\n",
    "        if 'page_label' in doc.metadata:\n",
    "            print(f\"Page: {doc.metadata['page_label']}\")\n",
    "        if 'chunk_size' in doc.metadata:\n",
    "            print(f\"Chunk Size: {doc.metadata['chunk_size']}\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7.4 Compare Different Chunk Sizes\n",
    "\n",
    "# %%\n",
    "def compare_chunk_sizes(\n",
    "    config: RAGConfig,\n",
    "    query: str,\n",
    "    chunk_sizes: List[int] = None,\n",
    "    k: int = 3\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare retrieval results across different chunk sizes.\n",
    "    \n",
    "    Args:\n",
    "        config: RAGConfig instance\n",
    "        query: Search query\n",
    "        chunk_sizes: List of chunk sizes to compare (None = all)\n",
    "        k: Number of results per chunk size\n",
    "    \"\"\"\n",
    "    if chunk_sizes is None:\n",
    "        chunk_sizes = config.chunk_sizes\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPARING CHUNK SIZES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for chunk_size in chunk_sizes:\n",
    "        print(f\"\\n--- Chunk Size: {chunk_size} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Load vector store\n",
    "            vectorstore = load_vector_store(config, chunk_size)\n",
    "            \n",
    "            # Search\n",
    "            docs_with_scores = vectorstore.similarity_search_with_score(query, k=k)\n",
    "            \n",
    "            results[chunk_size] = docs_with_scores\n",
    "            \n",
    "            # Display top result\n",
    "            if docs_with_scores:\n",
    "                doc, score = docs_with_scores[0]\n",
    "                print(f\"\\nTop Result (Score: {score:.4f}):\")\n",
    "                print(f\"{doc.page_content[:300]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading chunk size {chunk_size}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7.5 Test with FinanceBench Questions\n",
    "\n",
    "# %%\n",
    "def test_with_financebench_questions(\n",
    "    vectorstore: Chroma,\n",
    "    dataset,\n",
    "    num_questions: int = 5,\n",
    "    k: int = 3\n",
    "):\n",
    "    \"\"\"\n",
    "    Test retrieval using actual FinanceBench questions.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: Loaded Chroma vectorstore\n",
    "        dataset: FinanceBench dataset\n",
    "        num_questions: Number of questions to test\n",
    "        k: Number of documents to retrieve per question\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TESTING WITH FINANCEBENCH QUESTIONS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    import random\n",
    "    sample_indices = random.sample(range(len(dataset)), num_questions)\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        record = dataset[idx]\n",
    "        question = record['question']\n",
    "        answer = record['answer']\n",
    "        company = record['company']\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Company: {company}\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected Answer: {answer}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        docs = vectorstore.similarity_search(question, k=k)\n",
    "        \n",
    "        print(f\"\\nRetrieved {len(docs)} documents:\")\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            print(f\"\\n[Doc {i}]\")\n",
    "            print(f\"{doc.page_content[:200]}...\")\n",
    "            if 'file_name' in doc.metadata:\n",
    "                print(f\"Source: {doc.metadata['file_name']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7.6 Execute Tests\n",
    "\n",
    "# %%\n",
    "# Load a vector store (using chunk size 512 as example)\n",
    "vectorstore = load_vector_store(config, chunk_size=512)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Test 1: Simple Similarity Search\n",
    "\n",
    "# %%\n",
    "# Test with a financial question\n",
    "query = \"What was the capital expenditure in 2018?\"\n",
    "docs = test_similarity_search(vectorstore, query, k=3)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Test 2: Similarity Search with Scores\n",
    "\n",
    "# %%\n",
    "# Test with scores\n",
    "query = \"What is the total revenue for fiscal year 2022?\"\n",
    "results = test_similarity_search_with_scores(vectorstore, query, k=5)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Test 3: Compare Different Chunk Sizes\n",
    "\n",
    "# %%\n",
    "# Compare retrieval across chunk sizes\n",
    "query = \"What were the operating expenses in 2021?\"\n",
    "comparison_results = compare_chunk_sizes(\n",
    "    config=config,\n",
    "    query=query,\n",
    "    chunk_sizes=[512],  # Add more if you have them: [256, 512, 1024]\n",
    "    k=3\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Test 4: Test with Real FinanceBench Questions\n",
    "\n",
    "# %%\n",
    "# Test with actual questions from the dataset\n",
    "test_with_financebench_questions(\n",
    "    vectorstore=vectorstore,\n",
    "    dataset=dataset,\n",
    "    num_questions=3,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7.7 Custom Query Helper\n",
    "\n",
    "# %%\n",
    "def quick_query(query: str, chunk_size: int = 512, k: int = 5):\n",
    "    \"\"\"\n",
    "    Quick helper function for ad-hoc queries.\n",
    "    \n",
    "    Args:\n",
    "        query: Your question\n",
    "        chunk_size: Which chunk size to use\n",
    "        k: Number of results\n",
    "    \"\"\"\n",
    "    vs = load_vector_store(config, chunk_size)\n",
    "    results = test_similarity_search_with_scores(vs, query, k)\n",
    "    return results\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7.8 Try Your Own Queries\n",
    "# \n",
    "# Use the quick_query function for ad-hoc testing:\n",
    "\n",
    "# %%\n",
    "# Example: Your custom query\n",
    "# results = quick_query(\n",
    "#     query=\"What is the revenue growth rate?\",\n",
    "#     chunk_size=512,\n",
    "#     k=5\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
