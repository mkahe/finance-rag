{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee342d2a",
   "metadata": {},
   "source": [
    "# General Tips\n",
    "## Using virtual environments\n",
    "**Step 1:** CD to desired directory and Create a Virtual Environment `python3 -m venv myenv`. (Run `py -3.13 -m venv myenv` for a specific version of python)\n",
    "\n",
    "Check your python installed versions with `py -0` on Windows (`python3 --version` on Linux)\n",
    "\n",
    "**Step 2:** Activate the Environment `source myenv/bin/activate` (on Linux) and `myenv\\Scripts\\activate` (on Windows).\n",
    "\n",
    "**Step 3:** Install Any Needed Packages. e.g: `pip install requests pandas`. Or better to use `requirements.txt` file (`pip install -r requirements.txt`)\n",
    "\n",
    "**Step 4:** List All Installed Packages using `pip list`\n",
    "\n",
    "## Connecting the Jupyter Notebook to the vistual env\n",
    "1. Make sure that myenv is activate (`myenv\\Scripts\\activate`)\n",
    "2. Run this inside the virtual environment: `pip install ipykernel`\n",
    "3. Still inside the environment: `python -m ipykernel install --user --name=myenv --display-name \"Whatever Python Kernel Name\"`\n",
    "   \n",
    "   --name=myenv: internal identifier for the kernel\n",
    "   \n",
    "   --display-name: name that shows up in VS Code kernel picker\n",
    "4. Open VS Code and select the kernel\n",
    "\n",
    "   At the top-right, click \"Select Kernel\".\n",
    "   Look for “Whatever Python Kernel Name” — pick that.\n",
    "5. If you don’t see it right away, try: Reloading VS Code, Or running Reload Window from Command Palette (Ctrl+Shift+P)\n",
    "\n",
    "## Useful Commands\n",
    "1. Use `py -0` to check which python installation we have on Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc52eb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n",
      "✓ OpenAI API key loaded\n",
      "✓ Ollama URL: http://localhost:11434\n",
      "✓ Configuration set\n",
      "  PDF Directory: ../../financebench/documents\n",
      "  Vector DB Directory: ../../vector_databases\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 1: Setup and Imports\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # FinanceBench RAG Pipeline - Clean Modular Approach\n",
    "# \n",
    "# This notebook processes financial documents and creates vector embeddings\n",
    "# for retrieval-augmented generation (RAG).\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.1 Install Requirements\n",
    "# \n",
    "# Make sure you have installed:\n",
    "# ```bash\n",
    "# pip install -r requirements.txt\n",
    "# ```\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.2 Imports\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Progress\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Document processing\n",
    "from llama_index.core.schema import Document, BaseNode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "# Vector stores\n",
    "from langchain.docstore.document import Document as LCDocument\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.3 Load Environment Variables\n",
    "\n",
    "# %%\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    print(\"✓ OpenAI API key loaded\")\n",
    "else:\n",
    "    print(\"⚠ OpenAI API key not found (only needed if using OpenAI embeddings)\")\n",
    "\n",
    "print(f\"✓ Ollama URL: {OLLAMA_BASE_URL}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.4 Configuration Variables\n",
    "\n",
    "# %%\n",
    "# Paths\n",
    "PDF_DIR = \"../../financebench/documents\"\n",
    "VECTOR_DB_DIR = \"../../vector_databases\"\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = \"PatronusAI/financebench\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "\n",
    "# Processing\n",
    "COLLECTION_PREFIX = \"financebench_docs_chunk_\"\n",
    "BATCH_SIZE = 500\n",
    "CHUNK_OVERLAP_PERCENTAGE = 15\n",
    "\n",
    "print(\"✓ Configuration set\")\n",
    "print(f\"  PDF Directory: {PDF_DIR}\")\n",
    "print(f\"  Vector DB Directory: {VECTOR_DB_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "666927ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: PatronusAI/financebench\n",
      "✓ Loaded 150 records\n",
      "\n",
      "Sample record keys:\n",
      "  - financebench_id\n",
      "  - company\n",
      "  - doc_name\n",
      "  - question_type\n",
      "  - question_reasoning\n",
      "  - domain_question_num\n",
      "  - question\n",
      "  - answer\n",
      "  - justification\n",
      "  - dataset_subset_label\n",
      "  - evidence\n",
      "  - gics_sector\n",
      "  - doc_type\n",
      "  - doc_period\n",
      "  - doc_link\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7225ad5e4d0c46c6894c9a9d24d79487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning for PDFs:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found 84 unique PDFs required\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b565109e5d4e495baaf55a4dad719a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Verifying PDFs:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Available: 84 PDFs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc0519000864afd920c0c132e4d0038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading PDFs:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Loaded 12013 pages from 84 PDFs\n",
      "\n",
      "============================================================\n",
      "DOCUMENT STATISTICS\n",
      "============================================================\n",
      "Total Pages:           12,013\n",
      "Total Characters:      40,649,449\n",
      "Estimated Tokens:      10,162,362\n",
      "\n",
      "Per-Page Statistics:\n",
      "  Average:             3,384 chars\n",
      "  Min:                 0 chars\n",
      "  Max:                 10,738 chars\n",
      "============================================================\n",
      "\n",
      "✓ Step 2 complete!\n",
      "  Dataset records: 150\n",
      "  PDFs loaded: 84\n",
      "  Document pages: 12013\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Load Dataset and Documents\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.1 Load FinanceBench Dataset\n",
    "\n",
    "# %%\n",
    "def load_financebench_dataset(dataset_name: str, split: str):\n",
    "    \"\"\"Load the FinanceBench dataset from HuggingFace.\"\"\"\n",
    "    print(f\"Loading dataset: {dataset_name}\")\n",
    "    ds = load_dataset(dataset_name, split=split)\n",
    "    print(f\"✓ Loaded {len(ds)} records\")\n",
    "    return ds\n",
    "\n",
    "# %%\n",
    "# Load dataset\n",
    "dataset = load_financebench_dataset(DATASET_NAME, DATASET_SPLIT)\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample record keys:\")\n",
    "for key in dataset[0].keys():\n",
    "    print(f\"  - {key}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.2 Extract Required PDFs\n",
    "\n",
    "# %%\n",
    "def get_required_pdfs(dataset) -> set:\n",
    "    \"\"\"Extract unique PDF filenames needed.\"\"\"\n",
    "    unique_pdfs = set()\n",
    "    for record in tqdm(dataset, desc=\"Scanning for PDFs\"):\n",
    "        pdf_filename = record[\"doc_name\"] + \".pdf\"\n",
    "        unique_pdfs.add(pdf_filename)\n",
    "    print(f\"✓ Found {len(unique_pdfs)} unique PDFs required\")\n",
    "    return unique_pdfs\n",
    "\n",
    "# %%\n",
    "required_pdfs = get_required_pdfs(dataset)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.3 Verify PDF Availability\n",
    "\n",
    "# %%\n",
    "def verify_pdfs(pdf_dir: str, required_pdfs: set) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Check which PDFs are available.\"\"\"\n",
    "    available = []\n",
    "    missing = []\n",
    "    \n",
    "    for pdf in tqdm(required_pdfs, desc=\"Verifying PDFs\"):\n",
    "        path = os.path.join(pdf_dir, pdf)\n",
    "        if os.path.isfile(path):\n",
    "            available.append(pdf)\n",
    "        else:\n",
    "            missing.append(pdf)\n",
    "    \n",
    "    print(f\"\\n✓ Available: {len(available)} PDFs\")\n",
    "    if missing:\n",
    "        print(f\"✗ Missing: {len(missing)} PDFs\")\n",
    "        for f in missing[:5]:\n",
    "            print(f\"  - {f}\")\n",
    "        if len(missing) > 5:\n",
    "            print(f\"  ... and {len(missing)-5} more\")\n",
    "    \n",
    "    return available, missing\n",
    "\n",
    "# %%\n",
    "available_pdfs, missing_pdfs = verify_pdfs(PDF_DIR, required_pdfs)\n",
    "\n",
    "# %%\n",
    "# Check if we can proceed\n",
    "if missing_pdfs:\n",
    "    print(\"\\n⚠ Some PDFs are missing\")\n",
    "    proceed = input(\"Continue with available PDFs only? (y/n): \").lower().strip()\n",
    "    if proceed != 'y':\n",
    "        raise SystemExit(\"Stopped by user\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.4 Load PDF Documents\n",
    "\n",
    "# %%\n",
    "def load_pdf_documents(pdf_dir: str, pdf_files: List[str]) -> List[Document]:\n",
    "    \"\"\"Load PDFs using PyMuPDF.\"\"\"\n",
    "    reader = PyMuPDFReader()\n",
    "    documents = []\n",
    "    failed = []\n",
    "    \n",
    "    for pdf in tqdm(pdf_files, desc=\"Loading PDFs\"):\n",
    "        path = os.path.join(pdf_dir, pdf)\n",
    "        try:\n",
    "            docs = reader.load(path)\n",
    "            documents.extend(docs)\n",
    "        except Exception as e:\n",
    "            failed.append((pdf, str(e)))\n",
    "            print(f\"\\n✗ Failed: {pdf}: {e}\")\n",
    "    \n",
    "    print(f\"\\n✓ Loaded {len(documents)} pages from {len(pdf_files)-len(failed)} PDFs\")\n",
    "    if failed:\n",
    "        print(f\"✗ Failed to load {len(failed)} PDFs\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# %%\n",
    "documents = load_pdf_documents(PDF_DIR, available_pdfs)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.5 Analyze Documents\n",
    "\n",
    "# %%\n",
    "def analyze_documents(documents: List[Document]) -> Dict:\n",
    "    \"\"\"Analyze loaded documents.\"\"\"\n",
    "    total_pages = len(documents)\n",
    "    total_chars = sum(len(doc.text) for doc in documents)\n",
    "    estimated_tokens = total_chars // 4\n",
    "    \n",
    "    char_counts = [len(doc.text) for doc in documents]\n",
    "    avg_chars = total_chars / total_pages if total_pages > 0 else 0\n",
    "    \n",
    "    stats = {\n",
    "        'total_pages': total_pages,\n",
    "        'total_characters': total_chars,\n",
    "        'estimated_tokens': estimated_tokens,\n",
    "        'avg_chars_per_page': avg_chars,\n",
    "        'min_chars': min(char_counts) if char_counts else 0,\n",
    "        'max_chars': max(char_counts) if char_counts else 0\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DOCUMENT STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Pages:           {stats['total_pages']:,}\")\n",
    "    print(f\"Total Characters:      {stats['total_characters']:,}\")\n",
    "    print(f\"Estimated Tokens:      {stats['estimated_tokens']:,}\")\n",
    "    print(f\"\\nPer-Page Statistics:\")\n",
    "    print(f\"  Average:             {stats['avg_chars_per_page']:,.0f} chars\")\n",
    "    print(f\"  Min:                 {stats['min_chars']:,} chars\")\n",
    "    print(f\"  Max:                 {stats['max_chars']:,} chars\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# %%\n",
    "doc_stats = analyze_documents(documents)\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 2 complete!\")\n",
    "print(f\"  Dataset records: {len(dataset)}\")\n",
    "print(f\"  PDFs loaded: {len(available_pdfs)}\")\n",
    "print(f\"  Document pages: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a0f336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 3 chunk size(s)...\n",
      "\n",
      "============================================================\n",
      "PROCESSING CHUNK SIZE: 256\n",
      "============================================================\n",
      "Overlap: 38 chars (15%)\n",
      "Generating nodes (size=256, overlap=38)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a7f618fcd04989b04e2f86265cbec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/12013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 57,903 nodes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d543a1db979c44fd8b799c1cf1994b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting to LangChain:   0%|          | 0/57903 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted 57,903 documents\n",
      "\n",
      "============================================================\n",
      "CHUNK STATISTICS (Size: 256)\n",
      "============================================================\n",
      "Total Chunks:          57,903\n",
      "Total Characters:      43,436,009\n",
      "Estimated Tokens:      10,859,002\n",
      "\n",
      "Per-Chunk Statistics:\n",
      "  Average:             750 chars\n",
      "  Min:                 0 chars\n",
      "  Max:                 2,121 chars\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PROCESSING CHUNK SIZE: 512\n",
      "============================================================\n",
      "Overlap: 76 chars (15%)\n",
      "Generating nodes (size=512, overlap=76)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b80c5ed8b84f1bbbfee0ba8c3af116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/12013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 28,657 nodes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eeafee8976f4866a7205692808c9b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting to LangChain:   0%|          | 0/28657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted 28,657 documents\n",
      "\n",
      "============================================================\n",
      "CHUNK STATISTICS (Size: 512)\n",
      "============================================================\n",
      "Total Chunks:          28,657\n",
      "Total Characters:      43,783,209\n",
      "Estimated Tokens:      10,945,802\n",
      "\n",
      "Per-Chunk Statistics:\n",
      "  Average:             1,528 chars\n",
      "  Min:                 0 chars\n",
      "  Max:                 4,103 chars\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PROCESSING CHUNK SIZE: 1024\n",
      "============================================================\n",
      "Overlap: 153 chars (15%)\n",
      "Generating nodes (size=1024, overlap=153)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c2c960a628442fb4f597335df8529c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/12013 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 15,787 nodes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543b8ea12f7f47d29526a0fe49f4c752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting to LangChain:   0%|          | 0/15787 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Converted 15,787 documents\n",
      "\n",
      "============================================================\n",
      "CHUNK STATISTICS (Size: 1024)\n",
      "============================================================\n",
      "Total Chunks:          15,787\n",
      "Total Characters:      42,395,002\n",
      "Estimated Tokens:      10,598,750\n",
      "\n",
      "Per-Chunk Statistics:\n",
      "  Average:             2,685 chars\n",
      "  Min:                 0 chars\n",
      "  Max:                 7,205 chars\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PROCESSING SUMMARY\n",
      "============================================================\n",
      "Chunk 256: 57,903 chunks, ~10,859,002 tokens\n",
      "Chunk 512: 28,657 chunks, ~10,945,802 tokens\n",
      "Chunk 1024: 15,787 chunks, ~10,598,750 tokens\n",
      "============================================================\n",
      "\n",
      "✓ Step 3 complete!\n",
      "Processed chunk sizes: [256, 512, 1024]\n",
      "Total chunks across all sizes: 102,347\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Process Documents into Chunks\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.1 Generate Nodes (Chunks)\n",
    "\n",
    "# %%\n",
    "def generate_nodes(\n",
    "    documents: List[Document],\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int\n",
    ") -> List[BaseNode]:\n",
    "    \"\"\"Generate nodes from documents using SentenceSplitter.\"\"\"\n",
    "    parser = SentenceSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    print(f\"Generating nodes (size={chunk_size}, overlap={chunk_overlap})...\")\n",
    "    nodes = parser.get_nodes_from_documents(documents, show_progress=True)\n",
    "    print(f\"✓ Created {len(nodes):,} nodes\")\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.2 Convert to LangChain Documents\n",
    "\n",
    "# %%\n",
    "def nodes_to_langchain_docs(\n",
    "    nodes: List[BaseNode],\n",
    "    chunk_size: int\n",
    ") -> List[LCDocument]:\n",
    "    \"\"\"Convert LlamaIndex nodes to LangChain documents.\"\"\"\n",
    "    lc_docs = []\n",
    "    \n",
    "    for node in tqdm(nodes, desc=\"Converting to LangChain\"):\n",
    "        metadata = {\"chunk_size\": chunk_size}\n",
    "        \n",
    "        # Add original metadata\n",
    "        if hasattr(node, 'metadata'):\n",
    "            metadata.update(node.metadata)\n",
    "        \n",
    "        doc = LCDocument(\n",
    "            page_content=node.get_content(),\n",
    "            metadata=metadata\n",
    "        )\n",
    "        lc_docs.append(doc)\n",
    "    \n",
    "    print(f\"✓ Converted {len(lc_docs):,} documents\")\n",
    "    return lc_docs\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.3 Analyze Chunks\n",
    "\n",
    "# %%\n",
    "def analyze_chunks(lc_docs: List[LCDocument], chunk_size: int) -> Dict:\n",
    "    \"\"\"Analyze generated chunks.\"\"\"\n",
    "    chunk_lengths = [len(doc.page_content) for doc in lc_docs]\n",
    "    total_chunks = len(lc_docs)\n",
    "    total_chars = sum(chunk_lengths)\n",
    "    \n",
    "    stats = {\n",
    "        'total_chunks': total_chunks,\n",
    "        'total_characters': total_chars,\n",
    "        'estimated_tokens': total_chars // 4,\n",
    "        'avg_length': total_chars / total_chunks if total_chunks > 0 else 0,\n",
    "        'min_length': min(chunk_lengths) if chunk_lengths else 0,\n",
    "        'max_length': max(chunk_lengths) if chunk_lengths else 0\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"CHUNK STATISTICS (Size: {chunk_size})\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Chunks:          {stats['total_chunks']:,}\")\n",
    "    print(f\"Total Characters:      {stats['total_characters']:,}\")\n",
    "    print(f\"Estimated Tokens:      {stats['estimated_tokens']:,}\")\n",
    "    print(f\"\\nPer-Chunk Statistics:\")\n",
    "    print(f\"  Average:             {stats['avg_length']:,.0f} chars\")\n",
    "    print(f\"  Min:                 {stats['min_length']:,} chars\")\n",
    "    print(f\"  Max:                 {stats['max_length']:,} chars\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.4 Process Single Chunk Size\n",
    "\n",
    "# %%\n",
    "def process_chunk_size(\n",
    "    documents: List[Document],\n",
    "    chunk_size: int,\n",
    "    overlap_percentage: int = 15\n",
    ") -> Dict:\n",
    "    \"\"\"Process documents for a single chunk size.\"\"\"\n",
    "    # Calculate overlap\n",
    "    chunk_overlap = int(chunk_size * (overlap_percentage / 100))\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING CHUNK SIZE: {chunk_size}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Overlap: {chunk_overlap} chars ({overlap_percentage}%)\")\n",
    "    \n",
    "    # Generate nodes\n",
    "    nodes = generate_nodes(documents, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Convert to LangChain docs\n",
    "    lc_docs = nodes_to_langchain_docs(nodes, chunk_size)\n",
    "    \n",
    "    # Analyze\n",
    "    stats = analyze_chunks(lc_docs, chunk_size)\n",
    "    \n",
    "    return {\n",
    "        'chunk_size': chunk_size,\n",
    "        'chunk_overlap': chunk_overlap,\n",
    "        'nodes': nodes,\n",
    "        'lc_docs': lc_docs,\n",
    "        'stats': stats\n",
    "    }\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.5 Process Multiple Chunk Sizes\n",
    "\n",
    "# %%\n",
    "def process_multiple_chunk_sizes(\n",
    "    documents: List[Document],\n",
    "    chunk_sizes: List[int],\n",
    "    overlap_percentage: int = 15\n",
    ") -> Dict[int, Dict]:\n",
    "    \"\"\"Process documents for multiple chunk sizes.\"\"\"\n",
    "    print(f\"\\nProcessing {len(chunk_sizes)} chunk size(s)...\")\n",
    "    \n",
    "    processed_data = {}\n",
    "    \n",
    "    for chunk_size in chunk_sizes:\n",
    "        data = process_chunk_size(documents, chunk_size, overlap_percentage)\n",
    "        processed_data[chunk_size] = data\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    for cs, data in processed_data.items():\n",
    "        stats = data['stats']\n",
    "        print(f\"Chunk {cs}: {stats['total_chunks']:,} chunks, \"\n",
    "              f\"~{stats['estimated_tokens']:,} tokens\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.6 Execute Processing\n",
    "\n",
    "# %%\n",
    "# Define which chunk sizes you want to process\n",
    "CHUNK_SIZES = [256, 512, 1024]  # Add more as needed: [128, 256, 512, 1024]\n",
    "\n",
    "# %%\n",
    "# Process all chunk sizes\n",
    "processed_data = process_multiple_chunk_sizes(\n",
    "    documents=documents,\n",
    "    chunk_sizes=CHUNK_SIZES,\n",
    "    overlap_percentage=CHUNK_OVERLAP_PERCENTAGE\n",
    ")\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 3 complete!\")\n",
    "print(f\"Processed chunk sizes: {list(processed_data.keys())}\")\n",
    "print(f\"Total chunks across all sizes: {sum(d['stats']['total_chunks'] for d in processed_data.values()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff374659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SCANNING DATABASES\n",
      "============================================================\n",
      "Location: ../../vector_databases\n",
      "\n",
      "Database: openai_text-embedding-3-small\n",
      "  Provider: openai\n",
      "  Model: text-embedding-3-small\n",
      "    • Chunk 512: 29,298 documents\n",
      "  Total: 29,298 documents\n",
      "\n",
      "Database: ollama_nomic-embed-text\n",
      "  Provider: ollama\n",
      "  Model: nomic-embed-text\n",
      "    • Chunk 512: 28,657 documents\n",
      "    • Chunk 1024: 15,787 documents\n",
      "  Total: 44,444 documents\n",
      "\n",
      "\n",
      "============================================================\n",
      "DATABASE SUMMARY\n",
      "============================================================\n",
      "\n",
      "openai_text-embedding-3-small\n",
      "  Provider: openai\n",
      "  Model: text-embedding-3-small\n",
      "  Collections: 1\n",
      "  Documents: 29,298\n",
      "\n",
      "ollama_nomic-embed-text\n",
      "  Provider: ollama\n",
      "  Model: nomic-embed-text\n",
      "  Collections: 2\n",
      "  Documents: 44,444\n",
      "\n",
      "============================================================\n",
      "Total: 2 database(s), 3 collection(s), 73,742 documents\n",
      "============================================================\n",
      "\n",
      "✓ Step 4 complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 4: Inspect Existing Databases\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.1 Scan All Databases\n",
    "\n",
    "# %%\n",
    "def inspect_all_databases(base_dir: str = \"../../vector_databases\") -> Dict:\n",
    "    \"\"\"Scan and inspect all embedding databases.\"\"\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"No databases found at: {base_dir}\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SCANNING DATABASES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Location: {base_dir}\\n\")\n",
    "    \n",
    "    all_dbs = {}\n",
    "    \n",
    "    for item in os.listdir(base_dir):\n",
    "        item_path = os.path.join(base_dir, item)\n",
    "        if not os.path.isdir(item_path):\n",
    "            continue\n",
    "        \n",
    "        # Parse provider_model format\n",
    "        if '_' not in item:\n",
    "            continue\n",
    "        \n",
    "        parts = item.split('_', 1)\n",
    "        provider = parts[0]\n",
    "        model = parts[1]\n",
    "        \n",
    "        print(f\"Database: {item}\")\n",
    "        print(f\"  Provider: {provider}\")\n",
    "        print(f\"  Model: {model}\")\n",
    "        \n",
    "        # Check for ChromaDB\n",
    "        if not os.path.exists(os.path.join(item_path, \"chroma.sqlite3\")):\n",
    "            print(f\"  Status: Not a valid ChromaDB\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Inspect collections\n",
    "        collections = {}\n",
    "        try:\n",
    "            # Import appropriate embedding\n",
    "            if provider == \"ollama\":\n",
    "                from langchain_ollama import OllamaEmbeddings\n",
    "                emb = OllamaEmbeddings(model=model)\n",
    "            elif provider == \"openai\":\n",
    "                from langchain_openai import OpenAIEmbeddings\n",
    "                emb = OpenAIEmbeddings(model=model)\n",
    "            else:\n",
    "                print(f\"  Status: Unknown provider\\n\")\n",
    "                continue\n",
    "            \n",
    "            # Check common chunk sizes\n",
    "            for cs in [128, 256, 512, 1024, 2048]:\n",
    "                coll_name = f\"{COLLECTION_PREFIX}{cs}\"\n",
    "                try:\n",
    "                    vs = Chroma(\n",
    "                        collection_name=coll_name,\n",
    "                        embedding_function=emb,\n",
    "                        persist_directory=item_path\n",
    "                    )\n",
    "                    count = vs._collection.count()\n",
    "                    if count > 0:\n",
    "                        collections[cs] = count\n",
    "                        print(f\"    • Chunk {cs}: {count:,} documents\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            if collections:\n",
    "                all_dbs[item] = {\n",
    "                    'provider': provider,\n",
    "                    'model': model,\n",
    "                    'path': item_path,\n",
    "                    'collections': collections,\n",
    "                    'total_docs': sum(collections.values())\n",
    "                }\n",
    "                print(f\"  Total: {sum(collections.values()):,} documents\\n\")\n",
    "            else:\n",
    "                print(f\"  Status: No collections found\\n\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\\n\")\n",
    "    \n",
    "    return all_dbs\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.2 Display Summary\n",
    "\n",
    "# %%\n",
    "def display_summary(databases: Dict):\n",
    "    \"\"\"Display summary of all databases.\"\"\"\n",
    "    if not databases:\n",
    "        print(\"\\n❌ No databases found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DATABASE SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    total_colls = 0\n",
    "    total_docs = 0\n",
    "    \n",
    "    for db_name, info in databases.items():\n",
    "        print(f\"\\n{db_name}\")\n",
    "        print(f\"  Provider: {info['provider']}\")\n",
    "        print(f\"  Model: {info['model']}\")\n",
    "        print(f\"  Collections: {len(info['collections'])}\")\n",
    "        print(f\"  Documents: {info['total_docs']:,}\")\n",
    "        \n",
    "        total_colls += len(info['collections'])\n",
    "        total_docs += info['total_docs']\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Total: {len(databases)} database(s), {total_colls} collection(s), {total_docs:,} documents\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.3 Execute Inspection\n",
    "\n",
    "# %%\n",
    "# Scan all databases\n",
    "all_databases = inspect_all_databases(VECTOR_DB_DIR)\n",
    "\n",
    "# %%\n",
    "# Display summary\n",
    "display_summary(all_databases)\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 4 complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26a3b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 5: Add Embeddings Flexibly\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.1 Helper Functions\n",
    "\n",
    "# %%\n",
    "def get_embedding_function(provider: str, model: str):\n",
    "    \"\"\"Get embedding function for a provider/model.\"\"\"\n",
    "    if provider == \"ollama\":\n",
    "        from langchain_ollama import OllamaEmbeddings\n",
    "        return OllamaEmbeddings(model=model, base_url=OLLAMA_BASE_URL)\n",
    "    elif provider == \"openai\":\n",
    "        from langchain_openai import OpenAIEmbeddings\n",
    "        return OpenAIEmbeddings(model=model, openai_api_key=OPENAI_API_KEY)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "\n",
    "def get_db_path(base_dir: str, provider: str, model: str) -> str:\n",
    "    \"\"\"Get database path for embedding.\"\"\"\n",
    "    model_id = f\"{provider}_{model.replace('/', '_')}\"\n",
    "    return os.path.join(base_dir, model_id)\n",
    "\n",
    "\n",
    "def check_collection_exists(db_path: str, collection_name: str, embedding_fn) -> Tuple[bool, int]:\n",
    "    \"\"\"Check if collection exists and get count.\"\"\"\n",
    "    if not os.path.exists(db_path):\n",
    "        return False, 0\n",
    "    \n",
    "    try:\n",
    "        vs = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=embedding_fn,\n",
    "            persist_directory=db_path\n",
    "        )\n",
    "        count = vs._collection.count()\n",
    "        return count > 0, count\n",
    "    except Exception:\n",
    "        return False, 0\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.2 Add Single Chunk Size\n",
    "\n",
    "# %%\n",
    "def add_chunk_size_to_embedding(\n",
    "    processed_data: Dict[int, Dict],\n",
    "    chunk_size: int,\n",
    "    embedding_provider: str,\n",
    "    embedding_model: str,\n",
    "    base_db_dir: str = \"../../vector_databases\",\n",
    "    collection_prefix: str = \"financebench_docs_chunk_\",\n",
    "    batch_size: int = 500,\n",
    "    skip_if_exists: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Add a single chunk size to an embedding database.\n",
    "    \n",
    "    Args:\n",
    "        processed_data: Output from Step 3\n",
    "        chunk_size: Which chunk size (must exist in processed_data)\n",
    "        embedding_provider: \"ollama\" or \"openai\"\n",
    "        embedding_model: Model name\n",
    "        base_db_dir: Base database directory\n",
    "        collection_prefix: Collection name prefix\n",
    "        batch_size: Batch size for adding documents\n",
    "        skip_if_exists: Skip if collection already exists\n",
    "        \n",
    "    Returns:\n",
    "        Statistics dictionary\n",
    "    \"\"\"\n",
    "    # Validate\n",
    "    if chunk_size not in processed_data:\n",
    "        raise ValueError(f\"Chunk size {chunk_size} not in processed_data. \"\n",
    "                        f\"Available: {list(processed_data.keys())}\")\n",
    "    \n",
    "    # Setup\n",
    "    db_path = get_db_path(base_db_dir, embedding_provider, embedding_model)\n",
    "    collection_name = f\"{collection_prefix}{chunk_size}\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ADDING CHUNK SIZE {chunk_size}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Provider: {embedding_provider}\")\n",
    "    print(f\"Model: {embedding_model}\")\n",
    "    print(f\"Database: {db_path}\")\n",
    "    print(f\"Collection: {collection_name}\")\n",
    "    \n",
    "    # Get embedding function\n",
    "    emb_fn = get_embedding_function(embedding_provider, embedding_model)\n",
    "    \n",
    "    # Check if exists\n",
    "    exists, count = check_collection_exists(db_path, collection_name, emb_fn)\n",
    "    if exists and skip_if_exists:\n",
    "        print(f\"\\n✓ Already exists with {count:,} documents - SKIPPING\")\n",
    "        return {\n",
    "            'status': 'skipped',\n",
    "            'chunk_size': chunk_size,\n",
    "            'collection_name': collection_name,\n",
    "            'document_count': count\n",
    "        }\n",
    "    \n",
    "    # Get documents\n",
    "    lc_docs = processed_data[chunk_size]['lc_docs']\n",
    "    print(f\"Documents: {len(lc_docs):,}\")\n",
    "    \n",
    "    # Create database directory\n",
    "    os.makedirs(db_path, exist_ok=True)\n",
    "    \n",
    "    # Initialize vectorstore\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=emb_fn,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    \n",
    "    # Add in batches\n",
    "    total = len(lc_docs)\n",
    "    num_batches = (total + batch_size - 1) // batch_size\n",
    "    print(f\"\\nAdding in {num_batches} batch(es)...\")\n",
    "    \n",
    "    added = 0\n",
    "    with tqdm(total=total, desc=\"Progress\") as pbar:\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, total)\n",
    "            batch = lc_docs[start:end]\n",
    "            \n",
    "            try:\n",
    "                vectorstore.add_documents(batch)\n",
    "                added += len(batch)\n",
    "                pbar.update(len(batch))\n",
    "            except Exception as e:\n",
    "                print(f\"\\nBatch {i+1} failed: {e}\")\n",
    "                pbar.update(len(batch))\n",
    "    \n",
    "    # Persist\n",
    "    vectorstore.persist()\n",
    "    final_count = vectorstore._collection.count()\n",
    "    \n",
    "    print(f\"\\n✓ Complete: {added:,}/{total:,} added, {final_count:,} final\")\n",
    "    \n",
    "    return {\n",
    "        'status': 'completed',\n",
    "        'chunk_size': chunk_size,\n",
    "        'collection_name': collection_name,\n",
    "        'added': added,\n",
    "        'final_count': final_count\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8b4e833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 12:42:15,200 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ADDING CHUNK SIZE 256\n",
      "============================================================\n",
      "Provider: ollama\n",
      "Model: bge-m3\n",
      "Database: ../../vector_databases/ollama_bge-m3\n",
      "Collection: financebench_docs_chunk_256\n",
      "Documents: 57,903\n",
      "\n",
      "Adding in 116 batch(es)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3690f2bbfcac4a7c88306b59dc858e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/57903 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 12:42:54,906 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:43:32,475 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:44:12,987 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:45:09,872 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:46:09,396 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:47:09,319 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:48:13,320 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:49:09,288 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:50:03,704 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:51:01,643 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:52:05,977 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:53:06,471 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:54:06,213 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:55:07,380 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:56:09,064 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:57:10,607 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:58:14,898 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:59:09,771 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 12:59:46,639 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:00:29,526 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:01:30,467 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:02:28,205 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:03:25,740 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:04:11,326 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:04:47,799 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:05:35,222 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:06:28,142 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:07:28,935 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:08:23,862 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:09:21,192 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:10:11,735 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:10:53,022 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:11:43,206 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:12:34,594 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:13:30,996 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:14:05,944 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:14:41,328 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:15:17,337 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:15:56,872 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:16:33,135 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:17:09,245 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:18:05,694 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:19:04,422 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:19:59,843 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:20:57,148 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:21:54,253 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:22:51,417 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:23:53,692 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:24:54,139 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:25:59,194 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:26:58,396 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:27:42,813 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:28:41,133 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:29:38,878 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:30:37,214 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:31:38,271 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:32:39,398 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:33:38,573 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:34:36,703 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:35:36,361 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:36:37,150 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:37:32,326 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:38:34,598 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:39:32,961 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:40:30,044 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:41:27,551 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:42:28,762 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:43:26,054 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:44:22,472 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:44:55,356 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:45:33,451 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:46:15,473 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:47:22,641 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:48:30,653 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:49:36,244 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:50:14,605 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:50:51,931 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:51:43,506 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:52:45,978 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:53:45,887 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:54:52,499 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:55:56,286 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:56:55,066 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:57:57,078 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 13:59:05,220 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:00:04,017 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:00:43,490 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:01:29,605 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:02:26,757 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:03:29,911 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:04:34,206 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:05:28,870 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:06:32,215 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:07:46,888 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:08:46,219 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:09:43,825 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:10:40,917 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:11:41,260 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:12:44,675 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:13:48,960 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:14:57,418 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:16:03,011 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:16:57,686 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:17:53,927 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:18:40,056 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:19:17,709 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:20:19,484 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:21:22,484 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:22:29,549 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:23:25,614 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:24:26,871 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:25:30,822 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:26:33,693 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:27:34,531 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:28:13,473 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 14:28:43,888 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Complete: 57,903/57,903 added, 57,903 final\n",
      "\n",
      "✓ Step 5 complete!\n",
      "Status: completed\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 5.3 Example Usage\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Example 1: Add chunk 512 to Ollama\n",
    "\n",
    "# %%\n",
    "# Uncomment to run:\n",
    "# stats = add_chunk_size_to_embedding(\n",
    "#     processed_data=processed_data,\n",
    "#     chunk_size=512,\n",
    "#     embedding_provider=\"ollama\",\n",
    "#     embedding_model=\"nomic-embed-text\",\n",
    "#     skip_if_exists=True\n",
    "# )\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Example 2: Add chunk 1024 to same Ollama database\n",
    "\n",
    "# %%\n",
    "# Uncomment to run:\n",
    "# stats = add_chunk_size_to_embedding(\n",
    "#     processed_data=processed_data,\n",
    "#     chunk_size=1024,\n",
    "#     embedding_provider=\"ollama\",\n",
    "#     embedding_model=\"nomic-embed-text\",\n",
    "#     skip_if_exists=True\n",
    "# )\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Example 3: Add chunk 512 to OpenAI (different database)\n",
    "\n",
    "# %%\n",
    "# Uncomment to run:\n",
    "# stats = add_chunk_size_to_embedding(\n",
    "#     processed_data=processed_data,\n",
    "#     chunk_size=512,\n",
    "#     embedding_provider=\"openai\",\n",
    "#     embedding_model=\"text-embedding-3-small\",\n",
    "#     skip_if_exists=True\n",
    "# )\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.4 Your Turn: Add Your Embeddings\n",
    "\n",
    "# %%\n",
    "# Define what you want to add\n",
    "# Modify these parameters:\n",
    "\n",
    "# EMBED_PROVIDER = \"ollama\"  # or \"openai\"\n",
    "EMBED_PROVIDER = \"ollama\"\n",
    "EMBED_MODEL = \"bge-m3\"  # or \"text-embedding-3-small\"\n",
    "CHUNK_TO_ADD = 256  # Must exist in processed_data\n",
    "\n",
    "# %%\n",
    "# Execute\n",
    "stats = add_chunk_size_to_embedding(\n",
    "    processed_data=processed_data,\n",
    "    chunk_size=CHUNK_TO_ADD,\n",
    "    embedding_provider=EMBED_PROVIDER,\n",
    "    embedding_model=EMBED_MODEL,\n",
    "    skip_if_exists=True\n",
    ")\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 5 complete!\")\n",
    "print(f\"Status: {stats['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01602be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB Directory: ../../vector_databases\n",
      "\n",
      "============================================================\n",
      "SCANNING DATABASES\n",
      "============================================================\n",
      "Location: ../../vector_databases\n",
      "\n",
      "Database: openai_text-embedding-3-small\n",
      "  Provider: openai\n",
      "  Model: text-embedding-3-small\n",
      "    • Chunk 512: 29,298 documents\n",
      "  Total: 29,298 documents\n",
      "\n",
      "Database: ollama_bge-m3\n",
      "  Provider: ollama\n",
      "  Model: bge-m3\n",
      "    • Chunk 256: 57,903 documents\n",
      "  Total: 57,903 documents\n",
      "\n",
      "Database: ollama_nomic-embed-text\n",
      "  Provider: ollama\n",
      "  Model: nomic-embed-text\n",
      "    • Chunk 256: 57,903 documents\n",
      "    • Chunk 512: 28,657 documents\n",
      "    • Chunk 1024: 15,787 documents\n",
      "  Total: 102,347 documents\n",
      "\n",
      "\n",
      "============================================================\n",
      "DATABASE SUMMARY\n",
      "============================================================\n",
      "\n",
      "openai_text-embedding-3-small\n",
      "  Provider: openai\n",
      "  Model: text-embedding-3-small\n",
      "  Collections: 1\n",
      "  Documents: 29,298\n",
      "\n",
      "ollama_bge-m3\n",
      "  Provider: ollama\n",
      "  Model: bge-m3\n",
      "  Collections: 1\n",
      "  Documents: 57,903\n",
      "\n",
      "ollama_nomic-embed-text\n",
      "  Provider: ollama\n",
      "  Model: nomic-embed-text\n",
      "  Collections: 3\n",
      "  Documents: 102,347\n",
      "\n",
      "============================================================\n",
      "Total: 3 database(s), 5 collection(s), 189,548 documents\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Scan all databases\n",
    "print(f\"Vector DB Directory: {VECTOR_DB_DIR}\")\n",
    "all_databases = inspect_all_databases(VECTOR_DB_DIR)\n",
    "\n",
    "# %%\n",
    "# Display summary\n",
    "display_summary(all_databases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78311713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 6: Query and Test\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.1 Load Vector Store\n",
    "\n",
    "# %%\n",
    "def load_vector_store(\n",
    "    embedding_provider: str,\n",
    "    embedding_model: str,\n",
    "    chunk_size: int,\n",
    "    base_db_dir: str = \"../../vector_databases\",\n",
    "    collection_prefix: str = \"financebench_docs_chunk_\"\n",
    ") -> Chroma:\n",
    "    \"\"\"\n",
    "    Load a vector store for querying.\n",
    "    \n",
    "    Args:\n",
    "        embedding_provider: \"ollama\" or \"openai\"\n",
    "        embedding_model: Model name\n",
    "        chunk_size: Which chunk size collection to load\n",
    "        base_db_dir: Base database directory\n",
    "        collection_prefix: Collection name prefix\n",
    "        \n",
    "    Returns:\n",
    "        Chroma vectorstore instance\n",
    "    \"\"\"\n",
    "    # Get paths\n",
    "    db_path = get_db_path(base_db_dir, embedding_provider, embedding_model)\n",
    "    collection_name = f\"{collection_prefix}{chunk_size}\"\n",
    "    \n",
    "    print(f\"Loading vector store:\")\n",
    "    print(f\"  Provider: {embedding_provider}\")\n",
    "    print(f\"  Model: {embedding_model}\")\n",
    "    print(f\"  Database: {db_path}\")\n",
    "    print(f\"  Collection: {collection_name}\")\n",
    "    \n",
    "    # Get embedding function\n",
    "    emb_fn = get_embedding_function(embedding_provider, embedding_model)\n",
    "    \n",
    "    # Load\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=emb_fn,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    \n",
    "    count = vectorstore._collection.count()\n",
    "    print(f\"  Documents: {count:,}\")\n",
    "    print(\"✓ Loaded\")\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.2 Simple Search\n",
    "\n",
    "# %%\n",
    "def search(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    k: int = 5\n",
    ") -> List:\n",
    "    \"\"\"Perform similarity search.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SEARCH\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Top-{k} results:\\n\")\n",
    "    \n",
    "    docs = vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"[{i}] {doc.page_content[:200]}...\")\n",
    "        if 'file_name' in doc.metadata:\n",
    "            print(f\"    Source: {doc.metadata['file_name']}\")\n",
    "        print()\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.3 Search with Scores\n",
    "\n",
    "# %%\n",
    "def search_with_scores(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    k: int = 5\n",
    ") -> List[Tuple]:\n",
    "    \"\"\"Perform similarity search with relevance scores.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SEARCH WITH SCORES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Top-{k} results:\\n\")\n",
    "    \n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"[{i}] Score: {score:.4f}\")\n",
    "        print(f\"{doc.page_content[:200]}...\")\n",
    "        if 'file_name' in doc.metadata:\n",
    "            print(f\"Source: {doc.metadata['file_name']}\")\n",
    "        if 'page_label' in doc.metadata:\n",
    "            print(f\"Page: {doc.metadata['page_label']}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.4 Compare Chunk Sizes\n",
    "\n",
    "# %%\n",
    "def compare_chunk_sizes(\n",
    "    embedding_provider: str,\n",
    "    embedding_model: str,\n",
    "    query: str,\n",
    "    chunk_sizes: List[int],\n",
    "    k: int = 3,\n",
    "    base_db_dir: str = \"../../vector_databases\"\n",
    "):\n",
    "    \"\"\"Compare retrieval across different chunk sizes.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPARING CHUNK SIZES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    for chunk_size in chunk_sizes:\n",
    "        print(f\"--- Chunk Size: {chunk_size} ---\")\n",
    "        try:\n",
    "            vs = load_vector_store(\n",
    "                embedding_provider, \n",
    "                embedding_model, \n",
    "                chunk_size,\n",
    "                base_db_dir\n",
    "            )\n",
    "            \n",
    "            results = vs.similarity_search_with_score(query, k=k)\n",
    "            if results:\n",
    "                doc, score = results[0]\n",
    "                print(f\"Top result (Score: {score:.4f}):\")\n",
    "                print(f\"{doc.page_content[:250]}...\")\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\\n\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.5 Test with FinanceBench Questions\n",
    "\n",
    "# %%\n",
    "def test_with_dataset_questions(\n",
    "    vectorstore: Chroma,\n",
    "    dataset,\n",
    "    num_questions: int = 3,\n",
    "    k: int = 3\n",
    "):\n",
    "    \"\"\"Test with actual FinanceBench questions.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TESTING WITH FINANCEBENCH QUESTIONS\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    import random\n",
    "    indices = random.sample(range(len(dataset)), num_questions)\n",
    "    \n",
    "    for idx in indices:\n",
    "        record = dataset[idx]\n",
    "        question = record['question']\n",
    "        answer = record['answer']\n",
    "        company = record['company']\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Company: {company}\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Expected Answer: {answer}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Retrieve\n",
    "        docs = vectorstore.similarity_search(question, k=k)\n",
    "        \n",
    "        print(f\"Retrieved {len(docs)} documents:\\n\")\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            print(f\"[{i}] {doc.page_content[:150]}...\")\n",
    "            if 'file_name' in doc.metadata:\n",
    "                print(f\"    Source: {doc.metadata['file_name']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8869dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 10:57:38,824 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 10:57:38,873 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vector store:\n",
      "  Provider: ollama\n",
      "  Model: nomic-embed-text\n",
      "  Database: ../../vector_databases/ollama_nomic-embed-text\n",
      "  Collection: financebench_docs_chunk_512\n",
      "  Documents: 28,657\n",
      "✓ Loaded\n",
      "\n",
      "============================================================\n",
      "SEARCH\n",
      "============================================================\n",
      "Query: What was the capital expenditure in 2018?\n",
      "Top-3 results:\n",
      "\n",
      "[1] Capital Spending\n",
      " \n",
      "Capital spending was $1.6 billion in 2021, an increase of $260 million when compared to 2020. We expect our 2022 capital expenditures to be consistent with 2021.\n",
      "Cash Flows\n",
      " \n",
      "Summar...\n",
      "\n",
      "[2] Total U.S. capital expenditures decreased $478 million for fiscal 2018 , when compared to the previous fiscal year. Capital expenditures related to new stores and\n",
      "clubs, including expansions and reloc...\n",
      "\n",
      "[3] Year Ended December 31,\n",
      " \n",
      " \n",
      " \n",
      "2020\n",
      "  \n",
      "2019\n",
      "  \n",
      "2018\n",
      " \n",
      "Capital expenditures:\n",
      " \n",
      "(In thousands)\n",
      " \n",
      "Las Vegas Strip Resorts\n",
      " \n",
      "$\n",
      "87,511   \n",
      "$\n",
      "285,863   \n",
      "$\n",
      "501,044 \n",
      "Regional Operations\n",
      " \n",
      " \n",
      "41,456   \n",
      " \n",
      "187,489 ...\n",
      "\n",
      "\n",
      "============================================================\n",
      "SEARCH WITH SCORES\n",
      "============================================================\n",
      "Query: What is the total revenue for fiscal year 2022?\n",
      "Top-5 results:\n",
      "\n",
      "[1] Score: 0.4673\n",
      "Our segment revenue and results for fiscal 2022, 2021 and 2020 were as follows:\n",
      "(dollars in millions)\n",
      "Digital \n",
      "Media\n",
      "Digital \n",
      "Experience\n",
      "Publishing and \n",
      "Advertising\n",
      "Total\n",
      "Fiscal 2022\n",
      "Revenue\n",
      "$ \n",
      "12,842...\n",
      "\n",
      "[2] Score: 0.4702\n",
      "Revenue by geographic area for fiscal 2022, 2021 and 2020 were as follows:\n",
      "(in millions)\n",
      "2022\n",
      "2021\n",
      "2020\n",
      "Americas:\n",
      " \n",
      " \n",
      " \n",
      "United States\n",
      "$ \n",
      "9,217 $ \n",
      "8,104 $ \n",
      "6,745 \n",
      "Other\n",
      " \n",
      "1,034  \n",
      "892  \n",
      "709 \n",
      "Total Ameri...\n",
      "\n",
      "[3] Score: 0.5037\n",
      "2023\n",
      " \n",
      "2022\n",
      " \n",
      "2021\n",
      "Revenue by product category\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Domestic:\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Computing and Mobile Phones\n",
      "$\n",
      " 18,191 \n",
      "  \n",
      "$\n",
      " 20,693 \n",
      "  \n",
      "$\n",
      " 19,799 \n",
      " \n",
      "Consumer Electronics\n",
      " \n",
      " 13,040...\n",
      "\n",
      "[4] Score: 0.5073\n",
      "Operating revenues for the Regulated Businesses were $3,505 million for 2022,\n",
      "$3,384 million for 2021 and $3,255 million for 2020, accounting for 92%, 86% and 86%, respectively, of the Company’s total...\n",
      "\n",
      "[5] Score: 0.5109\n",
      "85 | 2022 Annual Report\n",
      "Consolidated Revenue and Operating Margin\n",
      "Year Ended December 31, 2022 Compared to Year Ended December 31, 2021\n",
      "Revenue\n",
      "(in millions)\n",
      "Consolidated Revenue — Revenue increased $...\n",
      "\n",
      "\n",
      "============================================================\n",
      "COMPARING CHUNK SIZES\n",
      "============================================================\n",
      "Query: What were the operating expenses in 2021?\n",
      "\n",
      "--- Chunk Size: 512 ---\n",
      "Loading vector store:\n",
      "  Provider: ollama\n",
      "  Model: nomic-embed-text\n",
      "  Database: ../../vector_databases/ollama_nomic-embed-text\n",
      "  Collection: financebench_docs_chunk_512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 10:57:38,952 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 10:57:39,042 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Documents: 28,657\n",
      "✓ Loaded\n",
      "Top result (Score: 0.4891):\n",
      "Table of Contents\n",
      "Operating Expenses\n",
      "Information about operating expenses is as follows (in millions):\n",
      " \n",
      " \n",
      "Year Ended December 31,\n",
      "  \n",
      "2015\n",
      "2016\n",
      "2017\n",
      "Operating expenses:\n",
      "Cost of sales\n",
      "$\n",
      "71,651\n",
      "$\n",
      "88,265\n",
      "$\n",
      "111,934\n",
      "Fulfillment\n",
      "13,410\n",
      "17,619\n",
      "25,249\n",
      "Market...\n",
      "\n",
      "--- Chunk Size: 1024 ---\n",
      "Loading vector store:\n",
      "  Provider: ollama\n",
      "  Model: nomic-embed-text\n",
      "  Database: ../../vector_databases/ollama_nomic-embed-text\n",
      "  Collection: financebench_docs_chunk_1024\n",
      "  Documents: 0\n",
      "✓ Loaded\n",
      "\n",
      "\n",
      "============================================================\n",
      "TESTING WITH FINANCEBENCH QUESTIONS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Company: Corning\n",
      "Question: Does Corning have positive working capital based on FY2022 data? If working capital is not a useful or relevant metric for this company, then please state that and explain why.\n",
      "Expected Answer: Yes. Corning had a positive working capital amount of $831 million by FY 2022 close. This answer considers only operating current assets and current liabilities that were clearly shown in the balance sheet.\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 10:57:39,128 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 10:57:39,168 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-06 10:57:39,209 - INFO - HTTP Request: POST http://localhost:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 documents:\n",
      "\n",
      "[1] Effective January 1, 2019, Corning began using constant-currency reporting for our Environmental Technologies \n",
      "and Life Sciences segments. The Company...\n",
      "[2] Despite the pandemic and resulting global disruptions, Corning adapted rapidly and remained resilient. We acted quickly to preserve our financial stre...\n",
      "[3] Our probability of success increases as we invest in our world-class capabilities.  Corning is concentrating approximately 80% of its research, develo...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Company: Boeing\n",
      "Question: Who are the primary customers of Boeing as of FY2022?\n",
      "Expected Answer: Boeing's primary customers as of FY2022 are a limited number of commercial airlines and the US government. The US government accounted for 40% of Boeing's total revenues in FY2022.\n",
      "============================================================\n",
      "\n",
      "Retrieved 3 documents:\n",
      "\n",
      "[1] We address employee concerns and take appropriate\n",
      "actions that uphold our Boeing values.\n",
      "Competition\n",
      "The commercial jet aircraft market and the airlin...\n",
      "[2] Table of Contents\n",
      "Backlog\n",
      "BGS total backlog of $19,338 million at December 31, 2022 decreased by 6% from $20,496 million at December 31, 2021, primari...\n",
      "[3] Susan Doniz\n",
      "53\n",
      "Chief Information Officer and Senior Vice President, Information Technology & Data Analytics\n",
      "since May 2020. Prior to joining Boeing, M...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Company: General Mills\n",
      "Question: By drawing conclusions from the information stated only in the statement of financial position, what is General Mills's FY2020 working capital ratio? Define working capital ratio as total current assets divided by total current liabilities. Round your answer to two decimal places.\n",
      "Expected Answer: 0.68\n",
      "============================================================\n",
      "\n",
      "Retrieved 3 documents:\n",
      "\n",
      "[1] Balance Sheet:\n",
      "3M’s strong balance sheet and liquidity provide the Company with significant flexibility to fund its numerous opportunities going forwa...\n",
      "[2] Balance Sheet:\n",
      "3M’s strong balance sheet and liquidity provide the Company with significant flexibility to fund its numerous opportunities going forwa...\n",
      "[3] The Company\n",
      "will continue to invest in its operations to drive growth, including continual review of acquisition opportunities.\n",
      " \n",
      "The Company uses wor...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "✓ Step 6 complete!\n",
      "You can now query your RAG system!\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 6.6 Execute Tests\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Load Vector Store\n",
    "\n",
    "# %%\n",
    "# Configure which embedding to test\n",
    "TEST_PROVIDER = \"ollama\"  # or \"openai\"\n",
    "TEST_MODEL = \"nomic-embed-text\"  # or \"text-embedding-3-small\"\n",
    "TEST_CHUNK_SIZE = 512\n",
    "\n",
    "# %%\n",
    "# Load vector store\n",
    "vectorstore = load_vector_store(\n",
    "    embedding_provider=TEST_PROVIDER,\n",
    "    embedding_model=TEST_MODEL,\n",
    "    chunk_size=TEST_CHUNK_SIZE\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Test 1: Simple Search\n",
    "\n",
    "# %%\n",
    "query = \"What was the capital expenditure in 2018?\"\n",
    "docs = search(vectorstore, query, k=3)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Test 2: Search with Scores\n",
    "\n",
    "# %%\n",
    "query = \"What is the total revenue for fiscal year 2022?\"\n",
    "results = search_with_scores(vectorstore, query, k=5)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Test 3: Compare Chunk Sizes\n",
    "\n",
    "# %%\n",
    "# Only works if you have multiple chunk sizes for the same embedding\n",
    "query = \"What were the operating expenses in 2021?\"\n",
    "compare_chunk_sizes(\n",
    "    embedding_provider=TEST_PROVIDER,\n",
    "    embedding_model=TEST_MODEL,\n",
    "    query=query,\n",
    "    chunk_sizes=[512, 1024],  # Adjust based on what you have\n",
    "    k=3\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Test 4: Test with Real Questions\n",
    "\n",
    "# %%\n",
    "test_with_dataset_questions(\n",
    "    vectorstore=vectorstore,\n",
    "    dataset=dataset,\n",
    "    num_questions=3,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.7 Quick Query Function\n",
    "\n",
    "# %%\n",
    "def quick_query(query: str, provider: str = \"openai\", model: str = \"nomic-embed-text\", chunk_size: int = 512, k: int = 5):\n",
    "    \"\"\"Quick helper for ad-hoc queries.\"\"\"\n",
    "    vs = load_vector_store(provider, model, chunk_size)\n",
    "    return search_with_scores(vs, query, k)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Your Custom Queries\n",
    "\n",
    "# %%\n",
    "# Try your own queries here\n",
    "# results = quick_query(\n",
    "#     query=\"Your question here\",\n",
    "#     provider=\"ollama\",\n",
    "#     model=\"nomic-embed-text\",\n",
    "#     chunk_size=512,\n",
    "#     k=5\n",
    "# )\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 6 complete!\")\n",
    "print(\"You can now query your RAG system!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
