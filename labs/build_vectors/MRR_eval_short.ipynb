{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce3a7f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "✓ OpenAI API key loaded\n",
      "✓ VoyageAI API key loaded\n",
      "✓ Ollama URL: http://localhost:11434\n",
      "✓ Configuration set\n",
      "  Vector DB Directory: ../../vector_databases\n",
      "  Output Directory: ../../evaluation_results/mrr_embeddings\n",
      "Loading FinanceBench dataset...\n",
      "✓ Loaded 150 queries\n",
      "\n",
      "Sample query:\n",
      "  ID: financebench_id_03029\n",
      "  Company: 3M\n",
      "  Question: What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the quest...\n",
      "  Doc: 3M_2018_10K\n",
      "  Evidence items: 1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FinanceBench Evaluation: MRR Analysis\n",
    "# Comparing Embedding Models and Chunk Sizes\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # FinanceBench RAG Evaluation\n",
    "# \n",
    "# This notebook evaluates different embedding models and chunk sizes using\n",
    "# Mean Reciprocal Rank (MRR) on the FinanceBench dataset.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.1 Imports\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Progress\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Data\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Vector stores\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.2 Configuration\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "VOYAGE_API_KEY = os.getenv(\"VOYAGE_API_KEY\")\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    print(\"✓ OpenAI API key loaded\")\n",
    "else:\n",
    "    print(\"⚠ OpenAI API key not found (only needed if using OpenAI embeddings)\")\n",
    "\n",
    "if VOYAGE_API_KEY:\n",
    "    print(\"✓ VoyageAI API key loaded\")\n",
    "else:\n",
    "    print(\"⚠ VoyageAI API key not found (only needed if using VoyageAI embeddings)\")\n",
    "\n",
    "print(f\"✓ Ollama URL: {OLLAMA_BASE_URL}\")\n",
    "\n",
    "# %%\n",
    "# Paths\n",
    "VECTOR_DB_BASE_DIR = \"../../vector_databases\"\n",
    "OUTPUT_DIR = \"../../evaluation_results/mrr_embeddings\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = \"PatronusAI/financebench\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "\n",
    "# Collection settings\n",
    "COLLECTION_PREFIX = \"financebench_docs_chunk_\"\n",
    "\n",
    "print(\"✓ Configuration set\")\n",
    "print(f\"  Vector DB Directory: {VECTOR_DB_BASE_DIR}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.3 Load Dataset\n",
    "\n",
    "# %%\n",
    "print(\"Loading FinanceBench dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=DATASET_SPLIT)\n",
    "print(f\"✓ Loaded {len(dataset)} queries\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample query:\")\n",
    "sample = dataset[0]\n",
    "print(f\"  ID: {sample['financebench_id']}\")\n",
    "print(f\"  Company: {sample['company']}\")\n",
    "print(f\"  Question: {sample['question'][:100]}...\")\n",
    "print(f\"  Doc: {sample['doc_name']}\")\n",
    "print(f\"  Evidence items: {len(sample['evidence'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3167cbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.1 Metadata Extraction\n",
    "\n",
    "# %%\n",
    "def extract_doc_name_from_path(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract document name from file path.\n",
    "    \n",
    "    Example: \"../../financebench/documents/GENERALMILLS_2019_10K.pdf\" \n",
    "             -> \"GENERALMILLS_2019_10K\"\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(file_path)\n",
    "    doc_name = filename.replace('.pdf', '')\n",
    "    return doc_name\n",
    "\n",
    "\n",
    "def extract_metadata_from_retrieved_doc(doc) -> Dict:\n",
    "    \"\"\"Extract relevant metadata from retrieved document.\"\"\"\n",
    "    file_path = doc.metadata.get('file_path', '')\n",
    "    doc_name = extract_doc_name_from_path(file_path)\n",
    "    page_num = doc.metadata.get('source', -1)\n",
    "    \n",
    "    # Ensure page_num is an integer\n",
    "    if isinstance(page_num, str):\n",
    "        page_num = int(page_num)\n",
    "    \n",
    "    return {\n",
    "        'doc_name': doc_name,\n",
    "        'page_number': page_num\n",
    "    }\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.2 Vector Store Loading\n",
    "\n",
    "# %%\n",
    "def get_embedding_function(provider: str, model: str):\n",
    "    \"\"\"Get embedding function.\"\"\"\n",
    "    if provider == \"ollama\":\n",
    "        return OllamaEmbeddings(model=model)\n",
    "    elif provider == \"openai\":\n",
    "        return OpenAIEmbeddings(model=model)\n",
    "    elif provider == \"voyage\":\n",
    "        return VoyageAIEmbeddings(model=model, api_key=VOYAGE_API_KEY)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "\n",
    "def load_vectorstore(\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    base_dir: str = VECTOR_DB_BASE_DIR,\n",
    "    collection_prefix: str = COLLECTION_PREFIX\n",
    ") -> Chroma:\n",
    "    \"\"\"Load a vector store.\"\"\"\n",
    "    model_id = f\"{provider}_{model.replace('/', '_')}\"\n",
    "    db_path = os.path.join(base_dir, model_id)\n",
    "    collection_name = f\"{collection_prefix}{chunk_size}\"\n",
    "    \n",
    "    emb_fn = get_embedding_function(provider, model)\n",
    "    \n",
    "    vectorstore = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=emb_fn,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.3 Evidence Matching\n",
    "\n",
    "# %%\n",
    "def check_match(\n",
    "    retrieved_doc: Dict, \n",
    "    evidence_list: List[Dict],\n",
    "    chunk_size: int = 512,\n",
    "    use_page_tolerance: bool = True\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Check if retrieved document matches any evidence.\n",
    "    \n",
    "    Uses chunk-size-aware page tolerance:\n",
    "    - Chunks can span multiple pages\n",
    "    - Only matches if retrieved page is BEFORE or AT evidence page (within tolerance)\n",
    "    - Retrieved page after evidence page = no match\n",
    "    \n",
    "    Page tolerance calculation (when use_page_tolerance=True):\n",
    "    - chunk_size <= 512: tolerance = 0 (exact match)\n",
    "    - chunk_size 513-1024: tolerance = 1\n",
    "    - chunk_size 1025-2048: tolerance = 2\n",
    "    - chunk_size > 2048: tolerance = 3\n",
    "    \n",
    "    Args:\n",
    "        retrieved_doc: {doc_name, page_number} (1-indexed)\n",
    "        evidence_list: List of evidence dicts from FinanceBench (0-indexed, will be converted)\n",
    "        chunk_size: Chunk size used for this retrieval\n",
    "        use_page_tolerance: If True, use chunk-size-aware tolerance. If False, exact match only.\n",
    "        \n",
    "    Returns:\n",
    "        True if match found\n",
    "    \"\"\"\n",
    "    retrieved_doc_name = retrieved_doc['doc_name']\n",
    "    retrieved_page = retrieved_doc['page_number']\n",
    "    \n",
    "    # Calculate page tolerance based on chunk size\n",
    "    if use_page_tolerance:\n",
    "        if chunk_size <= 512:\n",
    "            page_tolerance = 0\n",
    "        elif chunk_size <= 1024:\n",
    "            page_tolerance = 1\n",
    "        elif chunk_size <= 2048:\n",
    "            page_tolerance = 2\n",
    "        else:\n",
    "            page_tolerance = 2\n",
    "    else:\n",
    "        page_tolerance = 0  # Exact match only\n",
    "    \n",
    "    for evidence in evidence_list:\n",
    "        evidence_doc_name = evidence['doc_name']\n",
    "        evidence_page = evidence['evidence_page_num'] + 1  # Convert 0-indexed to 1-indexed\n",
    "        \n",
    "        # Check document name match\n",
    "        if retrieved_doc_name != evidence_doc_name:\n",
    "            continue\n",
    "        \n",
    "        # Check page match with tolerance\n",
    "        # Only match if retrieved page is BEFORE or AT evidence page\n",
    "        if retrieved_page <= evidence_page <= retrieved_page + page_tolerance:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.4 MRR Calculation (Modular)\n",
    "\n",
    "# %%\n",
    "def calculate_mrr_for_query(\n",
    "    retrieved_docs: List[Dict], \n",
    "    evidence_list: List[Dict],\n",
    "    chunk_size: int = 512,\n",
    "    use_page_tolerance: bool = True\n",
    ") -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    Calculate MRR for a single query.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs: List of {doc_name, page_number}\n",
    "        evidence_list: Ground truth evidence\n",
    "        chunk_size: Chunk size used (for page tolerance)\n",
    "        use_page_tolerance: If True, use chunk-size-aware tolerance. If False, exact match only.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (mrr_score, rank)\n",
    "        - mrr_score: 1/rank if found, 0 if not found\n",
    "        - rank: Position of first match (1-indexed), -1 if not found\n",
    "    \"\"\"\n",
    "    for rank, retrieved_doc in enumerate(retrieved_docs, start=1):\n",
    "        if check_match(retrieved_doc, evidence_list, chunk_size, use_page_tolerance):\n",
    "            mrr_score = 1.0 / rank\n",
    "            return mrr_score, rank\n",
    "    \n",
    "    # No match found\n",
    "    return 0.0, -1\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.5 Retrieval Functions\n",
    "\n",
    "# %%\n",
    "def retrieve_global(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve documents globally (search all documents).\n",
    "    \n",
    "    Returns:\n",
    "        List of {doc_name, page_number, rank}\n",
    "    \"\"\"\n",
    "    results = vectorstore.similarity_search(query, k=k)\n",
    "    retrieved = []\n",
    "    for rank, doc in enumerate(results, start=1):\n",
    "        metadata = extract_metadata_from_retrieved_doc(doc)\n",
    "        metadata['rank'] = rank\n",
    "        retrieved.append(metadata)\n",
    "    return retrieved\n",
    "\n",
    "\n",
    "def retrieve_single_doc(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    target_doc_name: str,\n",
    "    k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve documents filtered to single document.\n",
    "    \n",
    "    Since ChromaDB doesn't support $contains, we retrieve more documents\n",
    "    and filter them post-retrieval.\n",
    "    \n",
    "    Returns:\n",
    "        List of {doc_name, page_number, rank}\n",
    "    \"\"\"\n",
    "    # Retrieve more documents than needed (k * 10) to ensure we get enough from target doc\n",
    "    # Then filter to only the target document\n",
    "    \n",
    "    fetch_k = min(k * 10, 100)  # Fetch up to 10x k, max 100\n",
    "    \n",
    "    results = vectorstore.similarity_search(query, k=fetch_k)\n",
    "    \n",
    "    # Filter to only documents from target doc\n",
    "    filtered = []\n",
    "    for doc in results:\n",
    "        metadata = extract_metadata_from_retrieved_doc(doc)\n",
    "        if metadata['doc_name'] == target_doc_name:\n",
    "            filtered.append(metadata)\n",
    "            if len(filtered) >= k:\n",
    "                break\n",
    "    \n",
    "    # Add rank after filtering\n",
    "    for rank, doc_meta in enumerate(filtered[:k], start=1):\n",
    "        doc_meta['rank'] = rank\n",
    "    \n",
    "    # Return top k from target document\n",
    "    return filtered[:k]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.6 File Management\n",
    "\n",
    "# %%\n",
    "def get_output_filename(provider: str, model: str, chunk_size: int, k: int, mode: str) -> str:\n",
    "    \"\"\"Generate output filename.\"\"\"\n",
    "    model_clean = model.replace('/', '_')\n",
    "    filename = f\"{provider}_{model_clean}_chunk{chunk_size}_k{k}_{mode}.json\"\n",
    "    return filename\n",
    "\n",
    "\n",
    "def check_if_results_exist(provider: str, model: str, chunk_size: int, k: int, mode: str, output_dir: str) -> bool:\n",
    "    \"\"\"Check if results JSON already exists.\"\"\"\n",
    "    filename = get_output_filename(provider, model, chunk_size, k, mode)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    return os.path.exists(filepath)\n",
    "\n",
    "\n",
    "def save_results(results: List[Dict], provider: str, model: str, chunk_size: int, k: int, mode: str, output_dir: str):\n",
    "    \"\"\"Save results to JSON file.\"\"\"\n",
    "    filename = get_output_filename(provider, model, chunk_size, k, mode)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Saved: {filename}\")\n",
    "\n",
    "# %%\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e962333e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Evaluation Loop\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.1 Single Evaluation Run\n",
    "\n",
    "# %%\n",
    "def evaluate_single_configuration(\n",
    "    dataset,\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k: int,\n",
    "    mode: str,\n",
    "    use_page_tolerance: bool = True,\n",
    "    output_dir: str = OUTPUT_DIR\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a single configuration (provider, model, chunk_size, k, mode).\n",
    "    \n",
    "    Args:\n",
    "        dataset: FinanceBench dataset\n",
    "        provider: \"ollama\" or \"openai\"\n",
    "        model: Model name\n",
    "        chunk_size: Chunk size\n",
    "        k: Number of documents to retrieve\n",
    "        mode: \"global\" or \"singledoc\"\n",
    "        use_page_tolerance: If True, use chunk-size-aware page tolerance. If False, exact match only.\n",
    "        output_dir: Output directory\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATING: {provider}/{model}, chunk={chunk_size}, k={k}, mode={mode}\")\n",
    "    print(f\"Page Tolerance: {'ENABLED' if use_page_tolerance else 'DISABLED (exact match)'}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Check if already exists\n",
    "    if check_if_results_exist(provider, model, chunk_size, k, mode, output_dir):\n",
    "        print(\"✓ Results already exist - SKIPPING\")\n",
    "        return {'status': 'skipped'}\n",
    "    \n",
    "    # Load vectorstore\n",
    "    print(\"Loading vectorstore...\")\n",
    "    try:\n",
    "        vectorstore = load_vectorstore(provider, model, chunk_size)\n",
    "        doc_count = vectorstore._collection.count()\n",
    "        print(f\"✓ Loaded ({doc_count:,} documents)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load vectorstore: {e}\")\n",
    "        return {'status': 'failed', 'error': str(e)}\n",
    "    \n",
    "    # Process all queries\n",
    "    results = []\n",
    "    mrr_scores = []\n",
    "    \n",
    "    print(f\"\\nProcessing {len(dataset)} queries...\")\n",
    "    \n",
    "    for record in tqdm(dataset, desc=\"Queries\"):\n",
    "        query_id = record['financebench_id']\n",
    "        query = record['question']\n",
    "        evidence = record['evidence']\n",
    "        doc_name = record['doc_name']\n",
    "        \n",
    "        # Retrieve documents\n",
    "        try:\n",
    "            if mode == \"global\":\n",
    "                retrieved_docs = retrieve_global(vectorstore, query, k)\n",
    "            elif mode == \"singledoc\":\n",
    "                retrieved_docs = retrieve_single_doc(vectorstore, query, doc_name, k)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown mode: {mode}\")\n",
    "            \n",
    "            # Calculate MRR\n",
    "            mrr_score, rank = calculate_mrr_for_query(retrieved_docs, evidence, chunk_size, use_page_tolerance)\n",
    "            mrr_scores.append(mrr_score)\n",
    "            \n",
    "            # Store result (convert evidence to 1-indexed for consistency)\n",
    "            result = {\n",
    "                'query_id': query_id,\n",
    "                'query': query,\n",
    "                'expected_doc': doc_name,\n",
    "                'expected_evidence': [\n",
    "                    {\n",
    "                        'doc_name': ev['doc_name'],\n",
    "                        'page_number': ev['evidence_page_num'] + 1  # Convert to 1-indexed\n",
    "                    }\n",
    "                    for ev in evidence\n",
    "                ],\n",
    "                'retrieved_docs': retrieved_docs,\n",
    "                'mrr_score': mrr_score,\n",
    "                'rank': rank\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Error processing query {query_id}: {e}\")\n",
    "            results.append({\n",
    "                'query_id': query_id,\n",
    "                'query': query,\n",
    "                'error': str(e),\n",
    "                'mrr_score': 0.0,\n",
    "                'rank': -1\n",
    "            })\n",
    "            mrr_scores.append(0.0)\n",
    "    \n",
    "    # Calculate average MRR\n",
    "    avg_mrr = sum(mrr_scores) / len(mrr_scores) if mrr_scores else 0.0\n",
    "    \n",
    "    # Add summary\n",
    "    results.append({\n",
    "        'summary': {\n",
    "            'provider': provider,\n",
    "            'model': model,\n",
    "            'chunk_size': chunk_size,\n",
    "            'k': k,\n",
    "            'mode': mode,\n",
    "            'use_page_tolerance': use_page_tolerance,\n",
    "            'total_queries': len(dataset),\n",
    "            'average_mrr': avg_mrr\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Save results\n",
    "    save_results(results, provider, model, chunk_size, k, mode, output_dir)\n",
    "    \n",
    "    print(f\"\\n✓ Average MRR: {avg_mrr:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'status': 'completed',\n",
    "        'average_mrr': avg_mrr,\n",
    "        'total_queries': len(dataset)\n",
    "    }\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.2 Batch Evaluation\n",
    "\n",
    "# %%\n",
    "def evaluate_multiple_configurations(\n",
    "    dataset,\n",
    "    configurations: List[Dict],\n",
    "    k_values: List[int],\n",
    "    modes: List[str],\n",
    "    use_page_tolerance: bool = True,\n",
    "    output_dir: str = OUTPUT_DIR\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate multiple configurations.\n",
    "    \n",
    "    Args:\n",
    "        dataset: FinanceBench dataset\n",
    "        configurations: List of {provider, model, chunk_sizes}\n",
    "        k_values: List of k values to test\n",
    "        modes: List of modes [\"global\", \"singledoc\"]\n",
    "        use_page_tolerance: If True, use chunk-size-aware page tolerance. If False, exact match only.\n",
    "        output_dir: Output directory\n",
    "        \n",
    "    Returns:\n",
    "        Summary of all evaluations\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BATCH EVALUATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Configurations: {len(configurations)}\")\n",
    "    print(f\"K values: {k_values}\")\n",
    "    print(f\"Modes: {modes}\")\n",
    "    print(f\"Page Tolerance: {'ENABLED' if use_page_tolerance else 'DISABLED (exact match)'}\")\n",
    "    \n",
    "    # Calculate total runs\n",
    "    total_runs = 0\n",
    "    for config in configurations:\n",
    "        total_runs += len(config['chunk_sizes']) * len(k_values) * len(modes)\n",
    "    \n",
    "    print(f\"Total evaluation runs: {total_runs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Track results\n",
    "    all_results = []\n",
    "    completed = 0\n",
    "    skipped = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # Iterate through all combinations\n",
    "    for config in configurations:\n",
    "        provider = config['provider']\n",
    "        model = config['model']\n",
    "        chunk_sizes = config['chunk_sizes']\n",
    "        \n",
    "        for chunk_size in chunk_sizes:\n",
    "            for k in k_values:\n",
    "                for mode in modes:\n",
    "                    result = evaluate_single_configuration(\n",
    "                        dataset=dataset,\n",
    "                        provider=provider,\n",
    "                        model=model,\n",
    "                        chunk_size=chunk_size,\n",
    "                        k=k,\n",
    "                        mode=mode,\n",
    "                        use_page_tolerance=use_page_tolerance,\n",
    "                        output_dir=output_dir\n",
    "                    )\n",
    "                    \n",
    "                    all_results.append({\n",
    "                        'provider': provider,\n",
    "                        'model': model,\n",
    "                        'chunk_size': chunk_size,\n",
    "                        'k': k,\n",
    "                        'mode': mode,\n",
    "                        'result': result\n",
    "                    })\n",
    "                    \n",
    "                    if result['status'] == 'completed':\n",
    "                        completed += 1\n",
    "                    elif result['status'] == 'skipped':\n",
    "                        skipped += 1\n",
    "                    else:\n",
    "                        failed += 1\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BATCH EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total runs: {total_runs}\")\n",
    "    print(f\"Completed: {completed}\")\n",
    "    print(f\"Skipped: {skipped}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return {\n",
    "        'total_runs': total_runs,\n",
    "        'completed': completed,\n",
    "        'skipped': skipped,\n",
    "        'failed': failed,\n",
    "        'results': all_results\n",
    "    }\n",
    "\n",
    "# %%\n",
    "print(\"✓ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a114f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Test Retrieval Function\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Test Retrieval with Sample Text\n",
    "# \n",
    "# Use this to verify embeddings are working correctly by testing with \n",
    "# known text from your documents.\n",
    "\n",
    "# %%\n",
    "def test_retrieval(\n",
    "    query_text: str,\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k: int = 10,\n",
    "    mode: str = \"global\",\n",
    "    target_doc_name: str = None,\n",
    "    use_page_tolerance: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Test retrieval with a sample query.\n",
    "    \n",
    "    Args:\n",
    "        query_text: Text to search for (copy from actual document)\n",
    "        provider: \"ollama\" or \"openai\"\n",
    "        model: Model name\n",
    "        chunk_size: Chunk size to test\n",
    "        k: Number of results\n",
    "        mode: \"global\" or \"singledoc\"\n",
    "        target_doc_name: Required if mode is \"singledoc\"\n",
    "        use_page_tolerance: If True, use chunk-size-aware tolerance\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"TEST RETRIEVAL\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Provider: {provider}\")\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Chunk size: {chunk_size}\")\n",
    "    print(f\"Mode: {mode}\")\n",
    "    print(f\"K: {k}\")\n",
    "    print(f\"Page Tolerance: {'ENABLED' if use_page_tolerance else 'DISABLED'}\")\n",
    "    print(f\"\\nQuery (first 200 chars):\")\n",
    "    print(f\"{query_text[:200]}...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load vectorstore\n",
    "    try:\n",
    "        vectorstore = load_vectorstore(provider, model, chunk_size)\n",
    "        print(f\"✓ Vectorstore loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Retrieve\n",
    "    try:\n",
    "        if mode == \"global\":\n",
    "            results = retrieve_global(vectorstore, query_text, k)\n",
    "        elif mode == \"singledoc\":\n",
    "            if not target_doc_name:\n",
    "                print(\"✗ target_doc_name required for singledoc mode\")\n",
    "                return\n",
    "            results = retrieve_single_doc(vectorstore, query_text, target_doc_name, k)\n",
    "        else:\n",
    "            print(f\"✗ Unknown mode: {mode}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n✓ Retrieved {len(results)} documents\")\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for doc_meta in results:\n",
    "            print(f\"\\nRank {doc_meta['rank']}: {doc_meta['doc_name']}, Page {doc_meta['page_number']}\")\n",
    "        \n",
    "        # Check if top result seems correct\n",
    "        if results:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"TOP RESULT ANALYSIS\")\n",
    "            print(\"=\"*60)\n",
    "            top = results[0]\n",
    "            print(f\"Document: {top['doc_name']}\")\n",
    "            print(f\"Page: {top['page_number']}\")\n",
    "            print(f\"Rank: {top['rank']}\")\n",
    "            \n",
    "            # Try to get the actual content\n",
    "            try:\n",
    "                full_results = vectorstore.similarity_search(query_text, k=1)\n",
    "                if full_results:\n",
    "                    content = full_results[0].page_content\n",
    "                    print(f\"\\nTop result content (first 300 chars):\")\n",
    "                    print(f\"{content[:300]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not fetch content: {e}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Retrieval failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c3002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f096ca2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Í' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Step 4: Configuration and Execution\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mÍ\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# ## 4.1 Define Configurations to Test\u001b[39;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Define which embedding models and chunk sizes to evaluate\u001b[39;00m\n\u001b[32m     10\u001b[39m configurations = [\n\u001b[32m     11\u001b[39m     {\n\u001b[32m     12\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mprovider\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mollama\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     },\n\u001b[32m     41\u001b[39m ]\n",
      "\u001b[31mNameError\u001b[39m: name 'Í' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 4: Configuration and Execution\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.1 Define Configurations to Test\n",
    "\n",
    "# %%\n",
    "# Define which embedding models and chunk sizes to evaluate\n",
    "configurations = [\n",
    "    {\n",
    "        'provider': 'ollama',\n",
    "        'model': 'bge-m3',\n",
    "        'chunk_sizes': [256, 512, 1024, 2048]\n",
    "    },\n",
    "    {\n",
    "        'provider': 'ollama',\n",
    "        'model': 'nomic-embed-text',\n",
    "        'chunk_sizes': [256, 512, 1024, 2048]\n",
    "    },\n",
    "    {\n",
    "        'provider': 'openai',\n",
    "        'model': 'text-embedding-3-small',\n",
    "        'chunk_sizes': [256, 512, 1024, 2048]\n",
    "    },\n",
    "    {\n",
    "        'provider': 'openai',\n",
    "        'model': 'text-embedding-3-large',\n",
    "        'chunk_sizes': [512, 1024]\n",
    "    },\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-3-large',\n",
    "        'chunk_sizes': [512, 1024, 2048, 4096]\n",
    "    },\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-finance-2',\n",
    "        'chunk_sizes': [512, 1024, 2048]\n",
    "    },\n",
    "]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.2 Define Evaluation Parameters\n",
    "\n",
    "# %%\n",
    "# K values to test (number of documents to retrieve)\n",
    "k_values = [20]\n",
    "\n",
    "# Modes to test\n",
    "modes = ['global', 'singledoc']\n",
    "\n",
    "# Page tolerance setting\n",
    "# - True: Use chunk-size-aware page tolerance (lenient matching for large chunks)\n",
    "# - False: Exact page match only (strict evaluation)\n",
    "USE_PAGE_TOLERANCE = True\n",
    "\n",
    "# %%\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATION PLAN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_runs = 0\n",
    "for config in configurations:\n",
    "    provider = config['provider']\n",
    "    model = config['model']\n",
    "    chunk_sizes = config['chunk_sizes']\n",
    "    \n",
    "    print(f\"\\n{provider}/{model}\")\n",
    "    print(f\"  Chunk sizes: {chunk_sizes}\")\n",
    "    \n",
    "    runs_for_config = len(chunk_sizes) * len(k_values) * len(modes)\n",
    "    total_runs += runs_for_config\n",
    "    \n",
    "    print(f\"  Evaluation runs: {runs_for_config}\")\n",
    "    \n",
    "    # Show output filenames that will be generated\n",
    "    print(f\"  Output files:\")\n",
    "    for chunk_size in chunk_sizes:\n",
    "        for k in k_values:\n",
    "            for mode in modes:\n",
    "                filename = get_output_filename(provider, model, chunk_size, k, mode)\n",
    "                exists = check_if_results_exist(provider, model, chunk_size, k, mode, OUTPUT_DIR)\n",
    "                status = \"EXISTS\" if exists else \"TO CREATE\"\n",
    "                print(f\"    - {filename} [{status}]\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total evaluation runs: {total_runs}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Page Tolerance: {'ENABLED' if USE_PAGE_TOLERANCE else 'DISABLED'}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.4 Execute Evaluation\n",
    "\n",
    "# %%\n",
    "# Run batch evaluation\n",
    "summary = evaluate_multiple_configurations(\n",
    "    dataset=dataset,\n",
    "    configurations=configurations,\n",
    "    k_values=k_values,\n",
    "    modes=modes,\n",
    "    use_page_tolerance=USE_PAGE_TOLERANCE,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.5 View Results Summary\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for result in summary['results']:\n",
    "    if result['result']['status'] == 'completed':\n",
    "        print(f\"\\n{result['provider']}/{result['model']}, \"\n",
    "              f\"chunk={result['chunk_size']}, \"\n",
    "              f\"k={result['k']}, \"\n",
    "              f\"mode={result['mode']}\")\n",
    "        print(f\"  Average MRR: {result['result']['average_mrr']:.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.6 List Generated Files\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATED FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "json_files = sorted(output_path.glob(\"*.json\"))\n",
    "\n",
    "print(f\"\\nTotal JSON files: {len(json_files)}\\n\")\n",
    "\n",
    "for filepath in json_files:\n",
    "    file_size = filepath.stat().st_size / 1024  # KB\n",
    "    print(f\"  {filepath.name} ({file_size:.1f} KB)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPage Tolerance was: {'ENABLED' if USE_PAGE_TOLERANCE else 'DISABLED'}\")\n",
    "print(f\"To run with different tolerance setting:\")\n",
    "print(f\"  1. Change USE_PAGE_TOLERANCE in section 4.2\")\n",
    "print(f\"  2. Delete existing JSON files (or they'll be skipped)\")\n",
    "print(f\"  3. Re-run section 4.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaff51d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
