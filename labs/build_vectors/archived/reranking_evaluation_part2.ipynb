{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d784e904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 6: Configuration & Execution\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.1: Define Configurations to Test\n",
    "\n",
    "# %%\n",
    "# Define which embedding models, chunk sizes, and re-rankers to evaluate\n",
    "configurations = [\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-3-large',\n",
    "        'chunk_sizes': [512, 1024],\n",
    "        'k_retrieve': 40,       # Baseline: retrieve 40 documents\n",
    "        'k_rerank': 20,         # Re-ranking: keep top 20 after re-ranking\n",
    "        'reranker_models': [\n",
    "            # 'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "            # 'BAAI/bge-reranker-large',\n",
    "            'voyage-rerank-2.5'\n",
    "        ]\n",
    "    }\n",
    "    # {\n",
    "    #     'provider': 'openai',\n",
    "    #     'model': 'text-embedding-3-large',\n",
    "    #     'chunk_sizes': [512, 1024],\n",
    "    #     'k_retrieve': 40,\n",
    "    #     'k_rerank': 10,\n",
    "    #     'reranker_models': [\n",
    "    #         'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
    "    #         'BAAI/bge-reranker-large'\n",
    "    #     ]\n",
    "    # },\n",
    "    # {\n",
    "    #     'provider': 'ollama',\n",
    "    #     'model': 'nomic-embed-text',\n",
    "    #     'chunk_sizes': [1024],\n",
    "    #     'k_retrieve': 40,\n",
    "    #     'k_rerank': 20,\n",
    "    #     'reranker_models': [\n",
    "    #         'voyage-rerank-2.5'\n",
    "    #     ]\n",
    "    # }\n",
    "]\n",
    "\n",
    "print(\"✓ Configurations defined\")\n",
    "print(f\"  Total configurations: {len(configurations)}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.2: Define Evaluation Parameters\n",
    "\n",
    "# %%\n",
    "# Modes to test\n",
    "modes = ['global', 'singledoc']\n",
    "\n",
    "# Page tolerance setting\n",
    "USE_PAGE_TOLERANCE = True\n",
    "\n",
    "print(\"✓ Evaluation parameters set\")\n",
    "print(f\"  Modes: {modes}\")\n",
    "print(f\"  Page Tolerance: {'ENABLED' if USE_PAGE_TOLERANCE else 'DISABLED'}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.3: Display Evaluation Plan\n",
    "\n",
    "# %%\n",
    "def display_evaluation_plan(configurations, modes):\n",
    "    \"\"\"Display what will be evaluated.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EVALUATION PLAN\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    total_runs = 0\n",
    "    \n",
    "    for config in configurations:\n",
    "        provider = config['provider']\n",
    "        model = config['model']\n",
    "        chunk_sizes = config['chunk_sizes']\n",
    "        k_retrieve = config['k_retrieve']\n",
    "        k_rerank = config['k_rerank']\n",
    "        reranker_models = config.get('reranker_models', [])\n",
    "        \n",
    "        print(f\"\\n{provider}/{model}\")\n",
    "        print(f\"  Chunk sizes: {chunk_sizes}\")\n",
    "        print(f\"  Baseline: retrieve k={k_retrieve}\")\n",
    "        print(f\"  Re-ranking: retrieve k={k_retrieve}, keep top k={k_rerank}\")\n",
    "        print(f\"  Re-rankers: {len(reranker_models)}\")\n",
    "        for rm in reranker_models:\n",
    "            print(f\"    • {rm}\")\n",
    "        \n",
    "        # Calculate runs for this config\n",
    "        # For each chunk + mode: 1 baseline + N re-rankers\n",
    "        runs_per_config = len(chunk_sizes) * len(modes) * (1 + len(reranker_models))\n",
    "        total_runs += runs_per_config\n",
    "        print(f\"  Total runs: {runs_per_config} (baseline + {len(reranker_models)} re-rankers)\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TOTAL EVALUATION RUNS: {total_runs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return total_runs\n",
    "\n",
    "# %%\n",
    "total_runs = display_evaluation_plan(configurations, modes)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.4: Show Output Files That Will Be Created\n",
    "\n",
    "# %%\n",
    "def display_output_files(configurations, modes, output_dir):\n",
    "    \"\"\"Display all output files that will be created.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"OUTPUT FILES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Output directory: {output_dir}\\n\")\n",
    "    \n",
    "    all_files = []\n",
    "    existing_count = 0\n",
    "    \n",
    "    for config in configurations:\n",
    "        provider = config['provider']\n",
    "        model = config['model']\n",
    "        chunk_sizes = config['chunk_sizes']\n",
    "        k_retrieve = config['k_retrieve']\n",
    "        k_rerank = config['k_rerank']\n",
    "        reranker_models = config.get('reranker_models', [])\n",
    "        \n",
    "        print(f\"\\n{provider}/{model}:\")\n",
    "        \n",
    "        for chunk_size in chunk_sizes:\n",
    "            for mode in modes:\n",
    "                # Baseline file\n",
    "                baseline_file = get_output_filename(provider, model, chunk_size, k_retrieve, mode)\n",
    "                exists = check_if_results_exist(provider, model, chunk_size, k_retrieve, mode, output_dir)\n",
    "                status = \"✓ EXISTS\" if exists else \"○ TO CREATE\"\n",
    "                if exists:\n",
    "                    existing_count += 1\n",
    "                print(f\"  {status} {baseline_file}\")\n",
    "                all_files.append((baseline_file, exists))\n",
    "                \n",
    "                # Re-ranking files\n",
    "                for reranker in reranker_models:\n",
    "                    rerank_file = get_output_filename(provider, model, chunk_size, k_retrieve, mode, reranker, k_rerank)\n",
    "                    exists = check_if_results_exist(provider, model, chunk_size, k_retrieve, mode, output_dir, reranker, k_rerank)\n",
    "                    status = \"✓ EXISTS\" if exists else \"○ TO CREATE\"\n",
    "                    if exists:\n",
    "                        existing_count += 1\n",
    "                    print(f\"  {status} {rerank_file}\")\n",
    "                    all_files.append((rerank_file, exists))\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Total files: {len(all_files)}\")\n",
    "    print(f\"  Existing: {existing_count}\")\n",
    "    print(f\"  To create: {len(all_files) - existing_count}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return all_files\n",
    "\n",
    "# %%\n",
    "output_files = display_output_files(configurations, modes, OUTPUT_DIR)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.5: Execute Batch Evaluation\n",
    "\n",
    "# %%\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STARTING BATCH EVALUATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"This will process {len(configurations)} configuration(s)\")\n",
    "print(f\"Existing results will be skipped automatically\\n\")\n",
    "\n",
    "# Confirm before starting (optional - comment out to run without confirmation)\n",
    "# proceed = input(\"Proceed with evaluation? (y/n): \").lower().strip()\n",
    "# if proceed != 'y':\n",
    "#     print(\"Evaluation cancelled by user\")\n",
    "# else:\n",
    "\n",
    "# Run evaluation for each configuration\n",
    "all_results = []\n",
    "\n",
    "for i, config in enumerate(configurations, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING CONFIGURATION {i}/{len(configurations)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    config_results = evaluate_single_configuration(\n",
    "        dataset=dataset,\n",
    "        config=config,\n",
    "        modes=modes,\n",
    "        use_page_tolerance=USE_PAGE_TOLERANCE,\n",
    "        output_dir=OUTPUT_DIR\n",
    "    )\n",
    "    \n",
    "    all_results.extend(config_results)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BATCH EVALUATION COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.6: Display Results Summary\n",
    "\n",
    "# %%\n",
    "def display_results_summary(all_results):\n",
    "    \"\"\"Display summary of all evaluation results.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EVALUATION RESULTS SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Group by configuration\n",
    "    by_config = defaultdict(list)\n",
    "    \n",
    "    for result in all_results:\n",
    "        if result['result']['status'] == 'completed':\n",
    "            key = (result['provider'], result['model'], result['chunk_size'], result['mode'], result.get('k_retrieve', 0))\n",
    "            by_config[key].append(result)\n",
    "    \n",
    "    # Display results\n",
    "    for key, results in sorted(by_config.items()):\n",
    "        provider, model, chunk_size, mode, k_retrieve = key\n",
    "        \n",
    "        print(f\"\\n{provider}/{model}, chunk={chunk_size}, mode={mode}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Find baseline\n",
    "        baseline = next((r for r in results if r['type'] == 'baseline'), None)\n",
    "        if baseline and 'average_mrr' in baseline['result']:\n",
    "            baseline_mrr = baseline['result']['average_mrr']\n",
    "            print(f\"  Baseline (k={k_retrieve}):  MRR = {baseline_mrr:.4f}\")\n",
    "        else:\n",
    "            baseline_mrr = None\n",
    "            print(f\"  Baseline:  (skipped)\")\n",
    "        \n",
    "        # Show re-ranking results\n",
    "        rerank_results = [r for r in results if r['type'] == 'rerank']\n",
    "        for r in sorted(rerank_results, key=lambda x: x['result'].get('average_mrr', 0), reverse=True):\n",
    "            if 'average_mrr' in r['result']:\n",
    "                mrr = r['result']['average_mrr']\n",
    "                k_rerank = r.get('k_rerank', 0)\n",
    "                improvement = r['result'].get('improvement', 0)\n",
    "                improvement_pct = r['result'].get('improvement_percentage', 0)\n",
    "                reranker_short = get_reranker_short_name(r['reranker'])\n",
    "                print(f\"  {reranker_short:25s} (k={k_rerank})  MRR = {mrr:.4f}  ({improvement:+.4f}, {improvement_pct:+.2f}%)\")\n",
    "            else:\n",
    "                reranker_short = get_reranker_short_name(r['reranker'])\n",
    "                print(f\"  {reranker_short:25s} (skipped)\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "\n",
    "# %%\n",
    "display_results_summary(all_results)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.7: List All Generated Files\n",
    "\n",
    "# %%\n",
    "def list_generated_files(output_dir):\n",
    "    \"\"\"List all JSON files in output directory.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"GENERATED FILES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Directory: {output_dir}\\n\")\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    json_files = sorted(output_path.glob(\"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"No JSON files found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Total files: {len(json_files)}\\n\")\n",
    "    \n",
    "    # Group by type\n",
    "    baseline_files = []\n",
    "    rerank_files = []\n",
    "    \n",
    "    for filepath in json_files:\n",
    "        file_size = filepath.stat().st_size / 1024  # KB\n",
    "        if 'rerank-' in filepath.name:\n",
    "            rerank_files.append((filepath.name, file_size))\n",
    "        else:\n",
    "            baseline_files.append((filepath.name, file_size))\n",
    "    \n",
    "    if baseline_files:\n",
    "        print(f\"Baseline files ({len(baseline_files)}):\")\n",
    "        for name, size in baseline_files:\n",
    "            print(f\"  {name} ({size:.1f} KB)\")\n",
    "    \n",
    "    if rerank_files:\n",
    "        print(f\"\\nRe-ranking files ({len(rerank_files)}):\")\n",
    "        for name, size in rerank_files:\n",
    "            print(f\"  {name} ({size:.1f} KB)\")\n",
    "    \n",
    "    total_size = sum(f.stat().st_size for f in json_files) / (1024 * 1024)  # MB\n",
    "    print(f\"\\nTotal size: {total_size:.2f} MB\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# %%\n",
    "list_generated_files(OUTPUT_DIR)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.8: Quick Comparison Function\n",
    "\n",
    "# %%\n",
    "def quick_comparison(provider, model, chunk_size, mode, k_retrieve, k_rerank=None, output_dir=OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Quick comparison of baseline vs all re-rankers for a specific configuration.\n",
    "    \n",
    "    Args:\n",
    "        provider: Embedding provider\n",
    "        model: Embedding model name\n",
    "        chunk_size: Chunk size\n",
    "        mode: \"global\" or \"singledoc\"\n",
    "        k_retrieve: Number retrieved for baseline\n",
    "        k_rerank: Number kept after re-ranking (optional, will find all if None)\n",
    "        output_dir: Output directory\n",
    "    \n",
    "    Usage:\n",
    "        quick_comparison('voyage', 'voyage-finance-2', 512, 'global', 40, 10)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"QUICK COMPARISON\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{provider}/{model}, chunk={chunk_size}, mode={mode}\\n\")\n",
    "    \n",
    "    # Load baseline\n",
    "    baseline_file = get_output_filename(provider, model, chunk_size, k_retrieve, mode)\n",
    "    baseline_path = os.path.join(output_dir, baseline_file)\n",
    "    \n",
    "    if not os.path.exists(baseline_path):\n",
    "        print(f\"✗ Baseline file not found: {baseline_file}\")\n",
    "        return\n",
    "    \n",
    "    with open(baseline_path, 'r') as f:\n",
    "        baseline_data = json.load(f)\n",
    "    \n",
    "    baseline_summary = next((item['summary'] for item in baseline_data if 'summary' in item), None)\n",
    "    if not baseline_summary:\n",
    "        print(\"✗ No summary found in baseline file\")\n",
    "        return\n",
    "    \n",
    "    baseline_mrr = baseline_summary['average_mrr']\n",
    "    baseline_k = baseline_summary.get('k_retrieve', k_retrieve)\n",
    "    print(f\"Baseline (k={baseline_k}): MRR = {baseline_mrr:.4f}\\n\")\n",
    "    \n",
    "    # Find all rerank files for this config\n",
    "    model_clean = model.replace('/', '_')\n",
    "    pattern = f\"{provider}_{model_clean}_chunk{chunk_size}_k{k_retrieve}_{mode}_rerank_k*\"\n",
    "    \n",
    "    rerank_files = []\n",
    "    for f in Path(output_dir).glob(pattern):\n",
    "        rerank_files.append(f)\n",
    "    \n",
    "    if not rerank_files:\n",
    "        print(\"No re-ranking files found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Re-ranking results:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    for rerank_path in rerank_files:\n",
    "        with open(rerank_path, 'r') as f:\n",
    "            rerank_data = json.load(f)\n",
    "        \n",
    "        rerank_summary = next((item['summary'] for item in rerank_data if 'summary' in item), None)\n",
    "        if rerank_summary:\n",
    "            reranker = rerank_summary['retrieval_config']['reranker_model']\n",
    "            reranker_short = get_reranker_short_name(reranker)\n",
    "            k_r = rerank_summary.get('k_rerank', 0)\n",
    "            mrr = rerank_summary['average_mrr']\n",
    "            improvement = rerank_summary['mrr_improvement']\n",
    "            improvement_pct = rerank_summary['mrr_improvement_percentage']\n",
    "            \n",
    "            results.append((reranker_short, k_r, mrr, improvement, improvement_pct))\n",
    "    \n",
    "    # Sort by MRR descending\n",
    "    for reranker_short, k_r, mrr, improvement, improvement_pct in sorted(results, key=lambda x: x[2], reverse=True):\n",
    "        print(f\"{reranker_short:25s} (k={k_r})  MRR = {mrr:.4f}  ({improvement:+.4f}, {improvement_pct:+.2f}%)\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "print(\"✓ Quick comparison function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 6.9: Example Usage of Quick Comparison\n",
    "\n",
    "# %%\n",
    "# Uncomment to use quick comparison after evaluation is complete\n",
    "\"\"\"\n",
    "# Example: Compare all re-rankers for a specific configuration\n",
    "quick_comparison(\n",
    "    provider='voyage',\n",
    "    model='voyage-finance-2',\n",
    "    chunk_size=512,\n",
    "    mode='global',\n",
    "    k_retrieve=40,\n",
    "    k_rerank=10\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ STEP 6 COMPLETE - EVALUATION READY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nConfiguration format:\")\n",
    "print(\"  {\")\n",
    "print(\"    'provider': 'voyage',\")\n",
    "print(\"    'model': 'voyage-finance-2',\")\n",
    "print(\"    'chunk_sizes': [512, 1024],\")\n",
    "print(\"    'k_retrieve': 40,  # Baseline retrieves 40\")\n",
    "print(\"    'k_rerank': 10,    # Re-ranking keeps top 10\")\n",
    "print(\"    'reranker_models': ['cross-encoder/ms-marco-MiniLM-L-12-v2']\")\n",
    "print(\"  }\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Review the evaluation plan (6.3)\")\n",
    "print(\"  2. Check output files (6.4)\")\n",
    "print(\"  3. Run batch evaluation (6.5)\")\n",
    "print(\"  4. View results summary (6.6)\")\n",
    "print(\"  5. Check generated files (6.7)\")\n",
    "print(\"  6. Use quick_comparison() for specific configs (6.8)\")\n",
    "print(\"\\nAll results are saved to:\", OUTPUT_DIR)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28c26a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Load results functions defined\n",
      "✓ Overall statistics function defined\n",
      "✓ Top improved queries function defined\n",
      "✓ Degraded queries function defined\n",
      "✓ Question type analysis function defined\n",
      "✓ Rank movement analysis function defined\n",
      "✓ Multi-config comparison function defined\n",
      "✓ Export to CSV function defined\n",
      "\n",
      "✓ Step 7 complete!\n",
      "  Functions available:\n",
      "    • load_baseline_and_rerank() - Load results\n",
      "    • analyze_overall_statistics() - Overall stats with t-test\n",
      "    • show_top_improved_queries() - Best improvements\n",
      "    • show_degraded_queries() - Worst degradations\n",
      "    • analyze_by_question_type() - Group by question type\n",
      "    • analyze_rank_movements() - Rank change distribution\n",
      "    • compare_configurations() - Multi-config comparison\n",
      "    • export_analysis_to_csv() - Export to CSV\n",
      "\n",
      "============================================================\n",
      "OVERALL STATISTICS: voyage/voyage-3-large, chunk=1024\n",
      "============================================================\n",
      "\n",
      "MRR Scores:\n",
      "  Baseline:    0.6392\n",
      "  Re-ranking:  0.7844\n",
      "  Improvement: +0.1451 (+22.70%)\n",
      "\n",
      "Statistical Significance (Paired t-test):\n",
      "  t-statistic: 5.2812\n",
      "  p-value:     0.000000\n",
      "  Result:      *** (p < 0.001) - Highly significant\n",
      "\n",
      "Query-Level Changes:\n",
      "  Improved:   60 (40.0%)\n",
      "  Degraded:   13 (8.7%)\n",
      "  Unchanged:  77 (51.3%)\n",
      "\n",
      "============================================================\n",
      "TOP 10 IMPROVED QUERIES\n",
      "============================================================\n",
      "\n",
      "[1] Improvement: +13 positions\n",
      "    Company: Pfizer\n",
      "    Question Type: novel-generated\n",
      "    Question: Were there any potential events that are not in Pfizer's standard business operations that substanti...\n",
      "    Baseline rank: 18 → Re-rank: 5\n",
      "\n",
      "[2] Improvement: +11 positions\n",
      "    Company: 3M\n",
      "    Question Type: novel-generated\n",
      "    Question: If we exclude the impact of M&A, which segment has dragged down 3M's overall growth in 2022?...\n",
      "    Baseline rank: 15 → Re-rank: 4\n",
      "\n",
      "[3] Improvement: +11 positions\n",
      "    Company: Activision Blizzard\n",
      "    Question Type: metrics-generated\n",
      "    Question: What is the FY2019 fixed asset turnover ratio for Activision Blizzard? Fixed asset turnover ratio is...\n",
      "    Baseline rank: 12 → Re-rank: 1\n",
      "\n",
      "[4] Improvement: +11 positions\n",
      "    Company: American Express\n",
      "    Question Type: domain-relevant\n",
      "    Question: Does AMEX have an improving operating margin profile as of 2022? If operating margin is not a useful...\n",
      "    Baseline rank: 14 → Re-rank: 3\n",
      "\n",
      "[5] Improvement: +11 positions\n",
      "    Company: Pfizer\n",
      "    Question Type: novel-generated\n",
      "    Question: Did Pfizer grow its PPNE between FY20 and FY21?...\n",
      "    Baseline rank: 15 → Re-rank: 4\n",
      "\n",
      "[6] Improvement: +10 positions\n",
      "    Company: AES Corporation\n",
      "    Question Type: domain-relevant\n",
      "    Question: What is the quantity of restructuring costs directly outlined in AES Corporation's income statements...\n",
      "    Baseline rank: 11 → Re-rank: 1\n",
      "\n",
      "[7] Improvement: +10 positions\n",
      "    Company: Boeing\n",
      "    Question Type: domain-relevant\n",
      "    Question: Does Boeing have an improving gross margin profile as of FY2022? If gross margin is not a useful met...\n",
      "    Baseline rank: 11 → Re-rank: 1\n",
      "\n",
      "[8] Improvement: +9 positions\n",
      "    Company: JPMorgan\n",
      "    Question Type: novel-generated\n",
      "    Question: Which of JPM's business segments had the lowest net revenue in 2021 Q1?...\n",
      "    Baseline rank: 12 → Re-rank: 3\n",
      "\n",
      "[9] Improvement: +9 positions\n",
      "    Company: PepsiCo\n",
      "    Question Type: metrics-generated\n",
      "    Question: What is the FY2022 unadjusted EBITDA % margin for PepsiCo? Calculate unadjusted EBITDA using unadjus...\n",
      "    Baseline rank: 10 → Re-rank: 1\n",
      "\n",
      "[10] Improvement: +7 positions\n",
      "    Company: Microsoft\n",
      "    Question Type: domain-relevant\n",
      "    Question: Has Microsoft increased its debt on balance sheet between FY2023 and the FY2022 period?...\n",
      "    Baseline rank: 9 → Re-rank: 2\n",
      "\n",
      "============================================================\n",
      "DEGRADED QUERIES (Top 5 worst)\n",
      "============================================================\n",
      "\n",
      "[1] Degradation: -14 positions\n",
      "    Company: Boeing\n",
      "    Question Type: domain-relevant\n",
      "    Question: Who are the primary customers of Boeing as of FY2022?...\n",
      "    Baseline rank: 3 → Re-rank: 17\n",
      "\n",
      "[2] Degradation: -7 positions\n",
      "    Company: AMD\n",
      "    Question Type: domain-relevant\n",
      "    Question: What are the major products and services that AMD sells as of FY22?...\n",
      "    Baseline rank: 2 → Re-rank: 9\n",
      "\n",
      "[3] Degradation: -5 positions\n",
      "    Company: Johnson & Johnson\n",
      "    Question Type: domain-relevant\n",
      "    Question: Are JnJ's FY2022 financials that of a high growth company?...\n",
      "    Baseline rank: 1 → Re-rank: 6\n",
      "\n",
      "[4] Degradation: -4 positions\n",
      "    Company: PepsiCo\n",
      "    Question Type: domain-relevant\n",
      "    Question: What is the quantity of restructuring costs directly outlined in Pepsico's income statements for FY2...\n",
      "    Baseline rank: 1 → Re-rank: 5\n",
      "\n",
      "[5] Degradation: -3 positions\n",
      "    Company: Amcor\n",
      "    Question Type: novel-generated\n",
      "    Question: How much was the Real change in Sales for AMCOR in FY 2023 vs FY 2022, if we exclude the impact of F...\n",
      "    Baseline rank: 1 → Re-rank: 4\n",
      "\n",
      "============================================================\n",
      "ANALYSIS BY QUESTION TYPE\n",
      "============================================================\n",
      "\n",
      "Question Type               Count    Avg MRR   Baseline  Improved    p-value\n",
      "-------------------------------------------------------------------------------------\n",
      "domain-relevant                50     0.6741     0.5668    +18.9%  0.055849 \n",
      "metrics-generated              50     0.9033     0.6937    +30.2%  0.000022 ***\n",
      "novel-generated                50     0.7757     0.6543    +18.5%  0.005925 **\n",
      "\n",
      "Significance levels: *** p<0.001, ** p<0.01, * p<0.05\n",
      "\n",
      "============================================================\n",
      "RANK MOVEMENT ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Rank Change Statistics:\n",
      "  Average change:     +1.07 positions\n",
      "  Best improvement:   +13 positions\n",
      "  Worst degradation:  -14 positions\n",
      "\n",
      "Rank Change Distribution:\n",
      "  Large improvement (+10 or more)        7 (  4.7%) ██\n",
      "  Medium improvement (+5 to +9)          6 (  4.0%) ██\n",
      "  Small improvement (+1 to +4)          47 ( 31.3%) ███████████████\n",
      "  No change (0)                         77 ( 51.3%) █████████████████████████\n",
      "  Small degradation (-1 to -4)          10 (  6.7%) ███\n",
      "  Medium degradation (-5 to -9)          1 (  0.7%) \n",
      "  Large degradation (-10 or less)        1 (  0.7%) \n",
      "✓ Exported 150 queries to: analysis_voyage_voyage-3-large_chunk1024_singledoc.csv\n",
      "\n",
      "============================================================\n",
      "MULTI-CONFIGURATION COMPARISON\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "OVERALL STATISTICS: voyage/voyage-3-large, chunk=512, mode=global\n",
      "============================================================\n",
      "\n",
      "MRR Scores:\n",
      "  Baseline:    0.5204\n",
      "  Re-ranking:  0.6532\n",
      "  Improvement: +0.1327 (+25.51%)\n",
      "\n",
      "Statistical Significance (Paired t-test):\n",
      "  t-statistic: 4.1610\n",
      "  p-value:     0.000053\n",
      "  Result:      *** (p < 0.001) - Highly significant\n",
      "\n",
      "Query-Level Changes:\n",
      "  Improved:   59 (39.3%)\n",
      "  Degraded:   24 (16.0%)\n",
      "  Unchanged:  67 (44.7%)\n",
      "\n",
      "============================================================\n",
      "OVERALL STATISTICS: voyage/voyage-3-large, chunk=512, mode=singledoc\n",
      "============================================================\n",
      "\n",
      "MRR Scores:\n",
      "  Baseline:    0.5956\n",
      "  Re-ranking:  0.7720\n",
      "  Improvement: +0.1763 (+29.60%)\n",
      "\n",
      "Statistical Significance (Paired t-test):\n",
      "  t-statistic: 5.9730\n",
      "  p-value:     0.000000\n",
      "  Result:      *** (p < 0.001) - Highly significant\n",
      "\n",
      "Query-Level Changes:\n",
      "  Improved:   64 (42.7%)\n",
      "  Degraded:   14 (9.3%)\n",
      "  Unchanged:  72 (48.0%)\n",
      "\n",
      "============================================================\n",
      "OVERALL STATISTICS: voyage/voyage-3-large, chunk=1024, mode=global\n",
      "============================================================\n",
      "\n",
      "MRR Scores:\n",
      "  Baseline:    0.5556\n",
      "  Re-ranking:  0.6840\n",
      "  Improvement: +0.1284 (+23.11%)\n",
      "\n",
      "Statistical Significance (Paired t-test):\n",
      "  t-statistic: 4.3857\n",
      "  p-value:     0.000022\n",
      "  Result:      *** (p < 0.001) - Highly significant\n",
      "\n",
      "Query-Level Changes:\n",
      "  Improved:   57 (38.0%)\n",
      "  Degraded:   23 (15.3%)\n",
      "  Unchanged:  70 (46.7%)\n",
      "\n",
      "============================================================\n",
      "OVERALL STATISTICS: voyage/voyage-3-large, chunk=1024, mode=singledoc\n",
      "============================================================\n",
      "\n",
      "MRR Scores:\n",
      "  Baseline:    0.6392\n",
      "  Re-ranking:  0.7844\n",
      "  Improvement: +0.1451 (+22.70%)\n",
      "\n",
      "Statistical Significance (Paired t-test):\n",
      "  t-statistic: 5.2812\n",
      "  p-value:     0.000000\n",
      "  Result:      *** (p < 0.001) - Highly significant\n",
      "\n",
      "Query-Level Changes:\n",
      "  Improved:   60 (40.0%)\n",
      "  Degraded:   13 (8.7%)\n",
      "  Unchanged:  77 (51.3%)\n",
      "\n",
      "============================================================\n",
      "SUMMARY TABLE\n",
      "============================================================\n",
      "\n",
      "Configuration                              Baseline    Re-rank   Improve    p-value\n",
      "-------------------------------------------------------------------------------------\n",
      "voyage/voyage-3-large, chunk=512, mode=global     0.5204     0.6532    +25.5%  0.000053 ***\n",
      "voyage/voyage-3-large, chunk=512, mode=singledoc     0.5956     0.7720    +29.6%  0.000000 ***\n",
      "voyage/voyage-3-large, chunk=1024, mode=global     0.5556     0.6840    +23.1%  0.000022 ***\n",
      "voyage/voyage-3-large, chunk=1024, mode=singledoc     0.6392     0.7844    +22.7%  0.000000 ***\n",
      "\n",
      "Significance: *** p<0.001, ** p<0.01, * p<0.05\n",
      "\n",
      "============================================================\n",
      "STEP 7 READY FOR USE!\n",
      "============================================================\n",
      "\n",
      "Uncomment the examples above and run to analyze your results.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 7: Query-Level Analysis\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 7.1: Load Results Functions\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def load_results_file(filepath):\n",
    "    \"\"\"Load a results JSON file.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Separate queries from summary\n",
    "    queries = [item for item in data if 'summary' not in item]\n",
    "    summary = next((item['summary'] for item in data if 'summary' in item), None)\n",
    "    \n",
    "    return queries, summary\n",
    "\n",
    "\n",
    "def load_baseline_and_rerank(provider, model, chunk_size, mode, k_retrieve, k_rerank, reranker_model, output_dir=OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Load both baseline and re-ranking results for comparison.\n",
    "    \n",
    "    Returns:\n",
    "        baseline_queries, baseline_summary, rerank_queries, rerank_summary\n",
    "    \"\"\"\n",
    "    # Baseline file\n",
    "    baseline_file = get_output_filename(provider, model, chunk_size, k_retrieve, mode)\n",
    "    baseline_path = os.path.join(output_dir, baseline_file)\n",
    "    \n",
    "    # Re-ranking file\n",
    "    rerank_file = get_output_filename(provider, model, chunk_size, k_retrieve, mode, reranker_model, k_rerank)\n",
    "    rerank_path = os.path.join(output_dir, rerank_file)\n",
    "    \n",
    "    if not os.path.exists(baseline_path):\n",
    "        raise FileNotFoundError(f\"Baseline file not found: {baseline_file}\")\n",
    "    if not os.path.exists(rerank_path):\n",
    "        raise FileNotFoundError(f\"Re-ranking file not found: {rerank_file}\")\n",
    "    \n",
    "    baseline_queries, baseline_summary = load_results_file(baseline_path)\n",
    "    rerank_queries, rerank_summary = load_results_file(rerank_path)\n",
    "    \n",
    "    return baseline_queries, baseline_summary, rerank_queries, rerank_summary\n",
    "\n",
    "\n",
    "print(\"✓ Load results functions defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 7.2: Overall Statistics with Statistical Tests\n",
    "\n",
    "# %%\n",
    "def analyze_overall_statistics(baseline_queries, rerank_queries, config_name=\"Configuration\"):\n",
    "    \"\"\"\n",
    "    Compute overall statistics and perform statistical significance tests.\n",
    "    \n",
    "    Args:\n",
    "        baseline_queries: List of baseline query results\n",
    "        rerank_queries: List of re-ranking query results\n",
    "        config_name: Name for display\n",
    "    \"\"\"\n",
    "    from scipy import stats as scipy_stats  # Import with alias to avoid conflicts\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"OVERALL STATISTICS: {config_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Extract MRR scores\n",
    "    baseline_mrrs = [q['mrr_score'] for q in baseline_queries if 'mrr_score' in q]\n",
    "    rerank_mrrs = [q['mrr_score'] for q in rerank_queries if 'mrr_score' in q]\n",
    "    \n",
    "    # Basic statistics\n",
    "    avg_baseline = sum(baseline_mrrs) / len(baseline_mrrs)\n",
    "    avg_rerank = sum(rerank_mrrs) / len(rerank_mrrs)\n",
    "    improvement = avg_rerank - avg_baseline\n",
    "    improvement_pct = (improvement / avg_baseline * 100) if avg_baseline > 0 else 0\n",
    "    \n",
    "    print(f\"\\nMRR Scores:\")\n",
    "    print(f\"  Baseline:    {avg_baseline:.4f}\")\n",
    "    print(f\"  Re-ranking:  {avg_rerank:.4f}\")\n",
    "    print(f\"  Improvement: {improvement:+.4f} ({improvement_pct:+.2f}%)\")\n",
    "    \n",
    "    # Statistical significance test (Paired t-test)\n",
    "    t_statistic, p_value = scipy_stats.ttest_rel(rerank_mrrs, baseline_mrrs)\n",
    "    \n",
    "    print(f\"\\nStatistical Significance (Paired t-test):\")\n",
    "    print(f\"  t-statistic: {t_statistic:.4f}\")\n",
    "    print(f\"  p-value:     {p_value:.6f}\")\n",
    "    \n",
    "    if p_value < 0.001:\n",
    "        significance = \"*** (p < 0.001) - Highly significant\"\n",
    "    elif p_value < 0.01:\n",
    "        significance = \"** (p < 0.01) - Very significant\"\n",
    "    elif p_value < 0.05:\n",
    "        significance = \"* (p < 0.05) - Significant\"\n",
    "    else:\n",
    "        significance = \"(p >= 0.05) - Not significant\"\n",
    "    \n",
    "    print(f\"  Result:      {significance}\")\n",
    "    \n",
    "    # Query-level improvements\n",
    "    improvements_count = sum(1 for q in rerank_queries if q.get('rank_improvement', 0) > 0)\n",
    "    degradations_count = sum(1 for q in rerank_queries if q.get('rank_improvement', 0) < 0)\n",
    "    unchanged_count = sum(1 for q in rerank_queries if q.get('rank_improvement', 0) == 0)\n",
    "    \n",
    "    print(f\"\\nQuery-Level Changes:\")\n",
    "    print(f\"  Improved:   {improvements_count} ({improvements_count/len(rerank_queries)*100:.1f}%)\")\n",
    "    print(f\"  Degraded:   {degradations_count} ({degradations_count/len(rerank_queries)*100:.1f}%)\")\n",
    "    print(f\"  Unchanged:  {unchanged_count} ({unchanged_count/len(rerank_queries)*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'avg_baseline': avg_baseline,\n",
    "        'avg_rerank': avg_rerank,\n",
    "        'improvement': improvement,\n",
    "        'improvement_pct': improvement_pct,\n",
    "        't_statistic': t_statistic,\n",
    "        'p_value': p_value,\n",
    "        'improved': improvements_count,\n",
    "        'degraded': degradations_count,\n",
    "        'unchanged': unchanged_count\n",
    "    }\n",
    "\n",
    "print(\"✓ Overall statistics function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 7.3: Top Improved Queries\n",
    "\n",
    "# %%\n",
    "def show_top_improved_queries(rerank_queries, dataset, top_n=10):\n",
    "    \"\"\"\n",
    "    Show queries with the biggest improvements.\n",
    "    \n",
    "    Args:\n",
    "        rerank_queries: List of re-ranking query results\n",
    "        dataset: FinanceBench dataset (to get question details)\n",
    "        top_n: Number of top queries to show\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TOP {top_n} IMPROVED QUERIES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Filter queries with improvements\n",
    "    improved = [q for q in rerank_queries if q.get('rank_improvement', 0) > 0]\n",
    "    \n",
    "    if not improved:\n",
    "        print(\"No improved queries found.\")\n",
    "        return []\n",
    "    \n",
    "    # Sort by rank improvement (descending)\n",
    "    improved_sorted = sorted(improved, key=lambda x: x.get('rank_improvement', 0), reverse=True)\n",
    "    \n",
    "    for i, query_result in enumerate(improved_sorted[:top_n], 1):\n",
    "        query_id = query_result['query_id']\n",
    "        question = query_result['query']\n",
    "        baseline_rank = query_result.get('rank_baseline', -1)\n",
    "        rerank_rank = query_result.get('rank', -1)\n",
    "        improvement = query_result.get('rank_improvement', 0)\n",
    "        \n",
    "        # Get additional info from dataset\n",
    "        dataset_record = next((r for r in dataset if r['financebench_id'] == query_id), None)\n",
    "        company = dataset_record['company'] if dataset_record else \"Unknown\"\n",
    "        question_type = dataset_record.get('question_type', 'N/A') if dataset_record else 'N/A'\n",
    "        \n",
    "        print(f\"\\n[{i}] Improvement: +{improvement} positions\")\n",
    "        print(f\"    Company: {company}\")\n",
    "        print(f\"    Question Type: {question_type}\")\n",
    "        print(f\"    Question: {question[:100]}...\")\n",
    "        print(f\"    Baseline rank: {baseline_rank} → Re-rank: {rerank_rank}\")\n",
    "    \n",
    "    return improved_sorted[:top_n]\n",
    "\n",
    "print(\"✓ Top improved queries function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 7.4: Degraded Queries Analysis\n",
    "\n",
    "# %%\n",
    "def show_degraded_queries(rerank_queries, dataset, top_n=5):\n",
    "    \"\"\"\n",
    "    Show queries that got worse after re-ranking.\n",
    "    \n",
    "    Args:\n",
    "        rerank_queries: List of re-ranking query results\n",
    "        dataset: FinanceBench dataset\n",
    "        top_n: Number of queries to show\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DEGRADED QUERIES (Top {top_n} worst)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Filter degraded queries\n",
    "    degraded = [q for q in rerank_queries if q.get('rank_improvement', 0) < 0]\n",
    "    \n",
    "    if not degraded:\n",
    "        print(\"No degraded queries found - excellent!\")\n",
    "        return []\n",
    "    \n",
    "    # Sort by rank improvement (ascending - most negative first)\n",
    "    degraded_sorted = sorted(degraded, key=lambda x: x.get('rank_improvement', 0))\n",
    "    \n",
    "    for i, query_result in enumerate(degraded_sorted[:top_n], 1):\n",
    "        query_id = query_result['query_id']\n",
    "        question = query_result['query']\n",
    "        baseline_rank = query_result.get('rank_baseline', -1)\n",
    "        rerank_rank = query_result.get('rank', -1)\n",
    "        degradation = query_result.get('rank_improvement', 0)\n",
    "        \n",
    "        # Get additional info from dataset\n",
    "        dataset_record = next((r for r in dataset if r['financebench_id'] == query_id), None)\n",
    "        company = dataset_record['company'] if dataset_record else \"Unknown\"\n",
    "        question_type = dataset_record.get('question_type', 'N/A') if dataset_record else 'N/A'\n",
    "        \n",
    "        print(f\"\\n[{i}] Degradation: {degradation} positions\")\n",
    "        print(f\"    Company: {company}\")\n",
    "        print(f\"    Question Type: {question_type}\")\n",
    "        print(f\"    Question: {question[:100]}...\")\n",
    "        print(f\"    Baseline rank: {baseline_rank} → Re-rank: {rerank_rank}\")\n",
    "    \n",
    "    return degraded_sorted[:top_n]\n",
    "\n",
    "print(\"✓ Degraded queries function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 7.5: Analysis by Question Type\n",
    "\n",
    "# %%\n",
    "def analyze_by_question_type(rerank_queries, dataset):\n",
    "    \"\"\"\n",
    "    Analyze improvements grouped by question type.\n",
    "    \n",
    "    Args:\n",
    "        rerank_queries: List of re-ranking query results\n",
    "        dataset: FinanceBench dataset\n",
    "    \"\"\"\n",
    "    from scipy import stats as scipy_stats  # Import with alias to avoid conflicts\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ANALYSIS BY QUESTION TYPE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Group by question type\n",
    "    by_type = defaultdict(list)\n",
    "    \n",
    "    for query_result in rerank_queries:\n",
    "        query_id = query_result['query_id']\n",
    "        dataset_record = next((r for r in dataset if r['financebench_id'] == query_id), None)\n",
    "        \n",
    "        if dataset_record:\n",
    "            q_type = dataset_record.get('question_type', 'Unknown')\n",
    "            by_type[q_type].append(query_result)\n",
    "    \n",
    "    # Analyze each type\n",
    "    print(f\"\\n{'Question Type':<25} {'Count':>7} {'Avg MRR':>10} {'Baseline':>10} {'Improved':>9} {'p-value':>10}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    type_stats = []\n",
    "    \n",
    "    for q_type, queries in sorted(by_type.items()):\n",
    "        count = len(queries)\n",
    "        \n",
    "        # MRR scores\n",
    "        rerank_mrrs = [q['mrr_score'] for q in queries]\n",
    "        baseline_mrrs = [q.get('mrr_baseline', 0) for q in queries]\n",
    "        \n",
    "        avg_rerank = sum(rerank_mrrs) / len(rerank_mrrs)\n",
    "        avg_baseline = sum(baseline_mrrs) / len(baseline_mrrs)\n",
    "        improvement = avg_rerank - avg_baseline\n",
    "        improvement_pct = (improvement / avg_baseline * 100) if avg_baseline > 0 else 0\n",
    "        \n",
    "        # Statistical test\n",
    "        if len(rerank_mrrs) > 1:\n",
    "            _, p_value = scipy_stats.ttest_rel(rerank_mrrs, baseline_mrrs)\n",
    "        else:\n",
    "            p_value = 1.0\n",
    "        \n",
    "        # Significance marker\n",
    "        if p_value < 0.001:\n",
    "            sig = \"***\"\n",
    "        elif p_value < 0.01:\n",
    "            sig = \"**\"\n",
    "        elif p_value < 0.05:\n",
    "            sig = \"*\"\n",
    "        else:\n",
    "            sig = \"\"\n",
    "        \n",
    "        print(f\"{q_type:<25} {count:>7} {avg_rerank:>10.4f} {avg_baseline:>10.4f} \"\n",
    "              f\"{improvement_pct:>+8.1f}% {p_value:>9.6f} {sig}\")\n",
    "        \n",
    "        type_stats.append({\n",
    "            'question_type': q_type,\n",
    "            'count': count,\n",
    "            'avg_rerank': avg_rerank,\n",
    "            'avg_baseline': avg_baseline,\n",
    "            'improvement': improvement,\n",
    "            'improvement_pct': improvement_pct,\n",
    "            'p_value': p_value\n",
    "        })\n",
    "    \n",
    "    print(\"\\nSignificance levels: *** p<0.001, ** p<0.01, * p<0.05\")\n",
    "    \n",
    "    return type_stats\n",
    "\n",
    "print(\"✓ Question type analysis function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 7.6: Rank Movement Analysis\n",
    "\n",
    "# %%\n",
    "def analyze_rank_movements(rerank_queries):\n",
    "    \"\"\"\n",
    "    Analyze how much ranks changed (histogram of rank improvements).\n",
    "    \n",
    "    Args:\n",
    "        rerank_queries: List of re-ranking query results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RANK MOVEMENT ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Extract rank improvements\n",
    "    improvements = [q.get('rank_improvement', 0) for q in rerank_queries]\n",
    "    \n",
    "    # Statistics\n",
    "    avg_improvement = sum(improvements) / len(improvements)\n",
    "    max_improvement = max(improvements)\n",
    "    max_degradation = min(improvements)\n",
    "    \n",
    "    print(f\"\\nRank Change Statistics:\")\n",
    "    print(f\"  Average change:     {avg_improvement:+.2f} positions\")\n",
    "    print(f\"  Best improvement:   +{max_improvement} positions\")\n",
    "    print(f\"  Worst degradation:  {max_degradation} positions\")\n",
    "    \n",
    "    # Distribution\n",
    "    print(f\"\\nRank Change Distribution:\")\n",
    "    \n",
    "    # Create bins\n",
    "    bins = {\n",
    "        'Large improvement (+10 or more)': sum(1 for x in improvements if x >= 10),\n",
    "        'Medium improvement (+5 to +9)': sum(1 for x in improvements if 5 <= x < 10),\n",
    "        'Small improvement (+1 to +4)': sum(1 for x in improvements if 1 <= x < 5),\n",
    "        'No change (0)': sum(1 for x in improvements if x == 0),\n",
    "        'Small degradation (-1 to -4)': sum(1 for x in improvements if -4 <= x < 0),\n",
    "        'Medium degradation (-5 to -9)': sum(1 for x in improvements if -9 <= x < -5),\n",
    "        'Large degradation (-10 or less)': sum(1 for x in improvements if x <= -10)\n",
    "    }\n",
    "    \n",
    "    for category, count in bins.items():\n",
    "        pct = count / len(improvements) * 100\n",
    "        bar = '█' * int(pct / 2)  # Simple bar chart\n",
    "        print(f\"  {category:<35} {count:>4} ({pct:>5.1f}%) {bar}\")\n",
    "    \n",
    "    return improvements\n",
    "\n",
    "print(\"✓ Rank movement analysis function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 7.7: Compare Multiple Configurations\n",
    "\n",
    "# %%\n",
    "def compare_configurations(configs_to_compare, dataset, output_dir=OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Compare multiple configurations side-by-side.\n",
    "    \n",
    "    Args:\n",
    "        configs_to_compare: List of dicts with provider, model, chunk_size, mode, k_retrieve, k_rerank, reranker_model\n",
    "        dataset: FinanceBench dataset\n",
    "        output_dir: Output directory\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MULTI-CONFIGURATION COMPARISON\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    all_stats = []\n",
    "    \n",
    "    for config in configs_to_compare:\n",
    "        provider = config['provider']\n",
    "        model = config['model']\n",
    "        chunk_size = config['chunk_size']\n",
    "        mode = config['mode']\n",
    "        k_retrieve = config['k_retrieve']\n",
    "        k_rerank = config['k_rerank']\n",
    "        reranker_model = config['reranker_model']\n",
    "        \n",
    "        config_name = f\"{provider}/{model}, chunk={chunk_size}, mode={mode}\"\n",
    "        \n",
    "        try:\n",
    "            # Load results\n",
    "            baseline_queries, baseline_summary, rerank_queries, rerank_summary = load_baseline_and_rerank(\n",
    "                provider, model, chunk_size, mode, k_retrieve, k_rerank, reranker_model, output_dir\n",
    "            )\n",
    "            \n",
    "            # Analyze\n",
    "            stats = analyze_overall_statistics(baseline_queries, rerank_queries, config_name)\n",
    "            stats['config_name'] = config_name\n",
    "            stats['config'] = config\n",
    "            all_stats.append(stats)\n",
    "            \n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"\\n✗ Skipping {config_name}: {e}\")\n",
    "    \n",
    "    # Summary table\n",
    "    if all_stats:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"SUMMARY TABLE\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        print(f\"{'Configuration':<40} {'Baseline':>10} {'Re-rank':>10} {'Improve':>9} {'p-value':>10}\")\n",
    "        print(\"-\" * 85)\n",
    "        \n",
    "        for stat in all_stats:\n",
    "            sig = \"\"\n",
    "            if stat['p_value'] < 0.001:\n",
    "                sig = \"***\"\n",
    "            elif stat['p_value'] < 0.01:\n",
    "                sig = \"**\"\n",
    "            elif stat['p_value'] < 0.05:\n",
    "                sig = \"*\"\n",
    "            \n",
    "            print(f\"{stat['config_name']:<40} {stat['avg_baseline']:>10.4f} {stat['avg_rerank']:>10.4f} \"\n",
    "                  f\"{stat['improvement_pct']:>+8.1f}% {stat['p_value']:>9.6f} {sig}\")\n",
    "        \n",
    "        print(\"\\nSignificance: *** p<0.001, ** p<0.01, * p<0.05\")\n",
    "    \n",
    "    return all_stats\n",
    "\n",
    "print(\"✓ Multi-config comparison function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 7.8: Export to CSV\n",
    "\n",
    "# %%\n",
    "def export_analysis_to_csv(rerank_queries, dataset, output_file, output_dir=OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Export detailed query-level analysis to CSV.\n",
    "    \n",
    "    Args:\n",
    "        rerank_queries: List of re-ranking query results\n",
    "        dataset: FinanceBench dataset\n",
    "        output_file: Output CSV filename\n",
    "        output_dir: Output directory\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for query_result in rerank_queries:\n",
    "        query_id = query_result['query_id']\n",
    "        \n",
    "        # Get dataset info\n",
    "        dataset_record = next((r for r in dataset if r['financebench_id'] == query_id), None)\n",
    "        \n",
    "        row = {\n",
    "            'query_id': query_id,\n",
    "            'question': query_result['query'],\n",
    "            'company': dataset_record['company'] if dataset_record else 'Unknown',\n",
    "            'question_type': dataset_record.get('question_type', 'N/A') if dataset_record else 'N/A',\n",
    "            'baseline_rank': query_result.get('rank_baseline', -1),\n",
    "            'baseline_mrr': query_result.get('mrr_baseline', 0),\n",
    "            'rerank_rank': query_result.get('rank', -1),\n",
    "            'rerank_mrr': query_result.get('mrr_score', 0),\n",
    "            'rank_improvement': query_result.get('rank_improvement', 0),\n",
    "            'mrr_improvement': query_result.get('mrr_score', 0) - query_result.get('mrr_baseline', 0)\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df = pd.DataFrame(rows)\n",
    "    output_path = os.path.join(output_dir, output_file)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"✓ Exported {len(rows)} queries to: {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"✓ Export to CSV function defined\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 7 complete!\")\n",
    "print(\"  Functions available:\")\n",
    "print(\"    • load_baseline_and_rerank() - Load results\")\n",
    "print(\"    • analyze_overall_statistics() - Overall stats with t-test\")\n",
    "print(\"    • show_top_improved_queries() - Best improvements\")\n",
    "print(\"    • show_degraded_queries() - Worst degradations\")\n",
    "print(\"    • analyze_by_question_type() - Group by question type\")\n",
    "print(\"    • analyze_rank_movements() - Rank change distribution\")\n",
    "print(\"    • compare_configurations() - Multi-config comparison\")\n",
    "print(\"    • export_analysis_to_csv() - Export to CSV\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 7.9: Example Usage\n",
    "\n",
    "# %%\n",
    "# Example 1: Analyze a single configuration\n",
    "\n",
    "provider = 'voyage'\n",
    "model = 'voyage-3-large'\n",
    "chunk_size = 1024\n",
    "mode = 'singledoc'\n",
    "k_retrieve = 40\n",
    "k_rerank = 20\n",
    "reranker_model = 'voyage-rerank-2.5'\n",
    "\n",
    "# Load data\n",
    "baseline_queries, baseline_summary, rerank_queries, rerank_summary = load_baseline_and_rerank(\n",
    "    provider, model, chunk_size, mode, k_retrieve, k_rerank, reranker_model\n",
    ")\n",
    "\n",
    "# Overall statistics\n",
    "stats = analyze_overall_statistics(baseline_queries, rerank_queries, \n",
    "                                   f\"{provider}/{model}, chunk={chunk_size}\")\n",
    "\n",
    "# Top improved\n",
    "show_top_improved_queries(rerank_queries, dataset, top_n=10)\n",
    "\n",
    "# Degraded queries\n",
    "show_degraded_queries(rerank_queries, dataset, top_n=5)\n",
    "\n",
    "# Question type analysis\n",
    "type_stats = analyze_by_question_type(rerank_queries, dataset)\n",
    "\n",
    "# Rank movements\n",
    "analyze_rank_movements(rerank_queries)\n",
    "\n",
    "# Export to CSV\n",
    "export_analysis_to_csv(rerank_queries, dataset, \n",
    "                       f\"analysis_{provider}_{model}_chunk{chunk_size}_{mode}.csv\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# Example 2: Compare multiple configurations\n",
    "\n",
    "configs = [\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-3-large',\n",
    "        'chunk_size': 512,\n",
    "        'mode': 'global',\n",
    "        'k_retrieve': 40,\n",
    "        'k_rerank': 20,\n",
    "        'reranker_model': 'voyage-rerank-2.5'\n",
    "    },\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-3-large',\n",
    "        'chunk_size': 512,\n",
    "        'mode': 'singledoc',\n",
    "        'k_retrieve': 40,\n",
    "        'k_rerank': 20,\n",
    "        'reranker_model': 'voyage-rerank-2.5'\n",
    "    },\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-3-large',\n",
    "        'chunk_size': 1024,\n",
    "        'mode': 'global',\n",
    "        'k_retrieve': 40,\n",
    "        'k_rerank': 20,\n",
    "        'reranker_model': 'voyage-rerank-2.5'\n",
    "    },\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-3-large',\n",
    "        'chunk_size': 1024,\n",
    "        'mode': 'singledoc',\n",
    "        'k_retrieve': 40,\n",
    "        'k_rerank': 20,\n",
    "        'reranker_model': 'voyage-rerank-2.5'\n",
    "    }\n",
    "]\n",
    "\n",
    "all_stats = compare_configurations(configs, dataset)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 7 READY FOR USE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nUncomment the examples above and run to analyze your results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393a1ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 8: Visualization & Comparison\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 8.1: Import Visualization Libraries\n",
    "\n",
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set style for publication-quality plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "plt.rcParams['legend.fontsize'] = 9\n",
    "\n",
    "print(\"✓ Visualization libraries imported\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 8.2: Baseline vs Re-ranking Comparison (Bar Chart)\n",
    "\n",
    "# %%\n",
    "def plot_baseline_vs_reranking(all_stats, output_file=None):\n",
    "    \"\"\"\n",
    "    Create bar chart comparing baseline vs re-ranking MRR.\n",
    "    \n",
    "    Args:\n",
    "        all_stats: List of statistics from compare_configurations()\n",
    "        output_file: Optional filename to save plot\n",
    "    \"\"\"\n",
    "    if not all_stats:\n",
    "        print(\"No statistics to plot\")\n",
    "        return\n",
    "    \n",
    "    # Prepare data\n",
    "    config_names = [s['config_name'] for s in all_stats]\n",
    "    baseline_mrrs = [s['avg_baseline'] for s in all_stats]\n",
    "    rerank_mrrs = [s['avg_rerank'] for s in all_stats]\n",
    "    \n",
    "    # Shorten config names for display\n",
    "    short_names = []\n",
    "    for name in config_names:\n",
    "        # Extract key info: \"chunk=512, mode=global\" -> \"512-global\"\n",
    "        parts = name.split(', ')\n",
    "        chunk = parts[1].replace('chunk=', '')\n",
    "        mode = parts[2].replace('mode=', '')\n",
    "        short_names.append(f\"{chunk}-{mode}\")\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    x = np.arange(len(short_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, baseline_mrrs, width, label='Baseline', \n",
    "                   color='#3498db', alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    bars2 = ax.bar(x + width/2, rerank_mrrs, width, label='Re-ranking', \n",
    "                   color='#e74c3c', alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Add improvement percentage labels\n",
    "    for i, stat in enumerate(all_stats):\n",
    "        improvement_pct = stat['improvement_pct']\n",
    "        y_pos = max(baseline_mrrs[i], rerank_mrrs[i]) + 0.02\n",
    "        \n",
    "        # Add significance stars\n",
    "        p_val = stat['p_value']\n",
    "        if p_val < 0.001:\n",
    "            sig = \"***\"\n",
    "        elif p_val < 0.01:\n",
    "            sig = \"**\"\n",
    "        elif p_val < 0.05:\n",
    "            sig = \"*\"\n",
    "        else:\n",
    "            sig = \"\"\n",
    "        \n",
    "        ax.text(x[i], y_pos, f'+{improvement_pct:.1f}%{sig}',\n",
    "               ha='center', va='bottom', fontsize=8, fontweight='bold', color='green')\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Configuration (chunk-mode)', fontweight='bold')\n",
    "    ax.set_ylabel('Mean Reciprocal Rank (MRR)', fontweight='bold')\n",
    "    ax.set_title('Baseline vs Re-ranking Performance Comparison', fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(short_names, rotation=0)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_ylim(0, max(max(baseline_mrrs), max(rerank_mrrs)) * 1.15)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_file:\n",
    "        plt.savefig(output_file, bbox_inches='tight', dpi=300)\n",
    "        print(f\"✓ Saved plot to: {output_file}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Baseline vs re-ranking plot function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 8.3: Improvement Heatmap (Chunk × Mode)\n",
    "\n",
    "# %%\n",
    "def plot_improvement_heatmap(all_stats, output_file=None):\n",
    "    \"\"\"\n",
    "    Create heatmap showing improvement percentage by chunk size and mode.\n",
    "    \n",
    "    Args:\n",
    "        all_stats: List of statistics from compare_configurations()\n",
    "        output_file: Optional filename to save plot\n",
    "    \"\"\"\n",
    "    if not all_stats:\n",
    "        print(\"No statistics to plot\")\n",
    "        return\n",
    "    \n",
    "    # Extract chunk sizes and modes\n",
    "    chunk_sizes = []\n",
    "    modes = []\n",
    "    improvements = {}\n",
    "    \n",
    "    for stat in all_stats:\n",
    "        config = stat['config']\n",
    "        chunk = config['chunk_size']\n",
    "        mode = config['mode']\n",
    "        improvement_pct = stat['improvement_pct']\n",
    "        \n",
    "        if chunk not in chunk_sizes:\n",
    "            chunk_sizes.append(chunk)\n",
    "        if mode not in modes:\n",
    "            modes.append(mode)\n",
    "        \n",
    "        improvements[(chunk, mode)] = improvement_pct\n",
    "    \n",
    "    # Sort\n",
    "    chunk_sizes = sorted(chunk_sizes)\n",
    "    modes = sorted(modes)\n",
    "    \n",
    "    # Create matrix\n",
    "    data = np.zeros((len(chunk_sizes), len(modes)))\n",
    "    for i, chunk in enumerate(chunk_sizes):\n",
    "        for j, mode in enumerate(modes):\n",
    "            data[i, j] = improvements.get((chunk, mode), 0)\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    im = ax.imshow(data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=max(improvements.values()))\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('MRR Improvement (%)', rotation=270, labelpad=20, fontweight='bold')\n",
    "    \n",
    "    # Set ticks\n",
    "    ax.set_xticks(np.arange(len(modes)))\n",
    "    ax.set_yticks(np.arange(len(chunk_sizes)))\n",
    "    ax.set_xticklabels(modes)\n",
    "    ax.set_yticklabels(chunk_sizes)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(chunk_sizes)):\n",
    "        for j in range(len(modes)):\n",
    "            text = ax.text(j, i, f'{data[i, j]:.1f}%',\n",
    "                          ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel('Retrieval Mode', fontweight='bold')\n",
    "    ax.set_ylabel('Chunk Size', fontweight='bold')\n",
    "    ax.set_title('MRR Improvement Heatmap: Chunk Size × Mode', fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_file:\n",
    "        plt.savefig(output_file, bbox_inches='tight', dpi=300)\n",
    "        print(f\"✓ Saved plot to: {output_file}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Improvement heatmap function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 8.4: Question Type Performance (Bar Chart)\n",
    "\n",
    "# %%\n",
    "def plot_question_type_performance(type_stats, output_file=None):\n",
    "    \"\"\"\n",
    "    Create bar chart showing performance by question type.\n",
    "    \n",
    "    Args:\n",
    "        type_stats: Output from analyze_by_question_type()\n",
    "        output_file: Optional filename to save plot\n",
    "    \"\"\"\n",
    "    if not type_stats:\n",
    "        print(\"No question type statistics to plot\")\n",
    "        return\n",
    "    \n",
    "    # Prepare data\n",
    "    question_types = [s['question_type'] for s in type_stats]\n",
    "    baseline_mrrs = [s['avg_baseline'] for s in type_stats]\n",
    "    rerank_mrrs = [s['avg_rerank'] for s in type_stats]\n",
    "    p_values = [s['p_value'] for s in type_stats]\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    x = np.arange(len(question_types))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, baseline_mrrs, width, label='Baseline', \n",
    "                   color='#3498db', alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    bars2 = ax.bar(x + width/2, rerank_mrrs, width, label='Re-ranking', \n",
    "                   color='#e74c3c', alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Add improvement and significance\n",
    "    for i, stat in enumerate(type_stats):\n",
    "        improvement_pct = stat['improvement_pct']\n",
    "        p_val = stat['p_value']\n",
    "        \n",
    "        y_pos = max(baseline_mrrs[i], rerank_mrrs[i]) + 0.03\n",
    "        \n",
    "        # Significance marker\n",
    "        if p_val < 0.001:\n",
    "            sig = \"***\"\n",
    "        elif p_val < 0.01:\n",
    "            sig = \"**\"\n",
    "        elif p_val < 0.05:\n",
    "            sig = \"*\"\n",
    "        else:\n",
    "            sig = \"\"\n",
    "        \n",
    "        color = 'green' if improvement_pct > 0 else 'red'\n",
    "        ax.text(x[i], y_pos, f'{improvement_pct:+.1f}%{sig}',\n",
    "               ha='center', va='bottom', fontsize=9, fontweight='bold', color=color)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Question Type', fontweight='bold')\n",
    "    ax.set_ylabel('Mean Reciprocal Rank (MRR)', fontweight='bold')\n",
    "    ax.set_title('Re-ranking Performance by Question Type', fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(question_types, rotation=15, ha='right')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_ylim(0, max(max(baseline_mrrs), max(rerank_mrrs)) * 1.2)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add note\n",
    "    ax.text(0.02, 0.98, 'Significance: *** p<0.001, ** p<0.01, * p<0.05',\n",
    "           transform=ax.transAxes, fontsize=8, verticalalignment='top',\n",
    "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_file:\n",
    "        plt.savefig(output_file, bbox_inches='tight', dpi=300)\n",
    "        print(f\"✓ Saved plot to: {output_file}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Question type performance plot function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 8.5: Rank Improvement Distribution (Histogram)\n",
    "\n",
    "# %%\n",
    "def plot_rank_improvement_distribution(improvements, output_file=None):\n",
    "    \"\"\"\n",
    "    Create histogram of rank improvements.\n",
    "    \n",
    "    Args:\n",
    "        improvements: List of rank improvements from analyze_rank_movements()\n",
    "        output_file: Optional filename to save plot\n",
    "    \"\"\"\n",
    "    if not improvements:\n",
    "        print(\"No improvements data to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Create histogram\n",
    "    bins = range(min(improvements)-1, max(improvements)+2, 1)\n",
    "    n, bins, patches = ax.hist(improvements, bins=bins, edgecolor='black', \n",
    "                               linewidth=0.5, alpha=0.7, color='steelblue')\n",
    "    \n",
    "    # Color bars: green for improvements, red for degradations, gray for no change\n",
    "    for i, patch in enumerate(patches):\n",
    "        bin_center = (bins[i] + bins[i+1]) / 2\n",
    "        if bin_center > 0:\n",
    "            patch.set_facecolor('green')\n",
    "            patch.set_alpha(0.7)\n",
    "        elif bin_center < 0:\n",
    "            patch.set_facecolor('red')\n",
    "            patch.set_alpha(0.7)\n",
    "        else:\n",
    "            patch.set_facecolor('gray')\n",
    "            patch.set_alpha(0.5)\n",
    "    \n",
    "    # Add vertical line at zero\n",
    "    ax.axvline(x=0, color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "    \n",
    "    # Statistics\n",
    "    improved_pct = sum(1 for x in improvements if x > 0) / len(improvements) * 100\n",
    "    degraded_pct = sum(1 for x in improvements if x < 0) / len(improvements) * 100\n",
    "    unchanged_pct = sum(1 for x in improvements if x == 0) / len(improvements) * 100\n",
    "    \n",
    "    # Add text box with statistics\n",
    "    stats_text = f'Improved: {improved_pct:.1f}%\\nUnchanged: {unchanged_pct:.1f}%\\nDegraded: {degraded_pct:.1f}%'\n",
    "    ax.text(0.98, 0.97, stats_text, transform=ax.transAxes, fontsize=10,\n",
    "           verticalalignment='top', horizontalalignment='right',\n",
    "           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8, edgecolor='black'))\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Rank Change (Positive = Improvement)', fontweight='bold')\n",
    "    ax.set_ylabel('Number of Queries', fontweight='bold')\n",
    "    ax.set_title('Distribution of Rank Changes After Re-ranking', fontweight='bold', pad=20)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_file:\n",
    "        plt.savefig(output_file, bbox_inches='tight', dpi=300)\n",
    "        print(f\"✓ Saved plot to: {output_file}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Rank improvement distribution plot function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 8.6: Baseline vs Re-rank Scatter Plot\n",
    "\n",
    "# %%\n",
    "def plot_baseline_vs_rerank_scatter(baseline_queries, rerank_queries, output_file=None):\n",
    "    \"\"\"\n",
    "    Create scatter plot showing baseline rank vs re-rank rank.\n",
    "    \n",
    "    Args:\n",
    "        baseline_queries: Baseline query results\n",
    "        rerank_queries: Re-ranking query results\n",
    "        output_file: Optional filename to save plot\n",
    "    \"\"\"\n",
    "    # Extract ranks (only for queries where we have valid ranks)\n",
    "    baseline_ranks = []\n",
    "    rerank_ranks = []\n",
    "    \n",
    "    for rq in rerank_queries:\n",
    "        baseline_rank = rq.get('rank_baseline', -1)\n",
    "        rerank_rank = rq.get('rank', -1)\n",
    "        \n",
    "        if baseline_rank > 0 and rerank_rank > 0:\n",
    "            baseline_ranks.append(baseline_rank)\n",
    "            rerank_ranks.append(rerank_rank)\n",
    "    \n",
    "    if not baseline_ranks:\n",
    "        print(\"No valid rank data to plot\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(baseline_ranks, rerank_ranks, alpha=0.5, s=50, color='steelblue', edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Add diagonal line (y=x) - represents no change\n",
    "    max_rank = max(max(baseline_ranks), max(rerank_ranks))\n",
    "    ax.plot([0, max_rank], [0, max_rank], 'r--', linewidth=2, alpha=0.7, label='No change (y=x)')\n",
    "    \n",
    "    # Count improvements/degradations\n",
    "    improvements = sum(1 for b, r in zip(baseline_ranks, rerank_ranks) if r < b)\n",
    "    degradations = sum(1 for b, r in zip(baseline_ranks, rerank_ranks) if r > b)\n",
    "    unchanged = sum(1 for b, r in zip(baseline_ranks, rerank_ranks) if r == b)\n",
    "    \n",
    "    # Add shaded regions\n",
    "    ax.fill_between([0, max_rank], [0, max_rank], max_rank, alpha=0.1, color='red', \n",
    "                    label=f'Degraded ({degradations})')\n",
    "    ax.fill_between([0, max_rank], 0, [0, max_rank], alpha=0.1, color='green', \n",
    "                    label=f'Improved ({improvements})')\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Baseline Rank', fontweight='bold')\n",
    "    ax.set_ylabel('Re-ranking Rank', fontweight='bold')\n",
    "    ax.set_title('Rank Position Comparison: Baseline vs Re-ranking', fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    # Invert axes (rank 1 is best, should be at top)\n",
    "    ax.invert_yaxis()\n",
    "    ax.invert_xaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_file:\n",
    "        plt.savefig(output_file, bbox_inches='tight', dpi=300)\n",
    "        print(f\"✓ Saved plot to: {output_file}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Baseline vs re-rank scatter plot function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 8.7: Multi-Panel Summary Figure\n",
    "\n",
    "# %%\n",
    "def create_summary_figure(all_stats, type_stats, improvements, output_file=None):\n",
    "    \"\"\"\n",
    "    Create a multi-panel figure with all key visualizations.\n",
    "    \n",
    "    Args:\n",
    "        all_stats: Statistics from compare_configurations()\n",
    "        type_stats: Statistics from analyze_by_question_type()\n",
    "        improvements: Rank improvements list\n",
    "        output_file: Optional filename to save plot\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Panel 1: Baseline vs Re-ranking bars\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    config_names = [s['config_name'].split(', ')[1:] for s in all_stats]\n",
    "    short_names = [f\"{c[0].replace('chunk=', '')}-{c[1].replace('mode=', '')}\" for c in config_names]\n",
    "    baseline_mrrs = [s['avg_baseline'] for s in all_stats]\n",
    "    rerank_mrrs = [s['avg_rerank'] for s in all_stats]\n",
    "    \n",
    "    x = np.arange(len(short_names))\n",
    "    width = 0.35\n",
    "    ax1.bar(x - width/2, baseline_mrrs, width, label='Baseline', color='#3498db', alpha=0.8)\n",
    "    ax1.bar(x + width/2, rerank_mrrs, width, label='Re-ranking', color='#e74c3c', alpha=0.8)\n",
    "    ax1.set_xlabel('Configuration', fontweight='bold')\n",
    "    ax1.set_ylabel('MRR', fontweight='bold')\n",
    "    ax1.set_title('(A) Baseline vs Re-ranking Performance', fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(short_names, rotation=15, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Panel 2: Question type performance\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    q_types = [s['question_type'] for s in type_stats]\n",
    "    improvements_pct = [s['improvement_pct'] for s in type_stats]\n",
    "    colors = ['green' if x > 0 else 'red' for x in improvements_pct]\n",
    "    ax2.barh(q_types, improvements_pct, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax2.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "    ax2.set_xlabel('MRR Improvement (%)', fontweight='bold')\n",
    "    ax2.set_title('(B) Improvement by Question Type', fontweight='bold')\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Panel 3: Rank improvement histogram\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    bins = range(min(improvements)-1, max(improvements)+2, 1)\n",
    "    n, bins_edges, patches = ax3.hist(improvements, bins=bins, edgecolor='black', alpha=0.7)\n",
    "    for i, patch in enumerate(patches):\n",
    "        bin_center = (bins_edges[i] + bins_edges[i+1]) / 2\n",
    "        if bin_center > 0:\n",
    "            patch.set_facecolor('green')\n",
    "        elif bin_center < 0:\n",
    "            patch.set_facecolor('red')\n",
    "        else:\n",
    "            patch.set_facecolor('gray')\n",
    "    ax3.axvline(x=0, color='black', linestyle='--', linewidth=1.5)\n",
    "    ax3.set_xlabel('Rank Change', fontweight='bold')\n",
    "    ax3.set_ylabel('Frequency', fontweight='bold')\n",
    "    ax3.set_title('(C) Distribution of Rank Changes', fontweight='bold')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Panel 4: Summary statistics table\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_data = []\n",
    "    for stat in all_stats:\n",
    "        config = stat['config_name'].split(', ')\n",
    "        chunk_mode = f\"{config[1].replace('chunk=', '')}-{config[2].replace('mode=', '')}\"\n",
    "        summary_data.append([\n",
    "            chunk_mode,\n",
    "            f\"{stat['avg_baseline']:.3f}\",\n",
    "            f\"{stat['avg_rerank']:.3f}\",\n",
    "            f\"+{stat['improvement_pct']:.1f}%\",\n",
    "            \"***\" if stat['p_value'] < 0.001 else \"**\" if stat['p_value'] < 0.01 else \"*\" if stat['p_value'] < 0.05 else \"\"\n",
    "        ])\n",
    "    \n",
    "    table = ax4.table(cellText=summary_data,\n",
    "                     colLabels=['Config', 'Baseline', 'Re-rank', 'Improve', 'Sig.'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     bbox=[0, 0, 1, 1])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # Style header\n",
    "    for i in range(5):\n",
    "        table[(0, i)].set_facecolor('#3498db')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Alternate row colors\n",
    "    for i in range(1, len(summary_data) + 1):\n",
    "        for j in range(5):\n",
    "            if i % 2 == 0:\n",
    "                table[(i, j)].set_facecolor('#ecf0f1')\n",
    "    \n",
    "    ax4.set_title('(D) Summary Statistics', fontweight='bold', pad=20)\n",
    "    \n",
    "    # Main title\n",
    "    fig.suptitle('Re-ranking Evaluation Summary', fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    if output_file:\n",
    "        plt.savefig(output_file, bbox_inches='tight', dpi=300)\n",
    "        print(f\"✓ Saved summary figure to: {output_file}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Multi-panel summary figure function defined\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 8 complete!\")\n",
    "print(\"  Visualization functions available:\")\n",
    "print(\"    • plot_baseline_vs_reranking() - Bar chart comparison\")\n",
    "print(\"    • plot_improvement_heatmap() - Chunk × Mode heatmap\")\n",
    "print(\"    • plot_question_type_performance() - Question type bars\")\n",
    "print(\"    • plot_rank_improvement_distribution() - Histogram\")\n",
    "print(\"    • plot_baseline_vs_rerank_scatter() - Scatter plot\")\n",
    "print(\"    • create_summary_figure() - Multi-panel summary\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 8.8: Example Usage\n",
    "\n",
    "# %%\n",
    "# Example: Create all visualizations for a configuration\n",
    "\n",
    "# First, run Step 7 analysis\n",
    "PLOT_OUTPUT_DIR = OUTPUT_DIR + \"/plots/\"\n",
    "os.makedirs(PLOT_OUTPUT_DIR, exist_ok=True)\n",
    "configs = [\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-3-large',\n",
    "        'chunk_size': 512,\n",
    "        'mode': 'global',\n",
    "        'k_retrieve': 40,\n",
    "        'k_rerank': 20,\n",
    "        'reranker_model': 'voyage-rerank-2.5'\n",
    "    },\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-3-large',\n",
    "        'chunk_size': 512,\n",
    "        'mode': 'singledoc',\n",
    "        'k_retrieve': 40,\n",
    "        'k_rerank': 20,\n",
    "        'reranker_model': 'voyage-rerank-2.5'\n",
    "    },\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-3-large',\n",
    "        'chunk_size': 1024,\n",
    "        'mode': 'global',\n",
    "        'k_retrieve': 40,\n",
    "        'k_rerank': 20,\n",
    "        'reranker_model': 'voyage-rerank-2.5'\n",
    "    },\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-3-large',\n",
    "        'chunk_size': 1024,\n",
    "        'mode': 'singledoc',\n",
    "        'k_retrieve': 40,\n",
    "        'k_rerank': 20,\n",
    "        'reranker_model': 'voyage-rerank-2.5'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get statistics\n",
    "all_stats = compare_configurations(configs, dataset)\n",
    "\n",
    "# Load one config for detailed analysis\n",
    "baseline_queries, baseline_summary, rerank_queries, rerank_summary = load_baseline_and_rerank(\n",
    "    'voyage', 'voyage-3-large', 1024, 'singledoc', 40, 20, 'voyage-rerank-2.5'\n",
    ")\n",
    "\n",
    "# Get question type stats and rank improvements\n",
    "type_stats = analyze_by_question_type(rerank_queries, dataset)\n",
    "improvements = analyze_rank_movements(rerank_queries)\n",
    "\n",
    "# Create visualizations\n",
    "plot_baseline_vs_reranking(all_stats, PLOT_OUTPUT_DIR + 'fig_baseline_vs_rerank.png')\n",
    "plot_improvement_heatmap(all_stats, PLOT_OUTPUT_DIR + 'fig_improvement_heatmap.png')\n",
    "plot_question_type_performance(type_stats, PLOT_OUTPUT_DIR + 'fig_question_types.png')\n",
    "plot_rank_improvement_distribution(improvements, PLOT_OUTPUT_DIR + 'fig_rank_distribution.png')\n",
    "plot_baseline_vs_rerank_scatter(baseline_queries, rerank_queries, PLOT_OUTPUT_DIR + 'fig_scatter.png')\n",
    "create_summary_figure(all_stats, type_stats, improvements, PLOT_OUTPUT_DIR + 'fig_summary.png')\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 8 READY FOR USE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nUncomment the examples above to create visualizations.\")\n",
    "print(\"All plots are publication-quality (300 DPI) and can be saved as PNG files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
