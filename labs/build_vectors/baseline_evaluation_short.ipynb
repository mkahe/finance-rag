{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Text-Based Evaluation Notebook - FinanceBench RAG\n",
    "# Evaluating Retrieval with Page-Based AND Text-Based Metrics\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # FinanceBench Text-Based Evaluation\n",
    "# \n",
    "# This notebook evaluates RAG retrieval performance using BOTH:\n",
    "# 1. **Page-based metrics**: MRR, Recall, Precision, F1 (based on page number matching)\n",
    "# 2. **Text-based metrics**: MRR, Recall, Precision, F1 (based on semantic similarity)\n",
    "# \n",
    "# We use Sentence-BERT (all-MiniLM-L6-v2) to compute cosine similarity between\n",
    "# retrieved chunks and ground truth evidence text.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.1 Standard Imports\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Data handling\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Vector stores and embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "\n",
    "print(\"✓ Standard imports successful\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.2 Text Similarity Imports (NEW)\n",
    "\n",
    "# %%\n",
    "# Sentence-BERT for semantic similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Cosine similarity calculation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"✓ Text similarity imports successful\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.3 Load Environment Variables\n",
    "\n",
    "# %%\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# API Keys and URLs\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "VOYAGE_API_KEY = os.getenv(\"VOYAGE_API_KEY\")\n",
    "\n",
    "# Verify API keys\n",
    "if OPENAI_API_KEY:\n",
    "    print(\"✓ OpenAI API key loaded\")\n",
    "else:\n",
    "    print(\"⚠ OpenAI API key not found (only needed if using OpenAI embeddings)\")\n",
    "\n",
    "if VOYAGE_API_KEY:\n",
    "    print(\"✓ VoyageAI API key loaded\")\n",
    "else:\n",
    "    print(\"⚠ VoyageAI API key not found (only needed if using VoyageAI embeddings)\")\n",
    "\n",
    "print(f\"✓ Ollama URL: {OLLAMA_BASE_URL}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.4 Configuration Variables\n",
    "\n",
    "# %%\n",
    "# Directory paths\n",
    "VECTOR_DB_BASE_DIR = \"../../vector_databases\"\n",
    "OUTPUT_DIR = \"../../evaluation_results/text_based_evaluation\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_NAME = \"PatronusAI/financebench\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "\n",
    "# Vector database configuration\n",
    "COLLECTION_PREFIX = \"financebench_docs_chunk_\"\n",
    "\n",
    "# ============================================================================\n",
    "# TEXT-BASED EVALUATION PARAMETERS (NEW)\n",
    "# ============================================================================\n",
    "\n",
    "# Sentence-BERT model for semantic similarity\n",
    "SBERT_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# Similarity threshold for text-based matching\n",
    "# Chunks with cosine similarity >= this threshold are considered matches\n",
    "TEXT_SIMILARITY_THRESHOLD = 0.7\n",
    "\n",
    "# Chunk text preview settings\n",
    "# We store abbreviated chunk text: \"first N chars...last N chars\"\n",
    "CHUNK_TEXT_PREFIX_CHARS = 100  # Characters to keep from start\n",
    "CHUNK_TEXT_SUFFIX_CHARS = 100  # Characters to keep from end\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "print(\"✓ Configuration set\")\n",
    "print(f\"  Vector DB Directory: {VECTOR_DB_BASE_DIR}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"\\n  Sentence-BERT Model: {SBERT_MODEL_NAME}\")\n",
    "print(f\"  Text Similarity Threshold: {TEXT_SIMILARITY_THRESHOLD}\")\n",
    "print(f\"  Chunk Text Preview: First {CHUNK_TEXT_PREFIX_CHARS} + Last {CHUNK_TEXT_SUFFIX_CHARS} chars\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1.5 Load FinanceBench Dataset\n",
    "\n",
    "# %%\n",
    "print(\"\\nLoading FinanceBench dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=DATASET_SPLIT)\n",
    "print(f\"✓ Loaded {len(dataset)} queries\")\n",
    "\n",
    "# Show sample query with evidence structure\n",
    "print(\"\\nSample query:\")\n",
    "sample = dataset[0]\n",
    "print(f\"  ID: {sample['financebench_id']}\")\n",
    "print(f\"  Company: {sample['company']}\")\n",
    "print(f\"  Question: {sample['question'][:100]}...\")\n",
    "print(f\"  Doc: {sample['doc_name']}\")\n",
    "print(f\"  Evidence items: {len(sample['evidence'])}\")\n",
    "\n",
    "# Show evidence structure\n",
    "if len(sample['evidence']) > 0:\n",
    "    print(\"\\n  First evidence item structure:\")\n",
    "    evidence_item = sample['evidence'][0]\n",
    "    print(f\"    - doc_name: {evidence_item['doc_name']}\")\n",
    "    print(f\"    - evidence_page_num: {evidence_item['evidence_page_num']}\")\n",
    "    print(f\"    - evidence_text (first 100 chars): {evidence_item['evidence_text'][:100]}...\")\n",
    "    print(f\"    - Has 'evidence_text_full_page': {'evidence_text_full_page' in evidence_item}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ STEP 1 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"  ✓ All imports loaded\")\n",
    "print(\"  ✓ Environment variables configured\")\n",
    "print(\"  ✓ Paths set up\")\n",
    "print(f\"  ✓ Dataset loaded: {len(dataset)} queries\")\n",
    "print(f\"  ✓ Text similarity threshold: {TEXT_SIMILARITY_THRESHOLD}\")\n",
    "print(f\"  ✓ Chunk preview length: {CHUNK_TEXT_PREFIX_CHARS} + {CHUNK_TEXT_SUFFIX_CHARS} chars\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d166028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Load Sentence-BERT Model\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.1 Load Sentence-BERT Model\n",
    "# \n",
    "# We load the `all-MiniLM-L6-v2` model once at the start.\n",
    "# This model will be used to:\n",
    "# 1. Encode evidence texts (done once and cached)\n",
    "# 2. Encode retrieved chunk texts (done for each retrieval)\n",
    "# 3. Calculate cosine similarity between them\n",
    "\n",
    "# %%\n",
    "def load_sentence_bert_model(model_name: str = SBERT_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Load Sentence-BERT model for semantic similarity computation.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the sentence-transformers model\n",
    "        \n",
    "    Returns:\n",
    "        SentenceTransformer model instance\n",
    "    \n",
    "    Notes:\n",
    "        - all-MiniLM-L6-v2: 384-dimensional embeddings, ~80MB model\n",
    "        - First load downloads model from HuggingFace\n",
    "        - Subsequent loads use cached model\n",
    "        - Uses CPU by default (can be moved to GPU if available)\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading Sentence-BERT model: {model_name}\")\n",
    "    print(\"  (First run will download model from HuggingFace...)\")\n",
    "    \n",
    "    try:\n",
    "        model = SentenceTransformer(model_name)\n",
    "        print(f\"✓ Model loaded successfully\")\n",
    "        print(f\"  Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "        print(f\"  Max sequence length: {model.max_seq_length}\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load model: {e}\")\n",
    "        raise\n",
    "\n",
    "# %%\n",
    "# Load the model\n",
    "sbert_model = load_sentence_bert_model()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.2 Test the Model\n",
    "# \n",
    "# Let's verify the model works correctly by encoding sample texts\n",
    "\n",
    "# %%\n",
    "def test_sentence_bert_model(model):\n",
    "    \"\"\"\n",
    "    Test Sentence-BERT model with sample texts.\n",
    "    Verifies encoding and similarity calculation work correctly.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TESTING SENTENCE-BERT MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Sample texts\n",
    "    text1 = \"The company's revenue increased by 15% in Q4 2023.\"\n",
    "    text2 = \"Revenue grew 15 percent in the fourth quarter of 2023.\"\n",
    "    text3 = \"The weather was sunny and pleasant today.\"\n",
    "    \n",
    "    print(\"\\nTest texts:\")\n",
    "    print(f\"  Text 1: {text1}\")\n",
    "    print(f\"  Text 2: {text2}\")\n",
    "    print(f\"  Text 3: {text3}\")\n",
    "    \n",
    "    # Encode texts\n",
    "    print(\"\\nEncoding texts...\")\n",
    "    embeddings = model.encode([text1, text2, text3])\n",
    "    \n",
    "    print(f\"✓ Generated embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"  (3 texts × {embeddings.shape[1]} dimensions)\")\n",
    "    \n",
    "    # Calculate similarities\n",
    "    print(\"\\nCalculating cosine similarities:\")\n",
    "    \n",
    "    # Similarity between text1 and text2 (semantically similar)\n",
    "    sim_1_2 = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    print(f\"  Text 1 ↔ Text 2: {sim_1_2:.4f} (should be HIGH - same meaning)\")\n",
    "    \n",
    "    # Similarity between text1 and text3 (semantically different)\n",
    "    sim_1_3 = cosine_similarity([embeddings[0]], [embeddings[2]])[0][0]\n",
    "    print(f\"  Text 1 ↔ Text 3: {sim_1_3:.4f} (should be LOW - different topics)\")\n",
    "    \n",
    "    # Similarity between text2 and text3 (semantically different)\n",
    "    sim_2_3 = cosine_similarity([embeddings[1]], [embeddings[2]])[0][0]\n",
    "    print(f\"  Text 2 ↔ Text 3: {sim_2_3:.4f} (should be LOW - different topics)\")\n",
    "    \n",
    "    # Verify results make sense\n",
    "    print(\"\\nValidation:\")\n",
    "    if sim_1_2 > 0.7:\n",
    "        print(f\"  ✓ Similar texts have high similarity ({sim_1_2:.4f} > 0.7)\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Similar texts have lower similarity than expected ({sim_1_2:.4f})\")\n",
    "    \n",
    "    if sim_1_3 < 0.5:\n",
    "        print(f\"  ✓ Different texts have low similarity ({sim_1_3:.4f} < 0.5)\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Different texts have higher similarity than expected ({sim_1_3:.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ MODEL TEST COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# %%\n",
    "# Run the test\n",
    "test_result = test_sentence_bert_model(sbert_model)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2.3 Test with Actual FinanceBench Evidence\n",
    "# \n",
    "# Let's test with real evidence text from the dataset\n",
    "\n",
    "# %%\n",
    "def test_with_real_evidence(model, dataset):\n",
    "    \"\"\"\n",
    "    Test model with actual FinanceBench evidence text.\n",
    "    This helps verify the model works well with financial domain text.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TESTING WITH REAL FINANCEBENCH EVIDENCE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get first query with evidence\n",
    "    sample = dataset[0]\n",
    "    evidence_text = sample['evidence'][0]['evidence_text']\n",
    "    \n",
    "    print(f\"\\nQuery: {sample['question'][:100]}...\")\n",
    "    print(f\"\\nEvidence text (first 200 chars):\")\n",
    "    print(f\"  {evidence_text[:200]}...\")\n",
    "    \n",
    "    # Create some test chunks\n",
    "    # Chunk 1: Exact match (should have very high similarity)\n",
    "    chunk1 = evidence_text\n",
    "    \n",
    "    # Chunk 2: Paraphrased version (should have high similarity)\n",
    "    chunk2 = \"Capital expenditures totaled $1,577 million in fiscal year 2018.\"\n",
    "    \n",
    "    # Chunk 3: Different financial topic (should have lower similarity)\n",
    "    chunk3 = \"The company reported strong earnings growth driven by increased sales.\"\n",
    "    \n",
    "    print(\"\\nTest chunks:\")\n",
    "    print(f\"  Chunk 1: Exact match - {chunk1[:80]}...\")\n",
    "    print(f\"  Chunk 2: Paraphrased - {chunk2}\")\n",
    "    print(f\"  Chunk 3: Different topic - {chunk3}\")\n",
    "    \n",
    "    # Encode\n",
    "    evidence_embedding = model.encode([evidence_text])\n",
    "    chunk_embeddings = model.encode([chunk1, chunk2, chunk3])\n",
    "    \n",
    "    # Calculate similarities\n",
    "    print(\"\\nSimilarities with evidence:\")\n",
    "    for i, chunk_emb in enumerate(chunk_embeddings):\n",
    "        sim = cosine_similarity(evidence_embedding, [chunk_emb])[0][0]\n",
    "        match_status = \"✓ MATCH\" if sim >= TEXT_SIMILARITY_THRESHOLD else \"✗ NO MATCH\"\n",
    "        print(f\"  Chunk {i+1}: {sim:.4f} {match_status}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ REAL EVIDENCE TEST COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# %%\n",
    "# Run test with real evidence\n",
    "real_evidence_test = test_with_real_evidence(sbert_model, dataset)\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ STEP 2 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"  ✓ Sentence-BERT model loaded\")\n",
    "print(f\"  ✓ Model: {SBERT_MODEL_NAME}\")\n",
    "print(f\"  ✓ Embedding dimension: {sbert_model.get_sentence_embedding_dimension()}\")\n",
    "print(\"  ✓ Model tested with sample texts\")\n",
    "print(\"  ✓ Model tested with real FinanceBench evidence\")\n",
    "print(\"  ✓ Ready for evidence embedding pre-computation\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcb8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Pre-compute Evidence Embeddings\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.1 Extract All Evidence Texts\n",
    "# \n",
    "# We need to:\n",
    "# 1. Extract all unique evidence texts from the dataset\n",
    "# 2. Create a mapping structure for quick lookup\n",
    "# 3. Pre-compute embeddings once (instead of computing them 150 times)\n",
    "\n",
    "# %%\n",
    "def extract_all_evidence_from_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Extract all evidence items from the dataset.\n",
    "    \n",
    "    Returns a list of evidence items with metadata:\n",
    "    - query_id: Which query this evidence belongs to\n",
    "    - evidence_index: Index within that query's evidence list\n",
    "    - doc_name: Source document\n",
    "    - page_number: Evidence page (1-indexed for consistency)\n",
    "    - evidence_text: The actual text content\n",
    "    \n",
    "    This structure allows us to:\n",
    "    1. Pre-compute embeddings for all evidence\n",
    "    2. Map back to original queries during evaluation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXTRACTING EVIDENCE FROM DATASET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    all_evidence = []\n",
    "    evidence_texts = []\n",
    "    \n",
    "    print(f\"\\nProcessing {len(dataset)} queries...\")\n",
    "    \n",
    "    for record in tqdm(dataset, desc=\"Extracting evidence\"):\n",
    "        query_id = record['financebench_id']\n",
    "        evidence_list = record['evidence']\n",
    "        \n",
    "        for evidence_idx, evidence_item in enumerate(evidence_list):\n",
    "            # Extract evidence information\n",
    "            evidence_entry = {\n",
    "                'query_id': query_id,\n",
    "                'evidence_index': evidence_idx,\n",
    "                'doc_name': evidence_item['doc_name'],\n",
    "                'page_number': evidence_item['evidence_page_num'] + 1,  # Convert to 1-indexed\n",
    "                'evidence_text': evidence_item['evidence_text']\n",
    "            }\n",
    "            \n",
    "            all_evidence.append(evidence_entry)\n",
    "            evidence_texts.append(evidence_item['evidence_text'])\n",
    "    \n",
    "    print(f\"\\n✓ Extracted {len(all_evidence)} evidence items\")\n",
    "    print(f\"  From {len(dataset)} queries\")\n",
    "    print(f\"  Average evidence per query: {len(all_evidence)/len(dataset):.2f}\")\n",
    "    \n",
    "    # Show statistics\n",
    "    evidence_per_query = {}\n",
    "    for record in dataset:\n",
    "        query_id = record['financebench_id']\n",
    "        evidence_per_query[query_id] = len(record['evidence'])\n",
    "    \n",
    "    print(f\"\\nEvidence distribution:\")\n",
    "    print(f\"  Min evidence per query: {min(evidence_per_query.values())}\")\n",
    "    print(f\"  Max evidence per query: {max(evidence_per_query.values())}\")\n",
    "    print(f\"  Median evidence per query: {sorted(evidence_per_query.values())[len(evidence_per_query)//2]}\")\n",
    "    \n",
    "    return all_evidence, evidence_texts\n",
    "\n",
    "# %%\n",
    "# Extract all evidence\n",
    "all_evidence, evidence_texts = extract_all_evidence_from_dataset(dataset)\n",
    "\n",
    "# Show sample evidence\n",
    "print(\"\\nSample evidence items:\")\n",
    "for i in range(min(3, len(all_evidence))):\n",
    "    ev = all_evidence[i]\n",
    "    print(f\"\\n  Evidence {i+1}:\")\n",
    "    print(f\"    Query ID: {ev['query_id']}\")\n",
    "    print(f\"    Doc: {ev['doc_name']}, Page: {ev['page_number']}\")\n",
    "    print(f\"    Text (first 100 chars): {ev['evidence_text'][:100]}...\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.2 Pre-compute Evidence Embeddings\n",
    "# \n",
    "# This is a critical optimization:\n",
    "# - Without pre-computation: 150 queries × avg 1.5 evidence × encoding time\n",
    "# - With pre-computation: Encode once, reuse 150 times\n",
    "# - Estimated time savings: ~98%\n",
    "\n",
    "# %%\n",
    "def compute_evidence_embeddings(\n",
    "    evidence_texts: List[str],\n",
    "    model: SentenceTransformer,\n",
    "    batch_size: int = 32\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pre-compute embeddings for all evidence texts.\n",
    "    \n",
    "    Args:\n",
    "        evidence_texts: List of evidence text strings\n",
    "        model: Sentence-BERT model\n",
    "        batch_size: Number of texts to encode at once (larger = faster but more memory)\n",
    "        \n",
    "    Returns:\n",
    "        numpy array of shape (n_evidence, embedding_dim)\n",
    "        \n",
    "    Notes:\n",
    "        - Processes in batches for efficiency\n",
    "        - Shows progress bar\n",
    "        - Uses CPU by default (can be moved to GPU if available)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPUTING EVIDENCE EMBEDDINGS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nEncoding {len(evidence_texts)} evidence texts...\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "    \n",
    "    # Encode all texts with progress bar\n",
    "    # show_progress_bar=True displays tqdm progress\n",
    "    embeddings = model.encode(\n",
    "        evidence_texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Embeddings computed\")\n",
    "    print(f\"  Shape: {embeddings.shape}\")\n",
    "    print(f\"  Memory: {embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# %%\n",
    "# Compute embeddings\n",
    "evidence_embeddings = compute_evidence_embeddings(\n",
    "    evidence_texts=evidence_texts,\n",
    "    model=sbert_model,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.3 Create Evidence Lookup Structure\n",
    "# \n",
    "# Create a convenient structure to look up evidence by query_id\n",
    "\n",
    "# %%\n",
    "def create_evidence_lookup(all_evidence: List[Dict], evidence_embeddings: np.ndarray) -> Dict:\n",
    "    \"\"\"\n",
    "    Create a lookup dictionary mapping query_id to evidence items with embeddings.\n",
    "    \n",
    "    Structure:\n",
    "    {\n",
    "        'query_id_1': [\n",
    "            {\n",
    "                'evidence_index': 0,\n",
    "                'doc_name': 'DOC_NAME',\n",
    "                'page_number': 60,\n",
    "                'evidence_text': 'text...',\n",
    "                'embedding': numpy array\n",
    "            },\n",
    "            ...\n",
    "        ],\n",
    "        ...\n",
    "    }\n",
    "    \n",
    "    This allows fast lookup: evidence_lookup[query_id] returns all evidence for that query\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CREATING EVIDENCE LOOKUP STRUCTURE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    lookup = defaultdict(list)\n",
    "    \n",
    "    print(f\"\\nBuilding lookup for {len(all_evidence)} evidence items...\")\n",
    "    \n",
    "    for i, evidence_item in enumerate(all_evidence):\n",
    "        query_id = evidence_item['query_id']\n",
    "        \n",
    "        # Add embedding to evidence item\n",
    "        evidence_with_embedding = evidence_item.copy()\n",
    "        evidence_with_embedding['embedding'] = evidence_embeddings[i]\n",
    "        \n",
    "        lookup[query_id].append(evidence_with_embedding)\n",
    "    \n",
    "    print(f\"✓ Lookup created for {len(lookup)} queries\")\n",
    "    \n",
    "    # Verify\n",
    "    sample_query_id = list(lookup.keys())[0]\n",
    "    print(f\"\\nVerification - Sample query: {sample_query_id}\")\n",
    "    print(f\"  Evidence items: {len(lookup[sample_query_id])}\")\n",
    "    print(f\"  First evidence embedding shape: {lookup[sample_query_id][0]['embedding'].shape}\")\n",
    "    \n",
    "    return dict(lookup)\n",
    "\n",
    "# %%\n",
    "# Create lookup\n",
    "evidence_lookup = create_evidence_lookup(all_evidence, evidence_embeddings)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3.4 Test Evidence Lookup\n",
    "# \n",
    "# Verify we can retrieve evidence for any query\n",
    "\n",
    "# %%\n",
    "def test_evidence_lookup(dataset, evidence_lookup):\n",
    "    \"\"\"\n",
    "    Test that evidence lookup works correctly.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TESTING EVIDENCE LOOKUP\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test with first query\n",
    "    sample_record = dataset[0]\n",
    "    query_id = sample_record['financebench_id']\n",
    "    \n",
    "    print(f\"\\nTest query: {query_id}\")\n",
    "    print(f\"  Question: {sample_record['question'][:100]}...\")\n",
    "    \n",
    "    # Retrieve from lookup\n",
    "    evidence_items = evidence_lookup.get(query_id, [])\n",
    "    \n",
    "    print(f\"\\n✓ Retrieved {len(evidence_items)} evidence items\")\n",
    "    \n",
    "    for i, ev in enumerate(evidence_items):\n",
    "        print(f\"\\n  Evidence {i+1}:\")\n",
    "        print(f\"    Doc: {ev['doc_name']}, Page: {ev['page_number']}\")\n",
    "        print(f\"    Text (first 80 chars): {ev['evidence_text'][:80]}...\")\n",
    "        print(f\"    Embedding shape: {ev['embedding'].shape}\")\n",
    "        print(f\"    Embedding sample (first 5 dims): {ev['embedding'][:5]}\")\n",
    "    \n",
    "    # Verify count matches original\n",
    "    original_evidence_count = len(sample_record['evidence'])\n",
    "    retrieved_evidence_count = len(evidence_items)\n",
    "    \n",
    "    if original_evidence_count == retrieved_evidence_count:\n",
    "        print(f\"\\n✓ Count matches: {original_evidence_count} evidence items\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Count mismatch: {original_evidence_count} vs {retrieved_evidence_count}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ LOOKUP TEST COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# %%\n",
    "# Test lookup\n",
    "test_evidence_lookup(dataset, evidence_lookup)\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ STEP 3 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  ✓ Extracted {len(all_evidence)} evidence items from {len(dataset)} queries\")\n",
    "print(f\"  ✓ Computed {evidence_embeddings.shape[0]} embeddings\")\n",
    "print(f\"  ✓ Embedding dimension: {evidence_embeddings.shape[1]}\")\n",
    "print(f\"  ✓ Memory used: {evidence_embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  ✓ Evidence lookup created for {len(evidence_lookup)} queries\")\n",
    "print(\"  ✓ Ready for evaluation with pre-computed embeddings\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3645f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 4: Helper Functions - Metadata Extraction and Vector Store Loading\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.1 Metadata Extraction Functions\n",
    "# \n",
    "# These functions extract document name and page number from retrieved chunks\n",
    "\n",
    "# %%\n",
    "def extract_doc_name_from_path(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract document name from file path.\n",
    "    \n",
    "    Example:\n",
    "        \"../../documents/3M_2018_10K.pdf\" → \"3M_2018_10K\"\n",
    "    \n",
    "    Args:\n",
    "        file_path: Full path to document\n",
    "        \n",
    "    Returns:\n",
    "        Document name without extension\n",
    "    \"\"\"\n",
    "    return Path(file_path).stem\n",
    "\n",
    "\n",
    "def extract_metadata_from_retrieved_doc(doc) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract metadata from a retrieved LangChain document.\n",
    "    \n",
    "    FIXED: Correctly extracts from ChromaDB metadata structure:\n",
    "        - file_path: Full path to PDF\n",
    "        - source: Page number (as integer or string)\n",
    "    \n",
    "    Args:\n",
    "        doc: LangChain Document object from vectorstore.similarity_search()\n",
    "        \n",
    "    Returns:\n",
    "        Dict with:\n",
    "            - doc_name: Document name (e.g., \"3M_2018_10K\")\n",
    "            - page_number: Page number (integer, 0-indexed from ChromaDB)\n",
    "            - chunk_text: The chunk content\n",
    "    \"\"\"\n",
    "    metadata = doc.metadata\n",
    "    \n",
    "    # Extract file path and convert to doc name\n",
    "    file_path = metadata.get('file_path', '')\n",
    "    doc_name = extract_doc_name_from_path(file_path)\n",
    "    \n",
    "    # Extract page number from 'source' field\n",
    "    page_num = metadata.get('source', 0)\n",
    "    \n",
    "    # Ensure page_num is an integer\n",
    "    if isinstance(page_num, str):\n",
    "        try:\n",
    "            page_num = int(page_num)\n",
    "        except ValueError:\n",
    "            page_num = 0\n",
    "    \n",
    "    return {\n",
    "        'doc_name': doc_name,\n",
    "        'page_number': page_num,  # Keep 0-indexed as stored in ChromaDB\n",
    "        'chunk_text': doc.page_content\n",
    "    }\n",
    "\n",
    "print(\"✓ Metadata extraction functions defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.2 Embedding Function Factory\n",
    "# \n",
    "# Creates the appropriate embedding function based on provider\n",
    "\n",
    "# %%\n",
    "def get_embedding_function(provider: str, model: str):\n",
    "    \"\"\"\n",
    "    Get embedding function for vector store loading.\n",
    "    \n",
    "    Args:\n",
    "        provider: \"ollama\", \"openai\", or \"voyage\"\n",
    "        model: Model name (e.g., \"nomic-embed-text\", \"text-embedding-3-small\")\n",
    "        \n",
    "    Returns:\n",
    "        Embedding function compatible with LangChain/ChromaDB\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If provider is unknown\n",
    "    \"\"\"\n",
    "    if provider == \"ollama\":\n",
    "        return OllamaEmbeddings(\n",
    "            model=model,\n",
    "            base_url=OLLAMA_BASE_URL\n",
    "        )\n",
    "    elif provider == \"openai\":\n",
    "        return OpenAIEmbeddings(\n",
    "            model=model,\n",
    "            openai_api_key=OPENAI_API_KEY\n",
    "        )\n",
    "    elif provider == \"voyage\":\n",
    "        return VoyageAIEmbeddings(\n",
    "            model=model,\n",
    "            voyage_api_key=VOYAGE_API_KEY\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "print(\"✓ Embedding function factory defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.3 Vector Store Loading\n",
    "# \n",
    "# Load pre-built vector databases from disk\n",
    "\n",
    "# %%\n",
    "def load_vectorstore(\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    base_dir: str = VECTOR_DB_BASE_DIR,\n",
    "    collection_prefix: str = COLLECTION_PREFIX\n",
    ") -> Chroma:\n",
    "    \"\"\"\n",
    "    Load a pre-built vector store from disk.\n",
    "    \n",
    "    The vector databases were created by build_vectore_database.ipynb\n",
    "    and stored in a specific directory structure:\n",
    "    \n",
    "    {base_dir}/{provider}_{model}/financebench_docs_chunk_{chunk_size}/\n",
    "    \n",
    "    Args:\n",
    "        provider: \"ollama\", \"openai\", or \"voyage\"\n",
    "        model: Model name\n",
    "        chunk_size: Chunk size (256, 512, 1024, 2048, 4096)\n",
    "        base_dir: Base directory for vector databases\n",
    "        collection_prefix: Prefix for collection names\n",
    "        \n",
    "    Returns:\n",
    "        Loaded ChromaDB vectorstore\n",
    "        \n",
    "    Example:\n",
    "        vectorstore = load_vectorstore(\"voyage\", \"voyage-finance-2\", 1024)\n",
    "    \"\"\"\n",
    "    # Construct paths\n",
    "    model_id = f\"{provider}_{model.replace('/', '_')}\"\n",
    "    db_path = os.path.join(base_dir, model_id)\n",
    "    collection_name = f\"{collection_prefix}{chunk_size}\"\n",
    "    \n",
    "    # Get embedding function\n",
    "    emb_fn = get_embedding_function(provider, model)\n",
    "    \n",
    "    # Load vectorstore\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=emb_fn,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "print(\"✓ Vector store loading function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4.4 Test Vector Store Loading\n",
    "# \n",
    "# Verify we can load a vector store and retrieve documents\n",
    "\n",
    "# %%\n",
    "def test_vectorstore_loading():\n",
    "    \"\"\"\n",
    "    Test loading a vector store and performing a sample retrieval.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TESTING VECTOR STORE LOADING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test with a common configuration\n",
    "    test_provider = \"voyage\"\n",
    "    test_model = \"voyage-finance-2\"\n",
    "    test_chunk_size = 1024\n",
    "    \n",
    "    print(f\"\\nTest configuration:\")\n",
    "    print(f\"  Provider: {test_provider}\")\n",
    "    print(f\"  Model: {test_model}\")\n",
    "    print(f\"  Chunk size: {test_chunk_size}\")\n",
    "    \n",
    "    try:\n",
    "        # Load vectorstore\n",
    "        print(\"\\nLoading vectorstore...\")\n",
    "        vectorstore = load_vectorstore(test_provider, test_model, test_chunk_size)\n",
    "        \n",
    "        # Check collection\n",
    "        doc_count = vectorstore._collection.count()\n",
    "        print(f\"✓ Vectorstore loaded\")\n",
    "        print(f\"  Documents in collection: {doc_count:,}\")\n",
    "        \n",
    "        # Test retrieval\n",
    "        print(\"\\nTesting retrieval...\")\n",
    "        test_query = \"What was the revenue in 2018?\"\n",
    "        results = vectorstore.similarity_search(test_query, k=3)\n",
    "        \n",
    "        print(f\"✓ Retrieved {len(results)} documents\")\n",
    "        \n",
    "        # Show sample result\n",
    "        print(\"\\nSample retrieved document:\")\n",
    "        sample_doc = results[0]\n",
    "        metadata = extract_metadata_from_retrieved_doc(sample_doc)\n",
    "        \n",
    "        print(f\"  Doc name: {metadata['doc_name']}\")\n",
    "        print(f\"  Page number: {metadata['page_number']}\")\n",
    "        print(f\"  Chunk text (first 150 chars): {metadata['chunk_text'][:150]}...\")\n",
    "        print(f\"  Chunk text length: {len(metadata['chunk_text'])} characters\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✓ VECTOR STORE TEST COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error during test: {e}\")\n",
    "        print(\"\\nPossible issues:\")\n",
    "        print(\"  1. Vector database doesn't exist for this configuration\")\n",
    "        print(\"  2. Path is incorrect\")\n",
    "        print(\"  3. ChromaDB version mismatch\")\n",
    "        print(f\"\\nExpected path: {VECTOR_DB_BASE_DIR}/{test_provider}_{test_model}/\")\n",
    "        return False\n",
    "\n",
    "# %%\n",
    "# Run test\n",
    "test_result = test_vectorstore_loading()\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ STEP 4 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"  ✓ Metadata extraction functions defined\")\n",
    "print(\"  ✓ Embedding function factory defined\")\n",
    "print(\"  ✓ Vector store loading function defined\")\n",
    "print(\"  ✓ Vector store loading tested successfully\")\n",
    "print(\"  ✓ Ready to perform retrievals with chunk text extraction\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0211bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 5: Helper Functions - Page-Based Evaluation\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.1 Page-Based Matching Function\n",
    "# \n",
    "# This function checks if a retrieved chunk matches evidence based on page numbers\n",
    "\n",
    "# %%\n",
    "def check_page_match(\n",
    "    retrieved_doc: Dict, \n",
    "    evidence_list: List[Dict],\n",
    "    chunk_size: int = 512,\n",
    "    use_page_tolerance: bool = True\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Check if retrieved document matches any evidence based on PAGE NUMBERS.\n",
    "    \n",
    "    Uses chunk-size-aware page tolerance:\n",
    "    - Larger chunks can span multiple pages\n",
    "    - Retrieved page must be BEFORE or AT evidence page (within tolerance)\n",
    "    - Retrieved page AFTER evidence page = no match\n",
    "    \n",
    "    Page tolerance (when use_page_tolerance=True):\n",
    "    - chunk_size <= 512: tolerance = 0 (exact match)\n",
    "    - chunk_size 513-1024: tolerance = 1\n",
    "    - chunk_size 1025-2048: tolerance = 2\n",
    "    - chunk_size > 2048: tolerance = 2\n",
    "    \n",
    "    Args:\n",
    "        retrieved_doc: Dict with 'doc_name' and 'page_number' (1-indexed)\n",
    "        evidence_list: List of evidence dicts (page_number is 1-indexed)\n",
    "        chunk_size: Chunk size for tolerance calculation\n",
    "        use_page_tolerance: If True, use tolerance; if False, exact match only\n",
    "        \n",
    "    Returns:\n",
    "        True if match found, False otherwise\n",
    "        \n",
    "    Example:\n",
    "        Evidence on page 50, chunk_size=1024, tolerance=1:\n",
    "        - Page 49: MATCH (within tolerance, before evidence)\n",
    "        - Page 50: MATCH (exact match)\n",
    "        - Page 51: NO MATCH (after evidence page)\n",
    "    \"\"\"\n",
    "    retrieved_doc_name = retrieved_doc['doc_name']\n",
    "    retrieved_page = retrieved_doc['page_number']\n",
    "    \n",
    "    # Calculate page tolerance based on chunk size\n",
    "    if use_page_tolerance:\n",
    "        if chunk_size <= 512:\n",
    "            page_tolerance = 0\n",
    "        elif chunk_size <= 1024:\n",
    "            page_tolerance = 1\n",
    "        elif chunk_size <= 2048:\n",
    "            page_tolerance = 2\n",
    "        else:\n",
    "            page_tolerance = 2\n",
    "    else:\n",
    "        page_tolerance = 0  # Exact match only\n",
    "    \n",
    "    # Check against all evidence items\n",
    "    for evidence in evidence_list:\n",
    "        evidence_doc_name = evidence['doc_name']\n",
    "        evidence_page = evidence['page_number']  # Already 1-indexed from evidence_lookup\n",
    "        \n",
    "        # Check document name match\n",
    "        if retrieved_doc_name != evidence_doc_name:\n",
    "            continue\n",
    "        \n",
    "        # Check page match with tolerance\n",
    "        # Only match if retrieved page is BEFORE or AT evidence page\n",
    "        if retrieved_page <= evidence_page <= retrieved_page + page_tolerance:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "print(\"✓ Page-based matching function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.2 Page-Based MRR Calculation\n",
    "# \n",
    "# Calculate Mean Reciprocal Rank based on page matching\n",
    "\n",
    "# %%\n",
    "def calculate_page_mrr_for_query(\n",
    "    retrieved_docs: List[Dict], \n",
    "    evidence_list: List[Dict],\n",
    "    chunk_size: int = 512,\n",
    "    use_page_tolerance: bool = True\n",
    ") -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    Calculate PAGE-BASED MRR for a single query.\n",
    "    \n",
    "    Finds the rank of the first retrieved document that matches\n",
    "    any evidence based on page numbers.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs: List of retrieved docs with 'doc_name', 'page_number'\n",
    "        evidence_list: List of evidence items from evidence_lookup\n",
    "        chunk_size: Chunk size for tolerance calculation\n",
    "        use_page_tolerance: If True, use chunk-size-aware tolerance\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (mrr_score, rank):\n",
    "        - mrr_score: 1/rank if found, 0 if not found\n",
    "        - rank: Position of first match (1-indexed), -1 if not found\n",
    "        \n",
    "    Example:\n",
    "        First match at position 3: mrr_score = 1/3 = 0.333, rank = 3\n",
    "        No match found: mrr_score = 0.0, rank = -1\n",
    "    \"\"\"\n",
    "    for rank, retrieved_doc in enumerate(retrieved_docs, start=1):\n",
    "        if check_page_match(retrieved_doc, evidence_list, chunk_size, use_page_tolerance):\n",
    "            mrr_score = 1.0 / rank\n",
    "            return mrr_score, rank\n",
    "    \n",
    "    # No match found\n",
    "    return 0.0, -1\n",
    "\n",
    "print(\"✓ Page-based MRR calculation defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.3 Page-Based Recall, Precision, and F1\n",
    "# \n",
    "# NEW: Calculate precision, recall, and F1 based on page matching\n",
    "\n",
    "# %%\n",
    "def calculate_page_metrics_for_query(\n",
    "    retrieved_docs: List[Dict],\n",
    "    evidence_list: List[Dict],\n",
    "    chunk_size: int = 512,\n",
    "    use_page_tolerance: bool = True\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculate PAGE-BASED Recall, Precision, and F1 for a single query.\n",
    "    \n",
    "    Recall: What proportion of evidence pages were found in retrieved chunks?\n",
    "        recall = (# evidence items matched) / (# total evidence items)\n",
    "    \n",
    "    Precision: What proportion of retrieved chunks matched evidence?\n",
    "        precision = (# retrieved chunks matching evidence) / (# total retrieved chunks)\n",
    "    \n",
    "    F1: Harmonic mean of precision and recall\n",
    "        f1 = 2 × (precision × recall) / (precision + recall)\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs: List of retrieved docs with 'doc_name', 'page_number'\n",
    "        evidence_list: List of evidence items from evidence_lookup\n",
    "        chunk_size: Chunk size for tolerance calculation\n",
    "        use_page_tolerance: If True, use chunk-size-aware tolerance\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (recall, precision, f1)\n",
    "        \n",
    "    Example:\n",
    "        Evidence items: 2 (pages 50, 75)\n",
    "        Retrieved: 20 chunks\n",
    "        Matches: Found page 50 in 2 chunks, page 75 in 1 chunk\n",
    "        \n",
    "        Evidence matched: {page 50, page 75} = 2 unique evidence\n",
    "        Chunks matching: 3 chunks matched at least one evidence\n",
    "        \n",
    "        Recall = 2/2 = 1.0 (found all evidence)\n",
    "        Precision = 3/20 = 0.15 (3 out of 20 chunks matched)\n",
    "        F1 = 2 × (1.0 × 0.15) / (1.0 + 0.15) = 0.26\n",
    "    \"\"\"\n",
    "    if len(evidence_list) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    if len(retrieved_docs) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    \n",
    "    # Track which evidence items were found\n",
    "    evidence_found = set()  # Set of evidence indices that were matched\n",
    "    \n",
    "    # Track which retrieved chunks matched at least one evidence\n",
    "    chunks_matching = 0\n",
    "    \n",
    "    # Check each retrieved chunk\n",
    "    for retrieved_doc in retrieved_docs:\n",
    "        chunk_matched_any_evidence = False\n",
    "        \n",
    "        # Check against each evidence item\n",
    "        for evidence_idx, evidence in enumerate(evidence_list):\n",
    "            # Create single-item list for check_page_match\n",
    "            if check_page_match(retrieved_doc, [evidence], chunk_size, use_page_tolerance):\n",
    "                evidence_found.add(evidence_idx)\n",
    "                chunk_matched_any_evidence = True\n",
    "        \n",
    "        if chunk_matched_any_evidence:\n",
    "            chunks_matching += 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    recall = len(evidence_found) / len(evidence_list)\n",
    "    precision = chunks_matching / len(retrieved_docs)\n",
    "    \n",
    "    # Calculate F1\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    \n",
    "    return recall, precision, f1\n",
    "\n",
    "print(\"✓ Page-based metrics (recall, precision, F1) defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5.4 Test Page-Based Evaluation Functions\n",
    "# \n",
    "# Verify all page-based metrics work correctly\n",
    "\n",
    "# %%\n",
    "def test_page_based_evaluation():\n",
    "    \"\"\"\n",
    "    Test page-based evaluation functions with sample data.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TESTING PAGE-BASED EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create sample evidence (using 1-indexed pages)\n",
    "    evidence_list = [\n",
    "        {'doc_name': 'TEST_DOC', 'page_number': 50},\n",
    "        {'doc_name': 'TEST_DOC', 'page_number': 75}\n",
    "    ]\n",
    "    \n",
    "    # Create sample retrieved documents\n",
    "    retrieved_docs = [\n",
    "        {'doc_name': 'OTHER_DOC', 'page_number': 10},  # No match - wrong doc\n",
    "        {'doc_name': 'TEST_DOC', 'page_number': 50},   # MATCH - exact evidence page 50\n",
    "        {'doc_name': 'TEST_DOC', 'page_number': 49},   # MATCH - within tolerance of page 50\n",
    "        {'doc_name': 'TEST_DOC', 'page_number': 30},   # No match - not near evidence\n",
    "        {'doc_name': 'TEST_DOC', 'page_number': 75},   # MATCH - exact evidence page 75\n",
    "    ]\n",
    "    \n",
    "    chunk_size = 1024  # tolerance = 1\n",
    "    \n",
    "    print(\"\\nTest setup:\")\n",
    "    print(f\"  Evidence pages: [50, 75]\")\n",
    "    print(f\"  Retrieved pages: [10 (OTHER_DOC), 50, 49, 30, 75]\")\n",
    "    print(f\"  Chunk size: {chunk_size} (tolerance = 1)\")\n",
    "    \n",
    "    # Test MRR\n",
    "    print(\"\\n--- Page-Based MRR ---\")\n",
    "    mrr_score, rank = calculate_page_mrr_for_query(\n",
    "        retrieved_docs, evidence_list, chunk_size, use_page_tolerance=True\n",
    "    )\n",
    "    print(f\"  First match at rank: {rank}\")\n",
    "    print(f\"  MRR score: {mrr_score:.4f}\")\n",
    "    print(f\"  Expected: rank=2 (second doc matches page 50), MRR=0.5000\")\n",
    "    \n",
    "    # Test Recall, Precision, F1\n",
    "    print(\"\\n--- Page-Based Recall, Precision, F1 ---\")\n",
    "    recall, precision, f1 = calculate_page_metrics_for_query(\n",
    "        retrieved_docs, evidence_list, chunk_size, use_page_tolerance=True\n",
    "    )\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  F1: {f1:.4f}\")\n",
    "    \n",
    "    print(\"\\n  Expected calculations:\")\n",
    "    print(\"    Evidence found: {page 50, page 75} = 2/2 evidence items\")\n",
    "    print(\"    Chunks matching: 3 chunks (pages 50, 49, 75) matched evidence\")\n",
    "    print(\"    Recall = 2/2 = 1.0000\")\n",
    "    print(\"    Precision = 3/5 = 0.6000\")\n",
    "    print(\"    F1 = 2 × (1.0 × 0.6) / (1.0 + 0.6) = 0.7500\")\n",
    "    \n",
    "    # Verify results\n",
    "    print(\"\\n--- Verification ---\")\n",
    "    if rank == 2 and abs(mrr_score - 0.5) < 0.001:\n",
    "        print(\"  ✓ MRR calculation correct\")\n",
    "    else:\n",
    "        print(\"  ✗ MRR calculation incorrect\")\n",
    "    \n",
    "    if abs(recall - 1.0) < 0.001 and abs(precision - 0.6) < 0.001 and abs(f1 - 0.75) < 0.001:\n",
    "        print(\"  ✓ Recall, Precision, F1 calculations correct\")\n",
    "    else:\n",
    "        print(\"  ✗ Metrics calculation incorrect\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ PAGE-BASED EVALUATION TEST COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# %%\n",
    "# Run test\n",
    "test_page_based = test_page_based_evaluation()\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ STEP 5 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"  ✓ Page-based matching function defined\")\n",
    "print(\"  ✓ Page-based MRR calculation defined\")\n",
    "print(\"  ✓ Page-based Recall, Precision, F1 calculation defined\")\n",
    "print(\"  ✓ All page-based functions tested successfully\")\n",
    "print(\"  ✓ Ready to implement text-based evaluation\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62267f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 6: Helper Functions - Text-Based Evaluation\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.1 Chunk Text Preview Formatting\n",
    "# \n",
    "# Format chunk text as \"first N chars...last N chars\" for JSON storage\n",
    "\n",
    "# %%\n",
    "def format_chunk_text_preview(\n",
    "    text: str,\n",
    "    prefix_chars: int = CHUNK_TEXT_PREFIX_CHARS,\n",
    "    suffix_chars: int = CHUNK_TEXT_SUFFIX_CHARS\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Format chunk text as abbreviated preview for JSON storage.\n",
    "    \n",
    "    Format: \"first N characters...last N characters\"\n",
    "    \n",
    "    This keeps JSON files manageable while providing enough context\n",
    "    to manually verify matches.\n",
    "    \n",
    "    Args:\n",
    "        text: Full chunk text\n",
    "        prefix_chars: Number of characters from start\n",
    "        suffix_chars: Number of characters from end\n",
    "        \n",
    "    Returns:\n",
    "        Formatted preview string\n",
    "        \n",
    "    Examples:\n",
    "        Short text (< prefix + suffix): Returns full text\n",
    "        Long text: \"Capital expenditures were $1,577...in fiscal year 2018.\"\n",
    "    \"\"\"\n",
    "    if len(text) <= prefix_chars + suffix_chars:\n",
    "        # Text is short enough, return as-is\n",
    "        return text\n",
    "    \n",
    "    # Extract prefix and suffix\n",
    "    prefix = text[:prefix_chars]\n",
    "    suffix = text[-suffix_chars:]\n",
    "    \n",
    "    # Format with ellipsis\n",
    "    return f\"{prefix}...{suffix}\"\n",
    "\n",
    "print(\"✓ Chunk text preview formatting defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.2 Cosine Similarity Calculation\n",
    "# \n",
    "# Calculate cosine similarity between chunk and evidence embeddings\n",
    "\n",
    "# %%\n",
    "def compute_cosine_similarity(\n",
    "    chunk_embedding: np.ndarray,\n",
    "    evidence_embedding: np.ndarray\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two embeddings.\n",
    "    \n",
    "    Cosine similarity ranges from -1 to 1:\n",
    "    - 1.0: Identical/very similar\n",
    "    - 0.7-0.9: Strong similarity\n",
    "    - 0.5-0.7: Moderate similarity\n",
    "    - 0.0-0.5: Weak/no similarity\n",
    "    - Negative: Opposite meaning (rare in practice)\n",
    "    \n",
    "    Args:\n",
    "        chunk_embedding: Embedding vector for retrieved chunk (384-dim)\n",
    "        evidence_embedding: Embedding vector for evidence (384-dim)\n",
    "        \n",
    "    Returns:\n",
    "        Cosine similarity score (float)\n",
    "        \n",
    "    Note:\n",
    "        sklearn's cosine_similarity expects 2D arrays, so we reshape\n",
    "    \"\"\"\n",
    "    # Reshape to 2D arrays: (1, 384)\n",
    "    chunk_emb_2d = chunk_embedding.reshape(1, -1)\n",
    "    evidence_emb_2d = evidence_embedding.reshape(1, -1)\n",
    "    \n",
    "    # Calculate similarity\n",
    "    similarity = cosine_similarity(chunk_emb_2d, evidence_emb_2d)[0][0]\n",
    "    \n",
    "    return float(similarity)\n",
    "\n",
    "print(\"✓ Cosine similarity calculation defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.3 Calculate Text Similarities for Retrieved Chunk\n",
    "# \n",
    "# For each retrieved chunk, calculate similarity with ALL evidence items\n",
    "\n",
    "# %%\n",
    "def calculate_text_similarities_for_chunk(\n",
    "    chunk_text: str,\n",
    "    evidence_items: List[Dict],\n",
    "    sbert_model: SentenceTransformer\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between a chunk and all evidence items.\n",
    "    \n",
    "    Args:\n",
    "        chunk_text: Text content of retrieved chunk\n",
    "        evidence_items: List of evidence items (each has 'embedding', 'doc_name', 'page_number')\n",
    "        sbert_model: Sentence-BERT model for encoding chunk\n",
    "        \n",
    "    Returns:\n",
    "        List of similarity results:\n",
    "        [\n",
    "            {\n",
    "                'evidence_index': 0,\n",
    "                'evidence_doc': 'DOC_NAME',\n",
    "                'evidence_page': 60,\n",
    "                'cosine_similarity': 0.7823\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "        \n",
    "    Note:\n",
    "        Evidence embeddings are pre-computed, so we only encode the chunk once\n",
    "    \"\"\"\n",
    "    # Encode chunk text\n",
    "    chunk_embedding = sbert_model.encode(chunk_text, convert_to_numpy=True)\n",
    "    \n",
    "    # Calculate similarity with each evidence\n",
    "    similarities = []\n",
    "    \n",
    "    for evidence_idx, evidence in enumerate(evidence_items):\n",
    "        similarity_score = compute_cosine_similarity(\n",
    "            chunk_embedding,\n",
    "            evidence['embedding']\n",
    "        )\n",
    "        \n",
    "        similarities.append({\n",
    "            'evidence_index': evidence_idx,\n",
    "            'evidence_doc': evidence['doc_name'],\n",
    "            'evidence_page': evidence['page_number'],\n",
    "            'cosine_similarity': similarity_score\n",
    "        })\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "print(\"✓ Text similarities calculation for chunk defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.4 Text-Based Metrics Calculation\n",
    "# \n",
    "# Calculate text-based MRR, Recall, Precision, and F1\n",
    "\n",
    "# %%\n",
    "def calculate_text_metrics_for_query(\n",
    "    retrieved_docs: List[Dict],\n",
    "    evidence_items: List[Dict],\n",
    "    sbert_model: SentenceTransformer,\n",
    "    threshold: float = TEXT_SIMILARITY_THRESHOLD\n",
    ") -> Tuple[float, int, float, float, float, List[List[Dict]]]:\n",
    "    \"\"\"\n",
    "    Calculate TEXT-BASED metrics for a single query.\n",
    "    \n",
    "    For each retrieved chunk:\n",
    "    1. Encode chunk text\n",
    "    2. Calculate similarity with all evidence\n",
    "    3. Determine if chunk matches (max_similarity >= threshold)\n",
    "    \n",
    "    Metrics:\n",
    "    - Text MRR: Rank of first chunk where max(similarities) >= threshold\n",
    "    - Text Recall: # evidence matched / # total evidence\n",
    "    - Text Precision: # chunks matching / # total chunks\n",
    "    - Text F1: Harmonic mean of precision and recall\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs: List of retrieved docs with 'chunk_text'\n",
    "        evidence_items: List of evidence items with 'embedding'\n",
    "        sbert_model: Sentence-BERT model for encoding chunks\n",
    "        threshold: Similarity threshold for matching (default: 0.7)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (text_mrr, text_rank, text_recall, text_precision, text_f1, all_similarities)\n",
    "        - text_mrr: MRR score (0.0 if no match)\n",
    "        - text_rank: Rank of first match (-1 if no match)\n",
    "        - text_recall: Proportion of evidence found\n",
    "        - text_precision: Proportion of chunks matching\n",
    "        - text_f1: F1 score\n",
    "        - all_similarities: List of similarity lists for each chunk (for JSON storage)\n",
    "        \n",
    "    Example:\n",
    "        Evidence: 2 items\n",
    "        Retrieved: 20 chunks\n",
    "        Chunk 5 has max_similarity=0.82 with evidence[0] (FIRST MATCH)\n",
    "        Chunk 12 has max_similarity=0.75 with evidence[1]\n",
    "        \n",
    "        text_mrr = 1/5 = 0.2\n",
    "        text_rank = 5\n",
    "        evidence_matched = {evidence[0], evidence[1]} = 2\n",
    "        chunks_matching = 2\n",
    "        text_recall = 2/2 = 1.0\n",
    "        text_precision = 2/20 = 0.1\n",
    "        text_f1 = 2 × (1.0 × 0.1) / (1.0 + 0.1) = 0.18\n",
    "    \"\"\"\n",
    "    if len(evidence_items) == 0 or len(retrieved_docs) == 0:\n",
    "        return 0.0, -1, 0.0, 0.0, 0.0, []\n",
    "    \n",
    "    # Track results\n",
    "    all_similarities = []  # Store all similarities for JSON\n",
    "    text_mrr = 0.0\n",
    "    text_rank = -1\n",
    "    evidence_found = set()  # Set of evidence indices matched\n",
    "    chunks_matching = 0\n",
    "    \n",
    "    # Process each retrieved chunk\n",
    "    for rank, retrieved_doc in enumerate(retrieved_docs, start=1):\n",
    "        chunk_text = retrieved_doc.get('chunk_text', '')\n",
    "        \n",
    "        if not chunk_text:\n",
    "            # No text available\n",
    "            all_similarities.append([])\n",
    "            continue\n",
    "        \n",
    "        # Calculate similarities with all evidence\n",
    "        similarities = calculate_text_similarities_for_chunk(\n",
    "            chunk_text,\n",
    "            evidence_items,\n",
    "            sbert_model\n",
    "        )\n",
    "        \n",
    "        all_similarities.append(similarities)\n",
    "        \n",
    "        # Find maximum similarity\n",
    "        max_similarity = max([s['cosine_similarity'] for s in similarities])\n",
    "        \n",
    "        # Check if this chunk matches (above threshold)\n",
    "        chunk_matches_any_evidence = (max_similarity >= threshold)\n",
    "        \n",
    "        if chunk_matches_any_evidence:\n",
    "            chunks_matching += 1\n",
    "            \n",
    "            # Record which evidence items this chunk matched\n",
    "            for i, sim in enumerate(similarities):\n",
    "                if sim['cosine_similarity'] >= threshold:\n",
    "                    evidence_found.add(i)\n",
    "            \n",
    "            # Check for MRR (first match)\n",
    "            if text_mrr == 0.0:  # First match found\n",
    "                text_mrr = 1.0 / rank\n",
    "                text_rank = rank\n",
    "    \n",
    "    # Calculate recall and precision\n",
    "    text_recall = len(evidence_found) / len(evidence_items)\n",
    "    text_precision = chunks_matching / len(retrieved_docs)\n",
    "    \n",
    "    # Calculate F1\n",
    "    if text_precision + text_recall > 0:\n",
    "        text_f1 = 2 * (text_precision * text_recall) / (text_precision + text_recall)\n",
    "    else:\n",
    "        text_f1 = 0.0\n",
    "    \n",
    "    return text_mrr, text_rank, text_recall, text_precision, text_f1, all_similarities\n",
    "\n",
    "print(\"✓ Text-based metrics calculation defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6.5 Test Text-Based Evaluation Functions\n",
    "# \n",
    "# Verify text-based metrics work correctly with sample data\n",
    "\n",
    "# %%\n",
    "def test_text_based_evaluation():\n",
    "    \"\"\"\n",
    "    Test text-based evaluation functions with real FinanceBench data.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TESTING TEXT-BASED EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get sample query\n",
    "    sample_record = dataset[0]\n",
    "    query_id = sample_record['financebench_id']\n",
    "    \n",
    "    print(f\"\\nTest query: {query_id}\")\n",
    "    print(f\"  Question: {sample_record['question'][:100]}...\")\n",
    "    \n",
    "    # Get evidence for this query\n",
    "    evidence_items = evidence_lookup[query_id]\n",
    "    print(f\"\\n  Evidence items: {len(evidence_items)}\")\n",
    "    for i, ev in enumerate(evidence_items):\n",
    "        print(f\"    {i+1}. {ev['doc_name']}, page {ev['page_number']}\")\n",
    "        print(f\"       Text (first 80 chars): {ev['evidence_text'][:80]}...\")\n",
    "    \n",
    "    # Create sample retrieved chunks\n",
    "    # Chunk 1: Contains exact evidence text (should have very high similarity)\n",
    "    chunk1_text = evidence_items[0]['evidence_text']\n",
    "    \n",
    "    # Chunk 2: Paraphrased financial content (moderate similarity)\n",
    "    chunk2_text = \"The company's capital spending was approximately $1.6 billion for the fiscal year.\"\n",
    "    \n",
    "    # Chunk 3: Different financial topic (low similarity)\n",
    "    chunk3_text = \"Revenue increased by 8% year-over-year driven by strong product sales.\"\n",
    "    \n",
    "    # Chunk 4: Unrelated content (very low similarity)\n",
    "    chunk4_text = \"The weather forecast predicts sunny skies for the weekend.\"\n",
    "    \n",
    "    retrieved_docs = [\n",
    "        {'chunk_text': chunk1_text},\n",
    "        {'chunk_text': chunk2_text},\n",
    "        {'chunk_text': chunk3_text},\n",
    "        {'chunk_text': chunk4_text}\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n  Retrieved chunks: 4\")\n",
    "    print(\"    1. Exact evidence text\")\n",
    "    print(\"    2. Paraphrased financial content\")\n",
    "    print(\"    3. Different financial topic\")\n",
    "    print(\"    4. Unrelated content\")\n",
    "    \n",
    "    # Calculate text-based metrics\n",
    "    print(f\"\\n  Calculating similarities with threshold={TEXT_SIMILARITY_THRESHOLD}...\")\n",
    "    \n",
    "    text_mrr, text_rank, text_recall, text_precision, text_f1, all_similarities = \\\n",
    "        calculate_text_metrics_for_query(\n",
    "            retrieved_docs,\n",
    "            evidence_items,\n",
    "            sbert_model,\n",
    "            threshold=TEXT_SIMILARITY_THRESHOLD\n",
    "        )\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n--- Text-Based Metrics ---\")\n",
    "    print(f\"  Text MRR: {text_mrr:.4f}\")\n",
    "    print(f\"  Text Rank: {text_rank}\")\n",
    "    print(f\"  Text Recall: {text_recall:.4f}\")\n",
    "    print(f\"  Text Precision: {text_precision:.4f}\")\n",
    "    print(f\"  Text F1: {text_f1:.4f}\")\n",
    "    \n",
    "    # Show similarities for each chunk\n",
    "    print(\"\\n--- Chunk Similarities ---\")\n",
    "    for i, (chunk, similarities) in enumerate(zip(retrieved_docs, all_similarities), start=1):\n",
    "        print(f\"\\n  Chunk {i}:\")\n",
    "        print(f\"    Text (first 60 chars): {chunk['chunk_text'][:60]}...\")\n",
    "        for sim in similarities:\n",
    "            match_status = \"✓ MATCH\" if sim['cosine_similarity'] >= TEXT_SIMILARITY_THRESHOLD else \"✗ NO MATCH\"\n",
    "            print(f\"    Evidence {sim['evidence_index']}: {sim['cosine_similarity']:.4f} {match_status}\")\n",
    "    \n",
    "    # Expected behavior\n",
    "    print(\"\\n--- Expected Behavior ---\")\n",
    "    print(\"  Chunk 1 (exact evidence): Should have similarity ~0.99, MATCH\")\n",
    "    print(\"  Chunk 2 (paraphrased): Should have similarity ~0.7-0.8, likely MATCH\")\n",
    "    print(\"  Chunk 3 (different topic): Should have similarity ~0.3-0.5, NO MATCH\")\n",
    "    print(\"  Chunk 4 (unrelated): Should have similarity ~0.1-0.2, NO MATCH\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ TEXT-BASED EVALUATION TEST COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# %%\n",
    "# Run test\n",
    "test_text_based = test_text_based_evaluation()\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ STEP 6 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"  ✓ Chunk text preview formatting defined\")\n",
    "print(\"  ✓ Cosine similarity calculation defined\")\n",
    "print(\"  ✓ Text similarities for chunks defined\")\n",
    "print(\"  ✓ Text-based MRR, Recall, Precision, F1 calculation defined\")\n",
    "print(\"  ✓ All text-based functions tested with real data\")\n",
    "print(f\"  ✓ Similarity threshold: {TEXT_SIMILARITY_THRESHOLD}\")\n",
    "print(\"  ✓ Ready for retrieval functions\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 7: Retrieval Functions\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7.1 Global Retrieval\n",
    "# \n",
    "# Retrieve documents from the entire corpus (all documents)\n",
    "\n",
    "# %%\n",
    "def retrieve_global(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve documents globally (search all documents in the corpus).\n",
    "    \n",
    "    This mode searches across all 84 documents in FinanceBench.\n",
    "    Use case: Testing if the system can identify the correct document\n",
    "    among many documents.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: Loaded ChromaDB vectorstore\n",
    "        query: Query text\n",
    "        k: Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        List of retrieved documents with metadata:\n",
    "        [\n",
    "            {\n",
    "                'doc_name': 'DOC_NAME',\n",
    "                'page_number': 60,\n",
    "                'rank': 1,\n",
    "                'chunk_text': 'Full chunk text...'\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "        \n",
    "    Note:\n",
    "        Results are ordered by similarity score (most similar first)\n",
    "    \"\"\"\n",
    "    # Perform similarity search\n",
    "    results = vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "    # Extract metadata and add rank\n",
    "    retrieved = []\n",
    "    for rank, doc in enumerate(results, start=1):\n",
    "        metadata = extract_metadata_from_retrieved_doc(doc)\n",
    "        metadata['rank'] = rank\n",
    "        retrieved.append(metadata)\n",
    "    \n",
    "    return retrieved\n",
    "\n",
    "print(\"✓ Global retrieval function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7.2 Single-Document Retrieval\n",
    "# \n",
    "# Retrieve documents filtered to a specific target document\n",
    "\n",
    "# %%\n",
    "def retrieve_single_doc(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    target_doc_name: str,\n",
    "    k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve documents filtered to a single target document.\n",
    "    \n",
    "    This mode assumes we already know which document contains the answer\n",
    "    and only searches within that document.\n",
    "    Use case: Testing passage retrieval accuracy when document is known.\n",
    "    \n",
    "    Implementation:\n",
    "        ChromaDB doesn't support substring matching in filters, so we:\n",
    "        1. Retrieve more documents (k × 10)\n",
    "        2. Filter to target document\n",
    "        3. Return top k from filtered results\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: Loaded ChromaDB vectorstore\n",
    "        query: Query text\n",
    "        target_doc_name: Target document name (e.g., \"3M_2018_10K\")\n",
    "        k: Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        List of retrieved documents from target document:\n",
    "        [\n",
    "            {\n",
    "                'doc_name': '3M_2018_10K',\n",
    "                'page_number': 47,\n",
    "                'rank': 1,\n",
    "                'chunk_text': 'Full chunk text...'\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "        \n",
    "    Note:\n",
    "        If target document has fewer than k chunks, returns all available chunks\n",
    "    \"\"\"\n",
    "    # Retrieve more documents to ensure we get enough from target doc\n",
    "    # Factor of 10 is usually sufficient\n",
    "    retrieve_count = k * 10\n",
    "    results = vectorstore.similarity_search(query, k=retrieve_count)\n",
    "    \n",
    "    # Filter to target document and extract metadata\n",
    "    filtered = []\n",
    "    for doc in results:\n",
    "        metadata = extract_metadata_from_retrieved_doc(doc)\n",
    "        if metadata['doc_name'] == target_doc_name:\n",
    "            filtered.append(metadata)\n",
    "            # Stop once we have enough\n",
    "            if len(filtered) >= k:\n",
    "                break\n",
    "    \n",
    "    # Take top k from filtered results\n",
    "    top_k_filtered = filtered[:k]\n",
    "    \n",
    "    # Add rank\n",
    "    for rank, doc_metadata in enumerate(top_k_filtered, start=1):\n",
    "        doc_metadata['rank'] = rank\n",
    "    \n",
    "    return top_k_filtered\n",
    "\n",
    "print(\"✓ Single-document retrieval function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7.3 Test Retrieval Functions\n",
    "# \n",
    "# Verify both retrieval modes work correctly\n",
    "\n",
    "# %%\n",
    "def test_retrieval_functions():\n",
    "    \"\"\"\n",
    "    Test both global and single-document retrieval.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TESTING RETRIEVAL FUNCTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test configuration\n",
    "    test_provider = \"voyage\"\n",
    "    test_model = \"voyage-finance-2\"\n",
    "    test_chunk_size = 1024\n",
    "    test_k = 5\n",
    "    \n",
    "    print(f\"\\nTest configuration:\")\n",
    "    print(f\"  Provider: {test_provider}\")\n",
    "    print(f\"  Model: {test_model}\")\n",
    "    print(f\"  Chunk size: {test_chunk_size}\")\n",
    "    print(f\"  K: {test_k}\")\n",
    "    \n",
    "    # Load vectorstore\n",
    "    print(\"\\nLoading vectorstore...\")\n",
    "    vectorstore = load_vectorstore(test_provider, test_model, test_chunk_size)\n",
    "    doc_count = vectorstore._collection.count()\n",
    "    print(f\"✓ Loaded ({doc_count:,} documents)\")\n",
    "    \n",
    "    # Test query\n",
    "    test_query = \"What was the capital expenditure in 2018?\"\n",
    "    print(f\"\\nTest query: {test_query}\")\n",
    "    \n",
    "    # Test 1: Global retrieval\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"TEST 1: Global Retrieval\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    global_results = retrieve_global(vectorstore, test_query, test_k)\n",
    "    \n",
    "    print(f\"✓ Retrieved {len(global_results)} documents\")\n",
    "    print(\"\\nTop 3 results:\")\n",
    "    for i, result in enumerate(global_results[:3], start=1):\n",
    "        print(f\"\\n  {i}. Rank {result['rank']}\")\n",
    "        print(f\"     Doc: {result['doc_name']}\")\n",
    "        print(f\"     Page: {result['page_number']}\")\n",
    "        print(f\"     Text (first 100 chars): {result['chunk_text'][:100]}...\")\n",
    "        print(f\"     Text length: {len(result['chunk_text'])} chars\")\n",
    "    \n",
    "    # Test 2: Single-document retrieval\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"TEST 2: Single-Document Retrieval\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    target_doc = \"3M_2018_10K\"\n",
    "    print(f\"Target document: {target_doc}\")\n",
    "    \n",
    "    singledoc_results = retrieve_single_doc(vectorstore, test_query, target_doc, test_k)\n",
    "    \n",
    "    print(f\"✓ Retrieved {len(singledoc_results)} documents from target\")\n",
    "    print(\"\\nTop 3 results:\")\n",
    "    for i, result in enumerate(singledoc_results[:3], start=1):\n",
    "        print(f\"\\n  {i}. Rank {result['rank']}\")\n",
    "        print(f\"     Doc: {result['doc_name']}\")\n",
    "        print(f\"     Page: {result['page_number']}\")\n",
    "        print(f\"     Text (first 100 chars): {result['chunk_text'][:100]}...\")\n",
    "        print(f\"     Text length: {len(result['chunk_text'])} chars\")\n",
    "    \n",
    "    # Verify all results are from target document\n",
    "    all_from_target = all(r['doc_name'] == target_doc for r in singledoc_results)\n",
    "    if all_from_target:\n",
    "        print(f\"\\n✓ All results correctly filtered to {target_doc}\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Some results not from target document!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ RETRIEVAL FUNCTIONS TEST COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# %%\n",
    "# Run test\n",
    "test_retrieval = test_retrieval_functions()\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ STEP 7 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"  ✓ Global retrieval function defined\")\n",
    "print(\"  ✓ Single-document retrieval function defined\")\n",
    "print(\"  ✓ Both retrieval modes tested successfully\")\n",
    "print(\"  ✓ Chunk text extraction verified\")\n",
    "print(\"  ✓ Ready for main evaluation function\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf390a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 8: Main Evaluation Function\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8.1 File Management Functions\n",
    "# \n",
    "# Helper functions for saving and checking results\n",
    "\n",
    "# %%\n",
    "def get_output_filename(\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k: int,\n",
    "    mode: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate standardized output filename.\n",
    "    \n",
    "    Format: {provider}_{model}_chunk{size}_k{k}_{mode}.json\n",
    "    \n",
    "    Example:\n",
    "        voyage_voyage-finance-2_chunk1024_k20_global.json\n",
    "    \"\"\"\n",
    "    # Replace slashes in model name\n",
    "    model_safe = model.replace('/', '_')\n",
    "    filename = f\"{provider}_{model_safe}_chunk{chunk_size}_k{k}_{mode}.json\"\n",
    "    return filename\n",
    "\n",
    "\n",
    "def check_if_results_exist(\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k: int,\n",
    "    mode: str,\n",
    "    output_dir: str\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Check if results file already exists.\n",
    "    \n",
    "    Used to skip configurations that have already been evaluated.\n",
    "    \"\"\"\n",
    "    filename = get_output_filename(provider, model, chunk_size, k, mode)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    return os.path.exists(filepath)\n",
    "\n",
    "\n",
    "def save_results(\n",
    "    results: List[Dict],\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k: int,\n",
    "    mode: str,\n",
    "    output_dir: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Save evaluation results to JSON file.\n",
    "    \n",
    "    Args:\n",
    "        results: List of result dictionaries (queries + summary)\n",
    "        provider: Embedding provider\n",
    "        model: Model name\n",
    "        chunk_size: Chunk size\n",
    "        k: Number of retrieved documents\n",
    "        mode: \"global\" or \"singledoc\"\n",
    "        output_dir: Output directory\n",
    "    \"\"\"\n",
    "    filename = get_output_filename(provider, model, chunk_size, k, mode)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Saved: {filename}\")\n",
    "\n",
    "print(\"✓ File management functions defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8.2 Single Configuration Evaluation\n",
    "# \n",
    "# Evaluate a single configuration: (provider, model, chunk_size, k, mode)\n",
    "\n",
    "# %%\n",
    "def evaluate_single_configuration(\n",
    "    dataset,\n",
    "    evidence_lookup: Dict,\n",
    "    sbert_model: SentenceTransformer,\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k: int,\n",
    "    mode: str,\n",
    "    use_page_tolerance: bool = True,\n",
    "    text_similarity_threshold: float = TEXT_SIMILARITY_THRESHOLD,\n",
    "    output_dir: str = OUTPUT_DIR\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a single configuration with BOTH page-based AND text-based metrics.\n",
    "    \n",
    "    This is the main evaluation function that:\n",
    "    1. Loads the vector store\n",
    "    2. For each query:\n",
    "       - Retrieves documents\n",
    "       - Calculates PAGE-based metrics (MRR, Recall, Precision, F1)\n",
    "       - Calculates TEXT-based metrics (MRR, Recall, Precision, F1)\n",
    "       - Stores all results\n",
    "    3. Calculates average metrics\n",
    "    4. Saves results to JSON\n",
    "    \n",
    "    Args:\n",
    "        dataset: FinanceBench dataset\n",
    "        evidence_lookup: Pre-computed evidence embeddings\n",
    "        sbert_model: Sentence-BERT model for text similarity\n",
    "        provider: \"ollama\", \"openai\", or \"voyage\"\n",
    "        model: Model name\n",
    "        chunk_size: Chunk size\n",
    "        k: Number of documents to retrieve\n",
    "        mode: \"global\" or \"singledoc\"\n",
    "        use_page_tolerance: If True, use chunk-size-aware page tolerance\n",
    "        text_similarity_threshold: Threshold for text-based matching\n",
    "        output_dir: Output directory for results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with status and metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATING: {provider}/{model}\")\n",
    "    print(f\"  Chunk size: {chunk_size}\")\n",
    "    print(f\"  K: {k}\")\n",
    "    print(f\"  Mode: {mode}\")\n",
    "    print(f\"  Page tolerance: {'ENABLED' if use_page_tolerance else 'DISABLED'}\")\n",
    "    print(f\"  Text similarity threshold: {text_similarity_threshold}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Check if already exists\n",
    "    if check_if_results_exist(provider, model, chunk_size, k, mode, output_dir):\n",
    "        print(\"✓ Results already exist - SKIPPING\")\n",
    "        return {'status': 'skipped'}\n",
    "    \n",
    "    # Load vectorstore\n",
    "    print(\"\\nLoading vectorstore...\")\n",
    "    try:\n",
    "        vectorstore = load_vectorstore(provider, model, chunk_size)\n",
    "        doc_count = vectorstore._collection.count()\n",
    "        print(f\"✓ Loaded ({doc_count:,} documents)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load vectorstore: {e}\")\n",
    "        return {'status': 'failed', 'error': str(e)}\n",
    "    \n",
    "    # Initialize tracking lists\n",
    "    results = []\n",
    "    \n",
    "    # Page-based metrics\n",
    "    page_mrr_scores = []\n",
    "    page_recall_scores = []\n",
    "    page_precision_scores = []\n",
    "    page_f1_scores = []\n",
    "    \n",
    "    # Text-based metrics\n",
    "    text_mrr_scores = []\n",
    "    text_recall_scores = []\n",
    "    text_precision_scores = []\n",
    "    text_f1_scores = []\n",
    "    \n",
    "    # Process all queries\n",
    "    print(f\"\\nProcessing {len(dataset)} queries...\")\n",
    "    print(\"(This may take a while due to text similarity calculations...)\")\n",
    "    \n",
    "    for record in tqdm(dataset, desc=\"Queries\"):\n",
    "        query_id = record['financebench_id']\n",
    "        query = record['question']\n",
    "        doc_name = record['doc_name']\n",
    "        \n",
    "        # Get evidence for this query\n",
    "        evidence_items = evidence_lookup.get(query_id, [])\n",
    "        \n",
    "        if len(evidence_items) == 0:\n",
    "            # No evidence for this query, skip\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # ========================================\n",
    "            # STEP 1: RETRIEVE DOCUMENTS\n",
    "            # ========================================\n",
    "            if mode == \"global\":\n",
    "                retrieved_docs = retrieve_global(vectorstore, query, k)\n",
    "            elif mode == \"singledoc\":\n",
    "                retrieved_docs = retrieve_single_doc(vectorstore, query, doc_name, k)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown mode: {mode}\")\n",
    "            \n",
    "            # ========================================\n",
    "            # STEP 2: CALCULATE PAGE-BASED METRICS\n",
    "            # ========================================\n",
    "            \n",
    "            # Page-based MRR\n",
    "            page_mrr, page_rank = calculate_page_mrr_for_query(\n",
    "                retrieved_docs, evidence_items, chunk_size, use_page_tolerance\n",
    "            )\n",
    "            page_mrr_scores.append(page_mrr)\n",
    "            \n",
    "            # Page-based Recall, Precision, F1\n",
    "            page_recall, page_precision, page_f1 = calculate_page_metrics_for_query(\n",
    "                retrieved_docs, evidence_items, chunk_size, use_page_tolerance\n",
    "            )\n",
    "            page_recall_scores.append(page_recall)\n",
    "            page_precision_scores.append(page_precision)\n",
    "            page_f1_scores.append(page_f1)\n",
    "            \n",
    "            # ========================================\n",
    "            # STEP 3: CALCULATE TEXT-BASED METRICS\n",
    "            # ========================================\n",
    "            \n",
    "            # Show progress for text similarity calculations\n",
    "            # (This is the slowest part)\n",
    "            text_mrr, text_rank, text_recall, text_precision, text_f1, all_similarities = \\\n",
    "                calculate_text_metrics_for_query(\n",
    "                    retrieved_docs,\n",
    "                    evidence_items,\n",
    "                    sbert_model,\n",
    "                    threshold=text_similarity_threshold\n",
    "                )\n",
    "            \n",
    "            text_mrr_scores.append(text_mrr)\n",
    "            text_recall_scores.append(text_recall)\n",
    "            text_precision_scores.append(text_precision)\n",
    "            text_f1_scores.append(text_f1)\n",
    "            \n",
    "            # ========================================\n",
    "            # STEP 4: FORMAT RESULTS FOR JSON\n",
    "            # ========================================\n",
    "            \n",
    "            # Format expected evidence\n",
    "            expected_evidence = [\n",
    "                {\n",
    "                    'doc_name': ev['doc_name'],\n",
    "                    'page_number': ev['page_number'],\n",
    "                    'evidence_text': ev['evidence_text'][:200] + '...' if len(ev['evidence_text']) > 200 else ev['evidence_text']\n",
    "                }\n",
    "                for ev in evidence_items\n",
    "            ]\n",
    "            \n",
    "            # Format retrieved docs with text similarities\n",
    "            retrieved_docs_formatted = []\n",
    "            for i, doc in enumerate(retrieved_docs):\n",
    "                doc_formatted = {\n",
    "                    'doc_name': doc['doc_name'],\n",
    "                    'page_number': doc['page_number'],\n",
    "                    'rank': doc['rank'],\n",
    "                    'chunk_text': format_chunk_text_preview(doc['chunk_text']),\n",
    "                    'text_similarities': all_similarities[i] if i < len(all_similarities) else []\n",
    "                }\n",
    "                retrieved_docs_formatted.append(doc_formatted)\n",
    "            \n",
    "            # Store complete result\n",
    "            result = {\n",
    "                'query_id': query_id,\n",
    "                'query': query,\n",
    "                'expected_doc': doc_name,\n",
    "                'expected_evidence': expected_evidence,\n",
    "                'retrieved_docs': retrieved_docs_formatted,\n",
    "                \n",
    "                # Page-based metrics\n",
    "                'page_mrr_score': page_mrr,\n",
    "                'page_rank': page_rank,\n",
    "                'page_recall': page_recall,\n",
    "                'page_precision': page_precision,\n",
    "                'page_f1': page_f1,\n",
    "                \n",
    "                # Text-based metrics\n",
    "                'text_mrr_score': text_mrr,\n",
    "                'text_rank': text_rank,\n",
    "                'text_recall': text_recall,\n",
    "                'text_precision': text_precision,\n",
    "                'text_f1': text_f1\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Error processing query {query_id}: {e}\")\n",
    "            # Store error result\n",
    "            results.append({\n",
    "                'query_id': query_id,\n",
    "                'query': query,\n",
    "                'error': str(e),\n",
    "                'page_mrr_score': 0.0,\n",
    "                'page_rank': -1,\n",
    "                'page_recall': 0.0,\n",
    "                'page_precision': 0.0,\n",
    "                'page_f1': 0.0,\n",
    "                'text_mrr_score': 0.0,\n",
    "                'text_rank': -1,\n",
    "                'text_recall': 0.0,\n",
    "                'text_precision': 0.0,\n",
    "                'text_f1': 0.0\n",
    "            })\n",
    "            # Append zeros to tracking lists\n",
    "            page_mrr_scores.append(0.0)\n",
    "            page_recall_scores.append(0.0)\n",
    "            page_precision_scores.append(0.0)\n",
    "            page_f1_scores.append(0.0)\n",
    "            text_mrr_scores.append(0.0)\n",
    "            text_recall_scores.append(0.0)\n",
    "            text_precision_scores.append(0.0)\n",
    "            text_f1_scores.append(0.0)\n",
    "    \n",
    "    # ========================================\n",
    "    # CALCULATE AVERAGE METRICS\n",
    "    # ========================================\n",
    "    \n",
    "    avg_page_mrr = sum(page_mrr_scores) / len(page_mrr_scores) if page_mrr_scores else 0.0\n",
    "    avg_page_recall = sum(page_recall_scores) / len(page_recall_scores) if page_recall_scores else 0.0\n",
    "    avg_page_precision = sum(page_precision_scores) / len(page_precision_scores) if page_precision_scores else 0.0\n",
    "    avg_page_f1 = sum(page_f1_scores) / len(page_f1_scores) if page_f1_scores else 0.0\n",
    "    \n",
    "    avg_text_mrr = sum(text_mrr_scores) / len(text_mrr_scores) if text_mrr_scores else 0.0\n",
    "    avg_text_recall = sum(text_recall_scores) / len(text_recall_scores) if text_recall_scores else 0.0\n",
    "    avg_text_precision = sum(text_precision_scores) / len(text_precision_scores) if text_precision_scores else 0.0\n",
    "    avg_text_f1 = sum(text_f1_scores) / len(text_f1_scores) if text_f1_scores else 0.0\n",
    "    \n",
    "    # Add summary to results\n",
    "    results.append({\n",
    "        'summary': {\n",
    "            'provider': provider,\n",
    "            'model': model,\n",
    "            'chunk_size': chunk_size,\n",
    "            'k': k,\n",
    "            'mode': mode,\n",
    "            'use_page_tolerance': use_page_tolerance,\n",
    "            'text_similarity_threshold': text_similarity_threshold,\n",
    "            'total_queries': len(dataset),\n",
    "            \n",
    "            # Page-based averages\n",
    "            'average_page_mrr': avg_page_mrr,\n",
    "            'average_page_recall': avg_page_recall,\n",
    "            'average_page_precision': avg_page_precision,\n",
    "            'average_page_f1': avg_page_f1,\n",
    "            \n",
    "            # Text-based averages\n",
    "            'average_text_mrr': avg_text_mrr,\n",
    "            'average_text_recall': avg_text_recall,\n",
    "            'average_text_precision': avg_text_precision,\n",
    "            'average_text_f1': avg_text_f1\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Save results\n",
    "    save_results(results, provider, model, chunk_size, k, mode, output_dir)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"\\nPage-Based Metrics:\")\n",
    "    print(f\"  Average MRR:       {avg_page_mrr:.4f}\")\n",
    "    print(f\"  Average Recall:    {avg_page_recall:.4f}\")\n",
    "    print(f\"  Average Precision: {avg_page_precision:.4f}\")\n",
    "    print(f\"  Average F1:        {avg_page_f1:.4f}\")\n",
    "    print(\"\\nText-Based Metrics:\")\n",
    "    print(f\"  Average MRR:       {avg_text_mrr:.4f}\")\n",
    "    print(f\"  Average Recall:    {avg_text_recall:.4f}\")\n",
    "    print(f\"  Average Precision: {avg_text_precision:.4f}\")\n",
    "    print(f\"  Average F1:        {avg_text_f1:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return {\n",
    "        'status': 'completed',\n",
    "        'average_page_mrr': avg_page_mrr,\n",
    "        'average_page_recall': avg_page_recall,\n",
    "        'average_page_precision': avg_page_precision,\n",
    "        'average_page_f1': avg_page_f1,\n",
    "        'average_text_mrr': avg_text_mrr,\n",
    "        'average_text_recall': avg_text_recall,\n",
    "        'average_text_precision': avg_text_precision,\n",
    "        'average_text_f1': avg_text_f1,\n",
    "        'total_queries': len(dataset)\n",
    "    }\n",
    "\n",
    "print(\"✓ Single configuration evaluation function defined\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ STEP 8 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"  ✓ File management functions defined\")\n",
    "print(\"  ✓ Main evaluation function defined\")\n",
    "print(\"  ✓ Processes both page-based AND text-based metrics\")\n",
    "print(\"  ✓ Saves comprehensive results to JSON\")\n",
    "print(\"  ✓ Ready for batch evaluation\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252640ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 9: Batch Evaluation Function\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9.1 Batch Evaluation\n",
    "# \n",
    "# Evaluate multiple configurations automatically\n",
    "\n",
    "# %%\n",
    "def evaluate_multiple_configurations(\n",
    "    dataset,\n",
    "    evidence_lookup: Dict,\n",
    "    sbert_model: SentenceTransformer,\n",
    "    configurations: List[Dict],\n",
    "    k_values: List[int],\n",
    "    modes: List[str],\n",
    "    use_page_tolerance: bool = True,\n",
    "    text_similarity_threshold: float = TEXT_SIMILARITY_THRESHOLD,\n",
    "    output_dir: str = OUTPUT_DIR\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate multiple configurations in batch.\n",
    "    \n",
    "    This function iterates through all combinations of:\n",
    "    - Configurations (provider, model, chunk_sizes)\n",
    "    - K values (number of documents to retrieve)\n",
    "    - Modes (global, singledoc)\n",
    "    \n",
    "    And evaluates each combination using evaluate_single_configuration().\n",
    "    \n",
    "    Args:\n",
    "        dataset: FinanceBench dataset\n",
    "        evidence_lookup: Pre-computed evidence embeddings\n",
    "        sbert_model: Sentence-BERT model\n",
    "        configurations: List of {provider, model, chunk_sizes}\n",
    "        k_values: List of k values to test\n",
    "        modes: List of modes [\"global\", \"singledoc\"]\n",
    "        use_page_tolerance: If True, use chunk-size-aware tolerance\n",
    "        text_similarity_threshold: Threshold for text-based matching\n",
    "        output_dir: Output directory\n",
    "        \n",
    "    Returns:\n",
    "        Summary dictionary with all results\n",
    "        \n",
    "    Example configurations:\n",
    "        [\n",
    "            {\n",
    "                'provider': 'voyage',\n",
    "                'model': 'voyage-finance-2',\n",
    "                'chunk_sizes': [512, 1024, 2048]\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BATCH EVALUATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Configurations: {len(configurations)}\")\n",
    "    print(f\"K values: {k_values}\")\n",
    "    print(f\"Modes: {modes}\")\n",
    "    print(f\"Page tolerance: {'ENABLED' if use_page_tolerance else 'DISABLED'}\")\n",
    "    print(f\"Text similarity threshold: {text_similarity_threshold}\")\n",
    "    \n",
    "    # Calculate total runs\n",
    "    total_runs = 0\n",
    "    for config in configurations:\n",
    "        total_runs += len(config['chunk_sizes']) * len(k_values) * len(modes)\n",
    "    \n",
    "    print(f\"Total evaluation runs: {total_runs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Track results\n",
    "    all_results = []\n",
    "    completed = 0\n",
    "    skipped = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # Start time\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Iterate through all combinations\n",
    "    for config in configurations:\n",
    "        provider = config['provider']\n",
    "        model = config['model']\n",
    "        chunk_sizes = config['chunk_sizes']\n",
    "        \n",
    "        for chunk_size in chunk_sizes:\n",
    "            for k in k_values:\n",
    "                for mode in modes:\n",
    "                    print(f\"\\n{'#'*60}\")\n",
    "                    print(f\"CONFIGURATION {completed + skipped + failed + 1}/{total_runs}\")\n",
    "                    print(f\"{'#'*60}\")\n",
    "                    \n",
    "                    result = evaluate_single_configuration(\n",
    "                        dataset=dataset,\n",
    "                        evidence_lookup=evidence_lookup,\n",
    "                        sbert_model=sbert_model,\n",
    "                        provider=provider,\n",
    "                        model=model,\n",
    "                        chunk_size=chunk_size,\n",
    "                        k=k,\n",
    "                        mode=mode,\n",
    "                        use_page_tolerance=use_page_tolerance,\n",
    "                        text_similarity_threshold=text_similarity_threshold,\n",
    "                        output_dir=output_dir\n",
    "                    )\n",
    "                    \n",
    "                    all_results.append({\n",
    "                        'provider': provider,\n",
    "                        'model': model,\n",
    "                        'chunk_size': chunk_size,\n",
    "                        'k': k,\n",
    "                        'mode': mode,\n",
    "                        'result': result\n",
    "                    })\n",
    "                    \n",
    "                    if result['status'] == 'completed':\n",
    "                        completed += 1\n",
    "                    elif result['status'] == 'skipped':\n",
    "                        skipped += 1\n",
    "                    else:\n",
    "                        failed += 1\n",
    "    \n",
    "    # End time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BATCH EVALUATION SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total runs: {total_runs}\")\n",
    "    print(f\"Completed: {completed}\")\n",
    "    print(f\"Skipped: {skipped}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Total time: {elapsed_time/60:.2f} minutes\")\n",
    "    print(f\"Average time per run: {elapsed_time/total_runs:.2f} seconds\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return {\n",
    "        'total_runs': total_runs,\n",
    "        'completed': completed,\n",
    "        'skipped': skipped,\n",
    "        'failed': failed,\n",
    "        'elapsed_time': elapsed_time,\n",
    "        'results': all_results\n",
    "    }\n",
    "\n",
    "print(\"✓ Batch evaluation function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9.2 Results Analysis Helper\n",
    "# \n",
    "# Helper function to display results in a readable format\n",
    "\n",
    "# %%\n",
    "def display_batch_results(summary: Dict):\n",
    "    \"\"\"\n",
    "    Display batch evaluation results in a readable table format.\n",
    "    \n",
    "    Shows both page-based and text-based metrics for easy comparison.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED RESULTS - ALL CONFIGURATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Group results by status\n",
    "    completed_results = [r for r in summary['results'] if r['result']['status'] == 'completed']\n",
    "    skipped_results = [r for r in summary['results'] if r['result']['status'] == 'skipped']\n",
    "    failed_results = [r for r in summary['results'] if r['result']['status'] == 'failed']\n",
    "    \n",
    "    if completed_results:\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(f\"COMPLETED EVALUATIONS ({len(completed_results)})\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Table header\n",
    "        print(f\"\\n{'Config':<40} {'Page Metrics':<25} {'Text Metrics':<25}\")\n",
    "        print(f\"{'-'*40} {'-'*25} {'-'*25}\")\n",
    "        print(f\"{'Provider/Model/Chunk/K/Mode':<40} {'MRR':>6} {'Rec':>6} {'Prec':>6} {'F1':>6} {'MRR':>6} {'Rec':>6} {'Prec':>6} {'F1':>6}\")\n",
    "        print(\"-\"*90)\n",
    "        \n",
    "        # Sort by provider, model, chunk_size, k, mode\n",
    "        sorted_results = sorted(\n",
    "            completed_results,\n",
    "            key=lambda x: (x['provider'], x['model'], x['chunk_size'], x['k'], x['mode'])\n",
    "        )\n",
    "        \n",
    "        for r in sorted_results:\n",
    "            config_str = f\"{r['provider']}/{r['model']}/ch{r['chunk_size']}/k{r['k']}/{r['mode']}\"\n",
    "            result = r['result']\n",
    "            \n",
    "            # Page-based metrics\n",
    "            page_mrr = result['average_page_mrr']\n",
    "            page_rec = result['average_page_recall']\n",
    "            page_prec = result['average_page_precision']\n",
    "            page_f1 = result['average_page_f1']\n",
    "            \n",
    "            # Text-based metrics\n",
    "            text_mrr = result['average_text_mrr']\n",
    "            text_rec = result['average_text_recall']\n",
    "            text_prec = result['average_text_precision']\n",
    "            text_f1 = result['average_text_f1']\n",
    "            \n",
    "            print(f\"{config_str:<40} {page_mrr:>6.3f} {page_rec:>6.3f} {page_prec:>6.3f} {page_f1:>6.3f} {text_mrr:>6.3f} {text_rec:>6.3f} {text_prec:>6.3f} {text_f1:>6.3f}\")\n",
    "    \n",
    "    if skipped_results:\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(f\"SKIPPED EVALUATIONS ({len(skipped_results)})\")\n",
    "        print(\"-\"*80)\n",
    "        for r in skipped_results:\n",
    "            config_str = f\"{r['provider']}/{r['model']}/chunk{r['chunk_size']}/k{r['k']}/{r['mode']}\"\n",
    "            print(f\"  - {config_str}\")\n",
    "    \n",
    "    if failed_results:\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(f\"FAILED EVALUATIONS ({len(failed_results)})\")\n",
    "        print(\"-\"*80)\n",
    "        for r in failed_results:\n",
    "            config_str = f\"{r['provider']}/{r['model']}/chunk{r['chunk_size']}/k{r['k']}/{r['mode']}\"\n",
    "            error = r['result'].get('error', 'Unknown error')\n",
    "            print(f\"  - {config_str}: {error}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"✓ Results analysis helper defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 9.3 List Generated Files\n",
    "# \n",
    "# Helper to show all generated JSON files\n",
    "\n",
    "# %%\n",
    "def list_generated_files(output_dir: str = OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    List all generated JSON files with their sizes.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GENERATED FILES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    json_files = sorted(output_path.glob(\"*.json\"))\n",
    "    \n",
    "    print(f\"\\nTotal JSON files: {len(json_files)}\")\n",
    "    print(f\"Location: {output_dir}\\n\")\n",
    "    \n",
    "    if json_files:\n",
    "        # Calculate total size\n",
    "        total_size = sum(f.stat().st_size for f in json_files)\n",
    "        \n",
    "        print(f\"{'Filename':<60} {'Size':>10}\")\n",
    "        print(\"-\"*72)\n",
    "        \n",
    "        for filepath in json_files:\n",
    "            file_size = filepath.stat().st_size / 1024  # KB\n",
    "            print(f\"{filepath.name:<60} {file_size:>8.1f} KB\")\n",
    "        \n",
    "        print(\"-\"*72)\n",
    "        print(f\"{'TOTAL':<60} {total_size/1024:>8.1f} KB\")\n",
    "    else:\n",
    "        print(\"No JSON files found.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"✓ File listing helper defined\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ STEP 9 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"  ✓ Batch evaluation function defined\")\n",
    "print(\"  ✓ Results display helper defined\")\n",
    "print(\"  ✓ File listing helper defined\")\n",
    "print(\"  ✓ Ready for configuration and execution\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d17e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 10: Configuration and Execution\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10.1 Define Configurations to Test\n",
    "# \n",
    "# Specify which embedding models and chunk sizes to evaluate\n",
    "\n",
    "# %%\n",
    "# Define configurations to evaluate\n",
    "# Each configuration specifies: provider, model, and chunk sizes to test\n",
    "\n",
    "configurations = [\n",
    "    {\n",
    "        'provider': 'ollama',\n",
    "        'model': 'nomic-embed-text',\n",
    "        'chunk_sizes': [512, 1024]\n",
    "    },\n",
    "    {\n",
    "        'provider': 'ollama',\n",
    "        'model': 'bge-m3',\n",
    "        'chunk_sizes': [512, 1024]\n",
    "    },\n",
    "    # {\n",
    "    #     'provider': 'openai',\n",
    "    #     'model': 'text-embedding-3-small',\n",
    "    #     'chunk_sizes': [256, 512, 1024, 2048]\n",
    "    # },\n",
    "    # {\n",
    "    #     'provider': 'openai',\n",
    "    #     'model': 'text-embedding-3-large',\n",
    "    #     'chunk_sizes': [512, 1024]\n",
    "    # },\n",
    "    {\n",
    "        'provider': 'voyage',\n",
    "        'model': 'voyage-3-large',\n",
    "        'chunk_sizes': [512, 1024, 2048, 4096]\n",
    "    },\n",
    "    # {\n",
    "    #     'provider': 'voyage',\n",
    "    #     'model': 'voyage-finance-2',\n",
    "    #     'chunk_sizes': [512, 1024]\n",
    "    # },\n",
    "]\n",
    "\n",
    "print(\"✓ Configurations defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10.2 Define Evaluation Parameters\n",
    "\n",
    "# %%\n",
    "# K values to test (number of documents to retrieve)\n",
    "k_values = [20, 40, 60, 80]\n",
    "\n",
    "# Modes to test\n",
    "modes = ['global', 'singledoc']\n",
    "\n",
    "# Page tolerance setting\n",
    "# - True: Use chunk-size-aware page tolerance (lenient matching for large chunks)\n",
    "# - False: Exact page match only (strict evaluation)\n",
    "USE_PAGE_TOLERANCE = True\n",
    "\n",
    "# Text similarity threshold\n",
    "# - Chunks with cosine similarity >= this value are considered matches\n",
    "# - Higher = stricter matching, Lower = more lenient matching\n",
    "TEXT_SIMILARITY_THRESHOLD = 0.7\n",
    "\n",
    "print(\"✓ Evaluation parameters defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10.3 Display Evaluation Plan\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION PLAN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDataset: FinanceBench ({len(dataset)} queries)\")\n",
    "print(f\"Evidence items: {len(all_evidence)}\")\n",
    "print(f\"Pre-computed embeddings: {evidence_embeddings.shape[0]}\")\n",
    "\n",
    "print(f\"\\nEvaluation Settings:\")\n",
    "print(f\"  K values: {k_values}\")\n",
    "print(f\"  Modes: {modes}\")\n",
    "print(f\"  Page tolerance: {'ENABLED' if USE_PAGE_TOLERANCE else 'DISABLED'}\")\n",
    "print(f\"  Text similarity threshold: {TEXT_SIMILARITY_THRESHOLD}\")\n",
    "\n",
    "print(f\"\\nConfigurations to evaluate:\")\n",
    "total_runs = 0\n",
    "for i, config in enumerate(configurations, start=1):\n",
    "    provider = config['provider']\n",
    "    model = config['model']\n",
    "    chunk_sizes = config['chunk_sizes']\n",
    "    \n",
    "    runs_for_config = len(chunk_sizes) * len(k_values) * len(modes)\n",
    "    total_runs += runs_for_config\n",
    "    \n",
    "    print(f\"\\n  {i}. {provider}/{model}\")\n",
    "    print(f\"     Chunk sizes: {chunk_sizes}\")\n",
    "    print(f\"     Evaluation runs: {runs_for_config}\")\n",
    "    \n",
    "    # Show output filenames that will be generated\n",
    "    print(f\"     Output files:\")\n",
    "    for chunk_size in chunk_sizes:\n",
    "        for k in k_values:\n",
    "            for mode in modes:\n",
    "                filename = get_output_filename(provider, model, chunk_size, k, mode)\n",
    "                exists = check_if_results_exist(provider, model, chunk_size, k, mode, OUTPUT_DIR)\n",
    "                status = \"EXISTS\" if exists else \"TO CREATE\"\n",
    "                print(f\"       - {filename} [{status}]\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total evaluation runs: {total_runs}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10.4 Execute Batch Evaluation\n",
    "# \n",
    "# **IMPORTANT**: This cell will run the full evaluation.\n",
    "# - Depending on configurations, this may take 30 minutes to several hours\n",
    "# - Progress will be shown for each configuration\n",
    "# - Results are saved incrementally (existing results are skipped)\n",
    "\n",
    "# %%\n",
    "# Run batch evaluation\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"STARTING BATCH EVALUATION\")\n",
    "print(\"#\"*60)\n",
    "print(\"\\nNOTE: This may take a while. Progress will be shown for each configuration.\")\n",
    "print(\"You can interrupt and resume later - completed evaluations will be skipped.\\n\")\n",
    "\n",
    "# Uncomment the line below to run the evaluation\n",
    "summary = evaluate_multiple_configurations(\n",
    "    dataset=dataset,\n",
    "    evidence_lookup=evidence_lookup,\n",
    "    sbert_model=sbert_model,\n",
    "    configurations=configurations,\n",
    "    k_values=k_values,\n",
    "    modes=modes,\n",
    "    use_page_tolerance=USE_PAGE_TOLERANCE,\n",
    "    text_similarity_threshold=TEXT_SIMILARITY_THRESHOLD,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "print(\"\\n⚠️  EVALUATION NOT RUN - Uncomment the code above to execute\")\n",
    "print(\"This is intentional to prevent accidental execution during testing.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10.5 Display Results (Run after evaluation completes)\n",
    "# \n",
    "# Uncomment and run this cell after the evaluation completes\n",
    "\n",
    "# %%\n",
    "# Display detailed results in table format\n",
    "# Uncomment after evaluation completes:\n",
    "display_batch_results(summary)\n",
    "\n",
    "print(\"\\n⚠️  Results display not run - uncomment after evaluation completes\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10.6 List Generated Files\n",
    "# \n",
    "# View all generated JSON files\n",
    "\n",
    "# %%\n",
    "# List all generated files\n",
    "list_generated_files(OUTPUT_DIR)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 10.7 Load and Analyze a Single Result\n",
    "# \n",
    "# Example: How to load and inspect a single result file\n",
    "\n",
    "# %%\n",
    "def load_and_inspect_result(filename: str, output_dir: str = OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Load and display a single result file.\n",
    "    \n",
    "    Args:\n",
    "        filename: Name of the JSON file (e.g., \"voyage_voyage-finance-2_chunk1024_k20_global.json\")\n",
    "        output_dir: Output directory\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"❌ File not found: {filename}\")\n",
    "        return None\n",
    "    \n",
    "    # Load JSON\n",
    "    with open(filepath, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Extract summary (last item)\n",
    "    summary = results[-1]['summary']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"RESULTS: {filename}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(f\"  Provider: {summary['provider']}\")\n",
    "    print(f\"  Model: {summary['model']}\")\n",
    "    print(f\"  Chunk size: {summary['chunk_size']}\")\n",
    "    print(f\"  K: {summary['k']}\")\n",
    "    print(f\"  Mode: {summary['mode']}\")\n",
    "    print(f\"  Page tolerance: {summary['use_page_tolerance']}\")\n",
    "    print(f\"  Text threshold: {summary['text_similarity_threshold']}\")\n",
    "    \n",
    "    print(\"\\nPage-Based Metrics:\")\n",
    "    print(f\"  Average MRR:       {summary['average_page_mrr']:.4f}\")\n",
    "    print(f\"  Average Recall:    {summary['average_page_recall']:.4f}\")\n",
    "    print(f\"  Average Precision: {summary['average_page_precision']:.4f}\")\n",
    "    print(f\"  Average F1:        {summary['average_page_f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\nText-Based Metrics:\")\n",
    "    print(f\"  Average MRR:       {summary['average_text_mrr']:.4f}\")\n",
    "    print(f\"  Average Recall:    {summary['average_text_recall']:.4f}\")\n",
    "    print(f\"  Average Precision: {summary['average_text_precision']:.4f}\")\n",
    "    print(f\"  Average F1:        {summary['average_text_f1']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTotal queries: {summary['total_queries']}\")\n",
    "    print(f\"Total results (queries + summary): {len(results)}\")\n",
    "    \n",
    "    # Show sample query result\n",
    "    if len(results) > 1:\n",
    "        sample_query = results[0]\n",
    "        print(\"\\nSample query result:\")\n",
    "        print(f\"  Query ID: {sample_query['query_id']}\")\n",
    "        print(f\"  Question: {sample_query['query'][:80]}...\")\n",
    "        print(f\"  Page MRR: {sample_query['page_mrr_score']:.4f}, Rank: {sample_query['page_rank']}\")\n",
    "        print(f\"  Text MRR: {sample_query['text_mrr_score']:.4f}, Rank: {sample_query['text_rank']}\")\n",
    "        print(f\"  Retrieved docs: {len(sample_query['retrieved_docs'])}\")\n",
    "        \n",
    "        if len(sample_query['retrieved_docs']) > 0:\n",
    "            first_doc = sample_query['retrieved_docs'][0]\n",
    "            print(f\"\\n  First retrieved doc:\")\n",
    "            print(f\"    Doc: {first_doc['doc_name']}, Page: {first_doc['page_number']}\")\n",
    "            print(f\"    Chunk text: {first_doc['chunk_text'][:100]}...\")\n",
    "            if len(first_doc['text_similarities']) > 0:\n",
    "                print(f\"    Text similarity with evidence 0: {first_doc['text_similarities'][0]['cosine_similarity']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Result inspection function defined\")\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# results = load_and_inspect_result(\"voyage_voyage-finance-2_chunk1024_k20_global.json\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ STEP 10 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"  ✓ Configurations defined\")\n",
    "print(\"  ✓ Evaluation parameters set\")\n",
    "print(\"  ✓ Evaluation plan displayed\")\n",
    "print(\"  ✓ Batch evaluation ready (uncomment to run)\")\n",
    "print(\"  ✓ Result analysis tools ready\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 ALL STEPS COMPLETE! 🎉\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✅ SETUP COMPLETE:\")\n",
    "print(\"  ✓ Step 1: Imports and configuration\")\n",
    "print(\"  ✓ Step 2: Sentence-BERT model loaded\")\n",
    "print(\"  ✓ Step 3: Evidence embeddings pre-computed\")\n",
    "print(\"  ✓ Step 4: Vector store loading functions\")\n",
    "print(\"  ✓ Step 5: Page-based evaluation functions\")\n",
    "print(\"  ✓ Step 6: Text-based evaluation functions\")\n",
    "print(\"  ✓ Step 7: Retrieval functions\")\n",
    "print(\"  ✓ Step 8: Main evaluation function\")\n",
    "print(\"  ✓ Step 9: Batch evaluation function\")\n",
    "print(\"  ✓ Step 10: Configuration and execution ready\")\n",
    "\n",
    "print(\"\\n📊 EVALUATION CAPABILITIES:\")\n",
    "print(\"  ✓ Page-based metrics: MRR, Recall, Precision, F1\")\n",
    "print(\"  ✓ Text-based metrics: MRR, Recall, Precision, F1\")\n",
    "print(\"  ✓ Both global and single-document modes\")\n",
    "print(\"  ✓ Comprehensive JSON output with all similarities\")\n",
    "print(f\"  ✓ Text similarity threshold: {TEXT_SIMILARITY_THRESHOLD}\")\n",
    "print(f\"  ✓ Pre-computed embeddings: {evidence_embeddings.shape[0]} evidence items\")\n",
    "\n",
    "print(\"\\n🚀 NEXT STEPS:\")\n",
    "print(\"  1. Review the evaluation plan above\")\n",
    "print(\"  2. Uncomment the evaluation code in section 10.4\")\n",
    "print(\"  3. Run the batch evaluation (may take 30+ minutes)\")\n",
    "print(\"  4. After completion, uncomment section 10.5 to view results\")\n",
    "print(\"  5. Use section 10.7 to inspect individual result files\")\n",
    "\n",
    "print(\"\\n💾 OUTPUT:\")\n",
    "print(f\"  Location: {OUTPUT_DIR}\")\n",
    "print(f\"  Format: JSON files with complete metrics and similarities\")\n",
    "print(f\"  Naming: {{provider}}_{{model}}_chunk{{size}}_k{{k}}_{{mode}}.json\")\n",
    "\n",
    "print(\"\\n⚠️  IMPORTANT NOTES:\")\n",
    "print(\"  - Evaluation runs incrementally (existing results are skipped)\")\n",
    "print(\"  - You can interrupt and resume anytime\")\n",
    "print(\"  - Progress is shown for each configuration\")\n",
    "print(\"  - Each query processes text similarities (slowest part)\")\n",
    "print(\"  - Results are saved immediately after each configuration\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Ready to evaluate! Uncomment section 10.4 when ready to start.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
