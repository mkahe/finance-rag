{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "588a57b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n",
      "✓ VoyageAI API key loaded\n",
      "✓ Configuration set\n",
      "  Vector DB: ../../vector_databases\n",
      "  BM25 Index: ../../bm25_indices\n",
      "  Embedding: voyage/voyage-finance-2\n",
      "  Chunk Sizes: [512, 1024]\n",
      "✓ BM25 directory ready: ../../bm25_indices\n",
      "✓ Helper functions defined\n",
      "Loading FinanceBench dataset...\n",
      "✓ Loaded 150 questions\n",
      "\n",
      "Sample question:\n",
      "  Question: What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.\n",
      "  Document: 3M_2018_10K\n",
      "\n",
      "============================================================\n",
      "✅ STEP 1 COMPLETE\n",
      "============================================================\n",
      "All imports successful, configuration loaded, dataset ready.\n",
      "\n",
      "Test this step, then I'll provide Step 2!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Setup and Imports\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # Hybrid Retrieval Implementation\n",
    "# \n",
    "# This notebook implements hybrid retrieval (Dense + BM25) for financial QA.\n",
    "# We'll build this step-by-step, testing each section before moving forward.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1: Setup and Imports\n",
    "# \n",
    "# **Goal:** Import all required libraries and load configuration\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1.1 Import Libraries\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Progress bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# LangChain for vector stores\n",
    "from langchain.docstore.document import Document as LCDocument\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# VoyageAI embeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "\n",
    "# BM25 for keyword-based retrieval\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1.2 Load Environment Variables\n",
    "\n",
    "# %%\n",
    "load_dotenv()\n",
    "\n",
    "VOYAGE_API_KEY = os.getenv(\"VOYAGE_API_KEY\")\n",
    "\n",
    "if VOYAGE_API_KEY:\n",
    "    print(\"✓ VoyageAI API key loaded\")\n",
    "else:\n",
    "    raise ValueError(\"❌ VoyageAI API key not found in .env file\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1.3 Configuration\n",
    "\n",
    "# %%\n",
    "# Paths\n",
    "VECTOR_DB_DIR = \"../../vector_databases\"\n",
    "BM25_INDEX_DIR = \"../../bm25_indices\"\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = \"PatronusAI/financebench\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "\n",
    "# Settings\n",
    "COLLECTION_PREFIX = \"financebench_docs_chunk_\"\n",
    "EMBEDDING_PROVIDER = \"voyage\"\n",
    "EMBEDDING_MODEL = \"voyage-finance-2\"\n",
    "CHUNK_SIZES = [512, 1024]\n",
    "\n",
    "print(\"✓ Configuration set\")\n",
    "print(f\"  Vector DB: {VECTOR_DB_DIR}\")\n",
    "print(f\"  BM25 Index: {BM25_INDEX_DIR}\")\n",
    "print(f\"  Embedding: {EMBEDDING_PROVIDER}/{EMBEDDING_MODEL}\")\n",
    "print(f\"  Chunk Sizes: {CHUNK_SIZES}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1.4 Create Directories\n",
    "\n",
    "# %%\n",
    "os.makedirs(BM25_INDEX_DIR, exist_ok=True)\n",
    "print(f\"✓ BM25 directory ready: {BM25_INDEX_DIR}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1.5 Helper Functions\n",
    "\n",
    "# %%\n",
    "def get_embedding_function(provider: str, model: str):\n",
    "    \"\"\"Get embedding function.\"\"\"\n",
    "    if provider == \"voyage\":\n",
    "        return VoyageAIEmbeddings(model=model, voyage_api_key=VOYAGE_API_KEY)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "\n",
    "def get_db_path(base_dir: str, provider: str, model: str) -> str:\n",
    "    \"\"\"Get database path for embedding.\"\"\"\n",
    "    model_id = f\"{provider}_{model.replace('/', '_')}\"\n",
    "    return os.path.join(base_dir, model_id)\n",
    "\n",
    "\n",
    "def get_bm25_path(base_dir: str, chunk_size: int) -> str:\n",
    "    \"\"\"Get path for BM25 index file.\"\"\"\n",
    "    return os.path.join(base_dir, f\"bm25_chunk_{chunk_size}.pkl\")\n",
    "\n",
    "\n",
    "print(\"✓ Helper functions defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1.6 Load Dataset\n",
    "\n",
    "# %%\n",
    "print(\"Loading FinanceBench dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=DATASET_SPLIT)\n",
    "print(f\"✓ Loaded {len(dataset)} questions\")\n",
    "\n",
    "# Show sample\n",
    "sample = dataset[0]\n",
    "print(f\"\\nSample question:\")\n",
    "print(f\"  Question: {sample['question']}\")\n",
    "print(f\"  Document: {sample['doc_name']}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ STEP 1 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"All imports successful, configuration loaded, dataset ready.\")\n",
    "print(\"\\nTest this step, then I'll provide Step 2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "042b30bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Load vectorstore function defined\n",
      "✓ Extract chunks function defined\n",
      "\n",
      "============================================================\n",
      "Processing Chunk Size 512\n",
      "============================================================\n",
      "\n",
      "Loading vectorstore:\n",
      "  Path: ../../vector_databases/voyage_voyage-finance-2\n",
      "  Collection: financebench_docs_chunk_512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lj/175ptt0d6knb0gg0lg2h4n2h0000gp/T/ipykernel_55770/107959040.py:34: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Documents: 28,634\n",
      "\n",
      "Extracting chunks...\n",
      "✓ Extracted 28,634 chunks\n",
      "\n",
      "Sample chunk:\n",
      "  Length: 1606 chars\n",
      "  Preview: Table of Contents\n",
      "UNITED STATES\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "Washington, D.C. 20549 \n",
      "FORM 10-K\n",
      "☒\n",
      "ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) O...\n",
      "\n",
      "============================================================\n",
      "Processing Chunk Size 1024\n",
      "============================================================\n",
      "\n",
      "Loading vectorstore:\n",
      "  Path: ../../vector_databases/voyage_voyage-finance-2\n",
      "  Collection: financebench_docs_chunk_1024\n",
      "  Documents: 15,765\n",
      "\n",
      "Extracting chunks...\n",
      "✓ Extracted 15,765 chunks\n",
      "\n",
      "Sample chunk:\n",
      "  Length: 3576 chars\n",
      "  Preview: Table of Contents\n",
      "UNITED STATES\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "Washington, D.C. 20549 \n",
      "FORM 10-K\n",
      "☒\n",
      "ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) O...\n",
      "\n",
      "============================================================\n",
      "EXTRACTION SUMMARY\n",
      "============================================================\n",
      "\n",
      "Chunk Size 512:\n",
      "  Total chunks: 28,634\n",
      "  Total chars: 43,780,927\n",
      "  Avg length: 1529 chars\n",
      "  Min length: 1 chars\n",
      "  Max length: 4,103 chars\n",
      "\n",
      "Chunk Size 1024:\n",
      "  Total chunks: 15,765\n",
      "  Total chars: 42,395,002\n",
      "  Avg length: 2689 chars\n",
      "  Min length: 1 chars\n",
      "  Max length: 7,205 chars\n",
      "\n",
      "============================================================\n",
      "✅ STEP 2 COMPLETE\n",
      "============================================================\n",
      "Chunks extracted and ready for BM25 indexing.\n",
      "\n",
      "Test this step, then I'll provide Step 3!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Load Vectorstores and Extract Chunks\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 2: Load Vectorstores and Extract Chunks\n",
    "# \n",
    "# **Goal:** Load existing ChromaDB vectorstores and extract all chunks for BM25 indexing\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2.1 Load Vectorstore Function\n",
    "\n",
    "# %%\n",
    "def load_vectorstore(\n",
    "    embedding_provider: str,\n",
    "    embedding_model: str,\n",
    "    chunk_size: int,\n",
    "    base_db_dir: str = \"../../vector_databases\",\n",
    "    collection_prefix: str = \"financebench_docs_chunk_\"\n",
    ") -> Chroma:\n",
    "    \"\"\"Load a vectorstore for a specific chunk size.\"\"\"\n",
    "    db_path = get_db_path(base_db_dir, embedding_provider, embedding_model)\n",
    "    collection_name = f\"{collection_prefix}{chunk_size}\"\n",
    "    \n",
    "    print(f\"\\nLoading vectorstore:\")\n",
    "    print(f\"  Path: {db_path}\")\n",
    "    print(f\"  Collection: {collection_name}\")\n",
    "    \n",
    "    if not os.path.exists(db_path):\n",
    "        raise ValueError(f\"Database not found at: {db_path}\")\n",
    "    \n",
    "    emb_fn = get_embedding_function(embedding_provider, embedding_model)\n",
    "    \n",
    "    vectorstore = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=emb_fn,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    \n",
    "    count = vectorstore._collection.count()\n",
    "    print(f\"  Documents: {count:,}\")\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "print(\"✓ Load vectorstore function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2.2 Extract Chunks Function\n",
    "\n",
    "# %%\n",
    "def extract_chunks_from_vectorstore(vectorstore: Chroma) -> Dict:\n",
    "    \"\"\"Extract all chunks with their texts and metadata.\"\"\"\n",
    "    print(\"\\nExtracting chunks...\")\n",
    "    \n",
    "    collection = vectorstore._collection\n",
    "    results = collection.get(include=[\"documents\", \"metadatas\"])\n",
    "    \n",
    "    chunk_data = {\n",
    "        'ids': results['ids'],\n",
    "        'texts': results['documents'],\n",
    "        'metadatas': results['metadatas']\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Extracted {len(chunk_data['texts']):,} chunks\")\n",
    "    \n",
    "    # Show sample\n",
    "    if chunk_data['texts']:\n",
    "        print(f\"\\nSample chunk:\")\n",
    "        print(f\"  Length: {len(chunk_data['texts'][0])} chars\")\n",
    "        print(f\"  Preview: {chunk_data['texts'][0][:150]}...\")\n",
    "    \n",
    "    return chunk_data\n",
    "\n",
    "\n",
    "print(\"✓ Extract chunks function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2.3 Load and Extract Chunk Size 512\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Processing Chunk Size 512\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vectorstore_512 = load_vectorstore(\n",
    "    embedding_provider=EMBEDDING_PROVIDER,\n",
    "    embedding_model=EMBEDDING_MODEL,\n",
    "    chunk_size=512,\n",
    "    base_db_dir=VECTOR_DB_DIR\n",
    ")\n",
    "\n",
    "chunks_512 = extract_chunks_from_vectorstore(vectorstore_512)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2.4 Load and Extract Chunk Size 1024\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Processing Chunk Size 1024\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vectorstore_1024 = load_vectorstore(\n",
    "    embedding_provider=EMBEDDING_PROVIDER,\n",
    "    embedding_model=EMBEDDING_MODEL,\n",
    "    chunk_size=1024,\n",
    "    base_db_dir=VECTOR_DB_DIR\n",
    ")\n",
    "\n",
    "chunks_1024 = extract_chunks_from_vectorstore(vectorstore_1024)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2.5 Summary Statistics\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for chunk_size, chunks in [(512, chunks_512), (1024, chunks_1024)]:\n",
    "    texts = chunks['texts']\n",
    "    print(f\"\\nChunk Size {chunk_size}:\")\n",
    "    print(f\"  Total chunks: {len(texts):,}\")\n",
    "    print(f\"  Total chars: {sum(len(t) for t in texts):,}\")\n",
    "    print(f\"  Avg length: {sum(len(t) for t in texts) / len(texts):.0f} chars\")\n",
    "    print(f\"  Min length: {min(len(t) for t in texts):,} chars\")\n",
    "    print(f\"  Max length: {max(len(t) for t in texts):,} chars\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ STEP 2 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"Chunks extracted and ready for BM25 indexing.\")\n",
    "print(\"\\nTest this step, then I'll provide Step 3!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca500091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenization function defined\n",
      "✓ Build BM25 index function defined\n",
      "✓ Save BM25 index function defined\n",
      "✓ Load BM25 index function defined\n",
      "\n",
      "============================================================\n",
      "Building BM25 Index for Chunk Size 512\n",
      "============================================================\n",
      "\n",
      "Building BM25 index for chunk size 512...\n",
      "  Tokenizing 28,634 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf97ec13e8046dca3ba050c7e0ca845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/28634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building BM25 index...\n",
      "✓ BM25 index built with 28,634 documents\n",
      "✓ Saved BM25 index to: ../../bm25_indices/bm25_chunk_512.pkl\n",
      "  File size: 87.73 MB\n",
      "\n",
      "============================================================\n",
      "Building BM25 Index for Chunk Size 1024\n",
      "============================================================\n",
      "\n",
      "Building BM25 index for chunk size 1024...\n",
      "  Tokenizing 15,765 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726ee00c2d8c4b5299cb9b86b647e6e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/15765 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building BM25 index...\n",
      "✓ BM25 index built with 15,765 documents\n",
      "✓ Saved BM25 index to: ../../bm25_indices/bm25_chunk_1024.pkl\n",
      "  File size: 78.69 MB\n",
      "\n",
      "============================================================\n",
      "Testing BM25 Search\n",
      "============================================================\n",
      "\n",
      "Test Query: What was Apple's revenue in 2022?\n",
      "Tokenized: ['what', 'was', \"apple's\", 'revenue', 'in', '2022?']\n",
      "\n",
      "Top 3 results from BM25 (chunk 512):\n",
      "\n",
      "  Rank 1 (Score: 17.2535):\n",
      "    Once revenue is allocated to software or software-related elements as a group, \n",
      "we recognize revenue in conformance with software revenue accounting guidance. Revenue is recognized when revenue recogn...\n",
      "\n",
      "  Rank 2 (Score: 17.0497):\n",
      "    Revenue is recognized when revenue recognition \n",
      "criteria are met for each element.\n",
      "We are generally unable to establish VSOE or TPE for non-software elements and as such, we use BESP. BESP is generall...\n",
      "\n",
      "  Rank 3 (Score: 17.0164):\n",
      "    Pricing practices taken into \n",
      "consideration include historic contractually stated prices, volume discounts where applicable and our price lists. We must estimate \n",
      "certain royalty revenue amounts due t...\n",
      "\n",
      "============================================================\n",
      "✅ STEP 3 COMPLETE\n",
      "============================================================\n",
      "BM25 indices built and saved successfully.\n",
      "Indices saved to: ../../bm25_indices\n",
      "\n",
      "Test this step, then I'll provide Step 4!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Build BM25 Indices\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3: Build BM25 Indices\n",
    "# \n",
    "# **Goal:** Create BM25 indices from extracted chunks for keyword-based retrieval\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3.1 Tokenization Function\n",
    "# \n",
    "# BM25 needs to tokenize text. We'll use simple whitespace tokenization.\n",
    "\n",
    "# %%\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Simple tokenization: lowercase and split by whitespace.\"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "print(\"✓ Tokenization function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3.2 Build BM25 Index Function\n",
    "\n",
    "# %%\n",
    "def build_bm25_index(chunks: Dict, chunk_size: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Build BM25 index from chunks.\n",
    "    \n",
    "    Returns a dictionary with:\n",
    "    - bm25: BM25Okapi object\n",
    "    - ids: List of chunk IDs (same order as corpus)\n",
    "    - metadatas: List of metadata dicts\n",
    "    \"\"\"\n",
    "    print(f\"\\nBuilding BM25 index for chunk size {chunk_size}...\")\n",
    "    \n",
    "    texts = chunks['texts']\n",
    "    ids = chunks['ids']\n",
    "    metadatas = chunks['metadatas']\n",
    "    \n",
    "    # Tokenize all texts\n",
    "    print(f\"  Tokenizing {len(texts):,} chunks...\")\n",
    "    tokenized_corpus = [simple_tokenize(text) for text in tqdm(texts, desc=\"Tokenizing\")]\n",
    "    \n",
    "    # Build BM25 index\n",
    "    print(f\"  Building BM25 index...\")\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    bm25_data = {\n",
    "        'bm25': bm25,\n",
    "        'ids': ids,\n",
    "        'metadatas': metadatas,\n",
    "        'texts': texts,  # Keep original texts for retrieval\n",
    "        'chunk_size': chunk_size\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ BM25 index built with {len(texts):,} documents\")\n",
    "    \n",
    "    return bm25_data\n",
    "\n",
    "\n",
    "print(\"✓ Build BM25 index function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3.3 Save BM25 Index Function\n",
    "\n",
    "# %%\n",
    "def save_bm25_index(bm25_data: Dict, chunk_size: int, output_dir: str):\n",
    "    \"\"\"Save BM25 index to pickle file.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = get_bm25_path(output_dir, chunk_size)\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(bm25_data, f)\n",
    "    \n",
    "    print(f\"✓ Saved BM25 index to: {output_path}\")\n",
    "    \n",
    "    # Print file size\n",
    "    file_size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "    print(f\"  File size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "\n",
    "print(\"✓ Save BM25 index function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3.4 Load BM25 Index Function (for later use)\n",
    "\n",
    "# %%\n",
    "def load_bm25_index(chunk_size: int, index_dir: str) -> Dict:\n",
    "    \"\"\"Load BM25 index from pickle file.\"\"\"\n",
    "    index_path = get_bm25_path(index_dir, chunk_size)\n",
    "    \n",
    "    if not os.path.exists(index_path):\n",
    "        raise ValueError(f\"BM25 index not found at: {index_path}\")\n",
    "    \n",
    "    with open(index_path, 'rb') as f:\n",
    "        bm25_data = pickle.load(f)\n",
    "    \n",
    "    print(f\"✓ Loaded BM25 index from: {index_path}\")\n",
    "    print(f\"  Documents: {len(bm25_data['ids']):,}\")\n",
    "    \n",
    "    return bm25_data\n",
    "\n",
    "\n",
    "print(\"✓ Load BM25 index function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3.5 Build and Save BM25 Index for Chunk Size 512\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Building BM25 Index for Chunk Size 512\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "bm25_index_512 = build_bm25_index(chunks_512, 512)\n",
    "save_bm25_index(bm25_index_512, 512, BM25_INDEX_DIR)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3.6 Build and Save BM25 Index for Chunk Size 1024\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Building BM25 Index for Chunk Size 1024\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "bm25_index_1024 = build_bm25_index(chunks_1024, 1024)\n",
    "save_bm25_index(bm25_index_1024, 1024, BM25_INDEX_DIR)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3.7 Test BM25 Search\n",
    "# \n",
    "# Let's test the BM25 index with a sample query\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing BM25 Search\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test query\n",
    "test_query = \"What was Apple's revenue in 2022?\"\n",
    "print(f\"\\nTest Query: {test_query}\")\n",
    "\n",
    "# Tokenize query\n",
    "tokenized_query = simple_tokenize(test_query)\n",
    "print(f\"Tokenized: {tokenized_query}\")\n",
    "\n",
    "# Search with BM25 (chunk 512)\n",
    "print(f\"\\nTop 3 results from BM25 (chunk 512):\")\n",
    "scores = bm25_index_512['bm25'].get_scores(tokenized_query)\n",
    "\n",
    "# Get top 3 indices\n",
    "import numpy as np\n",
    "top_indices = np.argsort(scores)[::-1][:3]\n",
    "\n",
    "for rank, idx in enumerate(top_indices, 1):\n",
    "    score = scores[idx]\n",
    "    text = bm25_index_512['texts'][idx]\n",
    "    print(f\"\\n  Rank {rank} (Score: {score:.4f}):\")\n",
    "    print(f\"    {text[:200]}...\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ STEP 3 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"BM25 indices built and saved successfully.\")\n",
    "print(f\"Indices saved to: {BM25_INDEX_DIR}\")\n",
    "print(\"\\nTest this step, then I'll provide Step 4!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c33c0da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dense retrieval function defined\n",
      "✓ BM25 retrieval function defined\n",
      "✓ RRF function defined\n",
      "✓ Hybrid retrieval function defined\n",
      "\n",
      "============================================================\n",
      "Testing Hybrid Retrieval (Chunk 512)\n",
      "============================================================\n",
      "\n",
      "Query: What was Apple's total revenue in 2022?\n",
      "\n",
      "Hybrid Retrieval for: 'What was Apple's total revenue in 2022?'\n",
      "  Retrieving top-40 from each method...\n",
      "  ✓ Dense retrieval: 40 results\n",
      "  ✓ BM25 retrieval: 40 results\n",
      "  Fusing with RRF (k=60)...\n",
      "  ✓ Hybrid results: 10 results\n",
      "\n",
      "============================================================\n",
      "HYBRID RETRIEVAL RESULTS\n",
      "============================================================\n",
      "\n",
      "Rank 1:\n",
      "  RRF Score: 0.016393\n",
      "  Dense Rank: 1 (score: 1.2486)\n",
      "  BM25 Rank: None (score: N/A)\n",
      "  Text: Table of Contents\n",
      "Net cash provided by operating activities was $3.6 billion in 2022, primarily due to our net income of $1.3 billion in 2022, adjusted for non-cash adjustments of\n",
      "$4.1 billion and net...\n",
      "\n",
      "Rank 2:\n",
      "  RRF Score: 0.016393\n",
      "  Dense Rank: None (score: N/A)\n",
      "  BM25 Rank: 1 (score: 17.9997)\n",
      "  Text: Pricing practices taken into \n",
      "consideration include historic contractually stated prices, volume discounts where applicable and our price lists. We must estimate \n",
      "certain royalty revenue amounts due t...\n",
      "\n",
      "Rank 3:\n",
      "  RRF Score: 0.016129\n",
      "  Dense Rank: 2 (score: 1.2543)\n",
      "  BM25 Rank: None (score: N/A)\n",
      "  Text: Table of Contents\n",
      "Gaming\n",
      "Gaming net revenue of $6.8 billion in 2022 increased by 21%, compared to net revenue of $5.6 billion in 2021. The increase in net revenue was driven by\n",
      "higher semi-custom prod...\n",
      "\n",
      "Rank 4:\n",
      "  RRF Score: 0.016129\n",
      "  Dense Rank: None (score: N/A)\n",
      "  BM25 Rank: 2 (score: 17.2535)\n",
      "  Text: Once revenue is allocated to software or software-related elements as a group, \n",
      "we recognize revenue in conformance with software revenue accounting guidance. Revenue is recognized when revenue recogn...\n",
      "\n",
      "Rank 5:\n",
      "  RRF Score: 0.015873\n",
      "  Dense Rank: 3 (score: 1.2591)\n",
      "  BM25 Rank: None (score: N/A)\n",
      "  Text: The decrease in operating income was primarily driven by\n",
      "amortization of intangible assets associated with the Xilinx acquisition. Net income for 2022 was $1.3 billion compared to $3.2 billion in the ...\n",
      "\n",
      "============================================================\n",
      "COMPARISON: Dense vs BM25 vs Hybrid\n",
      "============================================================\n",
      "\n",
      "Dense Only (Top 3):\n",
      "  1. Score: 1.2486 | Table of Contents\n",
      "Net cash provided by operating activities was $3.6 billion in 2022, primarily due ...\n",
      "  2. Score: 1.2543 | Table of Contents\n",
      "Gaming\n",
      "Gaming net revenue of $6.8 billion in 2022 increased by 21%, compared to ne...\n",
      "  3. Score: 1.2591 | The decrease in operating income was primarily driven by\n",
      "amortization of intangible assets associate...\n",
      "\n",
      "BM25 Only (Top 3):\n",
      "  1. Score: 17.9997 | Pricing practices taken into \n",
      "consideration include historic contractually stated prices, volume dis...\n",
      "  2. Score: 17.2535 | Once revenue is allocated to software or software-related elements as a group, \n",
      "we recognize revenue...\n",
      "  3. Score: 17.0497 | Revenue is recognized when revenue recognition \n",
      "criteria are met for each element.\n",
      "We are generally ...\n",
      "\n",
      "Hybrid (Top 3):\n",
      "  1. RRF: 0.016393 | Table of Contents\n",
      "Net cash provided by operating activities was $3.6 billion in 2022, primarily due ...\n",
      "  2. RRF: 0.016393 | Pricing practices taken into \n",
      "consideration include historic contractually stated prices, volume dis...\n",
      "  3. RRF: 0.016129 | Table of Contents\n",
      "Gaming\n",
      "Gaming net revenue of $6.8 billion in 2022 increased by 21%, compared to ne...\n",
      "\n",
      "============================================================\n",
      "✅ STEP 4 COMPLETE\n",
      "============================================================\n",
      "Hybrid retrieval with RRF fusion implemented successfully!\n",
      "\n",
      "Test this step, then I'll provide Step 5!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: Hybrid Retrieval with RRF Fusion\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 4: Hybrid Retrieval with RRF Fusion\n",
    "# \n",
    "# **Goal:** Implement hybrid retrieval that combines dense (embeddings) and sparse (BM25) retrieval using Reciprocal Rank Fusion\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.1 Dense Retrieval Function\n",
    "\n",
    "# %%\n",
    "def dense_retrieval(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    k: int = 40\n",
    ") -> List[Tuple[str, float, Dict]]:\n",
    "    \"\"\"\n",
    "    Perform dense retrieval using embeddings.\n",
    "    \n",
    "    Returns list of (chunk_id, score, metadata) tuples.\n",
    "    \"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    # Format results: (id, score, metadata)\n",
    "    dense_results = []\n",
    "    for doc, score in results:\n",
    "        # Get the document ID from metadata or generate one\n",
    "        doc_id = doc.metadata.get('id', str(hash(doc.page_content)))\n",
    "        dense_results.append((doc_id, score, doc.metadata, doc.page_content))\n",
    "    \n",
    "    return dense_results\n",
    "\n",
    "\n",
    "print(\"✓ Dense retrieval function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.2 BM25 Retrieval Function\n",
    "\n",
    "# %%\n",
    "def bm25_retrieval(\n",
    "    bm25_data: Dict,\n",
    "    query: str,\n",
    "    k: int = 40\n",
    ") -> List[Tuple[str, float, Dict]]:\n",
    "    \"\"\"\n",
    "    Perform BM25 retrieval using keyword matching.\n",
    "    \n",
    "    Returns list of (chunk_id, score, metadata) tuples.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Tokenize query\n",
    "    tokenized_query = simple_tokenize(query)\n",
    "    \n",
    "    # Get BM25 scores\n",
    "    scores = bm25_data['bm25'].get_scores(tokenized_query)\n",
    "    \n",
    "    # Get top k indices\n",
    "    top_indices = np.argsort(scores)[::-1][:k]\n",
    "    \n",
    "    # Format results: (id, score, metadata, text)\n",
    "    bm25_results = []\n",
    "    for idx in top_indices:\n",
    "        chunk_id = bm25_data['ids'][idx]\n",
    "        score = scores[idx]\n",
    "        metadata = bm25_data['metadatas'][idx]\n",
    "        text = bm25_data['texts'][idx]\n",
    "        bm25_results.append((chunk_id, score, metadata, text))\n",
    "    \n",
    "    return bm25_results\n",
    "\n",
    "\n",
    "print(\"✓ BM25 retrieval function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.3 Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "# %%\n",
    "def reciprocal_rank_fusion(\n",
    "    dense_results: List[Tuple],\n",
    "    bm25_results: List[Tuple],\n",
    "    k: int = 60,\n",
    "    final_k: int = 20\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Combine dense and BM25 results using Reciprocal Rank Fusion.\n",
    "    \n",
    "    Args:\n",
    "        dense_results: List of (id, score, metadata, text) from dense retrieval\n",
    "        bm25_results: List of (id, score, metadata, text) from BM25 retrieval\n",
    "        k: RRF constant (typically 60)\n",
    "        final_k: Number of final results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with combined results\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Store RRF scores and document info\n",
    "    rrf_scores = defaultdict(float)\n",
    "    doc_info = {}  # Store metadata and text for each doc\n",
    "    \n",
    "    # Add dense retrieval ranks\n",
    "    for rank, (doc_id, score, metadata, text) in enumerate(dense_results, 1):\n",
    "        rrf_scores[doc_id] += 1 / (k + rank)\n",
    "        if doc_id not in doc_info:\n",
    "            doc_info[doc_id] = {\n",
    "                'metadata': metadata,\n",
    "                'text': text,\n",
    "                'dense_rank': rank,\n",
    "                'dense_score': score,\n",
    "                'bm25_rank': None,\n",
    "                'bm25_score': None\n",
    "            }\n",
    "        else:\n",
    "            doc_info[doc_id]['dense_rank'] = rank\n",
    "            doc_info[doc_id]['dense_score'] = score\n",
    "    \n",
    "    # Add BM25 retrieval ranks\n",
    "    for rank, (doc_id, score, metadata, text) in enumerate(bm25_results, 1):\n",
    "        rrf_scores[doc_id] += 1 / (k + rank)\n",
    "        if doc_id not in doc_info:\n",
    "            doc_info[doc_id] = {\n",
    "                'metadata': metadata,\n",
    "                'text': text,\n",
    "                'dense_rank': None,\n",
    "                'dense_score': None,\n",
    "                'bm25_rank': rank,\n",
    "                'bm25_score': score\n",
    "            }\n",
    "        else:\n",
    "            doc_info[doc_id]['bm25_rank'] = rank\n",
    "            doc_info[doc_id]['bm25_score'] = score\n",
    "    \n",
    "    # Sort by RRF score and get top k\n",
    "    sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)[:final_k]\n",
    "    \n",
    "    # Format final results\n",
    "    final_results = []\n",
    "    for doc_id, rrf_score in sorted_docs:\n",
    "        result = {\n",
    "            'id': doc_id,\n",
    "            'rrf_score': rrf_score,\n",
    "            'text': doc_info[doc_id]['text'],\n",
    "            'metadata': doc_info[doc_id]['metadata'],\n",
    "            'dense_rank': doc_info[doc_id]['dense_rank'],\n",
    "            'dense_score': doc_info[doc_id]['dense_score'],\n",
    "            'bm25_rank': doc_info[doc_id]['bm25_rank'],\n",
    "            'bm25_score': doc_info[doc_id]['bm25_score']\n",
    "        }\n",
    "        final_results.append(result)\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "\n",
    "print(\"✓ RRF function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.4 Complete Hybrid Retrieval Function\n",
    "\n",
    "# %%\n",
    "def hybrid_retrieval(\n",
    "    vectorstore: Chroma,\n",
    "    bm25_data: Dict,\n",
    "    query: str,\n",
    "    k_retrieve: int = 40,\n",
    "    k_final: int = 20,\n",
    "    rrf_k: int = 60\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform hybrid retrieval: Dense + BM25 with RRF fusion.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: ChromaDB vectorstore\n",
    "        bm25_data: BM25 index data\n",
    "        query: Search query\n",
    "        k_retrieve: Number of results to retrieve from each method\n",
    "        k_final: Number of final results after fusion\n",
    "        rrf_k: RRF constant\n",
    "    \n",
    "    Returns:\n",
    "        List of results sorted by RRF score\n",
    "    \"\"\"\n",
    "    print(f\"\\nHybrid Retrieval for: '{query}'\")\n",
    "    print(f\"  Retrieving top-{k_retrieve} from each method...\")\n",
    "    \n",
    "    # Dense retrieval\n",
    "    dense_results = dense_retrieval(vectorstore, query, k=k_retrieve)\n",
    "    print(f\"  ✓ Dense retrieval: {len(dense_results)} results\")\n",
    "    \n",
    "    # BM25 retrieval\n",
    "    bm25_results = bm25_retrieval(bm25_data, query, k=k_retrieve)\n",
    "    print(f\"  ✓ BM25 retrieval: {len(bm25_results)} results\")\n",
    "    \n",
    "    # Fusion\n",
    "    print(f\"  Fusing with RRF (k={rrf_k})...\")\n",
    "    hybrid_results = reciprocal_rank_fusion(\n",
    "        dense_results, \n",
    "        bm25_results, \n",
    "        k=rrf_k, \n",
    "        final_k=k_final\n",
    "    )\n",
    "    print(f\"  ✓ Hybrid results: {len(hybrid_results)} results\")\n",
    "    \n",
    "    return hybrid_results\n",
    "\n",
    "\n",
    "print(\"✓ Hybrid retrieval function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.5 Test Hybrid Retrieval with Sample Query\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing Hybrid Retrieval (Chunk 512)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_query = \"What was Apple's total revenue in 2022?\"\n",
    "print(f\"\\nQuery: {test_query}\")\n",
    "\n",
    "hybrid_results = hybrid_retrieval(\n",
    "    vectorstore=vectorstore_512,\n",
    "    bm25_data=bm25_index_512,\n",
    "    query=test_query,\n",
    "    k_retrieve=40,\n",
    "    k_final=10\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.6 Display Results with Rankings\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYBRID RETRIEVAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, result in enumerate(hybrid_results[:5], 1):\n",
    "    print(f\"\\nRank {i}:\")\n",
    "    print(f\"  RRF Score: {result['rrf_score']:.6f}\")\n",
    "    \n",
    "    # Format dense score\n",
    "    dense_score_str = f\"{result['dense_score']:.4f}\" if result['dense_score'] is not None else \"N/A\"\n",
    "    print(f\"  Dense Rank: {result['dense_rank']} (score: {dense_score_str})\")\n",
    "    \n",
    "    # Format BM25 score\n",
    "    bm25_score_str = f\"{result['bm25_score']:.4f}\" if result['bm25_score'] is not None else \"N/A\"\n",
    "    print(f\"  BM25 Rank: {result['bm25_rank']} (score: {bm25_score_str})\")\n",
    "    \n",
    "    # Show source if available\n",
    "    if result['metadata'] and 'file_name' in result['metadata']:\n",
    "        print(f\"  Source: {result['metadata']['file_name']}\")\n",
    "    \n",
    "    # Show text preview\n",
    "    print(f\"  Text: {result['text'][:200]}...\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4.7 Compare: Dense Only vs BM25 Only vs Hybrid\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: Dense vs BM25 vs Hybrid\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dense only\n",
    "print(\"\\nDense Only (Top 3):\")\n",
    "dense_only = dense_retrieval(vectorstore_512, test_query, k=3)\n",
    "for i, (doc_id, score, metadata, text) in enumerate(dense_only, 1):\n",
    "    print(f\"  {i}. Score: {score:.4f} | {text[:100]}...\")\n",
    "\n",
    "# BM25 only\n",
    "print(\"\\nBM25 Only (Top 3):\")\n",
    "bm25_only = bm25_retrieval(bm25_index_512, test_query, k=3)\n",
    "for i, (doc_id, score, metadata, text) in enumerate(bm25_only, 1):\n",
    "    print(f\"  {i}. Score: {score:.4f} | {text[:100]}...\")\n",
    "\n",
    "# Hybrid\n",
    "print(\"\\nHybrid (Top 3):\")\n",
    "for i, result in enumerate(hybrid_results[:3], 1):\n",
    "    print(f\"  {i}. RRF: {result['rrf_score']:.6f} | {result['text'][:100]}...\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ STEP 4 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"Hybrid retrieval with RRF fusion implemented successfully!\")\n",
    "print(\"\\nTest this step, then I'll provide Step 5!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87583c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Page matching function defined\n",
      "✓ MRR calculation function defined\n",
      "✓ Evaluate single query function defined\n",
      "✓ Evaluate all queries function defined\n",
      "\n",
      "============================================================\n",
      "Debugging Metadata Structure\n",
      "============================================================\n",
      "\n",
      "Sample metadata from chunks_512:\n",
      "First metadata: {'total_pages': 76, 'source': '1', 'chunk_size': 512, 'file_path': '../../financebench/documents/COSTCO_2021_10K.pdf'}\n",
      "\n",
      "Metadata keys: ['total_pages', 'source', 'chunk_size', 'file_path']\n",
      "\n",
      "Sample from FinanceBench dataset:\n",
      "Expected doc_name: 3M_2018_10K\n",
      "Evidence: [{'evidence_text': 'Table of Contents \\n3M Company and Subsidiaries\\nConsolidated Statement of Cash Flow s\\nYears ended December 31\\n \\n(Millions)\\n \\n2018\\n \\n2017\\n \\n2016\\n \\nCash Flows from Operating Activities\\n \\n \\n \\n \\n \\n \\n \\nNet income including noncontrolling interest\\n \\n$\\n5,363 \\n$\\n4,869 \\n$\\n5,058 \\nAdjustments to reconcile net income including noncontrolling interest to net cash\\nprovided by operating activities\\n \\n \\n \\n \\n \\n \\n \\nDepreciation and amortization\\n \\n \\n1,488 \\n \\n1,544 \\n \\n1,474 \\nCompany pension and postretirement contributions\\n \\n \\n(370) \\n \\n(967) \\n \\n(383) \\nCompany pension and postretirement expense\\n \\n \\n410 \\n \\n334 \\n \\n250 \\nStock-based compensation expense\\n \\n \\n302 \\n \\n324 \\n \\n298 \\nGain on sale of businesses\\n \\n \\n(545) \\n \\n(586) \\n \\n(111) \\nDeferred income taxes\\n \\n \\n(57) \\n \\n107 \\n \\n 7 \\nChanges in assets and liabilities\\n \\n \\n \\n \\n \\n \\n \\nAccounts receivable\\n \\n \\n(305) \\n \\n(245) \\n \\n(313) \\nInventories\\n \\n \\n(509) \\n \\n(387) \\n \\n57 \\nAccounts payable\\n \\n \\n408 \\n \\n24 \\n \\n148 \\nAccrued income taxes (current and long-term)\\n \\n \\n134 \\n \\n967 \\n \\n101 \\nOther net\\n \\n \\n120 \\n \\n256 \\n \\n76 \\nNet cash provided by (used in) operating activities\\n \\n \\n6,439 \\n \\n6,240 \\n \\n6,662 \\n \\n \\n \\n \\n \\n \\n \\n \\nCash Flows from Investing Activities\\n \\n \\n \\n \\n \\n \\n \\nPurchases of property, plant and equipment (PP&E)\\n \\n \\n(1,577) \\n \\n(1,373) \\n \\n(1,420) \\nProceeds from sale of PP&E and other assets\\n \\n \\n262 \\n \\n49 \\n \\n58 \\nAcquisitions, net of cash acquired\\n \\n \\n13 \\n \\n(2,023) \\n \\n(16) \\nPurchases of marketable securities and investments\\n \\n \\n(1,828) \\n \\n(2,152) \\n \\n(1,410) \\nProceeds from maturities and sale of marketable securities and investments\\n \\n \\n2,497 \\n \\n1,354 \\n \\n1,247 \\nProceeds from sale of businesses, net of cash sold\\n \\n \\n846 \\n \\n1,065 \\n \\n142 \\nOther net\\n \\n \\n 9 \\n \\n(6) \\n \\n(4) \\nNet cash provided by (used in) investing activities\\n \\n \\n222 \\n \\n(3,086) \\n \\n(1,403) \\n \\n \\n \\n \\n \\n \\n \\n \\nCash Flows from Financing Activities\\n \\n \\n \\n \\n \\n \\n \\nChange in short-term debt net\\n \\n \\n(284) \\n \\n578 \\n \\n(797) \\nRepayment of debt (maturities greater than 90 days)\\n \\n \\n(1,034) \\n \\n(962) \\n \\n(992) \\nProceeds from debt (maturities greater than 90 days)\\n \\n \\n2,251 \\n \\n1,987 \\n \\n2,832 \\nPurchases of treasury stock\\n \\n \\n(4,870) \\n \\n(2,068) \\n \\n(3,753) \\nProceeds from issuance of treasury stock pursuant to stock option and benefit plans\\n \\n \\n485 \\n \\n734 \\n \\n804 \\nDividends paid to shareholders\\n \\n \\n(3,193) \\n \\n(2,803) \\n \\n(2,678) \\nOther net\\n \\n \\n(56) \\n \\n(121) \\n \\n(42) \\nNet cash provided by (used in) financing activities\\n \\n \\n(6,701) \\n \\n(2,655) \\n \\n(4,626) \\n \\n \\n \\n \\n \\n \\n \\n \\nEffect of exchange rate changes on cash and cash equivalents\\n \\n \\n(160) \\n \\n156 \\n \\n(33) \\n \\n \\n \\n \\n \\n \\n \\n \\nNet increase (decrease) in cash and cash equivalents\\n \\n \\n(200) \\n \\n655 \\n \\n600 \\nCash and cash equivalents at beginning of year\\n \\n \\n3,053 \\n \\n2,398 \\n \\n1,798 \\nCash and cash equivalents at end of period\\n \\n$\\n2,853 \\n$\\n3,053 \\n$\\n2,398 \\n \\nThe accompanying Notes to Consolidated Financial Statements are an integral part of this statement.\\n \\n60', 'doc_name': '3M_2018_10K', 'evidence_page_num': 59, 'evidence_text_full_page': 'Table of Contents \\n3M Company and Subsidiaries\\nConsolidated Statement of Cash Flow s\\nYears ended December 31\\n \\n(Millions)\\n \\n2018\\n \\n2017\\n \\n2016\\n \\nCash Flows from Operating Activities\\n \\n \\n \\n \\n \\n \\n \\nNet income including noncontrolling interest\\n \\n$\\n5,363 \\n$\\n4,869 \\n$\\n5,058 \\nAdjustments to reconcile net income including noncontrolling interest to net cash\\nprovided by operating activities\\n \\n \\n \\n \\n \\n \\n \\nDepreciation and amortization\\n \\n \\n1,488 \\n \\n1,544 \\n \\n1,474 \\nCompany pension and postretirement contributions\\n \\n \\n(370) \\n \\n(967) \\n \\n(383) \\nCompany pension and postretirement expense\\n \\n \\n410 \\n \\n334 \\n \\n250 \\nStock-based compensation expense\\n \\n \\n302 \\n \\n324 \\n \\n298 \\nGain on sale of businesses\\n \\n \\n(545) \\n \\n(586) \\n \\n(111) \\nDeferred income taxes\\n \\n \\n(57) \\n \\n107 \\n \\n 7 \\nChanges in assets and liabilities\\n \\n \\n \\n \\n \\n \\n \\nAccounts receivable\\n \\n \\n(305) \\n \\n(245) \\n \\n(313) \\nInventories\\n \\n \\n(509) \\n \\n(387) \\n \\n57 \\nAccounts payable\\n \\n \\n408 \\n \\n24 \\n \\n148 \\nAccrued income taxes (current and long-term)\\n \\n \\n134 \\n \\n967 \\n \\n101 \\nOther net\\n \\n \\n120 \\n \\n256 \\n \\n76 \\nNet cash provided by (used in) operating activities\\n \\n \\n6,439 \\n \\n6,240 \\n \\n6,662 \\n \\n \\n \\n \\n \\n \\n \\n \\nCash Flows from Investing Activities\\n \\n \\n \\n \\n \\n \\n \\nPurchases of property, plant and equipment (PP&E)\\n \\n \\n(1,577) \\n \\n(1,373) \\n \\n(1,420) \\nProceeds from sale of PP&E and other assets\\n \\n \\n262 \\n \\n49 \\n \\n58 \\nAcquisitions, net of cash acquired\\n \\n \\n13 \\n \\n(2,023) \\n \\n(16) \\nPurchases of marketable securities and investments\\n \\n \\n(1,828) \\n \\n(2,152) \\n \\n(1,410) \\nProceeds from maturities and sale of marketable securities and investments\\n \\n \\n2,497 \\n \\n1,354 \\n \\n1,247 \\nProceeds from sale of businesses, net of cash sold\\n \\n \\n846 \\n \\n1,065 \\n \\n142 \\nOther net\\n \\n \\n 9 \\n \\n(6) \\n \\n(4) \\nNet cash provided by (used in) investing activities\\n \\n \\n222 \\n \\n(3,086) \\n \\n(1,403) \\n \\n \\n \\n \\n \\n \\n \\n \\nCash Flows from Financing Activities\\n \\n \\n \\n \\n \\n \\n \\nChange in short-term debt net\\n \\n \\n(284) \\n \\n578 \\n \\n(797) \\nRepayment of debt (maturities greater than 90 days)\\n \\n \\n(1,034) \\n \\n(962) \\n \\n(992) \\nProceeds from debt (maturities greater than 90 days)\\n \\n \\n2,251 \\n \\n1,987 \\n \\n2,832 \\nPurchases of treasury stock\\n \\n \\n(4,870) \\n \\n(2,068) \\n \\n(3,753) \\nProceeds from issuance of treasury stock pursuant to stock option and benefit plans\\n \\n \\n485 \\n \\n734 \\n \\n804 \\nDividends paid to shareholders\\n \\n \\n(3,193) \\n \\n(2,803) \\n \\n(2,678) \\nOther net\\n \\n \\n(56) \\n \\n(121) \\n \\n(42) \\nNet cash provided by (used in) financing activities\\n \\n \\n(6,701) \\n \\n(2,655) \\n \\n(4,626) \\n \\n \\n \\n \\n \\n \\n \\n \\nEffect of exchange rate changes on cash and cash equivalents\\n \\n \\n(160) \\n \\n156 \\n \\n(33) \\n \\n \\n \\n \\n \\n \\n \\n \\nNet increase (decrease) in cash and cash equivalents\\n \\n \\n(200) \\n \\n655 \\n \\n600 \\nCash and cash equivalents at beginning of year\\n \\n \\n3,053 \\n \\n2,398 \\n \\n1,798 \\nCash and cash equivalents at end of period\\n \\n$\\n2,853 \\n$\\n3,053 \\n$\\n2,398 \\n \\nThe accompanying Notes to Consolidated Financial Statements are an integral part of this statement.\\n \\n60\\n \\n'}]\n",
      "\n",
      "============================================================\n",
      "Test Retrieval to Check Metadata\n",
      "============================================================\n",
      "\n",
      "Query: What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.\n",
      "\n",
      "Hybrid Retrieval for: 'What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.'\n",
      "  Retrieving top-5 from each method...\n",
      "  ✓ Dense retrieval: 5 results\n",
      "  ✓ BM25 retrieval: 5 results\n",
      "  Fusing with RRF (k=60)...\n",
      "  ✓ Hybrid results: 3 results\n",
      "\n",
      "First result metadata: {'source': '47', 'chunk_size': 512, 'file_path': '../../financebench/documents/3M_2018_10K.pdf', 'total_pages': 160}\n",
      "Expected doc_name: 3M_2018_10K\n",
      "⚠️ WARNING: 'file_name' not in metadata!\n",
      "Available keys: ['source', 'chunk_size', 'file_path', 'total_pages']\n",
      "\n",
      "============================================================\n",
      "Testing Evaluation (First 10 Queries)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EVALUATING: HYBRID (Chunk Size: 512, k=20)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4aa522d91fb4c8a94c7c0a3a5335583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating hybrid:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠️ WARNING: No results were evaluated!\n",
      "This might be due to metadata mismatch.\n",
      "\n",
      "============================================================\n",
      "FULL EVALUATION - CHUNK SIZE 512\n",
      "============================================================\n",
      "\n",
      "1. Dense Retrieval...\n",
      "\n",
      "============================================================\n",
      "EVALUATING: DENSE (Chunk Size: 512, k=20)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cfc80c9b3c4c7a8647cc4fa6d774f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating dense:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠️ WARNING: No results were evaluated!\n",
      "This might be due to metadata mismatch.\n",
      "\n",
      "2. BM25 Retrieval...\n",
      "\n",
      "============================================================\n",
      "EVALUATING: BM25 (Chunk Size: 512, k=20)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8914efcda6e248778bf34b98de3c9576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating bm25:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠️ WARNING: No results were evaluated!\n",
      "This might be due to metadata mismatch.\n",
      "\n",
      "3. Hybrid Retrieval...\n",
      "\n",
      "============================================================\n",
      "EVALUATING: HYBRID (Chunk Size: 512, k=20)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4227fdee460e41188140cac9e0d8058b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating hybrid:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠️ WARNING: No results were evaluated!\n",
      "This might be due to metadata mismatch.\n",
      "\n",
      "============================================================\n",
      "COMPARISON - CHUNK SIZE 512\n",
      "============================================================\n",
      "Dense           MRR: 0.0000\n",
      "BM25            MRR: 0.0000\n",
      "Hybrid          MRR: 0.0000\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mZeroDivisionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 399\u001b[39m\n\u001b[32m    397\u001b[39m dense_mrr = comparison_512[\u001b[33m'\u001b[39m\u001b[33mDense\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    398\u001b[39m hybrid_mrr = comparison_512[\u001b[33m'\u001b[39m\u001b[33mHybrid\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m improvement = (\u001b[43m(\u001b[49m\u001b[43mhybrid_mrr\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_mrr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_mrr\u001b[49m) * \u001b[32m100\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mImprovement (Hybrid vs Dense): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimprovement\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m+.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# ### 5.8 Full Evaluation - Chunk Size 1024\u001b[39;00m\n\u001b[32m    405\u001b[39m \n\u001b[32m    406\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n",
      "\u001b[31mZeroDivisionError\u001b[39m: float division by zero"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: Evaluation Framework\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 5: Evaluation Framework\n",
    "# \n",
    "# **Goal:** Evaluate and compare Dense, BM25, and Hybrid retrieval using MRR metric\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.1 Page Matching Function\n",
    "# \n",
    "# This matches your existing evaluation logic from the thesis\n",
    "\n",
    "# %%\n",
    "def check_page_match(retrieved_page: int, evidence_page: int, chunk_size: int) -> bool:\n",
    "    \"\"\"\n",
    "    Check if retrieved page matches evidence page with tolerance based on chunk size.\n",
    "    \n",
    "    Tolerance is directional: retrieved page must be before or at evidence page.\n",
    "    \"\"\"\n",
    "    # Determine tolerance based on chunk size\n",
    "    if chunk_size <= 512:\n",
    "        tolerance = 0\n",
    "    elif chunk_size <= 1024:\n",
    "        tolerance = 1\n",
    "    elif chunk_size <= 2048:\n",
    "        tolerance = 2\n",
    "    else:\n",
    "        tolerance = 3\n",
    "    \n",
    "    # Check if retrieved page is within tolerance and before/at evidence page\n",
    "    if retrieved_page <= evidence_page and retrieved_page >= evidence_page - tolerance:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "print(\"✓ Page matching function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.2 MRR Calculation Function\n",
    "\n",
    "# %%\n",
    "def calculate_mrr(results: List[Dict], evidence_pages: List[int], doc_name: str, chunk_size: int) -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank for a single query.\n",
    "    \n",
    "    Args:\n",
    "        results: List of retrieved results with metadata\n",
    "        evidence_pages: List of evidence page numbers\n",
    "        doc_name: Expected document name\n",
    "        chunk_size: Chunk size for tolerance calculation\n",
    "    \n",
    "    Returns:\n",
    "        (reciprocal_rank, rank_of_first_match)\n",
    "    \"\"\"\n",
    "    for rank, result in enumerate(results, 1):\n",
    "        metadata = result['metadata']\n",
    "        \n",
    "        # Extract document name from file_path\n",
    "        if 'file_path' in metadata:\n",
    "            file_path = metadata['file_path']\n",
    "            # Extract filename from path: '../../financebench/documents/3M_2018_10K.pdf' -> '3M_2018_10K'\n",
    "            import os\n",
    "            filename = os.path.basename(file_path)\n",
    "            retrieved_doc = filename.replace('.pdf', '')\n",
    "        elif 'file_name' in metadata:\n",
    "            retrieved_doc = metadata['file_name']\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Check document name match\n",
    "        if retrieved_doc != doc_name:\n",
    "            continue\n",
    "        \n",
    "        # Get page number from 'source' or 'page_label'\n",
    "        if 'source' in metadata:\n",
    "            try:\n",
    "                retrieved_page = int(metadata['source'])\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        elif 'page_label' in metadata:\n",
    "            try:\n",
    "                retrieved_page = int(metadata['page_label'])\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Check if any evidence page matches\n",
    "        for evidence_page in evidence_pages:\n",
    "            if check_page_match(retrieved_page, evidence_page, chunk_size):\n",
    "                return 1.0 / rank, rank\n",
    "    \n",
    "    # No match found\n",
    "    return 0.0, -1\n",
    "\n",
    "\n",
    "print(\"✓ MRR calculation function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.3 Evaluate Single Query\n",
    "\n",
    "# %%\n",
    "def evaluate_single_query(\n",
    "    query: str,\n",
    "    doc_name: str,\n",
    "    evidence_pages: List[int],\n",
    "    vectorstore: Chroma,\n",
    "    bm25_data: Dict,\n",
    "    chunk_size: int,\n",
    "    k: int = 20,\n",
    "    method: str = \"hybrid\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a single query with specified retrieval method.\n",
    "    \n",
    "    Args:\n",
    "        method: \"dense\", \"bm25\", or \"hybrid\"\n",
    "    \"\"\"\n",
    "    # Perform retrieval based on method\n",
    "    if method == \"dense\":\n",
    "        dense_results = dense_retrieval(vectorstore, query, k=k)\n",
    "        results = [\n",
    "            {\n",
    "                'id': doc_id,\n",
    "                'text': text,\n",
    "                'metadata': metadata,\n",
    "                'dense_score': score\n",
    "            }\n",
    "            for doc_id, score, metadata, text in dense_results\n",
    "        ]\n",
    "    \n",
    "    elif method == \"bm25\":\n",
    "        bm25_results = bm25_retrieval(bm25_data, query, k=k)\n",
    "        results = [\n",
    "            {\n",
    "                'id': doc_id,\n",
    "                'text': text,\n",
    "                'metadata': metadata,\n",
    "                'bm25_score': score\n",
    "            }\n",
    "            for doc_id, score, metadata, text in bm25_results\n",
    "        ]\n",
    "    \n",
    "    elif method == \"hybrid\":\n",
    "        results = hybrid_retrieval(\n",
    "            vectorstore=vectorstore,\n",
    "            bm25_data=bm25_data,\n",
    "            query=query,\n",
    "            k_retrieve=40,\n",
    "            k_final=k,\n",
    "            rrf_k=60\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    # Calculate MRR\n",
    "    reciprocal_rank, rank = calculate_mrr(results, evidence_pages, doc_name, chunk_size)\n",
    "    \n",
    "    return {\n",
    "        'query': query,\n",
    "        'doc_name': doc_name,\n",
    "        'evidence_pages': evidence_pages,\n",
    "        'reciprocal_rank': reciprocal_rank,\n",
    "        'rank': rank,\n",
    "        'found': rank != -1\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ Evaluate single query function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.4 Evaluate All Queries\n",
    "\n",
    "# %%\n",
    "def evaluate_all_queries(\n",
    "    dataset,\n",
    "    vectorstore: Chroma,\n",
    "    bm25_data: Dict,\n",
    "    chunk_size: int,\n",
    "    method: str = \"hybrid\",\n",
    "    k: int = 20\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate all queries in dataset.\n",
    "    \n",
    "    Returns dictionary with results and summary statistics.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATING: {method.upper()} (Chunk Size: {chunk_size}, k={k})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = []\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for record in tqdm(dataset, desc=f\"Evaluating {method}\"):\n",
    "        query = record['question']\n",
    "        doc_name = record['doc_name']\n",
    "        \n",
    "        # Parse evidence pages - try both 'page_number' and 'evidence_page_num'\n",
    "        evidence = record['evidence']\n",
    "        evidence_pages = []\n",
    "        for item in evidence:\n",
    "            if 'page_number' in item:\n",
    "                evidence_pages.append(item['page_number'])\n",
    "            elif 'evidence_page_num' in item:\n",
    "                evidence_pages.append(item['evidence_page_num'])\n",
    "        \n",
    "        if not evidence_pages:\n",
    "            continue\n",
    "        \n",
    "        # Evaluate query\n",
    "        result = evaluate_single_query(\n",
    "            query=query,\n",
    "            doc_name=doc_name,\n",
    "            evidence_pages=evidence_pages,\n",
    "            vectorstore=vectorstore,\n",
    "            bm25_data=bm25_data,\n",
    "            chunk_size=chunk_size,\n",
    "            k=k,\n",
    "            method=method\n",
    "        )\n",
    "        \n",
    "        results.append(result)\n",
    "        reciprocal_ranks.append(result['reciprocal_rank'])\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    if len(results) == 0:\n",
    "        print(\"\\n⚠️ WARNING: No results were evaluated!\")\n",
    "        print(\"This might be due to metadata mismatch.\")\n",
    "        return {\n",
    "            'method': method,\n",
    "            'chunk_size': chunk_size,\n",
    "            'k': k,\n",
    "            'total_queries': 0,\n",
    "            'found_count': 0,\n",
    "            'mrr': 0.0,\n",
    "            'results': []\n",
    "        }\n",
    "    \n",
    "    mrr = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "    found_count = sum(1 for r in results if r['found'])\n",
    "    \n",
    "    summary = {\n",
    "        'method': method,\n",
    "        'chunk_size': chunk_size,\n",
    "        'k': k,\n",
    "        'total_queries': len(results),\n",
    "        'found_count': found_count,\n",
    "        'mrr': mrr,\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RESULTS SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Method: {method.upper()}\")\n",
    "    print(f\"Chunk Size: {chunk_size}\")\n",
    "    print(f\"Total Queries: {len(results)}\")\n",
    "    print(f\"Found: {found_count} ({found_count/len(results)*100:.1f}%)\")\n",
    "    print(f\"MRR: {mrr:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "print(\"✓ Evaluate all queries function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.5 Debug: Check Metadata Structure\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Debugging Metadata Structure\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check what metadata looks like in your chunks\n",
    "print(\"\\nSample metadata from chunks_512:\")\n",
    "if chunks_512['metadatas']:\n",
    "    print(f\"First metadata: {chunks_512['metadatas'][0]}\")\n",
    "    print(f\"\\nMetadata keys: {list(chunks_512['metadatas'][0].keys())}\")\n",
    "\n",
    "# Check what FinanceBench expects\n",
    "print(\"\\nSample from FinanceBench dataset:\")\n",
    "sample = dataset[0]\n",
    "print(f\"Expected doc_name: {sample['doc_name']}\")\n",
    "print(f\"Evidence: {sample['evidence']}\")\n",
    "\n",
    "# Test a query to see what we retrieve\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test Retrieval to Check Metadata\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_query = sample['question']\n",
    "print(f\"\\nQuery: {test_query}\")\n",
    "\n",
    "# Get one result and check its metadata\n",
    "test_results = hybrid_retrieval(\n",
    "    vectorstore=vectorstore_512,\n",
    "    bm25_data=bm25_index_512,\n",
    "    query=test_query,\n",
    "    k_retrieve=5,\n",
    "    k_final=3\n",
    ")\n",
    "\n",
    "if test_results:\n",
    "    print(f\"\\nFirst result metadata: {test_results[0]['metadata']}\")\n",
    "    print(f\"Expected doc_name: {sample['doc_name']}\")\n",
    "    \n",
    "    # Extract doc name from file_path\n",
    "    import os\n",
    "    if 'file_path' in test_results[0]['metadata']:\n",
    "        file_path = test_results[0]['metadata']['file_path']\n",
    "        filename = os.path.basename(file_path)\n",
    "        retrieved_doc = filename.replace('.pdf', '')\n",
    "        print(f\"Extracted doc_name: {retrieved_doc}\")\n",
    "        print(f\"Does it match? {retrieved_doc == sample['doc_name']}\")\n",
    "    \n",
    "    # Check page number\n",
    "    if 'source' in test_results[0]['metadata']:\n",
    "        retrieved_source = test_results[0]['metadata']['source']\n",
    "        print(f\"Retrieved page (source): {retrieved_source}\")\n",
    "        \n",
    "        # Get evidence pages - try both keys\n",
    "        evidence_pages = []\n",
    "        for item in sample['evidence']:\n",
    "            if 'page_number' in item:\n",
    "                evidence_pages.append(item['page_number'])\n",
    "            elif 'evidence_page_num' in item:\n",
    "                evidence_pages.append(item['evidence_page_num'])\n",
    "        print(f\"Expected evidence pages: {evidence_pages}\")\n",
    "        \n",
    "        # Check all retrieved results to see if any match\n",
    "        print(f\"\\nChecking all {len(test_results)} retrieved results:\")\n",
    "        for i, res in enumerate(test_results):\n",
    "            if 'source' in res['metadata']:\n",
    "                page = res['metadata']['source']\n",
    "                file_path = res['metadata'].get('file_path', '')\n",
    "                doc = os.path.basename(file_path).replace('.pdf', '') if file_path else 'unknown'\n",
    "                print(f\"  Result {i+1}: doc={doc}, page={page}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.5b Test MRR Calculation Directly\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing MRR Calculation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test with the first sample\n",
    "sample = dataset[0]\n",
    "print(f\"\\nQuery: {sample['question']}\")\n",
    "print(f\"Expected doc: {sample['doc_name']}\")\n",
    "\n",
    "# Get evidence pages - try both keys\n",
    "evidence_pages = []\n",
    "for item in sample['evidence']:\n",
    "    if 'page_number' in item:\n",
    "        evidence_pages.append(item['page_number'])\n",
    "    elif 'evidence_page_num' in item:\n",
    "        evidence_pages.append(item['evidence_page_num'])\n",
    "print(f\"Evidence pages: {evidence_pages}\")\n",
    "\n",
    "# Perform retrieval\n",
    "test_eval = evaluate_single_query(\n",
    "    query=sample['question'],\n",
    "    doc_name=sample['doc_name'],\n",
    "    evidence_pages=evidence_pages,\n",
    "    vectorstore=vectorstore_512,\n",
    "    bm25_data=bm25_index_512,\n",
    "    chunk_size=512,\n",
    "    k=20,\n",
    "    method=\"hybrid\"\n",
    ")\n",
    "\n",
    "print(f\"\\nResult:\")\n",
    "print(f\"  Found: {test_eval['found']}\")\n",
    "print(f\"  Rank: {test_eval['rank']}\")\n",
    "print(f\"  Reciprocal Rank: {test_eval['reciprocal_rank']:.4f}\")\n",
    "\n",
    "# Let's also check what pages were actually retrieved\n",
    "print(f\"\\nAll 20 retrieved pages from {sample['doc_name']}:\")\n",
    "test_hybrid_results = hybrid_retrieval(\n",
    "    vectorstore=vectorstore_512,\n",
    "    bm25_data=bm25_index_512,\n",
    "    query=sample['question'],\n",
    "    k_retrieve=40,\n",
    "    k_final=20\n",
    ")\n",
    "\n",
    "pages_from_correct_doc = []\n",
    "for i, res in enumerate(test_hybrid_results, 1):\n",
    "    if 'file_path' in res['metadata']:\n",
    "        file_path = res['metadata']['file_path']\n",
    "        doc = os.path.basename(file_path).replace('.pdf', '')\n",
    "        page = res['metadata'].get('source', 'N/A')\n",
    "        if doc == sample['doc_name']:\n",
    "            pages_from_correct_doc.append((i, page))\n",
    "            if int(page) == evidence_pages[0]:\n",
    "                print(f\"  ✓ Rank {i}: page {page} ← MATCH!\")\n",
    "            else:\n",
    "                print(f\"    Rank {i}: page {page}\")\n",
    "\n",
    "if not pages_from_correct_doc:\n",
    "    print(f\"  ⚠️ No pages from {sample['doc_name']} in top 20!\")\n",
    "elif evidence_pages[0] not in [int(p) for _, p in pages_from_correct_doc]:\n",
    "    print(f\"\\n  ⚠️ Evidence page {evidence_pages[0]} NOT in top 20 results\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.6 Test Evaluation on Small Sample\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing Evaluation (First 10 Queries)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test with first 10 queries\n",
    "small_dataset = dataset.select(range(10))\n",
    "\n",
    "# Test hybrid retrieval\n",
    "test_results_hybrid = evaluate_all_queries(\n",
    "    dataset=small_dataset,\n",
    "    vectorstore=vectorstore_512,\n",
    "    bm25_data=bm25_index_512,\n",
    "    chunk_size=512,\n",
    "    method=\"hybrid\",\n",
    "    k=20\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.6 Full Evaluation - Compare All Methods (Chunk 512)\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FULL EVALUATION - CHUNK SIZE 512\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dense only\n",
    "print(\"\\n1. Dense Retrieval...\")\n",
    "results_dense_512 = evaluate_all_queries(\n",
    "    dataset=dataset,\n",
    "    vectorstore=vectorstore_512,\n",
    "    bm25_data=bm25_index_512,\n",
    "    chunk_size=512,\n",
    "    method=\"dense\",\n",
    "    k=20\n",
    ")\n",
    "\n",
    "# BM25 only\n",
    "print(\"\\n2. BM25 Retrieval...\")\n",
    "results_bm25_512 = evaluate_all_queries(\n",
    "    dataset=dataset,\n",
    "    vectorstore=vectorstore_512,\n",
    "    bm25_data=bm25_index_512,\n",
    "    chunk_size=512,\n",
    "    method=\"bm25\",\n",
    "    k=20\n",
    ")\n",
    "\n",
    "# Hybrid\n",
    "print(\"\\n3. Hybrid Retrieval...\")\n",
    "results_hybrid_512 = evaluate_all_queries(\n",
    "    dataset=dataset,\n",
    "    vectorstore=vectorstore_512,\n",
    "    bm25_data=bm25_index_512,\n",
    "    chunk_size=512,\n",
    "    method=\"hybrid\",\n",
    "    k=20\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.7 Compare Results (Chunk 512)\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON - CHUNK SIZE 512\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_512 = {\n",
    "    'Dense': results_dense_512['mrr'],\n",
    "    'BM25': results_bm25_512['mrr'],\n",
    "    'Hybrid': results_hybrid_512['mrr']\n",
    "}\n",
    "\n",
    "for method, mrr in comparison_512.items():\n",
    "    print(f\"{method:15s} MRR: {mrr:.4f}\")\n",
    "\n",
    "# Calculate improvement (with safety check)\n",
    "dense_mrr = comparison_512['Dense']\n",
    "hybrid_mrr = comparison_512['Hybrid']\n",
    "\n",
    "if dense_mrr > 0:\n",
    "    improvement = ((hybrid_mrr - dense_mrr) / dense_mrr) * 100\n",
    "    print(f\"\\nImprovement (Hybrid vs Dense): {improvement:+.2f}%\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Warning: Dense MRR is 0, cannot calculate improvement percentage\")\n",
    "    print(f\"Absolute improvement: {hybrid_mrr - dense_mrr:.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.8 Full Evaluation - Chunk Size 1024\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FULL EVALUATION - CHUNK SIZE 1024\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dense only\n",
    "print(\"\\n1. Dense Retrieval...\")\n",
    "results_dense_1024 = evaluate_all_queries(\n",
    "    dataset=dataset,\n",
    "    vectorstore=vectorstore_1024,\n",
    "    bm25_data=bm25_index_1024,\n",
    "    chunk_size=1024,\n",
    "    method=\"dense\",\n",
    "    k=20\n",
    ")\n",
    "\n",
    "# BM25 only\n",
    "print(\"\\n2. BM25 Retrieval...\")\n",
    "results_bm25_1024 = evaluate_all_queries(\n",
    "    dataset=dataset,\n",
    "    vectorstore=vectorstore_1024,\n",
    "    bm25_data=bm25_index_1024,\n",
    "    chunk_size=1024,\n",
    "    method=\"bm25\",\n",
    "    k=20\n",
    ")\n",
    "\n",
    "# Hybrid\n",
    "print(\"\\n3. Hybrid Retrieval...\")\n",
    "results_hybrid_1024 = evaluate_all_queries(\n",
    "    dataset=dataset,\n",
    "    vectorstore=vectorstore_1024,\n",
    "    bm25_data=bm25_index_1024,\n",
    "    chunk_size=1024,\n",
    "    method=\"hybrid\",\n",
    "    k=20\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.9 Compare Results (Chunk 1024)\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON - CHUNK SIZE 1024\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_1024 = {\n",
    "    'Dense': results_dense_1024['mrr'],\n",
    "    'BM25': results_bm25_1024['mrr'],\n",
    "    'Hybrid': results_hybrid_1024['mrr']\n",
    "}\n",
    "\n",
    "for method, mrr in comparison_1024.items():\n",
    "    print(f\"{method:15s} MRR: {mrr:.4f}\")\n",
    "\n",
    "# Calculate improvement (with safety check)\n",
    "dense_mrr = comparison_1024['Dense']\n",
    "hybrid_mrr = comparison_1024['Hybrid']\n",
    "\n",
    "if dense_mrr > 0:\n",
    "    improvement = ((hybrid_mrr - dense_mrr) / dense_mrr) * 100\n",
    "    print(f\"\\nImprovement (Hybrid vs Dense): {improvement:+.2f}%\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Warning: Dense MRR is 0, cannot calculate improvement percentage\")\n",
    "    print(f\"Absolute improvement: {hybrid_mrr - dense_mrr:.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.10 Final Summary\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY - ALL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nChunk Size 512:\")\n",
    "for method, mrr in comparison_512.items():\n",
    "    print(f\"  {method:10s}: {mrr:.4f}\")\n",
    "\n",
    "print(\"\\nChunk Size 1024:\")\n",
    "for method, mrr in comparison_1024.items():\n",
    "    print(f\"  {method:10s}: {mrr:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ STEP 5 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"Evaluation complete! You now have MRR scores for all methods.\")\n",
    "print(\"\\nYou can use these results for your thesis comparison!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd69bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Debugging Metadata Structure\n",
      "============================================================\n",
      "\n",
      "Sample metadata from chunks_512:\n",
      "First metadata: {'total_pages': 76, 'source': '1', 'chunk_size': 512, 'file_path': '../../financebench/documents/COSTCO_2021_10K.pdf'}\n",
      "\n",
      "Metadata keys: ['total_pages', 'source', 'chunk_size', 'file_path']\n",
      "\n",
      "Sample from FinanceBench dataset:\n",
      "Expected doc_name: 3M_2018_10K\n",
      "Evidence: [{'evidence_text': 'Table of Contents \\n3M Company and Subsidiaries\\nConsolidated Statement of Cash Flow s\\nYears ended December 31\\n \\n(Millions)\\n \\n2018\\n \\n2017\\n \\n2016\\n \\nCash Flows from Operating Activities\\n \\n \\n \\n \\n \\n \\n \\nNet income including noncontrolling interest\\n \\n$\\n5,363 \\n$\\n4,869 \\n$\\n5,058 \\nAdjustments to reconcile net income including noncontrolling interest to net cash\\nprovided by operating activities\\n \\n \\n \\n \\n \\n \\n \\nDepreciation and amortization\\n \\n \\n1,488 \\n \\n1,544 \\n \\n1,474 \\nCompany pension and postretirement contributions\\n \\n \\n(370) \\n \\n(967) \\n \\n(383) \\nCompany pension and postretirement expense\\n \\n \\n410 \\n \\n334 \\n \\n250 \\nStock-based compensation expense\\n \\n \\n302 \\n \\n324 \\n \\n298 \\nGain on sale of businesses\\n \\n \\n(545) \\n \\n(586) \\n \\n(111) \\nDeferred income taxes\\n \\n \\n(57) \\n \\n107 \\n \\n 7 \\nChanges in assets and liabilities\\n \\n \\n \\n \\n \\n \\n \\nAccounts receivable\\n \\n \\n(305) \\n \\n(245) \\n \\n(313) \\nInventories\\n \\n \\n(509) \\n \\n(387) \\n \\n57 \\nAccounts payable\\n \\n \\n408 \\n \\n24 \\n \\n148 \\nAccrued income taxes (current and long-term)\\n \\n \\n134 \\n \\n967 \\n \\n101 \\nOther net\\n \\n \\n120 \\n \\n256 \\n \\n76 \\nNet cash provided by (used in) operating activities\\n \\n \\n6,439 \\n \\n6,240 \\n \\n6,662 \\n \\n \\n \\n \\n \\n \\n \\n \\nCash Flows from Investing Activities\\n \\n \\n \\n \\n \\n \\n \\nPurchases of property, plant and equipment (PP&E)\\n \\n \\n(1,577) \\n \\n(1,373) \\n \\n(1,420) \\nProceeds from sale of PP&E and other assets\\n \\n \\n262 \\n \\n49 \\n \\n58 \\nAcquisitions, net of cash acquired\\n \\n \\n13 \\n \\n(2,023) \\n \\n(16) \\nPurchases of marketable securities and investments\\n \\n \\n(1,828) \\n \\n(2,152) \\n \\n(1,410) \\nProceeds from maturities and sale of marketable securities and investments\\n \\n \\n2,497 \\n \\n1,354 \\n \\n1,247 \\nProceeds from sale of businesses, net of cash sold\\n \\n \\n846 \\n \\n1,065 \\n \\n142 \\nOther net\\n \\n \\n 9 \\n \\n(6) \\n \\n(4) \\nNet cash provided by (used in) investing activities\\n \\n \\n222 \\n \\n(3,086) \\n \\n(1,403) \\n \\n \\n \\n \\n \\n \\n \\n \\nCash Flows from Financing Activities\\n \\n \\n \\n \\n \\n \\n \\nChange in short-term debt net\\n \\n \\n(284) \\n \\n578 \\n \\n(797) \\nRepayment of debt (maturities greater than 90 days)\\n \\n \\n(1,034) \\n \\n(962) \\n \\n(992) \\nProceeds from debt (maturities greater than 90 days)\\n \\n \\n2,251 \\n \\n1,987 \\n \\n2,832 \\nPurchases of treasury stock\\n \\n \\n(4,870) \\n \\n(2,068) \\n \\n(3,753) \\nProceeds from issuance of treasury stock pursuant to stock option and benefit plans\\n \\n \\n485 \\n \\n734 \\n \\n804 \\nDividends paid to shareholders\\n \\n \\n(3,193) \\n \\n(2,803) \\n \\n(2,678) \\nOther net\\n \\n \\n(56) \\n \\n(121) \\n \\n(42) \\nNet cash provided by (used in) financing activities\\n \\n \\n(6,701) \\n \\n(2,655) \\n \\n(4,626) \\n \\n \\n \\n \\n \\n \\n \\n \\nEffect of exchange rate changes on cash and cash equivalents\\n \\n \\n(160) \\n \\n156 \\n \\n(33) \\n \\n \\n \\n \\n \\n \\n \\n \\nNet increase (decrease) in cash and cash equivalents\\n \\n \\n(200) \\n \\n655 \\n \\n600 \\nCash and cash equivalents at beginning of year\\n \\n \\n3,053 \\n \\n2,398 \\n \\n1,798 \\nCash and cash equivalents at end of period\\n \\n$\\n2,853 \\n$\\n3,053 \\n$\\n2,398 \\n \\nThe accompanying Notes to Consolidated Financial Statements are an integral part of this statement.\\n \\n60', 'doc_name': '3M_2018_10K', 'evidence_page_num': 59, 'evidence_text_full_page': 'Table of Contents \\n3M Company and Subsidiaries\\nConsolidated Statement of Cash Flow s\\nYears ended December 31\\n \\n(Millions)\\n \\n2018\\n \\n2017\\n \\n2016\\n \\nCash Flows from Operating Activities\\n \\n \\n \\n \\n \\n \\n \\nNet income including noncontrolling interest\\n \\n$\\n5,363 \\n$\\n4,869 \\n$\\n5,058 \\nAdjustments to reconcile net income including noncontrolling interest to net cash\\nprovided by operating activities\\n \\n \\n \\n \\n \\n \\n \\nDepreciation and amortization\\n \\n \\n1,488 \\n \\n1,544 \\n \\n1,474 \\nCompany pension and postretirement contributions\\n \\n \\n(370) \\n \\n(967) \\n \\n(383) \\nCompany pension and postretirement expense\\n \\n \\n410 \\n \\n334 \\n \\n250 \\nStock-based compensation expense\\n \\n \\n302 \\n \\n324 \\n \\n298 \\nGain on sale of businesses\\n \\n \\n(545) \\n \\n(586) \\n \\n(111) \\nDeferred income taxes\\n \\n \\n(57) \\n \\n107 \\n \\n 7 \\nChanges in assets and liabilities\\n \\n \\n \\n \\n \\n \\n \\nAccounts receivable\\n \\n \\n(305) \\n \\n(245) \\n \\n(313) \\nInventories\\n \\n \\n(509) \\n \\n(387) \\n \\n57 \\nAccounts payable\\n \\n \\n408 \\n \\n24 \\n \\n148 \\nAccrued income taxes (current and long-term)\\n \\n \\n134 \\n \\n967 \\n \\n101 \\nOther net\\n \\n \\n120 \\n \\n256 \\n \\n76 \\nNet cash provided by (used in) operating activities\\n \\n \\n6,439 \\n \\n6,240 \\n \\n6,662 \\n \\n \\n \\n \\n \\n \\n \\n \\nCash Flows from Investing Activities\\n \\n \\n \\n \\n \\n \\n \\nPurchases of property, plant and equipment (PP&E)\\n \\n \\n(1,577) \\n \\n(1,373) \\n \\n(1,420) \\nProceeds from sale of PP&E and other assets\\n \\n \\n262 \\n \\n49 \\n \\n58 \\nAcquisitions, net of cash acquired\\n \\n \\n13 \\n \\n(2,023) \\n \\n(16) \\nPurchases of marketable securities and investments\\n \\n \\n(1,828) \\n \\n(2,152) \\n \\n(1,410) \\nProceeds from maturities and sale of marketable securities and investments\\n \\n \\n2,497 \\n \\n1,354 \\n \\n1,247 \\nProceeds from sale of businesses, net of cash sold\\n \\n \\n846 \\n \\n1,065 \\n \\n142 \\nOther net\\n \\n \\n 9 \\n \\n(6) \\n \\n(4) \\nNet cash provided by (used in) investing activities\\n \\n \\n222 \\n \\n(3,086) \\n \\n(1,403) \\n \\n \\n \\n \\n \\n \\n \\n \\nCash Flows from Financing Activities\\n \\n \\n \\n \\n \\n \\n \\nChange in short-term debt net\\n \\n \\n(284) \\n \\n578 \\n \\n(797) \\nRepayment of debt (maturities greater than 90 days)\\n \\n \\n(1,034) \\n \\n(962) \\n \\n(992) \\nProceeds from debt (maturities greater than 90 days)\\n \\n \\n2,251 \\n \\n1,987 \\n \\n2,832 \\nPurchases of treasury stock\\n \\n \\n(4,870) \\n \\n(2,068) \\n \\n(3,753) \\nProceeds from issuance of treasury stock pursuant to stock option and benefit plans\\n \\n \\n485 \\n \\n734 \\n \\n804 \\nDividends paid to shareholders\\n \\n \\n(3,193) \\n \\n(2,803) \\n \\n(2,678) \\nOther net\\n \\n \\n(56) \\n \\n(121) \\n \\n(42) \\nNet cash provided by (used in) financing activities\\n \\n \\n(6,701) \\n \\n(2,655) \\n \\n(4,626) \\n \\n \\n \\n \\n \\n \\n \\n \\nEffect of exchange rate changes on cash and cash equivalents\\n \\n \\n(160) \\n \\n156 \\n \\n(33) \\n \\n \\n \\n \\n \\n \\n \\n \\nNet increase (decrease) in cash and cash equivalents\\n \\n \\n(200) \\n \\n655 \\n \\n600 \\nCash and cash equivalents at beginning of year\\n \\n \\n3,053 \\n \\n2,398 \\n \\n1,798 \\nCash and cash equivalents at end of period\\n \\n$\\n2,853 \\n$\\n3,053 \\n$\\n2,398 \\n \\nThe accompanying Notes to Consolidated Financial Statements are an integral part of this statement.\\n \\n60\\n \\n'}]\n",
      "\n",
      "============================================================\n",
      "Test Retrieval to Check Metadata\n",
      "============================================================\n",
      "\n",
      "Query: What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.\n",
      "\n",
      "Hybrid Retrieval for: 'What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.'\n",
      "  Retrieving top-5 from each method...\n",
      "  ✓ Dense retrieval: 5 results\n",
      "  ✓ BM25 retrieval: 5 results\n",
      "  Fusing with RRF (k=60)...\n",
      "  ✓ Hybrid results: 3 results\n",
      "\n",
      "First result metadata: {'file_path': '../../financebench/documents/3M_2018_10K.pdf', 'source': '47', 'chunk_size': 512, 'total_pages': 160}\n",
      "Expected doc_name: 3M_2018_10K\n",
      "Extracted doc_name: 3M_2018_10K\n",
      "Does it match? True\n",
      "Retrieved page (source): 47\n",
      "Expected evidence pages: [59]\n",
      "\n",
      "Checking all 3 retrieved results:\n",
      "  Result 1: doc=3M_2018_10K, page=47\n",
      "  Result 2: doc=AMCOR_2023_10K, page=146\n",
      "  Result 3: doc=3M_2018_10K, page=39\n",
      "\n",
      "============================================================\n",
      "Testing MRR Calculation\n",
      "============================================================\n",
      "\n",
      "Query: What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.\n",
      "Expected doc: 3M_2018_10K\n",
      "Evidence pages: [59]\n",
      "\n",
      "Hybrid Retrieval for: 'What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.'\n",
      "  Retrieving top-40 from each method...\n",
      "  ✓ Dense retrieval: 40 results\n",
      "  ✓ BM25 retrieval: 40 results\n",
      "  Fusing with RRF (k=60)...\n",
      "  ✓ Hybrid results: 20 results\n",
      "\n",
      "Result:\n",
      "  Found: False\n",
      "  Rank: -1\n",
      "  Reciprocal Rank: 0.0000\n",
      "\n",
      "All 20 retrieved pages from 3M_2018_10K:\n",
      "\n",
      "Hybrid Retrieval for: 'What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.'\n",
      "  Retrieving top-40 from each method...\n",
      "  ✓ Dense retrieval: 40 results\n",
      "  ✓ BM25 retrieval: 40 results\n",
      "  Fusing with RRF (k=60)...\n",
      "  ✓ Hybrid results: 20 results\n",
      "    Rank 1: page 47\n",
      "    Rank 3: page 39\n",
      "    Rank 5: page 73\n",
      "    Rank 7: page 43\n",
      "    Rank 9: page 60\n",
      "    Rank 11: page 83\n",
      "    Rank 13: page 58\n",
      "    Rank 15: page 14\n",
      "    Rank 17: page 7\n",
      "    Rank 18: page 49\n",
      "    Rank 19: page 49\n",
      "\n",
      "  ⚠️ Evidence page 59 NOT in top 20 results\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ### 5.5 Debug: Check Metadata Structure\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Debugging Metadata Structure\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check what metadata looks like in your chunks\n",
    "print(\"\\nSample metadata from chunks_512:\")\n",
    "if chunks_512['metadatas']:\n",
    "    print(f\"First metadata: {chunks_512['metadatas'][0]}\")\n",
    "    print(f\"\\nMetadata keys: {list(chunks_512['metadatas'][0].keys())}\")\n",
    "\n",
    "# Check what FinanceBench expects\n",
    "print(\"\\nSample from FinanceBench dataset:\")\n",
    "sample = dataset[0]\n",
    "print(f\"Expected doc_name: {sample['doc_name']}\")\n",
    "print(f\"Evidence: {sample['evidence']}\")\n",
    "\n",
    "# Test a query to see what we retrieve\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test Retrieval to Check Metadata\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_query = sample['question']\n",
    "print(f\"\\nQuery: {test_query}\")\n",
    "\n",
    "# Get one result and check its metadata\n",
    "test_results = hybrid_retrieval(\n",
    "    vectorstore=vectorstore_512,\n",
    "    bm25_data=bm25_index_512,\n",
    "    query=test_query,\n",
    "    k_retrieve=5,\n",
    "    k_final=3\n",
    ")\n",
    "\n",
    "if test_results:\n",
    "    print(f\"\\nFirst result metadata: {test_results[0]['metadata']}\")\n",
    "    print(f\"Expected doc_name: {sample['doc_name']}\")\n",
    "    \n",
    "    # Extract doc name from file_path\n",
    "    import os\n",
    "    if 'file_path' in test_results[0]['metadata']:\n",
    "        file_path = test_results[0]['metadata']['file_path']\n",
    "        filename = os.path.basename(file_path)\n",
    "        retrieved_doc = filename.replace('.pdf', '')\n",
    "        print(f\"Extracted doc_name: {retrieved_doc}\")\n",
    "        print(f\"Does it match? {retrieved_doc == sample['doc_name']}\")\n",
    "    \n",
    "    # Check page number\n",
    "    if 'source' in test_results[0]['metadata']:\n",
    "        retrieved_source = test_results[0]['metadata']['source']\n",
    "        print(f\"Retrieved page (source): {retrieved_source}\")\n",
    "        \n",
    "        # Get evidence pages - try both keys\n",
    "        evidence_pages = []\n",
    "        for item in sample['evidence']:\n",
    "            if 'page_number' in item:\n",
    "                evidence_pages.append(item['page_number'])\n",
    "            elif 'evidence_page_num' in item:\n",
    "                evidence_pages.append(item['evidence_page_num'])\n",
    "        print(f\"Expected evidence pages: {evidence_pages}\")\n",
    "        \n",
    "        # Check all retrieved results to see if any match\n",
    "        print(f\"\\nChecking all {len(test_results)} retrieved results:\")\n",
    "        for i, res in enumerate(test_results):\n",
    "            if 'source' in res['metadata']:\n",
    "                page = res['metadata']['source']\n",
    "                file_path = res['metadata'].get('file_path', '')\n",
    "                doc = os.path.basename(file_path).replace('.pdf', '') if file_path else 'unknown'\n",
    "                print(f\"  Result {i+1}: doc={doc}, page={page}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5.5b Test MRR Calculation Directly\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing MRR Calculation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test with the first sample\n",
    "sample = dataset[0]\n",
    "print(f\"\\nQuery: {sample['question']}\")\n",
    "print(f\"Expected doc: {sample['doc_name']}\")\n",
    "\n",
    "# Get evidence pages - try both keys\n",
    "evidence_pages = []\n",
    "for item in sample['evidence']:\n",
    "    if 'page_number' in item:\n",
    "        evidence_pages.append(item['page_number'])\n",
    "    elif 'evidence_page_num' in item:\n",
    "        evidence_pages.append(item['evidence_page_num'])\n",
    "print(f\"Evidence pages: {evidence_pages}\")\n",
    "\n",
    "# Perform retrieval\n",
    "test_eval = evaluate_single_query(\n",
    "    query=sample['question'],\n",
    "    doc_name=sample['doc_name'],\n",
    "    evidence_pages=evidence_pages,\n",
    "    vectorstore=vectorstore_512,\n",
    "    bm25_data=bm25_index_512,\n",
    "    chunk_size=512,\n",
    "    k=20,\n",
    "    method=\"hybrid\"\n",
    ")\n",
    "\n",
    "print(f\"\\nResult:\")\n",
    "print(f\"  Found: {test_eval['found']}\")\n",
    "print(f\"  Rank: {test_eval['rank']}\")\n",
    "print(f\"  Reciprocal Rank: {test_eval['reciprocal_rank']:.4f}\")\n",
    "\n",
    "# Let's also check what pages were actually retrieved\n",
    "print(f\"\\nAll 20 retrieved pages from {sample['doc_name']}:\")\n",
    "test_hybrid_results = hybrid_retrieval(\n",
    "    vectorstore=vectorstore_512,\n",
    "    bm25_data=bm25_index_512,\n",
    "    query=sample['question'],\n",
    "    k_retrieve=40,\n",
    "    k_final=20\n",
    ")\n",
    "\n",
    "pages_from_correct_doc = []\n",
    "for i, res in enumerate(test_hybrid_results, 1):\n",
    "    if 'file_path' in res['metadata']:\n",
    "        file_path = res['metadata']['file_path']\n",
    "        doc = os.path.basename(file_path).replace('.pdf', '')\n",
    "        page = res['metadata'].get('source', 'N/A')\n",
    "        if doc == sample['doc_name']:\n",
    "            pages_from_correct_doc.append((i, page))\n",
    "            if int(page) == evidence_pages[0]:\n",
    "                print(f\"  ✓ Rank {i}: page {page} ← MATCH!\")\n",
    "            else:\n",
    "                print(f\"    Rank {i}: page {page}\")\n",
    "\n",
    "if not pages_from_correct_doc:\n",
    "    print(f\"  ⚠️ No pages from {sample['doc_name']} in top 20!\")\n",
    "elif evidence_pages[0] not in [int(p) for _, p in pages_from_correct_doc]:\n",
    "    print(f\"\\n  ⚠️ Evidence page {evidence_pages[0]} NOT in top 20 results\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
