{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fab246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n",
      "✓ OpenAI API key loaded\n",
      "✓ VoyageAI API key loaded\n",
      "✓ Ollama URL: http://localhost:11434\n",
      "✓ Configuration set\n",
      "  Vector DB Directory: ../../vector_databases\n",
      "  Output Directory: ../../evaluation_results/reranking\n",
      "Loading FinanceBench dataset...\n",
      "✓ Loaded 150 queries\n",
      "\n",
      "Sample query:\n",
      "  ID: financebench_id_03029\n",
      "  Company: 3M\n",
      "  Question: What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the quest...\n",
      "  Doc: 3M_2018_10K\n",
      "  Evidence items: 1\n",
      "\n",
      "✓ Step 1 complete!\n",
      "  Environment variables loaded\n",
      "  Paths configured\n",
      "  Dataset loaded: 150 queries\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Re-Ranking Evaluation Notebook - Step 1: Setup and Imports\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # Re-Ranking Evaluation for RAG Systems\n",
    "# \n",
    "# This notebook evaluates the impact of re-ranking on retrieval performance.\n",
    "# It compares baseline retrieval against two-stage retrieval (retrieve + re-rank).\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1.1: Imports\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Progress\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Data\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Vector stores\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "\n",
    "# Re-ranking\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_voyageai import VoyageAIRerank\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1.2: Load Environment Variables\n",
    "\n",
    "# %%\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "VOYAGE_API_KEY = os.getenv(\"VOYAGE_API_KEY\")\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    print(\"✓ OpenAI API key loaded\")\n",
    "else:\n",
    "    print(\"⚠ OpenAI API key not found (only needed if using OpenAI embeddings)\")\n",
    "\n",
    "if VOYAGE_API_KEY:\n",
    "    print(\"✓ VoyageAI API key loaded\")\n",
    "else:\n",
    "    print(\"⚠ VoyageAI API key not found (only needed if using VoyageAI embeddings/reranking)\")\n",
    "\n",
    "print(f\"✓ Ollama URL: {OLLAMA_BASE_URL}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1.3: Configuration Variables\n",
    "\n",
    "# %%\n",
    "# Paths\n",
    "VECTOR_DB_BASE_DIR = \"../../vector_databases\"\n",
    "OUTPUT_DIR = \"../../evaluation_results/reranking\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = \"PatronusAI/financebench\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "\n",
    "# Collection settings\n",
    "COLLECTION_PREFIX = \"financebench_docs_chunk_\"\n",
    "\n",
    "print(\"✓ Configuration set\")\n",
    "print(f\"  Vector DB Directory: {VECTOR_DB_BASE_DIR}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1.4: Load FinanceBench Dataset\n",
    "\n",
    "# %%\n",
    "print(\"Loading FinanceBench dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME, split=DATASET_SPLIT)\n",
    "print(f\"✓ Loaded {len(dataset)} queries\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample query:\")\n",
    "sample = dataset[0]\n",
    "print(f\"  ID: {sample['financebench_id']}\")\n",
    "print(f\"  Company: {sample['company']}\")\n",
    "print(f\"  Question: {sample['question'][:100]}...\")\n",
    "print(f\"  Doc: {sample['doc_name']}\")\n",
    "print(f\"  Evidence items: {len(sample['evidence'])}\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 1 complete!\")\n",
    "print(\"  Environment variables loaded\")\n",
    "print(\"  Paths configured\")\n",
    "print(f\"  Dataset loaded: {len(dataset)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d819cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Metadata extraction functions defined\n",
      "✓ Vector store loading functions defined\n",
      "✓ Evidence matching functions defined\n",
      "✓ File management functions defined\n",
      "\n",
      "✓ Step 2 complete!\n",
      "  Metadata extraction: extract_doc_name_from_path, extract_metadata_from_retrieved_doc\n",
      "  Vector store: get_embedding_function, load_vectorstore\n",
      "  Evidence matching: check_match, calculate_mrr_for_query\n",
      "  File management: get_output_filename, check_if_results_exist, save_results\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Helper Functions\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 2.1: Metadata Extraction\n",
    "\n",
    "# %%\n",
    "def extract_doc_name_from_path(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract document name from file path.\n",
    "    \n",
    "    Example: \"../../financebench/documents/3M_2018_10K.pdf\" -> \"3M_2018_10K\"\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(file_path)\n",
    "    doc_name = filename.replace('.pdf', '')\n",
    "    return doc_name\n",
    "\n",
    "\n",
    "def extract_metadata_from_retrieved_doc(doc) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract relevant metadata from retrieved document.\n",
    "    \n",
    "    Returns dict with: doc_name, page_number, content\n",
    "    \"\"\"\n",
    "    file_path = doc.metadata.get('file_path', '')\n",
    "    doc_name = extract_doc_name_from_path(file_path)\n",
    "    page_num = doc.metadata.get('source', -1)\n",
    "    \n",
    "    # Ensure page_num is an integer\n",
    "    if isinstance(page_num, str):\n",
    "        page_num = int(page_num)\n",
    "    \n",
    "    return {\n",
    "        'doc_name': doc_name,\n",
    "        'page_number': page_num,\n",
    "        'content': doc.page_content\n",
    "    }\n",
    "\n",
    "print(\"✓ Metadata extraction functions defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 2.2: Vector Store Loading\n",
    "\n",
    "# %%\n",
    "def get_embedding_function(provider: str, model: str):\n",
    "    \"\"\"Get embedding function for a provider/model.\"\"\"\n",
    "    if provider == \"ollama\":\n",
    "        return OllamaEmbeddings(model=model, base_url=OLLAMA_BASE_URL)\n",
    "    elif provider == \"openai\":\n",
    "        return OpenAIEmbeddings(model=model, openai_api_key=OPENAI_API_KEY)\n",
    "    elif provider == \"voyage\":\n",
    "        return VoyageAIEmbeddings(model=model, voyage_api_key=VOYAGE_API_KEY)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "\n",
    "def load_vectorstore(\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    base_dir: str = VECTOR_DB_BASE_DIR,\n",
    "    collection_prefix: str = COLLECTION_PREFIX\n",
    ") -> Chroma:\n",
    "    \"\"\"\n",
    "    Load a vector store.\n",
    "    \n",
    "    Args:\n",
    "        provider: \"ollama\", \"openai\", or \"voyage\"\n",
    "        model: Model name\n",
    "        chunk_size: Chunk size (must match existing collection)\n",
    "        base_dir: Base directory for vector databases\n",
    "        collection_prefix: Collection name prefix\n",
    "        \n",
    "    Returns:\n",
    "        Chroma vectorstore instance\n",
    "    \"\"\"\n",
    "    model_id = f\"{provider}_{model.replace('/', '_')}\"\n",
    "    db_path = os.path.join(base_dir, model_id)\n",
    "    collection_name = f\"{collection_prefix}{chunk_size}\"\n",
    "    \n",
    "    emb_fn = get_embedding_function(provider, model)\n",
    "    \n",
    "    vectorstore = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=emb_fn,\n",
    "        persist_directory=db_path\n",
    "    )\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "print(\"✓ Vector store loading functions defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 2.3: Evidence Matching\n",
    "\n",
    "# %%\n",
    "def check_match(\n",
    "    retrieved_doc: Dict, \n",
    "    evidence_list: List[Dict],\n",
    "    chunk_size: int = 512,\n",
    "    use_page_tolerance: bool = True\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Check if retrieved document matches any evidence.\n",
    "    \n",
    "    Uses chunk-size-aware page tolerance:\n",
    "    - chunk_size <= 512: tolerance = 0 (exact match)\n",
    "    - chunk_size 513-1024: tolerance = 1\n",
    "    - chunk_size 1025-2048: tolerance = 2\n",
    "    - chunk_size > 2048: tolerance = 2\n",
    "    \n",
    "    Args:\n",
    "        retrieved_doc: {doc_name, page_number}\n",
    "        evidence_list: Ground truth evidence\n",
    "        chunk_size: Chunk size (for page tolerance calculation)\n",
    "        use_page_tolerance: If True, use chunk-size-aware tolerance\n",
    "        \n",
    "    Returns:\n",
    "        True if match found\n",
    "    \"\"\"\n",
    "    retrieved_doc_name = retrieved_doc['doc_name']\n",
    "    retrieved_page = retrieved_doc['page_number']\n",
    "    \n",
    "    # Calculate page tolerance based on chunk size\n",
    "    if use_page_tolerance:\n",
    "        if chunk_size <= 512:\n",
    "            page_tolerance = 0\n",
    "        elif chunk_size <= 1024:\n",
    "            page_tolerance = 1\n",
    "        elif chunk_size <= 2048:\n",
    "            page_tolerance = 2\n",
    "        else:\n",
    "            page_tolerance = 2\n",
    "    else:\n",
    "        page_tolerance = 0  # Exact match only\n",
    "    \n",
    "    for evidence in evidence_list:\n",
    "        evidence_doc_name = evidence['doc_name']\n",
    "        evidence_page = evidence['evidence_page_num'] + 1  # Convert 0-indexed to 1-indexed\n",
    "        \n",
    "        # Check document name match\n",
    "        if retrieved_doc_name != evidence_doc_name:\n",
    "            continue\n",
    "        \n",
    "        # Check page match with tolerance\n",
    "        # Only match if retrieved page is BEFORE or AT evidence page\n",
    "        if retrieved_page <= evidence_page <= retrieved_page + page_tolerance:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def calculate_mrr_for_query(\n",
    "    retrieved_docs: List[Dict], \n",
    "    evidence_list: List[Dict],\n",
    "    chunk_size: int = 512,\n",
    "    use_page_tolerance: bool = True\n",
    ") -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    Calculate MRR for a single query.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_docs: List of {doc_name, page_number}\n",
    "        evidence_list: Ground truth evidence\n",
    "        chunk_size: Chunk size (for page tolerance)\n",
    "        use_page_tolerance: If True, use chunk-size-aware tolerance\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (mrr_score, rank)\n",
    "        - mrr_score: 1/rank if found, 0 if not found\n",
    "        - rank: Position of first match (1-indexed), -1 if not found\n",
    "    \"\"\"\n",
    "    for rank, retrieved_doc in enumerate(retrieved_docs, start=1):\n",
    "        if check_match(retrieved_doc, evidence_list, chunk_size, use_page_tolerance):\n",
    "            mrr_score = 1.0 / rank\n",
    "            return mrr_score, rank\n",
    "    \n",
    "    # No match found\n",
    "    return 0.0, -1\n",
    "\n",
    "print(\"✓ Evidence matching functions defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 2.4: File Management\n",
    "\n",
    "# %%\n",
    "def get_reranker_short_name(reranker_model: str) -> str:\n",
    "    \"\"\"\n",
    "    Get short name for reranker model for filename.\n",
    "    \n",
    "    Examples:\n",
    "        'cross-encoder/ms-marco-MiniLM-L-12-v2' -> 'ms-marco-L-12'\n",
    "        'voyage-rerank-2.5' -> 'voyage-rerank-2.5'\n",
    "    \"\"\"\n",
    "    name_mapping = {\n",
    "        'cross-encoder/ms-marco-MiniLM-L-6-v2': 'ms-marco-L-6',\n",
    "        'cross-encoder/ms-marco-MiniLM-L-12-v2': 'ms-marco-L-12',\n",
    "        'BAAI/bge-reranker-base': 'bge-base',\n",
    "        'BAAI/bge-reranker-large': 'bge-large',\n",
    "        'voyage-rerank-2.5': 'voyage-rerank-2.5',\n",
    "        'voyage-rerank-2.5-lite': 'voyage-rerank-2.5-lite',\n",
    "    }\n",
    "    return name_mapping.get(reranker_model, reranker_model.replace('/', '_'))\n",
    "\n",
    "\n",
    "def get_output_filename(\n",
    "    provider: str, \n",
    "    model: str, \n",
    "    chunk_size: int, \n",
    "    k: int, \n",
    "    mode: str,\n",
    "    reranker_model: Optional[str] = None,\n",
    "    k_rerank: Optional[int] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate output filename.\n",
    "    \n",
    "    Examples:\n",
    "        Baseline: \"voyage_voyage-finance-2_chunk512_k40_global.json\"\n",
    "        Rerank:   \"voyage_voyage-finance-2_chunk512_k40_global_rerank_k10-ms-marco-L-12.json\"\n",
    "    \n",
    "    Args:\n",
    "        provider: Embedding provider\n",
    "        model: Embedding model name\n",
    "        chunk_size: Chunk size\n",
    "        k: k_retrieve for baseline, or k_retrieve for rerank\n",
    "        mode: \"global\" or \"singledoc\"\n",
    "        reranker_model: Re-ranker model name (None for baseline)\n",
    "        k_rerank: Number kept after re-ranking (only for rerank files)\n",
    "    \"\"\"\n",
    "    model_clean = model.replace('/', '_')\n",
    "    \n",
    "    if reranker_model:\n",
    "        # Re-ranking file: show both k_retrieve and k_rerank\n",
    "        reranker_short = get_reranker_short_name(reranker_model)\n",
    "        filename = f\"{provider}_{model_clean}_chunk{chunk_size}_k{k}_{mode}_rerank_k{k_rerank}-{reranker_short}.json\"\n",
    "    else:\n",
    "        # Baseline file: only show k_retrieve\n",
    "        filename = f\"{provider}_{model_clean}_chunk{chunk_size}_k{k}_{mode}.json\"\n",
    "    \n",
    "    return filename\n",
    "\n",
    "\n",
    "def check_if_results_exist(\n",
    "    provider: str, \n",
    "    model: str, \n",
    "    chunk_size: int, \n",
    "    k: int, \n",
    "    mode: str,\n",
    "    output_dir: str,\n",
    "    reranker_model: Optional[str] = None,\n",
    "    k_rerank: Optional[int] = None\n",
    ") -> bool:\n",
    "    \"\"\"Check if results JSON already exists.\"\"\"\n",
    "    filename = get_output_filename(provider, model, chunk_size, k, mode, reranker_model, k_rerank)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    return os.path.exists(filepath)\n",
    "\n",
    "\n",
    "def save_results(\n",
    "    results: List[Dict], \n",
    "    provider: str, \n",
    "    model: str, \n",
    "    chunk_size: int, \n",
    "    k: int, \n",
    "    mode: str,\n",
    "    output_dir: str,\n",
    "    reranker_model: Optional[str] = None,\n",
    "    k_rerank: Optional[int] = None\n",
    "):\n",
    "    \"\"\"Save results to JSON file.\"\"\"\n",
    "    filename = get_output_filename(provider, model, chunk_size, k, mode, reranker_model, k_rerank)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Saved: {filename}\")\n",
    "\n",
    "print(\"✓ File management functions defined\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 2 complete!\")\n",
    "print(\"  Metadata extraction: extract_doc_name_from_path, extract_metadata_from_retrieved_doc\")\n",
    "print(\"  Vector store: get_embedding_function, load_vectorstore\")\n",
    "print(\"  Evidence matching: check_match, calculate_mrr_for_query\")\n",
    "print(\"  File management: get_output_filename, check_if_results_exist, save_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8acb100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Global retrieval function defined\n",
      "✓ Single-document retrieval function defined\n",
      "✓ Unified retrieval interface defined\n",
      "\\nTesting retrieval functions...\n",
      "Loading vectorstore: voyage/voyage-3-large, chunk=512\n",
      "\\nTest query: What was the capital expenditure in 2018?\n",
      "\\n--- Global Retrieval (k=5) ---\n",
      "1. BOEING_2018_10K, page 39, score: 0.7928\n",
      "2. CVSHEALTH_2018_10K, page 280, score: 0.8283\n",
      "3. MGMRESORTS_2018_10K, page 46, score: 0.8411\n",
      "4. ACTIVISIONBLIZZARD_2019_10K, page 52, score: 0.8509\n",
      "5. CORNING_2020_10K, page 126, score: 0.8647\n",
      "\\n--- Single-Doc Retrieval (k=5, doc='3M_2018_10K') ---\n",
      "1. 3M_2018_10K, page 39, score: 0.9703\n",
      "2. 3M_2018_10K, page 47, score: 0.9908\n",
      "3. 3M_2018_10K, page 109, score: 1.0054\n",
      "4. 3M_2018_10K, page 81, score: 1.0193\n",
      "\\n✓ Retrieval functions working correctly!\n",
      "\n",
      "✓ Step 3 complete!\n",
      "  Global retrieval: retrieve_global_with_scores\n",
      "  Single-doc retrieval: retrieve_single_doc_with_scores\n",
      "  Unified interface: retrieve_with_scores\n",
      "\n",
      "These functions return documents with similarity scores that will be\n",
      "preserved as 'initial_score' when we apply re-ranking in Step 4.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Baseline Retrieval Functions\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3.1: Global Retrieval with Scores\n",
    "\n",
    "# %%\n",
    "def retrieve_global_with_scores(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve documents globally (search all documents) with similarity scores.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: ChromaDB vectorstore instance\n",
    "        query: Search query\n",
    "        k: Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        List of {doc_name, page_number, content, rank, score}\n",
    "    \"\"\"\n",
    "    # Use similarity_search_with_score to get scores\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    retrieved = []\n",
    "    for rank, (doc, score) in enumerate(results, start=1):\n",
    "        metadata = extract_metadata_from_retrieved_doc(doc)\n",
    "        metadata['rank'] = rank\n",
    "        metadata['score'] = float(score)\n",
    "        retrieved.append(metadata)\n",
    "    \n",
    "    return retrieved\n",
    "\n",
    "print(\"✓ Global retrieval function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3.2: Single-Document Retrieval with Scores\n",
    "\n",
    "# %%\n",
    "def retrieve_single_doc_with_scores(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    target_doc_name: str,\n",
    "    k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve documents filtered to single document with scores.\n",
    "    \n",
    "    Since ChromaDB doesn't support filtering by document name directly,\n",
    "    we retrieve more documents (k * 10) and filter them post-retrieval.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: ChromaDB vectorstore instance\n",
    "        query: Search query\n",
    "        target_doc_name: Document name to filter to (e.g., \"3M_2018_10K\")\n",
    "        k: Number of documents to return after filtering\n",
    "        \n",
    "    Returns:\n",
    "        List of {doc_name, page_number, content, rank, score}\n",
    "    \"\"\"\n",
    "    # Retrieve more documents than needed to ensure we get enough from target doc\n",
    "    fetch_k = min(k * 10, 100)  # Fetch up to 10x k, max 100\n",
    "    \n",
    "    results = vectorstore.similarity_search_with_score(query, k=fetch_k)\n",
    "    \n",
    "    # Filter to only documents from target doc\n",
    "    filtered = []\n",
    "    for doc, score in results:\n",
    "        metadata = extract_metadata_from_retrieved_doc(doc)\n",
    "        if metadata['doc_name'] == target_doc_name:\n",
    "            metadata['score'] = float(score)\n",
    "            filtered.append(metadata)\n",
    "            if len(filtered) >= k:\n",
    "                break\n",
    "    \n",
    "    # Add rank after filtering\n",
    "    for rank, doc_meta in enumerate(filtered[:k], start=1):\n",
    "        doc_meta['rank'] = rank\n",
    "    \n",
    "    # Return top k from target document\n",
    "    return filtered[:k]\n",
    "\n",
    "print(\"✓ Single-document retrieval function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3.3: Unified Retrieval Interface\n",
    "\n",
    "# %%\n",
    "def retrieve_with_scores(\n",
    "    vectorstore: Chroma,\n",
    "    query: str,\n",
    "    k: int,\n",
    "    mode: str,\n",
    "    target_doc_name: Optional[str] = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Unified interface for retrieval with scores.\n",
    "    \n",
    "    Args:\n",
    "        vectorstore: ChromaDB vectorstore instance\n",
    "        query: Search query\n",
    "        k: Number of documents to retrieve\n",
    "        mode: \"global\" or \"singledoc\"\n",
    "        target_doc_name: Required if mode is \"singledoc\"\n",
    "        \n",
    "    Returns:\n",
    "        List of {doc_name, page_number, content, rank, score}\n",
    "    \"\"\"\n",
    "    if mode == \"global\":\n",
    "        return retrieve_global_with_scores(vectorstore, query, k)\n",
    "    elif mode == \"singledoc\":\n",
    "        if not target_doc_name:\n",
    "            raise ValueError(\"target_doc_name required for singledoc mode\")\n",
    "        return retrieve_single_doc_with_scores(vectorstore, query, target_doc_name, k)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}. Must be 'global' or 'singledoc'\")\n",
    "\n",
    "print(\"✓ Unified retrieval interface defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3.4: Test Retrieval (Optional)\n",
    "\n",
    "# %%\n",
    "# Uncomment to test retrieval functions\n",
    "\n",
    "# Test with a sample configuration\n",
    "test_provider = \"voyage\"\n",
    "test_model = \"voyage-3-large\"\n",
    "test_chunk_size = 512\n",
    "\n",
    "print(\"\\\\nTesting retrieval functions...\")\n",
    "print(f\"Loading vectorstore: {test_provider}/{test_model}, chunk={test_chunk_size}\")\n",
    "\n",
    "try:\n",
    "    test_vs = load_vectorstore(test_provider, test_model, test_chunk_size)\n",
    "    test_query = \"What was the capital expenditure in 2018?\"\n",
    "    \n",
    "    print(f\"\\\\nTest query: {test_query}\")\n",
    "    \n",
    "    # Test global retrieval\n",
    "    print(\"\\\\n--- Global Retrieval (k=5) ---\")\n",
    "    results_global = retrieve_global_with_scores(test_vs, test_query, k=5)\n",
    "    for i, doc in enumerate(results_global, 1):\n",
    "        print(f\"{i}. {doc['doc_name']}, page {doc['page_number']}, score: {doc['score']:.4f}\")\n",
    "    \n",
    "    # Test single-doc retrieval\n",
    "    print(\"\\\\n--- Single-Doc Retrieval (k=5, doc='3M_2018_10K') ---\")\n",
    "    results_single = retrieve_single_doc_with_scores(test_vs, test_query, \"3M_2018_10K\", k=5)\n",
    "    for i, doc in enumerate(results_single, 1):\n",
    "        print(f\"{i}. {doc['doc_name']}, page {doc['page_number']}, score: {doc['score']:.4f}\")\n",
    "    \n",
    "    print(\"\\\\n✓ Retrieval functions working correctly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Test failed: {e}\")\n",
    "\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 3 complete!\")\n",
    "print(\"  Global retrieval: retrieve_global_with_scores\")\n",
    "print(\"  Single-doc retrieval: retrieve_single_doc_with_scores\")\n",
    "print(\"  Unified interface: retrieve_with_scores\")\n",
    "print(\"\\nThese functions return documents with similarity scores that will be\")\n",
    "print(\"preserved as 'initial_score' when we apply re-ranking in Step 4.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 4: Re-Ranking Functions\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 4.1: Cross-Encoder Re-Ranking (Hugging Face Models)\n",
    "\n",
    "# %%\n",
    "def rerank_with_cross_encoder(\n",
    "    query: str,\n",
    "    retrieved_docs: List[Dict],\n",
    "    reranker_model: str,\n",
    "    top_k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Re-rank documents using a cross-encoder model from Hugging Face.\n",
    "    \n",
    "    Cross-encoders process query and document together, providing more\n",
    "    accurate relevance scores than bi-encoders (embeddings).\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        retrieved_docs: List from baseline retrieval (with 'content', 'rank', 'score')\n",
    "        reranker_model: Hugging Face model name (e.g., 'cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "        top_k: Number of documents to return after re-ranking\n",
    "        \n",
    "    Returns:\n",
    "        Re-ranked list with {doc_name, page_number, rank, initial_rank, initial_score, rerank_score}\n",
    "    \"\"\"\n",
    "    from sentence_transformers import CrossEncoder\n",
    "    \n",
    "    # Initialize cross-encoder\n",
    "    cross_encoder = CrossEncoder(reranker_model)\n",
    "    \n",
    "    # Prepare query-document pairs for cross-encoder\n",
    "    pairs = [[query, doc.get('content', '')] for doc in retrieved_docs]\n",
    "    \n",
    "    # Get cross-encoder relevance scores\n",
    "    rerank_scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Preserve initial ranking information and add rerank scores\n",
    "    for doc, rerank_score in zip(retrieved_docs, rerank_scores):\n",
    "        doc['initial_rank'] = doc['rank']\n",
    "        doc['initial_score'] = doc['score']\n",
    "        doc['rerank_score'] = float(rerank_score)\n",
    "        # Remove temporary fields\n",
    "        del doc['rank']\n",
    "        del doc['score']\n",
    "        # Remove content to save space in JSON (optional)\n",
    "        if 'content' in doc:\n",
    "            del doc['content']\n",
    "    \n",
    "    # Sort by rerank score (descending - higher is more relevant)\n",
    "    reranked = sorted(retrieved_docs, key=lambda x: x['rerank_score'], reverse=True)\n",
    "    \n",
    "    # Assign new ranks\n",
    "    for rank, doc in enumerate(reranked[:top_k], start=1):\n",
    "        doc['rank'] = rank\n",
    "    \n",
    "    return reranked[:top_k]\n",
    "\n",
    "print(\"✓ Cross-encoder re-ranking function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 4.2: Voyage AI Re-Ranking (API-based)\n",
    "\n",
    "# %%\n",
    "def rerank_with_voyage(\n",
    "    query: str,\n",
    "    retrieved_docs: List[Dict],\n",
    "    reranker_model: str,\n",
    "    top_k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Re-rank documents using Voyage AI reranker API.\n",
    "    \n",
    "    Voyage provides specialized rerankers optimized for different domains.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        retrieved_docs: List from baseline retrieval (with 'content', 'rank', 'score')\n",
    "        reranker_model: Voyage model name (e.g., 'voyage-rerank-2', 'voyage-rerank-2-lite')\n",
    "        top_k: Number of documents to return after re-ranking\n",
    "        \n",
    "    Returns:\n",
    "        Re-ranked list with {doc_name, page_number, rank, initial_rank, initial_score, rerank_score}\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document\n",
    "    from langchain_voyageai import VoyageAIRerank\n",
    "    \n",
    "    # Convert to LangChain documents (required by VoyageAIRerank)\n",
    "    lc_docs = [\n",
    "        Document(\n",
    "            page_content=doc['content'],\n",
    "            metadata={\n",
    "                'doc_name': doc['doc_name'], \n",
    "                'page_number': doc['page_number'],\n",
    "                'initial_rank': doc['rank'],\n",
    "                'initial_score': doc['score']\n",
    "            }\n",
    "        )\n",
    "        for doc in retrieved_docs\n",
    "    ]\n",
    "    \n",
    "    # Initialize Voyage reranker\n",
    "    # Extract model name (e.g., \"rerank-2\" from \"voyage-rerank-2\")\n",
    "    model_name = reranker_model.replace('voyage-', '')\n",
    "    \n",
    "    reranker = VoyageAIRerank(\n",
    "        model=model_name,\n",
    "        voyage_api_key=VOYAGE_API_KEY,\n",
    "        top_k=top_k\n",
    "    )\n",
    "    \n",
    "    # Rerank documents\n",
    "    reranked_docs = reranker.compress_documents(lc_docs, query)\n",
    "    \n",
    "    # Convert back to our format\n",
    "    results = []\n",
    "    for rank, doc in enumerate(reranked_docs, start=1):\n",
    "        result = {\n",
    "            'doc_name': doc.metadata['doc_name'],\n",
    "            'page_number': doc.metadata['page_number'],\n",
    "            'rank': rank,\n",
    "            'initial_rank': doc.metadata['initial_rank'],\n",
    "            'initial_score': doc.metadata['initial_score'],\n",
    "            'rerank_score': doc.metadata.get('relevance_score', 0.0)\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Voyage AI re-ranking function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 4.3: Unified Re-Ranking Interface\n",
    "\n",
    "# %%\n",
    "def rerank_documents(\n",
    "    query: str,\n",
    "    retrieved_docs: List[Dict],\n",
    "    reranker_model: str,\n",
    "    top_k: int\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Unified interface for re-ranking documents.\n",
    "    \n",
    "    Automatically selects the appropriate re-ranker based on model name:\n",
    "    - Voyage models (starting with 'voyage-'): Use Voyage API\n",
    "    - Others: Use Hugging Face cross-encoders\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        retrieved_docs: List from baseline retrieval\n",
    "        reranker_model: Model name (e.g., 'voyage-rerank-2' or 'cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "        top_k: Number of documents to return after re-ranking\n",
    "        \n",
    "    Returns:\n",
    "        Re-ranked list with scores and ranking information\n",
    "    \"\"\"\n",
    "    if reranker_model.startswith('voyage-'):\n",
    "        return rerank_with_voyage(query, retrieved_docs, reranker_model, top_k)\n",
    "    else:\n",
    "        return rerank_with_cross_encoder(query, retrieved_docs, reranker_model, top_k)\n",
    "\n",
    "print(\"✓ Unified re-ranking interface defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 4.4: Test Re-Ranking (Optional)\n",
    "\n",
    "# %%\n",
    "# Uncomment to test re-ranking functions\n",
    "\n",
    "# Test with a sample configuration\n",
    "test_provider = \"voyage\"\n",
    "test_model = \"voyage-finance-2\"\n",
    "test_chunk_size = 512\n",
    "test_reranker = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"  # or \"voyage-rerank-2\"\n",
    "\n",
    "print(\"\\\\nTesting re-ranking functions...\")\n",
    "print(f\"Embedding: {test_provider}/{test_model}, chunk={test_chunk_size}\")\n",
    "print(f\"Re-ranker: {test_reranker}\")\n",
    "\n",
    "try:\n",
    "    # Load vectorstore and retrieve\n",
    "    test_vs = load_vectorstore(test_provider, test_model, test_chunk_size)\n",
    "    test_query = \"What was the capital expenditure in 2018?\"\n",
    "    \n",
    "    print(f\"\\\\nTest query: {test_query}\")\n",
    "    print(\"\\\\n--- Step 1: Baseline Retrieval (k=20) ---\")\n",
    "    \n",
    "    baseline_results = retrieve_global_with_scores(test_vs, test_query, k=20)\n",
    "    \n",
    "    print(\"Top 5 from baseline:\")\n",
    "    for i, doc in enumerate(baseline_results[:5], 1):\n",
    "        print(f\"{i}. {doc['doc_name']}, page {doc['page_number']}, score: {doc['score']:.4f}\")\n",
    "    \n",
    "    print(\"\\\\n--- Step 2: Re-Ranking (top_k=10) ---\")\n",
    "    \n",
    "    reranked_results = rerank_documents(\n",
    "        query=test_query,\n",
    "        retrieved_docs=baseline_results,\n",
    "        reranker_model=test_reranker,\n",
    "        top_k=10\n",
    "    )\n",
    "    \n",
    "    print(\"Top 5 after re-ranking:\")\n",
    "    for i, doc in enumerate(reranked_results[:5], 1):\n",
    "        print(f\"{i}. {doc['doc_name']}, page {doc['page_number']}\")\n",
    "        print(f\"   Initial rank: {doc['initial_rank']}, Initial score: {doc['initial_score']:.4f}\")\n",
    "        print(f\"   Rerank score: {doc['rerank_score']:.4f}\")\n",
    "        print(f\"   Rank change: {doc['initial_rank'] - doc['rank']:+d}\")\n",
    "    \n",
    "    print(\"\\\\n✓ Re-ranking functions working correctly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 4 complete!\")\n",
    "print(\"  Cross-encoder re-ranking: rerank_with_cross_encoder\")\n",
    "print(\"  Voyage AI re-ranking: rerank_with_voyage\")\n",
    "print(\"  Unified interface: rerank_documents\")\n",
    "print(\"\\nSupported re-ranker models:\")\n",
    "print(\"  • Cross-encoders: cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "print(\"                    cross-encoder/ms-marco-MiniLM-L-12-v2\")\n",
    "print(\"                    BAAI/bge-reranker-base\")\n",
    "print(\"                    BAAI/bge-reranker-large\")\n",
    "print(\"  • Voyage AI:      voyage-rerank-2.5\")\n",
    "print(\"                    voyage-rerank-2.5-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4261c700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Baseline evaluation function defined\n",
      "✓ Re-ranking evaluation function defined\n",
      "✓ Batch evaluation helper defined\n",
      "\n",
      "✓ Step 5 complete!\n",
      "  Baseline evaluation: evaluate_baseline\n",
      "  Re-ranking evaluation: evaluate_with_reranking\n",
      "  Batch helper: evaluate_single_configuration\n",
      "\n",
      "These functions will:\n",
      "  • Skip existing results automatically\n",
      "  • Calculate MRR for both baseline and re-ranked results\n",
      "  • Track improvements/degradations per query\n",
      "  • Save comprehensive JSON with all statistics\n",
      "\n",
      "Configuration structure:\n",
      "  {\n",
      "    'provider': 'voyage',\n",
      "    'model': 'voyage-finance-2',\n",
      "    'chunk_sizes': [512, 1024],\n",
      "    'k_retrieve': 40,  # Retrieve 40 for baseline\n",
      "    'k_rerank': 10,    # Keep top 10 after re-ranking\n",
      "    'reranker_models': ['cross-encoder/ms-marco-MiniLM-L-12-v2']\n",
      "  }\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 5: Evaluation Functions\n",
    "# ============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 5.1: Baseline Evaluation (No Re-Ranking)\n",
    "\n",
    "# %%\n",
    "def evaluate_baseline(\n",
    "    dataset,\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k_retrieve: int,\n",
    "    mode: str,\n",
    "    use_page_tolerance: bool = True,\n",
    "    output_dir: str = OUTPUT_DIR\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate baseline retrieval without re-ranking.\n",
    "    \n",
    "    This creates the baseline results that we'll compare against re-ranking.\n",
    "    \n",
    "    Args:\n",
    "        dataset: FinanceBench dataset\n",
    "        provider: \"ollama\", \"openai\", or \"voyage\"\n",
    "        model: Embedding model name\n",
    "        chunk_size: Chunk size\n",
    "        k_retrieve: Number of documents to retrieve\n",
    "        mode: \"global\" or \"singledoc\"\n",
    "        use_page_tolerance: If True, use chunk-size-aware page tolerance\n",
    "        output_dir: Output directory for results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation status and statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"BASELINE EVALUATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Provider: {provider}\")\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Chunk: {chunk_size}, k_retrieve={k_retrieve}, mode={mode}\")\n",
    "    print(f\"Page Tolerance: {'ENABLED' if use_page_tolerance else 'DISABLED'}\")\n",
    "    \n",
    "    # Check if already exists\n",
    "    if check_if_results_exist(provider, model, chunk_size, k_retrieve, mode, output_dir):\n",
    "        print(\"✓ Baseline results already exist - SKIPPING\")\n",
    "        return {'status': 'skipped'}\n",
    "    \n",
    "    # Load vectorstore\n",
    "    print(\"Loading vectorstore...\")\n",
    "    try:\n",
    "        vectorstore = load_vectorstore(provider, model, chunk_size)\n",
    "        doc_count = vectorstore._collection.count()\n",
    "        print(f\"✓ Loaded ({doc_count:,} documents)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load vectorstore: {e}\")\n",
    "        return {'status': 'failed', 'error': str(e)}\n",
    "    \n",
    "    # Process all queries\n",
    "    results = []\n",
    "    mrr_scores = []\n",
    "    \n",
    "    print(f\"\\nProcessing {len(dataset)} queries...\")\n",
    "    \n",
    "    for record in tqdm(dataset, desc=\"Queries\"):\n",
    "        query_id = record['financebench_id']\n",
    "        query = record['question']\n",
    "        evidence = record['evidence']\n",
    "        doc_name = record['doc_name']\n",
    "        \n",
    "        try:\n",
    "            # Retrieve documents\n",
    "            retrieved_docs = retrieve_with_scores(\n",
    "                vectorstore=vectorstore,\n",
    "                query=query,\n",
    "                k=k_retrieve,\n",
    "                mode=mode,\n",
    "                target_doc_name=doc_name if mode == \"singledoc\" else None\n",
    "            )\n",
    "            \n",
    "            # Remove content before calculating MRR (save memory)\n",
    "            retrieved_for_mrr = [\n",
    "                {k: v for k, v in doc.items() if k != 'content'}\n",
    "                for doc in retrieved_docs\n",
    "            ]\n",
    "            \n",
    "            # Calculate MRR\n",
    "            mrr_score, rank = calculate_mrr_for_query(\n",
    "                retrieved_for_mrr, evidence, chunk_size, use_page_tolerance\n",
    "            )\n",
    "            mrr_scores.append(mrr_score)\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                'query_id': query_id,\n",
    "                'query': query,\n",
    "                'expected_doc': doc_name,\n",
    "                'expected_evidence': [\n",
    "                    {\n",
    "                        'doc_name': ev['doc_name'],\n",
    "                        'page_number': ev['evidence_page_num'] + 1  # Convert to 1-indexed\n",
    "                    }\n",
    "                    for ev in evidence\n",
    "                ],\n",
    "                'retrieved_docs': retrieved_for_mrr,\n",
    "                'mrr_score': mrr_score,\n",
    "                'rank': rank\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Error processing query {query_id}: {e}\")\n",
    "            results.append({\n",
    "                'query_id': query_id,\n",
    "                'query': query,\n",
    "                'error': str(e),\n",
    "                'mrr_score': 0.0,\n",
    "                'rank': -1\n",
    "            })\n",
    "            mrr_scores.append(0.0)\n",
    "    \n",
    "    # Calculate average MRR\n",
    "    avg_mrr = sum(mrr_scores) / len(mrr_scores) if mrr_scores else 0.0\n",
    "    \n",
    "    # Add summary\n",
    "    summary = {\n",
    "        'provider': provider,\n",
    "        'model': model,\n",
    "        'chunk_size': chunk_size,\n",
    "        'k_retrieve': k_retrieve,\n",
    "        'mode': mode,\n",
    "        'use_page_tolerance': use_page_tolerance,\n",
    "        'total_queries': len(dataset),\n",
    "        'average_mrr': avg_mrr\n",
    "    }\n",
    "    results.append({'summary': summary})\n",
    "    \n",
    "    # Save results (pass k_retrieve as k parameter)\n",
    "    save_results(results, provider, model, chunk_size, k_retrieve, mode, output_dir)\n",
    "    \n",
    "    print(f\"\\n✓ Average MRR: {avg_mrr:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'status': 'completed',\n",
    "        'average_mrr': avg_mrr,\n",
    "        'total_queries': len(dataset)\n",
    "    }\n",
    "\n",
    "print(\"✓ Baseline evaluation function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 5.2: Re-Ranking Evaluation\n",
    "\n",
    "# %%\n",
    "def evaluate_with_reranking(\n",
    "    dataset,\n",
    "    provider: str,\n",
    "    model: str,\n",
    "    chunk_size: int,\n",
    "    k_retrieve: int,\n",
    "    k_rerank: int,\n",
    "    mode: str,\n",
    "    reranker_model: str,\n",
    "    use_page_tolerance: bool = True,\n",
    "    output_dir: str = OUTPUT_DIR\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate retrieval with re-ranking.\n",
    "    \n",
    "    Two-stage process:\n",
    "    1. Retrieve k_retrieve documents (e.g., 40)\n",
    "    2. Re-rank all k_retrieve documents and keep top k_rerank (e.g., 10)\n",
    "    \n",
    "    Args:\n",
    "        dataset: FinanceBench dataset\n",
    "        provider: \"ollama\", \"openai\", or \"voyage\"\n",
    "        model: Embedding model name\n",
    "        chunk_size: Chunk size\n",
    "        k_retrieve: Number of documents to retrieve in first stage\n",
    "        k_rerank: Number of documents to keep after re-ranking\n",
    "        mode: \"global\" or \"singledoc\"\n",
    "        reranker_model: Re-ranker model name\n",
    "        use_page_tolerance: If True, use chunk-size-aware page tolerance\n",
    "        output_dir: Output directory for results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation status and statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RE-RANKING EVALUATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Embedding: {provider}/{model}\")\n",
    "    print(f\"Chunk: {chunk_size}, mode={mode}\")\n",
    "    print(f\"Retrieval: k_retrieve={k_retrieve} → Re-rank: k_rerank={k_rerank}\")\n",
    "    print(f\"Re-ranker: {reranker_model}\")\n",
    "    print(f\"Page Tolerance: {'ENABLED' if use_page_tolerance else 'DISABLED'}\")\n",
    "    \n",
    "    # Check if already exists\n",
    "    if check_if_results_exist(provider, model, chunk_size, k_retrieve, mode, output_dir, reranker_model, k_rerank):\n",
    "        print(\"✓ Re-ranking results already exist - SKIPPING\")\n",
    "        return {'status': 'skipped'}\n",
    "    \n",
    "    # Load vectorstore\n",
    "    print(\"Loading vectorstore...\")\n",
    "    try:\n",
    "        vectorstore = load_vectorstore(provider, model, chunk_size)\n",
    "        doc_count = vectorstore._collection.count()\n",
    "        print(f\"✓ Loaded ({doc_count:,} documents)\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load vectorstore: {e}\")\n",
    "        return {'status': 'failed', 'error': str(e)}\n",
    "    \n",
    "    # Process all queries\n",
    "    results = []\n",
    "    mrr_scores_rerank = []\n",
    "    mrr_scores_baseline = []\n",
    "    rank_improvements = []\n",
    "    queries_improved = 0\n",
    "    queries_degraded = 0\n",
    "    queries_unchanged = 0\n",
    "    \n",
    "    print(f\"\\nProcessing {len(dataset)} queries...\")\n",
    "    \n",
    "    for record in tqdm(dataset, desc=\"Queries\"):\n",
    "        query_id = record['financebench_id']\n",
    "        query = record['question']\n",
    "        evidence = record['evidence']\n",
    "        doc_name = record['doc_name']\n",
    "        \n",
    "        try:\n",
    "            # Stage 1: Retrieve k_retrieve documents\n",
    "            retrieved_docs = retrieve_with_scores(\n",
    "                vectorstore=vectorstore,\n",
    "                query=query,\n",
    "                k=k_retrieve,\n",
    "                mode=mode,\n",
    "                target_doc_name=doc_name if mode == \"singledoc\" else None\n",
    "            )\n",
    "            \n",
    "            # Calculate baseline MRR (on first k_rerank documents from initial retrieval)\n",
    "            baseline_docs_for_mrr = [\n",
    "                {k: v for k, v in doc.items() if k != 'content'}\n",
    "                for doc in retrieved_docs[:k_rerank]\n",
    "            ]\n",
    "            mrr_baseline, rank_baseline = calculate_mrr_for_query(\n",
    "                baseline_docs_for_mrr, evidence, chunk_size, use_page_tolerance\n",
    "            )\n",
    "            mrr_scores_baseline.append(mrr_baseline)\n",
    "            \n",
    "            # Stage 2: Re-rank all k_retrieve documents\n",
    "            reranked_docs = rerank_documents(\n",
    "                query=query,\n",
    "                retrieved_docs=retrieved_docs,\n",
    "                reranker_model=reranker_model,\n",
    "                top_k=k_rerank\n",
    "            )\n",
    "            \n",
    "            # Calculate MRR after re-ranking\n",
    "            mrr_rerank, rank_rerank = calculate_mrr_for_query(\n",
    "                reranked_docs, evidence, chunk_size, use_page_tolerance\n",
    "            )\n",
    "            mrr_scores_rerank.append(mrr_rerank)\n",
    "            \n",
    "            # Track improvements\n",
    "            rank_improvement = rank_baseline - rank_rerank if rank_baseline > 0 and rank_rerank > 0 else 0\n",
    "            rank_improvements.append(rank_improvement)\n",
    "            \n",
    "            if rank_improvement > 0:\n",
    "                queries_improved += 1\n",
    "            elif rank_improvement < 0:\n",
    "                queries_degraded += 1\n",
    "            else:\n",
    "                queries_unchanged += 1\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                'query_id': query_id,\n",
    "                'query': query,\n",
    "                'expected_doc': doc_name,\n",
    "                'expected_evidence': [\n",
    "                    {\n",
    "                        'doc_name': ev['doc_name'],\n",
    "                        'page_number': ev['evidence_page_num'] + 1\n",
    "                    }\n",
    "                    for ev in evidence\n",
    "                ],\n",
    "                'retrieval_config': {\n",
    "                    'k_retrieve': k_retrieve,\n",
    "                    'k_rerank': k_rerank,\n",
    "                    'reranker_model': reranker_model\n",
    "                },\n",
    "                'retrieved_docs': reranked_docs,\n",
    "                'mrr_score': mrr_rerank,\n",
    "                'rank': rank_rerank,\n",
    "                'mrr_baseline': mrr_baseline,\n",
    "                'rank_baseline': rank_baseline,\n",
    "                'rank_improvement': rank_improvement\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n✗ Error processing query {query_id}: {e}\")\n",
    "            results.append({\n",
    "                'query_id': query_id,\n",
    "                'query': query,\n",
    "                'error': str(e),\n",
    "                'mrr_score': 0.0,\n",
    "                'rank': -1,\n",
    "                'mrr_baseline': 0.0,\n",
    "                'rank_baseline': -1\n",
    "            })\n",
    "            mrr_scores_rerank.append(0.0)\n",
    "            mrr_scores_baseline.append(0.0)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_mrr_rerank = sum(mrr_scores_rerank) / len(mrr_scores_rerank) if mrr_scores_rerank else 0.0\n",
    "    avg_mrr_baseline = sum(mrr_scores_baseline) / len(mrr_scores_baseline) if mrr_scores_baseline else 0.0\n",
    "    mrr_improvement = avg_mrr_rerank - avg_mrr_baseline\n",
    "    mrr_improvement_pct = (mrr_improvement / avg_mrr_baseline * 100) if avg_mrr_baseline > 0 else 0.0\n",
    "    \n",
    "    avg_rank_improvement = sum(rank_improvements) / len(rank_improvements) if rank_improvements else 0.0\n",
    "    \n",
    "    # Add summary\n",
    "    summary = {\n",
    "        'provider': provider,\n",
    "        'model': model,\n",
    "        'chunk_size': chunk_size,\n",
    "        'k_retrieve': k_retrieve,\n",
    "        'k_rerank': k_rerank,\n",
    "        'mode': mode,\n",
    "        'use_page_tolerance': use_page_tolerance,\n",
    "        'retrieval_config': {\n",
    "            'k_retrieve': k_retrieve,\n",
    "            'k_rerank': k_rerank,\n",
    "            'reranker_model': reranker_model\n",
    "        },\n",
    "        'total_queries': len(dataset),\n",
    "        'average_mrr': avg_mrr_rerank,\n",
    "        'average_mrr_baseline': avg_mrr_baseline,\n",
    "        'mrr_improvement': mrr_improvement,\n",
    "        'mrr_improvement_percentage': mrr_improvement_pct,\n",
    "        'queries_improved': queries_improved,\n",
    "        'queries_degraded': queries_degraded,\n",
    "        'queries_unchanged': queries_unchanged,\n",
    "        'average_rank_improvement': avg_rank_improvement\n",
    "    }\n",
    "    results.append({'summary': summary})\n",
    "    \n",
    "    # Save results (pass k_retrieve as k, and k_rerank separately)\n",
    "    save_results(results, provider, model, chunk_size, k_retrieve, mode, output_dir, reranker_model, k_rerank)\n",
    "    \n",
    "    print(f\"\\n✓ Results:\")\n",
    "    print(f\"  Baseline MRR: {avg_mrr_baseline:.4f} (top {k_rerank} from {k_retrieve} retrieved)\")\n",
    "    print(f\"  Re-rank MRR:  {avg_mrr_rerank:.4f} (top {k_rerank} after re-ranking {k_retrieve})\")\n",
    "    print(f\"  Improvement:  {mrr_improvement:+.4f} ({mrr_improvement_pct:+.2f}%)\")\n",
    "    print(f\"  Queries improved: {queries_improved}, degraded: {queries_degraded}, unchanged: {queries_unchanged}\")\n",
    "    \n",
    "    return {\n",
    "        'status': 'completed',\n",
    "        'average_mrr': avg_mrr_rerank,\n",
    "        'average_mrr_baseline': avg_mrr_baseline,\n",
    "        'improvement': mrr_improvement,\n",
    "        'improvement_percentage': mrr_improvement_pct,\n",
    "        'total_queries': len(dataset)\n",
    "    }\n",
    "\n",
    "print(\"✓ Re-ranking evaluation function defined\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 5.3: Batch Evaluation Helper\n",
    "\n",
    "# %%\n",
    "def evaluate_single_configuration(\n",
    "    dataset,\n",
    "    config: Dict,\n",
    "    modes: List[str],\n",
    "    use_page_tolerance: bool = True,\n",
    "    output_dir: str = OUTPUT_DIR\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a single embedding configuration across all settings.\n",
    "    \n",
    "    For each configuration, this runs:\n",
    "    1. Baseline evaluation (if not exists)\n",
    "    2. Re-ranking evaluation for each re-ranker (if not exists)\n",
    "    \n",
    "    Args:\n",
    "        dataset: FinanceBench dataset\n",
    "        config: Configuration dict with:\n",
    "            - provider: Embedding provider\n",
    "            - model: Embedding model name\n",
    "            - chunk_sizes: List of chunk sizes to test\n",
    "            - k_retrieve: Number of documents to retrieve\n",
    "            - k_rerank: Number of documents to keep after re-ranking\n",
    "            - reranker_models: List of re-ranker models to test\n",
    "        modes: List of modes (e.g., ['global', 'singledoc'])\n",
    "        use_page_tolerance: If True, use chunk-size-aware tolerance\n",
    "        output_dir: Output directory\n",
    "        \n",
    "    Returns:\n",
    "        Summary of evaluations\n",
    "    \"\"\"\n",
    "    provider = config['provider']\n",
    "    model = config['model']\n",
    "    chunk_sizes = config['chunk_sizes']\n",
    "    k_retrieve = config['k_retrieve']\n",
    "    k_rerank = config['k_rerank']\n",
    "    reranker_models = config.get('reranker_models', [])\n",
    "    \n",
    "    results_summary = []\n",
    "    \n",
    "    for chunk_size in chunk_sizes:\n",
    "        for mode in modes:\n",
    "            # Baseline evaluation\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"CONFIG: {provider}/{model}, chunk={chunk_size}, k_retrieve={k_retrieve}, mode={mode}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            baseline_result = evaluate_baseline(\n",
    "                dataset=dataset,\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                chunk_size=chunk_size,\n",
    "                k_retrieve=k_retrieve,\n",
    "                mode=mode,\n",
    "                use_page_tolerance=use_page_tolerance,\n",
    "                output_dir=output_dir\n",
    "            )\n",
    "            \n",
    "            results_summary.append({\n",
    "                'type': 'baseline',\n",
    "                'provider': provider,\n",
    "                'model': model,\n",
    "                'chunk_size': chunk_size,\n",
    "                'k_retrieve': k_retrieve,\n",
    "                'mode': mode,\n",
    "                'result': baseline_result\n",
    "            })\n",
    "            \n",
    "            # Re-ranking evaluations\n",
    "            for reranker_model in reranker_models:\n",
    "                rerank_result = evaluate_with_reranking(\n",
    "                    dataset=dataset,\n",
    "                    provider=provider,\n",
    "                    model=model,\n",
    "                    chunk_size=chunk_size,\n",
    "                    k_retrieve=k_retrieve,\n",
    "                    k_rerank=k_rerank,\n",
    "                    mode=mode,\n",
    "                    reranker_model=reranker_model,\n",
    "                    use_page_tolerance=use_page_tolerance,\n",
    "                    output_dir=output_dir\n",
    "                )\n",
    "                \n",
    "                results_summary.append({\n",
    "                    'type': 'rerank',\n",
    "                    'provider': provider,\n",
    "                    'model': model,\n",
    "                    'chunk_size': chunk_size,\n",
    "                    'k_retrieve': k_retrieve,\n",
    "                    'k_rerank': k_rerank,\n",
    "                    'mode': mode,\n",
    "                    'reranker': reranker_model,\n",
    "                    'result': rerank_result\n",
    "                })\n",
    "    \n",
    "    return results_summary\n",
    "\n",
    "print(\"✓ Batch evaluation helper defined\")\n",
    "\n",
    "# %%\n",
    "print(\"\\n✓ Step 5 complete!\")\n",
    "print(\"  Baseline evaluation: evaluate_baseline\")\n",
    "print(\"  Re-ranking evaluation: evaluate_with_reranking\")\n",
    "print(\"  Batch helper: evaluate_single_configuration\")\n",
    "print(\"\\nThese functions will:\")\n",
    "print(\"  • Skip existing results automatically\")\n",
    "print(\"  • Calculate MRR for both baseline and re-ranked results\")\n",
    "print(\"  • Track improvements/degradations per query\")\n",
    "print(\"  • Save comprehensive JSON with all statistics\")\n",
    "print(\"\\nConfiguration structure:\")\n",
    "print(\"  {\")\n",
    "print(\"    'provider': 'voyage',\")\n",
    "print(\"    'model': 'voyage-finance-2',\")\n",
    "print(\"    'chunk_sizes': [512, 1024],\")\n",
    "print(\"    'k_retrieve': 40,  # Retrieve 40 for baseline\")\n",
    "print(\"    'k_rerank': 10,    # Keep top 10 after re-ranking\")\n",
    "print(\"    'reranker_models': ['cross-encoder/ms-marco-MiniLM-L-12-v2']\")\n",
    "print(\"  }\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
