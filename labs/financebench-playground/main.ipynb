{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade40fc8",
   "metadata": {},
   "source": [
    "# General Tips\n",
    "## Using virtual environments\n",
    "**Step 1:** CD to desired directory and Create a Virtual Environment `python3 -m venv myenv`. (Run `py -3.13 -m venv myenv` for a specific version of python)\n",
    "\n",
    "Check your python installed versions with `py -0` on Windows (`python3 --version` on Linux)\n",
    "\n",
    "**Step 2:** Activate the Environment `source myenv/bin/activate` (on Linux) and `myenv\\Scripts\\activate` (on Windows).\n",
    "\n",
    "**Step 3:** Install Any Needed Packages. e.g: `pip install requests pandas`. Or better to use `requirements.txt` file (`pip install -r requirements.txt`)\n",
    "\n",
    "**Step 4:** List All Installed Packages using `pip list`\n",
    "\n",
    "## Connecting the Jupyter Notebook to the vistual env\n",
    "1. Make sure that myenv is activate (`myenv\\Scripts\\activate`)\n",
    "2. Run this inside the virtual environment: `pip install ipykernel`\n",
    "3. Still inside the environment: `python -m ipykernel install --user --name=myenv --display-name \"Whatever Python Kernel Name\"`\n",
    "   \n",
    "   --name=myenv: internal identifier for the kernel\n",
    "   \n",
    "   --display-name: name that shows up in VS Code kernel picker\n",
    "4. Open VS Code and select the kernel\n",
    "\n",
    "   At the top-right, click \"Select Kernel\".\n",
    "   Look for “Whatever Python Kernel Name” — pick that.\n",
    "5. If you don’t see it right away, try: Reloading VS Code, Or running Reload Window from Command Palette (Ctrl+Shift+P)\n",
    "\n",
    "## Useful Commands\n",
    "1. Use `py -0` to check which python installation we have on Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25c85eb",
   "metadata": {},
   "source": [
    "## Step 0: Setup Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49666301",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"nomic-embed-text\"\n",
    "chunk_sizes = [128, 256, 512, 1024]\n",
    "chunk_overlap_percentage = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe242c59",
   "metadata": {},
   "source": [
    "# Step 1: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e8ba30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehrdad/projects/finance-rag/finance-rag/labs/financebench-playground/myenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mehrdad/projects/finance-rag/finance-rag/labs/financebench-playground/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 150/150 [00:00<00:00, 13482.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"PatronusAI/financebench\", split=\"train\")\n",
    "\n",
    "# Define PDF directory path\n",
    "pdf_dir = \"../pdfs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e16e2f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records:  150\n",
      "Keys:  {'financebench_id': 'financebench_id_03029', 'company': '3M', 'doc_name': '3M_2018_10K', 'question_type': 'metrics-generated', 'question_reasoning': 'Information extraction', 'domain_question_num': None, 'question': 'What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.', 'answer': '$1577.00', 'justification': 'The metric capital expenditures was directly extracted from the company 10K. The line item name, as seen in the 10K, was: Purchases of property, plant and equipment (PP&E).', 'dataset_subset_label': 'OPEN_SOURCE', 'evidence': [{'evidence_text': 'Table of Contents \\n3M Company and Subsidiaries\\nConsolidated Statement of Cash Flow s\\nYears ended December 31\\n \\n(Millions)\\n \\n2018\\n \\n2017\\n \\n2016\\n \\nCash Flows from Operating Activities\\n \\n \\n \\n \\n \\n \\n \\nNet income including noncontrolling interest\\n \\n$\\n5,363 \\n$\\n4,869 \\n$\\n5,058 \\nAdjustments to reconcile net income including noncontrolling interest to net cash\\nprovided by operating activities\\n \\n \\n \\n \\n \\n \\n \\nDepreciation and amortization\\n \\n \\n1,488 \\n \\n1,544 \\n \\n1,474 \\nCompany pension and postretirement contributions\\n \\n \\n(370) \\n \\n(967) \\n \\n(383) \\nCompany pension and postretirement expense\\n \\n \\n410 \\n \\n334 \\n \\n250 \\nStock-based compensation expense\\n \\n \\n302 \\n \\n324 \\n \\n298 \\nGain on sale of businesses\\n \\n \\n(545) \\n \\n(586) \\n \\n(111) \\nDeferred income taxes\\n \\n \\n(57) \\n \\n107 \\n \\n 7 \\nChanges in assets and liabilities\\n \\n \\n \\n \\n \\n \\n \\nAccounts receivable\\n \\n \\n(305) \\n \\n(245) \\n \\n(313) \\nInventories\\n \\n \\n(509) \\n \\n(387) \\n \\n57 \\nAccounts payable\\n \\n \\n408 \\n \\n24 \\n \\n148 \\nAccrued income taxes (current and long-term)\\n \\n \\n134 \\n \\n967 \\n \\n101 \\nOther net\\n \\n \\n120 \\n \\n256 \\n \\n76 \\nNet cash provided by (used in) operating activities\\n \\n \\n6,439 \\n \\n6,240 \\n \\n6,662 \\n \\n \\n \\n \\n \\n \\n \\n \\nCash Flows from Investing Activities\\n \\n \\n \\n \\n \\n \\n \\nPurchases of property, plant and equipment (PP&E)\\n \\n \\n(1,577) \\n \\n(1,373) \\n \\n(1,420) \\nProceeds from sale of PP&E and other assets\\n \\n \\n262 \\n \\n49 \\n \\n58 \\nAcquisitions, net of cash acquired\\n \\n \\n13 \\n \\n(2,023) \\n \\n(16) \\nPurchases of marketable securities and investments\\n \\n \\n(1,828) \\n \\n(2,152) \\n \\n(1,410) \\nProceeds from maturities and sale of marketable securities and investments\\n \\n \\n2,497 \\n \\n1,354 \\n \\n1,247 \\nProceeds from sale of businesses, net of cash sold\\n \\n \\n846 \\n \\n1,065 \\n \\n142 \\nOther net\\n \\n \\n 9 \\n \\n(6) \\n \\n(4) \\nNet cash provided by (used in) investing activities\\n \\n \\n222 \\n \\n(3,086) \\n \\n(1,403) \\n \\n \\n \\n \\n \\n \\n \\n \\nCash Flows from Financing Activities\\n \\n \\n \\n \\n \\n \\n \\nChange in short-term debt net\\n \\n \\n(284) \\n \\n578 \\n \\n(797) \\nRepayment of debt (maturities greater than 90 days)\\n \\n \\n(1,034) \\n \\n(962) \\n \\n(992) \\nProceeds from debt (maturities greater than 90 days)\\n \\n \\n2,251 \\n \\n1,987 \\n \\n2,832 \\nPurchases of treasury stock\\n \\n \\n(4,870) \\n \\n(2,068) \\n \\n(3,753) \\nProceeds from issuance of treasury stock pursuant to stock option and benefit plans\\n \\n \\n485 \\n \\n734 \\n \\n804 \\nDividends paid to shareholders\\n \\n \\n(3,193) \\n \\n(2,803) \\n \\n(2,678) \\nOther net\\n \\n \\n(56) \\n \\n(121) \\n \\n(42) \\nNet cash provided by (used in) financing activities\\n \\n \\n(6,701) \\n \\n(2,655) \\n \\n(4,626) \\n \\n \\n \\n \\n \\n \\n \\n \\nEffect of exchange rate changes on cash and cash equivalents\\n \\n \\n(160) \\n \\n156 \\n \\n(33) \\n \\n \\n \\n \\n \\n \\n \\n \\nNet increase (decrease) in cash and cash equivalents\\n \\n \\n(200) \\n \\n655 \\n \\n600 \\nCash and cash equivalents at beginning of year\\n \\n \\n3,053 \\n \\n2,398 \\n \\n1,798 \\nCash and cash equivalents at end of period\\n \\n$\\n2,853 \\n$\\n3,053 \\n$\\n2,398 \\n \\nThe accompanying Notes to Consolidated Financial Statements are an integral part of this statement.\\n \\n60', 'doc_name': '3M_2018_10K', 'evidence_page_num': 59, 'evidence_text_full_page': 'Table of Contents \\n3M Company and Subsidiaries\\nConsolidated Statement of Cash Flow s\\nYears ended December 31\\n \\n(Millions)\\n \\n2018\\n \\n2017\\n \\n2016\\n \\nCash Flows from Operating Activities\\n \\n \\n \\n \\n \\n \\n \\nNet income including noncontrolling interest\\n \\n$\\n5,363 \\n$\\n4,869 \\n$\\n5,058 \\nAdjustments to reconcile net income including noncontrolling interest to net cash\\nprovided by operating activities\\n \\n \\n \\n \\n \\n \\n \\nDepreciation and amortization\\n \\n \\n1,488 \\n \\n1,544 \\n \\n1,474 \\nCompany pension and postretirement contributions\\n \\n \\n(370) \\n \\n(967) \\n \\n(383) \\nCompany pension and postretirement expense\\n \\n \\n410 \\n \\n334 \\n \\n250 \\nStock-based compensation expense\\n \\n \\n302 \\n \\n324 \\n \\n298 \\nGain on sale of businesses\\n \\n \\n(545) \\n \\n(586) \\n \\n(111) \\nDeferred income taxes\\n \\n \\n(57) \\n \\n107 \\n \\n 7 \\nChanges in assets and liabilities\\n \\n \\n \\n \\n \\n \\n \\nAccounts receivable\\n \\n \\n(305) \\n \\n(245) \\n \\n(313) \\nInventories\\n \\n \\n(509) \\n \\n(387) \\n \\n57 \\nAccounts payable\\n \\n \\n408 \\n \\n24 \\n \\n148 \\nAccrued income taxes (current and long-term)\\n \\n \\n134 \\n \\n967 \\n \\n101 \\nOther net\\n \\n \\n120 \\n \\n256 \\n \\n76 \\nNet cash provided by (used in) operating activities\\n \\n \\n6,439 \\n \\n6,240 \\n \\n6,662 \\n \\n \\n \\n \\n \\n \\n \\n \\nCash Flows from Investing Activities\\n \\n \\n \\n \\n \\n \\n \\nPurchases of property, plant and equipment (PP&E)\\n \\n \\n(1,577) \\n \\n(1,373) \\n \\n(1,420) \\nProceeds from sale of PP&E and other assets\\n \\n \\n262 \\n \\n49 \\n \\n58 \\nAcquisitions, net of cash acquired\\n \\n \\n13 \\n \\n(2,023) \\n \\n(16) \\nPurchases of marketable securities and investments\\n \\n \\n(1,828) \\n \\n(2,152) \\n \\n(1,410) \\nProceeds from maturities and sale of marketable securities and investments\\n \\n \\n2,497 \\n \\n1,354 \\n \\n1,247 \\nProceeds from sale of businesses, net of cash sold\\n \\n \\n846 \\n \\n1,065 \\n \\n142 \\nOther net\\n \\n \\n 9 \\n \\n(6) \\n \\n(4) \\nNet cash provided by (used in) investing activities\\n \\n \\n222 \\n \\n(3,086) \\n \\n(1,403) \\n \\n \\n \\n \\n \\n \\n \\n \\nCash Flows from Financing Activities\\n \\n \\n \\n \\n \\n \\n \\nChange in short-term debt net\\n \\n \\n(284) \\n \\n578 \\n \\n(797) \\nRepayment of debt (maturities greater than 90 days)\\n \\n \\n(1,034) \\n \\n(962) \\n \\n(992) \\nProceeds from debt (maturities greater than 90 days)\\n \\n \\n2,251 \\n \\n1,987 \\n \\n2,832 \\nPurchases of treasury stock\\n \\n \\n(4,870) \\n \\n(2,068) \\n \\n(3,753) \\nProceeds from issuance of treasury stock pursuant to stock option and benefit plans\\n \\n \\n485 \\n \\n734 \\n \\n804 \\nDividends paid to shareholders\\n \\n \\n(3,193) \\n \\n(2,803) \\n \\n(2,678) \\nOther net\\n \\n \\n(56) \\n \\n(121) \\n \\n(42) \\nNet cash provided by (used in) financing activities\\n \\n \\n(6,701) \\n \\n(2,655) \\n \\n(4,626) \\n \\n \\n \\n \\n \\n \\n \\n \\nEffect of exchange rate changes on cash and cash equivalents\\n \\n \\n(160) \\n \\n156 \\n \\n(33) \\n \\n \\n \\n \\n \\n \\n \\n \\nNet increase (decrease) in cash and cash equivalents\\n \\n \\n(200) \\n \\n655 \\n \\n600 \\nCash and cash equivalents at beginning of year\\n \\n \\n3,053 \\n \\n2,398 \\n \\n1,798 \\nCash and cash equivalents at end of period\\n \\n$\\n2,853 \\n$\\n3,053 \\n$\\n2,398 \\n \\nThe accompanying Notes to Consolidated Financial Statements are an integral part of this statement.\\n \\n60\\n \\n'}], 'gics_sector': 'Industrials', 'doc_type': '10k', 'doc_period': 2018, 'doc_link': 'https://investors.3m.com/financials/sec-filings/content/0001558370-19-000470/0001558370-19-000470.pdf'}\n",
      "Dataset [0]:  {'financebench_id': 'financebench_id_03029', 'company': '3M', 'doc_name': '3M_2018_10K', 'question_type': 'metrics-generated', 'question_reasoning': 'Information extraction', 'domain_question_num': None, 'question': 'What is the FY2018 capital expenditure amount (in USD millions) for 3M? Give a response to the question by relying on the details shown in the cash flow statement.', 'answer': '$1577.00', 'justification': 'The metric capital expenditures was directly extracted from the company 10K. The line item name, as seen in the 10K, was: Purchases of property, plant and equipment (PP&E).', 'dataset_subset_label': 'OPEN_SOURCE', 'evidence': [{'evidence_text': 'Table of Contents \\n3M Company and Subsidiaries\\nConsolidated Statement of Cash Flow s\\nYears ended December 31\\n \\n(Millions)\\n \\n2018\\n \\n2017\\n \\n2016\\n \\nCash Flows from Operating Activities\\n \\n \\n \\n \\n \\n \\n \\nNet income including noncontrolling interest\\n \\n$\\n5,363 \\n$\\n4,869 \\n$\\n5,058 \\nAdjustments to reconcile net income including noncontrolling interest to net cash\\nprovided by operating activities\\n \\n \\n \\n \\n \\n \\n \\nDepreciation and amortization\\n \\n \\n1,488 \\n \\n1,544 \\n \\n1,474 \\nCompany pension and postretirement contributions\\n \\n \\n(370) \\n \\n(967) \\n \\n(383) \\nCompany pension and postretirement expense\\n \\n \\n410 \\n \\n334 \\n \\n250 \\nStock-based compensation expense\\n \\n \\n302 \\n \\n324 \\n \\n298 \\nGain on sale of businesses\\n \\n \\n(545) \\n \\n(586) \\n \\n(111) \\nDeferred income taxes\\n \\n \\n(57) \\n \\n107 \\n \\n 7 \\nChanges in assets and liabilities\\n \\n \\n \\n \\n \\n \\n \\nAccounts receivable\\n \\n \\n(305) \\n \\n(245) \\n \\n(313) \\nInventories\\n \\n \\n(509) \\n \\n(387) \\n \\n57 \\nAccounts payable\\n \\n \\n408 \\n \\n24 \\n \\n148 \\nAccrued income taxes (current and long-term)\\n \\n \\n134 \\n \\n967 \\n \\n101 \\nOther net\\n \\n \\n120 \\n \\n256 \\n \\n76 \\nNet cash provided by (used in) operating activities\\n \\n \\n6,439 \\n \\n6,240 \\n \\n6,662 \\n \\n \\n \\n \\n \\n \\n \\n \\nCash Flows from Investing Activities\\n \\n \\n \\n \\n \\n \\n \\nPurchases of property, plant and equipment (PP&E)\\n \\n \\n(1,577) \\n \\n(1,373) \\n \\n(1,420) \\nProceeds from sale of PP&E and other assets\\n \\n \\n262 \\n \\n49 \\n \\n58 \\nAcquisitions, net of cash acquired\\n \\n \\n13 \\n \\n(2,023) \\n \\n(16) \\nPurchases of marketable securities and investments\\n \\n \\n(1,828) \\n \\n(2,152) \\n \\n(1,410) \\nProceeds from maturities and sale of marketable securities and investments\\n \\n \\n2,497 \\n \\n1,354 \\n \\n1,247 \\nProceeds from sale of businesses, net of cash sold\\n \\n \\n846 \\n \\n1,065 \\n \\n142 \\nOther net\\n \\n \\n 9 \\n \\n(6) \\n \\n(4) \\nNet cash provided by (used in) investing activities\\n \\n \\n222 \\n \\n(3,086) \\n \\n(1,403) \\n \\n \\n \\n \\n \\n \\n \\n \\nCash Flows from Financing Activities\\n \\n \\n \\n \\n \\n \\n \\nChange in short-term debt net\\n \\n \\n(284) \\n \\n578 \\n \\n(797) \\nRepayment of debt (maturities greater than 90 days)\\n \\n \\n(1,034) \\n \\n(962) \\n \\n(992) \\nProceeds from debt (maturities greater than 90 days)\\n \\n \\n2,251 \\n \\n1,987 \\n \\n2,832 \\nPurchases of treasury stock\\n \\n \\n(4,870) \\n \\n(2,068) \\n \\n(3,753) \\nProceeds from issuance of treasury stock pursuant to stock option and benefit plans\\n \\n \\n485 \\n \\n734 \\n \\n804 \\nDividends paid to shareholders\\n \\n \\n(3,193) \\n \\n(2,803) \\n \\n(2,678) \\nOther net\\n \\n \\n(56) \\n \\n(121) \\n \\n(42) \\nNet cash provided by (used in) financing activities\\n \\n \\n(6,701) \\n \\n(2,655) \\n \\n(4,626) \\n \\n \\n \\n \\n \\n \\n \\n \\nEffect of exchange rate changes on cash and cash equivalents\\n \\n \\n(160) \\n \\n156 \\n \\n(33) \\n \\n \\n \\n \\n \\n \\n \\n \\nNet increase (decrease) in cash and cash equivalents\\n \\n \\n(200) \\n \\n655 \\n \\n600 \\nCash and cash equivalents at beginning of year\\n \\n \\n3,053 \\n \\n2,398 \\n \\n1,798 \\nCash and cash equivalents at end of period\\n \\n$\\n2,853 \\n$\\n3,053 \\n$\\n2,398 \\n \\nThe accompanying Notes to Consolidated Financial Statements are an integral part of this statement.\\n \\n60', 'doc_name': '3M_2018_10K', 'evidence_page_num': 59, 'evidence_text_full_page': 'Table of Contents \\n3M Company and Subsidiaries\\nConsolidated Statement of Cash Flow s\\nYears ended December 31\\n \\n(Millions)\\n \\n2018\\n \\n2017\\n \\n2016\\n \\nCash Flows from Operating Activities\\n \\n \\n \\n \\n \\n \\n \\nNet income including noncontrolling interest\\n \\n$\\n5,363 \\n$\\n4,869 \\n$\\n5,058 \\nAdjustments to reconcile net income including noncontrolling interest to net cash\\nprovided by operating activities\\n \\n \\n \\n \\n \\n \\n \\nDepreciation and amortization\\n \\n \\n1,488 \\n \\n1,544 \\n \\n1,474 \\nCompany pension and postretirement contributions\\n \\n \\n(370) \\n \\n(967) \\n \\n(383) \\nCompany pension and postretirement expense\\n \\n \\n410 \\n \\n334 \\n \\n250 \\nStock-based compensation expense\\n \\n \\n302 \\n \\n324 \\n \\n298 \\nGain on sale of businesses\\n \\n \\n(545) \\n \\n(586) \\n \\n(111) \\nDeferred income taxes\\n \\n \\n(57) \\n \\n107 \\n \\n 7 \\nChanges in assets and liabilities\\n \\n \\n \\n \\n \\n \\n \\nAccounts receivable\\n \\n \\n(305) \\n \\n(245) \\n \\n(313) \\nInventories\\n \\n \\n(509) \\n \\n(387) \\n \\n57 \\nAccounts payable\\n \\n \\n408 \\n \\n24 \\n \\n148 \\nAccrued income taxes (current and long-term)\\n \\n \\n134 \\n \\n967 \\n \\n101 \\nOther net\\n \\n \\n120 \\n \\n256 \\n \\n76 \\nNet cash provided by (used in) operating activities\\n \\n \\n6,439 \\n \\n6,240 \\n \\n6,662 \\n \\n \\n \\n \\n \\n \\n \\n \\nCash Flows from Investing Activities\\n \\n \\n \\n \\n \\n \\n \\nPurchases of property, plant and equipment (PP&E)\\n \\n \\n(1,577) \\n \\n(1,373) \\n \\n(1,420) \\nProceeds from sale of PP&E and other assets\\n \\n \\n262 \\n \\n49 \\n \\n58 \\nAcquisitions, net of cash acquired\\n \\n \\n13 \\n \\n(2,023) \\n \\n(16) \\nPurchases of marketable securities and investments\\n \\n \\n(1,828) \\n \\n(2,152) \\n \\n(1,410) \\nProceeds from maturities and sale of marketable securities and investments\\n \\n \\n2,497 \\n \\n1,354 \\n \\n1,247 \\nProceeds from sale of businesses, net of cash sold\\n \\n \\n846 \\n \\n1,065 \\n \\n142 \\nOther net\\n \\n \\n 9 \\n \\n(6) \\n \\n(4) \\nNet cash provided by (used in) investing activities\\n \\n \\n222 \\n \\n(3,086) \\n \\n(1,403) \\n \\n \\n \\n \\n \\n \\n \\n \\nCash Flows from Financing Activities\\n \\n \\n \\n \\n \\n \\n \\nChange in short-term debt net\\n \\n \\n(284) \\n \\n578 \\n \\n(797) \\nRepayment of debt (maturities greater than 90 days)\\n \\n \\n(1,034) \\n \\n(962) \\n \\n(992) \\nProceeds from debt (maturities greater than 90 days)\\n \\n \\n2,251 \\n \\n1,987 \\n \\n2,832 \\nPurchases of treasury stock\\n \\n \\n(4,870) \\n \\n(2,068) \\n \\n(3,753) \\nProceeds from issuance of treasury stock pursuant to stock option and benefit plans\\n \\n \\n485 \\n \\n734 \\n \\n804 \\nDividends paid to shareholders\\n \\n \\n(3,193) \\n \\n(2,803) \\n \\n(2,678) \\nOther net\\n \\n \\n(56) \\n \\n(121) \\n \\n(42) \\nNet cash provided by (used in) financing activities\\n \\n \\n(6,701) \\n \\n(2,655) \\n \\n(4,626) \\n \\n \\n \\n \\n \\n \\n \\n \\nEffect of exchange rate changes on cash and cash equivalents\\n \\n \\n(160) \\n \\n156 \\n \\n(33) \\n \\n \\n \\n \\n \\n \\n \\n \\nNet increase (decrease) in cash and cash equivalents\\n \\n \\n(200) \\n \\n655 \\n \\n600 \\nCash and cash equivalents at beginning of year\\n \\n \\n3,053 \\n \\n2,398 \\n \\n1,798 \\nCash and cash equivalents at end of period\\n \\n$\\n2,853 \\n$\\n3,053 \\n$\\n2,398 \\n \\nThe accompanying Notes to Consolidated Financial Statements are an integral part of this statement.\\n \\n60\\n \\n'}], 'gics_sector': 'Industrials', 'doc_type': '10k', 'doc_period': 2018, 'doc_link': 'https://investors.3m.com/financials/sec-filings/content/0001558370-19-000470/0001558370-19-000470.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Records: \", len(ds))\n",
    "print(\"Keys: \", ds[0])\n",
    "print(\"Dataset [0]: \", ds[0])\n",
    "\n",
    "# print(\"List of document links:\")\n",
    "# counter = 0\n",
    "# for doc in ds:\n",
    "#     counter += 1\n",
    "#     print(f\"{counter}: {doc['doc_link']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce91e89",
   "metadata": {},
   "source": [
    "## Verify if all of the pdfs are in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f31deff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique PDF files required: 84\n",
      "Total missing PDF files: 0\n",
      "All required PDF files are present.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Track missing and unique PDF filenames\n",
    "unique_pdfs = set()\n",
    "missing_pdfs = []\n",
    "\n",
    "# Collect unique PDF filenames from the dataset\n",
    "for record in ds:\n",
    "    pdf_filename = record[\"doc_name\"] + \".pdf\"\n",
    "    unique_pdfs.add(pdf_filename)\n",
    "\n",
    "# Check for existence of each unique PDF\n",
    "for pdf_filename in unique_pdfs:\n",
    "    pdf_path = os.path.join(pdf_dir, pdf_filename)\n",
    "    if not os.path.isfile(pdf_path):\n",
    "        missing_pdfs.append(pdf_filename)\n",
    "\n",
    "# Report\n",
    "print(f\"Total unique PDF files required: {len(unique_pdfs)}\")\n",
    "print(f\"Total missing PDF files: {len(missing_pdfs)}\")\n",
    "\n",
    "if missing_pdfs:\n",
    "    print(\"Missing PDF files:\")\n",
    "    for missing_file in missing_pdfs:\n",
    "        print(\" -\", missing_file)\n",
    "else:\n",
    "    print(\"All required PDF files are present.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda594c",
   "metadata": {},
   "source": [
    "## Move Required PDF files that the dataset needs to a new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b375ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 84 PDF files to '../financebench_pdfs'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Source and target directories\n",
    "source_dir = \"../pdfs\"\n",
    "target_dir = \"../financebench_pdfs\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Track unique doc_names\n",
    "unique_doc_names = {record[\"doc_name\"] for record in ds}\n",
    "\n",
    "# Copy only the needed PDFs\n",
    "copied_count = 0\n",
    "for doc_name in unique_doc_names:\n",
    "    filename = doc_name + \".pdf\"\n",
    "    source_path = os.path.join(source_dir, filename)\n",
    "    target_path = os.path.join(target_dir, filename)\n",
    "\n",
    "    if os.path.isfile(source_path):\n",
    "        shutil.copy2(source_path, target_path)\n",
    "        copied_count += 1\n",
    "    else:\n",
    "        print(f\"Missing file: {filename}\")\n",
    "\n",
    "print(f\"Copied {copied_count} PDF files to '{target_dir}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b866e1",
   "metadata": {},
   "source": [
    "## Load documents using LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "090cf7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: KRAFTHEINZ_2019_10K.pdf\n",
      "Loaded: KRAFTHEINZ_2019_10K.pdf\n",
      "Processing: JPMORGAN_2021Q1_10Q.pdf\n",
      "Loaded: JPMORGAN_2021Q1_10Q.pdf\n",
      "Processing: MGMRESORTS_2022_10K.pdf\n",
      "Loaded: MGMRESORTS_2022_10K.pdf\n",
      "Processing: AMERICANWATERWORKS_2021_10K.pdf\n",
      "Loaded: AMERICANWATERWORKS_2021_10K.pdf\n",
      "Processing: AMCOR_2020_10K.pdf\n",
      "Loaded: AMCOR_2020_10K.pdf\n",
      "Processing: 3M_2023Q2_10Q.pdf\n",
      "Loaded: 3M_2023Q2_10Q.pdf\n",
      "Processing: CVSHEALTH_2022_10K.pdf\n",
      "Loaded: CVSHEALTH_2022_10K.pdf\n",
      "Processing: AMERICANWATERWORKS_2020_10K.pdf\n",
      "Loaded: AMERICANWATERWORKS_2020_10K.pdf\n",
      "Processing: NETFLIX_2017_10K.pdf\n",
      "Loaded: NETFLIX_2017_10K.pdf\n",
      "Processing: AMERICANEXPRESS_2022_10K.pdf\n",
      "Loaded: AMERICANEXPRESS_2022_10K.pdf\n",
      "Processing: WALMART_2020_10K.pdf\n",
      "Loaded: WALMART_2020_10K.pdf\n",
      "Processing: COCACOLA_2022_10K.pdf\n",
      "Loaded: COCACOLA_2022_10K.pdf\n",
      "Processing: PEPSICO_2021_10K.pdf\n",
      "Loaded: PEPSICO_2021_10K.pdf\n",
      "Processing: MGMRESORTS_2022Q4_EARNINGS.pdf\n",
      "Loaded: MGMRESORTS_2022Q4_EARNINGS.pdf\n",
      "Processing: MICROSOFT_2016_10K.pdf\n",
      "Loaded: MICROSOFT_2016_10K.pdf\n",
      "Processing: BESTBUY_2023_10K.pdf\n",
      "Loaded: BESTBUY_2023_10K.pdf\n",
      "Processing: JOHNSON_JOHNSON_2022_10K.pdf\n",
      "Loaded: JOHNSON_JOHNSON_2022_10K.pdf\n",
      "Processing: CORNING_2022_10K.pdf\n",
      "Loaded: CORNING_2022_10K.pdf\n",
      "Processing: ACTIVISIONBLIZZARD_2019_10K.pdf\n",
      "Loaded: ACTIVISIONBLIZZARD_2019_10K.pdf\n",
      "Processing: BESTBUY_2017_10K.pdf\n",
      "Loaded: BESTBUY_2017_10K.pdf\n",
      "Processing: AMCOR_2023_10K.pdf\n",
      "Loaded: AMCOR_2023_10K.pdf\n",
      "Processing: NIKE_2019_10K.pdf\n",
      "Loaded: NIKE_2019_10K.pdf\n",
      "Processing: AMD_2015_10K.pdf\n",
      "Loaded: AMD_2015_10K.pdf\n",
      "Processing: AMERICANWATERWORKS_2022_10K.pdf\n",
      "Loaded: AMERICANWATERWORKS_2022_10K.pdf\n",
      "Processing: BOEING_2022_10K.pdf\n",
      "Loaded: BOEING_2022_10K.pdf\n",
      "Processing: AES_2022_10K.pdf\n",
      "Loaded: AES_2022_10K.pdf\n",
      "Processing: NETFLIX_2015_10K.pdf\n",
      "Loaded: NETFLIX_2015_10K.pdf\n",
      "Processing: AMAZON_2019_10K.pdf\n",
      "Loaded: AMAZON_2019_10K.pdf\n",
      "Processing: FOOTLOCKER_2022_8K_dated_2022-08-19.pdf\n",
      "Loaded: FOOTLOCKER_2022_8K_dated_2022-08-19.pdf\n",
      "Processing: CORNING_2020_10K.pdf\n",
      "Loaded: CORNING_2020_10K.pdf\n",
      "Processing: GENERALMILLS_2019_10K.pdf\n",
      "Loaded: GENERALMILLS_2019_10K.pdf\n",
      "Processing: BLOCK_2020_10K.pdf\n",
      "Loaded: BLOCK_2020_10K.pdf\n",
      "Processing: PEPSICO_2022_10K.pdf\n",
      "Loaded: PEPSICO_2022_10K.pdf\n",
      "Processing: COCACOLA_2021_10K.pdf\n",
      "Loaded: COCACOLA_2021_10K.pdf\n",
      "Processing: AMD_2022_10K.pdf\n",
      "Loaded: AMD_2022_10K.pdf\n",
      "Processing: CORNING_2021_10K.pdf\n",
      "Loaded: CORNING_2021_10K.pdf\n",
      "Processing: FOOTLOCKER_2022_8K_dated-2022-05-20.pdf\n",
      "Loaded: FOOTLOCKER_2022_8K_dated-2022-05-20.pdf\n",
      "Processing: MICROSOFT_2023_10K.pdf\n",
      "Loaded: MICROSOFT_2023_10K.pdf\n",
      "Processing: 3M_2022_10K.pdf\n",
      "Loaded: 3M_2022_10K.pdf\n",
      "Processing: NIKE_2018_10K.pdf\n",
      "Loaded: NIKE_2018_10K.pdf\n",
      "Processing: MGMRESORTS_2020_10K.pdf\n",
      "Loaded: MGMRESORTS_2020_10K.pdf\n",
      "Processing: COCACOLA_2017_10K.pdf\n",
      "Loaded: COCACOLA_2017_10K.pdf\n",
      "Processing: PEPSICO_2023_8K_dated-2023-05-30.pdf\n",
      "Loaded: PEPSICO_2023_8K_dated-2023-05-30.pdf\n",
      "Processing: ULTABEAUTY_2023Q4_EARNINGS.pdf\n",
      "Loaded: ULTABEAUTY_2023Q4_EARNINGS.pdf\n",
      "Processing: BLOCK_2016_10K.pdf\n",
      "Loaded: BLOCK_2016_10K.pdf\n",
      "Processing: 3M_2018_10K.pdf\n",
      "Loaded: 3M_2018_10K.pdf\n",
      "Processing: Pfizer_2023Q2_10Q.pdf\n",
      "Loaded: Pfizer_2023Q2_10Q.pdf\n",
      "Processing: ADOBE_2015_10K.pdf\n",
      "Loaded: ADOBE_2015_10K.pdf\n",
      "Processing: AMCOR_2023Q2_10Q.pdf\n",
      "Loaded: AMCOR_2023Q2_10Q.pdf\n",
      "Processing: WALMART_2019_10K.pdf\n",
      "Loaded: WALMART_2019_10K.pdf\n",
      "Processing: COSTCO_2021_10K.pdf\n",
      "Loaded: COSTCO_2021_10K.pdf\n",
      "Processing: JPMORGAN_2022Q2_10Q.pdf\n",
      "Loaded: JPMORGAN_2022Q2_10Q.pdf\n",
      "Processing: GENERALMILLS_2022_10K.pdf\n",
      "Loaded: GENERALMILLS_2022_10K.pdf\n",
      "Processing: JPMORGAN_2022_10K.pdf\n",
      "Loaded: JPMORGAN_2022_10K.pdf\n",
      "Processing: LOCKHEEDMARTIN_2022_10K.pdf\n",
      "Loaded: LOCKHEEDMARTIN_2022_10K.pdf\n",
      "Processing: VERIZON_2021_10K.pdf\n",
      "Loaded: VERIZON_2021_10K.pdf\n",
      "Processing: ADOBE_2022_10K.pdf\n",
      "Loaded: ADOBE_2022_10K.pdf\n",
      "Processing: NIKE_2023_10K.pdf\n",
      "Loaded: NIKE_2023_10K.pdf\n",
      "Processing: BOEING_2018_10K.pdf\n",
      "Loaded: BOEING_2018_10K.pdf\n",
      "Processing: PEPSICO_2023_8K_dated-2023-05-05.pdf\n",
      "Loaded: PEPSICO_2023_8K_dated-2023-05-05.pdf\n",
      "Processing: WALMART_2018_10K.pdf\n",
      "Loaded: WALMART_2018_10K.pdf\n",
      "Processing: ULTABEAUTY_2023_10K.pdf\n",
      "Loaded: ULTABEAUTY_2023_10K.pdf\n",
      "Processing: LOCKHEEDMARTIN_2020_10K.pdf\n",
      "Loaded: LOCKHEEDMARTIN_2020_10K.pdf\n",
      "Processing: GENERALMILLS_2020_10K.pdf\n",
      "Loaded: GENERALMILLS_2020_10K.pdf\n",
      "Processing: PEPSICO_2023Q1_EARNINGS.pdf\n",
      "Loaded: PEPSICO_2023Q1_EARNINGS.pdf\n",
      "Processing: JOHNSON_JOHNSON_2023_8K_dated-2023-08-30.pdf\n",
      "Loaded: JOHNSON_JOHNSON_2023_8K_dated-2023-08-30.pdf\n",
      "Processing: BESTBUY_2019_10K.pdf\n",
      "Loaded: BESTBUY_2019_10K.pdf\n",
      "Processing: JOHNSON_JOHNSON_2023Q2_EARNINGS.pdf\n",
      "Loaded: JOHNSON_JOHNSON_2023Q2_EARNINGS.pdf\n",
      "Processing: AMCOR_2022_8K_dated-2022-07-01.pdf\n",
      "Loaded: AMCOR_2022_8K_dated-2022-07-01.pdf\n",
      "Processing: MGMRESORTS_2023Q2_10Q.pdf\n",
      "Loaded: MGMRESORTS_2023Q2_10Q.pdf\n",
      "Processing: ADOBE_2016_10K.pdf\n",
      "Loaded: ADOBE_2016_10K.pdf\n",
      "Processing: NIKE_2021_10K.pdf\n",
      "Loaded: NIKE_2021_10K.pdf\n",
      "Processing: PAYPAL_2022_10K.pdf\n",
      "Loaded: PAYPAL_2022_10K.pdf\n",
      "Processing: JPMORGAN_2023Q2_10Q.pdf\n",
      "Loaded: JPMORGAN_2023Q2_10Q.pdf\n",
      "Processing: AMCOR_2023Q4_EARNINGS.pdf\n",
      "Loaded: AMCOR_2023Q4_EARNINGS.pdf\n",
      "Processing: AMAZON_2017_10K.pdf\n",
      "Loaded: AMAZON_2017_10K.pdf\n",
      "Processing: JOHNSON_JOHNSON_2022Q4_EARNINGS.pdf\n",
      "Loaded: JOHNSON_JOHNSON_2022Q4_EARNINGS.pdf\n",
      "Processing: PFIZER_2021_10K.pdf\n",
      "Loaded: PFIZER_2021_10K.pdf\n",
      "Processing: ADOBE_2017_10K.pdf\n",
      "Loaded: ADOBE_2017_10K.pdf\n",
      "Processing: MGMRESORTS_2018_10K.pdf\n",
      "Loaded: MGMRESORTS_2018_10K.pdf\n",
      "Processing: CVSHEALTH_2018_10K.pdf\n",
      "Loaded: CVSHEALTH_2018_10K.pdf\n",
      "Processing: LOCKHEEDMARTIN_2021_10K.pdf\n",
      "Loaded: LOCKHEEDMARTIN_2021_10K.pdf\n",
      "Processing: BESTBUY_2024Q2_10Q.pdf\n",
      "Loaded: BESTBUY_2024Q2_10Q.pdf\n",
      "Processing: VERIZON_2022_10K.pdf\n",
      "Loaded: VERIZON_2022_10K.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "pdf_dir = \"../financebench_pdfs\"\n",
    "pdf_reader = PyMuPDFReader()\n",
    "\n",
    "# List of PDF files\n",
    "pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]\n",
    "\n",
    "documents = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    file_path = os.path.join(pdf_dir, pdf_file)\n",
    "    print(f\"Processing: {pdf_file}\")\n",
    "    try:\n",
    "        doc = pdf_reader.load(file_path)\n",
    "        documents.extend(doc)  # note: `doc` is a list of Document objects\n",
    "        print(f\"Loaded: {pdf_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {pdf_file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6fc3f",
   "metadata": {},
   "source": [
    "## Create the nodes with specific chunk and overlap size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d0784b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from typing import List\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "def generate_nodes(\n",
    "    documents: List[Document],\n",
    "    chunk_size: int = 512,\n",
    "    chunk_overlap: int = 512 // 4 # 20% overlap\n",
    ") -> List:\n",
    "    \"\"\"\n",
    "    Generate nodes from documents using LlamaIndex SentenceSplitter.\n",
    "\n",
    "    Args:\n",
    "        documents: List of LlamaIndex Document objects to process\n",
    "        chunk_size: Maximum characters per chunk (default: 512)\n",
    "        chunk_overlap: Overlap between chunks to preserve context (default: 25% overlap)\n",
    "\n",
    "    Returns:\n",
    "        List of nodes generated from the documents\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If chunk_size or chunk_overlap is invalid\n",
    "        TypeError: If documents is not a list of Document objects\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(documents, list):\n",
    "        raise TypeError(\"Documents must be provided as a list\")\n",
    "    \n",
    "    if not all(isinstance(doc, Document) for doc in documents):\n",
    "        raise TypeError(\"All items in documents list must be LlamaIndex Document objects\")\n",
    "    \n",
    "    if chunk_size <= 0:\n",
    "        raise ValueError(\"Chunk size must be positive\")\n",
    "        \n",
    "    if chunk_overlap < 0:\n",
    "        raise ValueError(\"Chunk overlap cannot be negative\")\n",
    "        \n",
    "    if chunk_overlap >= chunk_size:\n",
    "        raise ValueError(\"Chunk overlap must be less than chunk size\")\n",
    "\n",
    "    # Initialize SentenceSplitter\n",
    "    parser = SentenceSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    # Generate nodes\n",
    "    nodes = parser.get_nodes_from_documents(documents)\n",
    "    \n",
    "    print(f\"Created {len(nodes)} chunks with chunk_size={chunk_size} and chunk_overlap={chunk_overlap}\")\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "# # Example usage\n",
    "# # Sample documents\n",
    "# sample_text = \"This is a sample document for testing. \" * 50\n",
    "# documents = [Document(text=sample_text)]\n",
    "\n",
    "# try:\n",
    "#     # Generate nodes with default parameters\n",
    "#     nodes = generate_nodes(documents)\n",
    "    \n",
    "#     # Generate nodes with custom parameters\n",
    "#     custom_nodes = generate_nodes(\n",
    "#         documents=documents,\n",
    "#         chunk_size=1000,\n",
    "#         chunk_overlap=200\n",
    "#     )\n",
    "# except (ValueError, TypeError) as e:\n",
    "#     print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a0b718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.docstore.document import Document as LCDocument\n",
    "from llama_index.core.schema import BaseNode\n",
    "\n",
    "def nodes_to_langchain_docs(\n",
    "    nodes: List[BaseNode],\n",
    "    chunk_size: int,\n",
    "    keep_node_metadata: bool = True\n",
    ") -> List[LCDocument]:\n",
    "    \"\"\"\n",
    "    Convert LlamaIndex nodes to LangChain documents.\n",
    "\n",
    "    Args:\n",
    "        nodes: List of LlamaIndex nodes to convert\n",
    "        chunk_size: Chunk size used for node creation (for metadata)\n",
    "        keep_node_metadata: If True, include original node metadata in addition to chunk_size\n",
    "\n",
    "    Returns:\n",
    "        List of LangChain Document objects\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If nodes is not a list of LlamaIndex BaseNode objects\n",
    "        ValueError: If chunk_size is invalid\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(nodes, list):\n",
    "        raise TypeError(\"Nodes must be provided as a list\")\n",
    "    \n",
    "    if not all(isinstance(node, BaseNode) for node in nodes):\n",
    "        raise TypeError(\"All items in nodes list must be LlamaIndex BaseNode objects\")\n",
    "    \n",
    "    if chunk_size <= 0:\n",
    "        raise ValueError(\"Chunk size must be positive\")\n",
    "\n",
    "    # Convert nodes to LangChain documents\n",
    "    lc_docs = []\n",
    "    for node in nodes:\n",
    "        # Base metadata with chunk_size\n",
    "        metadata = {\"chunk_size\": chunk_size}\n",
    "        \n",
    "        # Add original node metadata if keep_node_metadata is True\n",
    "        if keep_node_metadata:\n",
    "            metadata.update(node.metadata)\n",
    "        \n",
    "        # Create LangChain document\n",
    "        doc = LCDocument(\n",
    "            page_content=node.get_content(),\n",
    "            metadata=metadata\n",
    "        )\n",
    "        lc_docs.append(doc)\n",
    "    \n",
    "    print(f\"Converted {len(lc_docs)} nodes to LangChain documents \"\n",
    "          f\"(keep_node_metadata={keep_node_metadata})\")\n",
    "    \n",
    "    return lc_docs\n",
    "\n",
    "# # Example usage\n",
    "# # Create sample nodes\n",
    "# sample_text = \"This is a sample document for testing. \" * 50\n",
    "# doc = Document(text=sample_text)\n",
    "# doc.metadata = {\"source\": \"sample.pdf\", \"author\": \"John Doe\", \"page\": 1}\n",
    "# parser = SentenceSplitter(chunk_size=500, chunk_overlap=150)\n",
    "# nodes = parser.get_nodes_from_documents([doc])\n",
    "\n",
    "# try:\n",
    "#     # Convert nodes without keeping node metadata\n",
    "#     lc_docs_without_metadata = nodes_to_langchain_docs(\n",
    "#         nodes=nodes,\n",
    "#         chunk_size=500,\n",
    "#         keep_node_metadata=False\n",
    "#     )\n",
    "    \n",
    "#     # Convert nodes keeping node metadata\n",
    "#     lc_docs_with_metadata = nodes_to_langchain_docs(\n",
    "#         nodes=nodes,\n",
    "#         chunk_size=500,\n",
    "#         keep_node_metadata=True\n",
    "#     )\n",
    "    \n",
    "#     # Print sample results\n",
    "#     print(\"\\nSample document without node metadata:\")\n",
    "#     print(lc_docs_without_metadata[0].metadata)\n",
    "    \n",
    "#     print(\"\\nSample document with node metadata:\")\n",
    "#     print(lc_docs_with_metadata[0].metadata)\n",
    "    \n",
    "# except (TypeError, ValueError) as e:\n",
    "#     print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932462e9",
   "metadata": {},
   "source": [
    "## Populate documents to vectore storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f70186eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from llama_index.core.schema import Document, BaseNode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from langchain.docstore.document import Document as LCDocument\n",
    "import os\n",
    "import shutil\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "def clear_directory_with_retry(directory: str, max_attempts: int = 5, delay: float = 1.0) -> None:\n",
    "    \"\"\"\n",
    "    Attempt to clear a directory with retries to handle file access issues on Windows.\n",
    "\n",
    "    Args:\n",
    "        directory: Path to the directory to clear\n",
    "        max_attempts: Maximum number of retry attempts\n",
    "        delay: Delay between attempts in seconds\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        return\n",
    "\n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            shutil.rmtree(directory, ignore_errors=True)\n",
    "            print(f\"Cleared existing ChromaDB at {directory}\")\n",
    "            return\n",
    "        except PermissionError as e:\n",
    "            print(f\"Attempt {attempt + 1}/{max_attempts} failed: {e}\")\n",
    "            if attempt < max_attempts - 1:\n",
    "                time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to clear directory: {e}\")\n",
    "            raise\n",
    "    raise PermissionError(f\"Could not clear directory {directory} after {max_attempts} attempts\")\n",
    "\n",
    "def populate_vector_store(\n",
    "    documents: List[Document],\n",
    "    chunk_sizes: List[int],\n",
    "    embedding_model: str,\n",
    "    collection_name_prefix: str,\n",
    "    persist_directory: str,\n",
    "    chunk_overlap_percentage: int = 30,\n",
    "    keep_node_metadata: bool = False,\n",
    "    clear_old_db: bool = False,\n",
    "    max_batch_size: int = 5000\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Populate Chroma vector store with embeddings for multiple chunk sizes.\n",
    "\n",
    "    Args:\n",
    "        documents: List of LlamaIndex Document objects\n",
    "        chunk_sizes: List of chunk sizes to process\n",
    "        embedding_model: Ollama embedding model name\n",
    "        collection_name_prefix: Prefix for Chroma collection names\n",
    "        persist_directory: Directory to store ChromaDB\n",
    "        chunk_overlap_percentage: Overlap percentage (1-99) for chunks (default: 30)\n",
    "        keep_node_metadata: If True, keep original node metadata\n",
    "        clear_old_db: If True, remove existing ChromaDB directory\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If inputs are invalid\n",
    "        TypeError: If documents or chunk_sizes are not lists\n",
    "        PermissionError: If directory cannot be cleared\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(documents, list):\n",
    "        raise TypeError(\"Documents must be provided as a list\")\n",
    "    \n",
    "    if not all(isinstance(doc, Document) for doc in documents):\n",
    "        raise TypeError(\"All items in documents list must be LlamaIndex Document objects\")\n",
    "    \n",
    "    if not isinstance(chunk_sizes, list):\n",
    "        raise TypeError(\"Chunk sizes must be provided as a list\")\n",
    "    \n",
    "    if not all(isinstance(size, int) and size > 0 for size in chunk_sizes):\n",
    "        raise ValueError(\"All chunk sizes must be positive integers\")\n",
    "    \n",
    "    if not isinstance(chunk_overlap_percentage, int) or chunk_overlap_percentage < 1 or chunk_overlap_percentage > 99:\n",
    "        raise ValueError(\"Chunk overlap percentage must be an integer between 1 and 99\")\n",
    "    \n",
    "    if not embedding_model:\n",
    "        raise ValueError(\"Embedding model name must be provided\")\n",
    "\n",
    "    # Clear existing ChromaDB if requested\n",
    "    if clear_old_db:\n",
    "        clear_directory_with_retry(persist_directory)\n",
    "\n",
    "    # Initialize embeddings\n",
    "    embedding = OllamaEmbeddings(model=embedding_model)\n",
    "\n",
    "    # Process each chunk size\n",
    "    for chunk_size in chunk_sizes:\n",
    "        # Calculate chunk overlap based on percentage\n",
    "        chunk_overlap = int(chunk_size * (chunk_overlap_percentage / 100))\n",
    "        \n",
    "        if chunk_overlap >= chunk_size:\n",
    "            raise ValueError(f\"Calculated chunk overlap ({chunk_overlap}) must be less than chunk size ({chunk_size})\")\n",
    "        \n",
    "        collection_name = f\"{collection_name_prefix}{chunk_size}\"\n",
    "        \n",
    "        # Check if collection already exists\n",
    "        vectorstore = None\n",
    "        if not clear_old_db and os.path.exists(persist_directory):\n",
    "            try:\n",
    "                vectorstore = Chroma(\n",
    "                    collection_name=collection_name,\n",
    "                    embedding_function=embedding,\n",
    "                    persist_directory=persist_directory\n",
    "                )\n",
    "                # If collection exists and clear_old_db is False, skip\n",
    "                if vectorstore._collection.count() > 0:\n",
    "                    print(f\"Skipping chunk size {chunk_size}: Collection already exists\")\n",
    "                    vectorstore = None  # Explicitly release the connection\n",
    "                    continue\n",
    "            except Exception:\n",
    "                pass  # Collection doesn't exist, proceed with creation\n",
    "\n",
    "        # Generate nodes\n",
    "        nodes = generate_nodes(\n",
    "            documents=documents,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "        print(f\"Generated {len(nodes)} nodes for chunk size {chunk_size} \")\n",
    "\n",
    "        # Convert nodes to LangChain documents\n",
    "        lc_docs = nodes_to_langchain_docs(\n",
    "            nodes=nodes,\n",
    "            chunk_size=chunk_size,\n",
    "            keep_node_metadata=keep_node_metadata\n",
    "        )\n",
    "\n",
    "        print(f\"Converted {len(lc_docs)} nodes to LangChain documents for chunk size {chunk_size}\")\n",
    "\n",
    "        os.makedirs(persist_directory, exist_ok=True)\n",
    "        # Initialize Chroma vector store\n",
    "        vectorstore = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=embedding,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "\n",
    "        vectorstore.persist()\n",
    "\n",
    "        print(f\"Initialized Chroma vector store for chunk size {chunk_size}\")\n",
    "\n",
    "        # Add documents to vector store in batches\n",
    "        total_docs = len(lc_docs)\n",
    "        for batch_start in range(0, total_docs, max_batch_size):\n",
    "            print(f\"Adding batch {batch_start//max_batch_size + 1}\")\n",
    "            batch_end = min(batch_start + max_batch_size, total_docs)\n",
    "            batch = lc_docs[batch_start:batch_end]\n",
    "            try:\n",
    "                print(f\"Try adding {len(batch)} batche of documents\")\n",
    "                vectorstore.add_documents(batch)\n",
    "                print(f\"Added batch {batch_start//max_batch_size + 1} \"\n",
    "                      f\"({len(batch)} documents) for chunk size {chunk_size}\")\n",
    "                vectorstore.persist()\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding batch {batch_start//max_batch_size + 1} \"\n",
    "                      f\"for chunk size {chunk_size}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        vectorstore.persist()\n",
    "\n",
    "        print(f\"Populated vector store for chunk size {chunk_size} with {len(lc_docs)} documents \"\n",
    "              f\"(overlap={chunk_overlap_percentage}% -> {chunk_overlap} tokens)\")\n",
    "\n",
    "        # Explicitly release the vectorstore connection\n",
    "        vectorstore = None\n",
    "\n",
    "# # Example usage\n",
    "# # Sample documents\n",
    "# sample_text = \"This is a sample document for testing. \" * 50\n",
    "# documents = [Document(text=sample_text)]\n",
    "\n",
    "# try:\n",
    "#     populate_vector_store(\n",
    "#         documents=documents,\n",
    "#         chunk_sizes=[128, 256, 512, 1024],\n",
    "#         embedding_model=\"all-minilm\",\n",
    "#         collection_name_prefix=\"rag_docs_chunk_\",\n",
    "#         persist_directory=\"chroma_db2\",\n",
    "#         chunk_overlap_percentage=20,\n",
    "#         keep_node_metadata=True,\n",
    "#         clear_old_db=False,\n",
    "#         max_batch_size=5000\n",
    "#     )\n",
    "# except (ValueError, TypeError) as e:\n",
    "#     print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe1a7ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 29356 chunks with chunk_size=512 and chunk_overlap=102\n",
      "Generated 29356 nodes for chunk size 512 \n",
      "Converted 29356 nodes to LangChain documents (keep_node_metadata=True)\n",
      "Converted 29356 nodes to LangChain documents for chunk size 512\n",
      "Initialized Chroma vector store for chunk size 512\n",
      "Adding batch 1\n",
      "Try adding 500 batche of documents\n",
      "Added batch 1 (500 documents) for chunk size 512\n",
      "Adding batch 2\n",
      "Try adding 500 batche of documents\n",
      "Added batch 2 (500 documents) for chunk size 512\n",
      "Adding batch 3\n",
      "Try adding 500 batche of documents\n",
      "Added batch 3 (500 documents) for chunk size 512\n",
      "Adding batch 4\n",
      "Try adding 500 batche of documents\n",
      "Added batch 4 (500 documents) for chunk size 512\n",
      "Adding batch 5\n",
      "Try adding 500 batche of documents\n",
      "Added batch 5 (500 documents) for chunk size 512\n",
      "Adding batch 6\n",
      "Try adding 500 batche of documents\n",
      "Added batch 6 (500 documents) for chunk size 512\n",
      "Adding batch 7\n",
      "Try adding 500 batche of documents\n",
      "Added batch 7 (500 documents) for chunk size 512\n",
      "Adding batch 8\n",
      "Try adding 500 batche of documents\n",
      "Added batch 8 (500 documents) for chunk size 512\n",
      "Adding batch 9\n",
      "Try adding 500 batche of documents\n",
      "Added batch 9 (500 documents) for chunk size 512\n",
      "Adding batch 10\n",
      "Try adding 500 batche of documents\n",
      "Added batch 10 (500 documents) for chunk size 512\n",
      "Adding batch 11\n",
      "Try adding 500 batche of documents\n",
      "Added batch 11 (500 documents) for chunk size 512\n",
      "Adding batch 12\n",
      "Try adding 500 batche of documents\n",
      "Added batch 12 (500 documents) for chunk size 512\n",
      "Adding batch 13\n",
      "Try adding 500 batche of documents\n",
      "Added batch 13 (500 documents) for chunk size 512\n",
      "Adding batch 14\n",
      "Try adding 500 batche of documents\n",
      "Added batch 14 (500 documents) for chunk size 512\n",
      "Adding batch 15\n",
      "Try adding 500 batche of documents\n",
      "Added batch 15 (500 documents) for chunk size 512\n",
      "Adding batch 16\n",
      "Try adding 500 batche of documents\n",
      "Added batch 16 (500 documents) for chunk size 512\n",
      "Adding batch 17\n",
      "Try adding 500 batche of documents\n",
      "Added batch 17 (500 documents) for chunk size 512\n",
      "Adding batch 18\n",
      "Try adding 500 batche of documents\n",
      "Added batch 18 (500 documents) for chunk size 512\n",
      "Adding batch 19\n",
      "Try adding 500 batche of documents\n",
      "Added batch 19 (500 documents) for chunk size 512\n",
      "Adding batch 20\n",
      "Try adding 500 batche of documents\n",
      "Added batch 20 (500 documents) for chunk size 512\n",
      "Adding batch 21\n",
      "Try adding 500 batche of documents\n",
      "Added batch 21 (500 documents) for chunk size 512\n",
      "Adding batch 22\n",
      "Try adding 500 batche of documents\n",
      "Added batch 22 (500 documents) for chunk size 512\n",
      "Adding batch 23\n",
      "Try adding 500 batche of documents\n",
      "Added batch 23 (500 documents) for chunk size 512\n",
      "Adding batch 24\n",
      "Try adding 500 batche of documents\n",
      "Added batch 24 (500 documents) for chunk size 512\n",
      "Adding batch 25\n",
      "Try adding 500 batche of documents\n",
      "Added batch 25 (500 documents) for chunk size 512\n",
      "Adding batch 26\n",
      "Try adding 500 batche of documents\n",
      "Added batch 26 (500 documents) for chunk size 512\n",
      "Adding batch 27\n",
      "Try adding 500 batche of documents\n",
      "Added batch 27 (500 documents) for chunk size 512\n",
      "Adding batch 28\n",
      "Try adding 500 batche of documents\n",
      "Added batch 28 (500 documents) for chunk size 512\n",
      "Adding batch 29\n",
      "Try adding 500 batche of documents\n",
      "Added batch 29 (500 documents) for chunk size 512\n",
      "Adding batch 30\n",
      "Try adding 500 batche of documents\n",
      "Added batch 30 (500 documents) for chunk size 512\n",
      "Adding batch 31\n",
      "Try adding 500 batche of documents\n",
      "Added batch 31 (500 documents) for chunk size 512\n",
      "Adding batch 32\n",
      "Try adding 500 batche of documents\n",
      "Added batch 32 (500 documents) for chunk size 512\n",
      "Adding batch 33\n",
      "Try adding 500 batche of documents\n",
      "Added batch 33 (500 documents) for chunk size 512\n",
      "Adding batch 34\n",
      "Try adding 500 batche of documents\n",
      "Added batch 34 (500 documents) for chunk size 512\n",
      "Adding batch 35\n",
      "Try adding 500 batche of documents\n",
      "Added batch 35 (500 documents) for chunk size 512\n",
      "Adding batch 36\n",
      "Try adding 500 batche of documents\n",
      "Added batch 36 (500 documents) for chunk size 512\n",
      "Adding batch 37\n",
      "Try adding 500 batche of documents\n",
      "Added batch 37 (500 documents) for chunk size 512\n",
      "Adding batch 38\n",
      "Try adding 500 batche of documents\n",
      "Added batch 38 (500 documents) for chunk size 512\n",
      "Adding batch 39\n",
      "Try adding 500 batche of documents\n",
      "Added batch 39 (500 documents) for chunk size 512\n",
      "Adding batch 40\n",
      "Try adding 500 batche of documents\n",
      "Added batch 40 (500 documents) for chunk size 512\n",
      "Adding batch 41\n",
      "Try adding 500 batche of documents\n",
      "Added batch 41 (500 documents) for chunk size 512\n",
      "Adding batch 42\n",
      "Try adding 500 batche of documents\n",
      "Added batch 42 (500 documents) for chunk size 512\n",
      "Adding batch 43\n",
      "Try adding 500 batche of documents\n",
      "Added batch 43 (500 documents) for chunk size 512\n",
      "Adding batch 44\n",
      "Try adding 500 batche of documents\n",
      "Added batch 44 (500 documents) for chunk size 512\n",
      "Adding batch 45\n",
      "Try adding 500 batche of documents\n",
      "Added batch 45 (500 documents) for chunk size 512\n",
      "Adding batch 46\n",
      "Try adding 500 batche of documents\n",
      "Added batch 46 (500 documents) for chunk size 512\n",
      "Adding batch 47\n",
      "Try adding 500 batche of documents\n",
      "Added batch 47 (500 documents) for chunk size 512\n",
      "Adding batch 48\n",
      "Try adding 500 batche of documents\n",
      "Added batch 48 (500 documents) for chunk size 512\n",
      "Adding batch 49\n",
      "Try adding 500 batche of documents\n",
      "Added batch 49 (500 documents) for chunk size 512\n",
      "Adding batch 50\n",
      "Try adding 500 batche of documents\n",
      "Added batch 50 (500 documents) for chunk size 512\n",
      "Adding batch 51\n",
      "Try adding 500 batche of documents\n",
      "Added batch 51 (500 documents) for chunk size 512\n",
      "Adding batch 52\n",
      "Try adding 500 batche of documents\n",
      "Added batch 52 (500 documents) for chunk size 512\n",
      "Adding batch 53\n",
      "Try adding 500 batche of documents\n",
      "Added batch 53 (500 documents) for chunk size 512\n",
      "Adding batch 54\n",
      "Try adding 500 batche of documents\n",
      "Added batch 54 (500 documents) for chunk size 512\n",
      "Adding batch 55\n",
      "Try adding 500 batche of documents\n",
      "Added batch 55 (500 documents) for chunk size 512\n",
      "Adding batch 56\n",
      "Try adding 500 batche of documents\n",
      "Added batch 56 (500 documents) for chunk size 512\n",
      "Adding batch 57\n",
      "Try adding 500 batche of documents\n",
      "Added batch 57 (500 documents) for chunk size 512\n",
      "Adding batch 58\n",
      "Try adding 500 batche of documents\n",
      "Added batch 58 (500 documents) for chunk size 512\n",
      "Adding batch 59\n",
      "Try adding 356 batche of documents\n",
      "Added batch 59 (356 documents) for chunk size 512\n",
      "Populated vector store for chunk size 512 with 29356 documents (overlap=20% -> 102 tokens)\n"
     ]
    }
   ],
   "source": [
    "embedding_model = \"nomic-embed-text\"\n",
    "# chunk_sizes = [128, 256, 512, 1024]\n",
    "chunk_sizes = [512]\n",
    "chunk_overlap_percentage = 20\n",
    "\n",
    "# Generate real vectore store from one of the datasets\n",
    "try:\n",
    "    populate_vector_store(\n",
    "        documents=documents,\n",
    "        chunk_sizes=chunk_sizes,\n",
    "        embedding_model=embedding_model,\n",
    "        collection_name_prefix=\"financebench_docs_chunk_\",\n",
    "        persist_directory=\"./financebench_db2\",\n",
    "        chunk_overlap_percentage=chunk_overlap_percentage,\n",
    "        keep_node_metadata=True,\n",
    "        clear_old_db=True,\n",
    "        max_batch_size=500\n",
    "    )\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
